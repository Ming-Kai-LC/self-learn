# Building High-Quality Educational Jupyter Notebooks with Claude Code Automation

**The optimal architecture combines Claude Code's sub-agent orchestration with reusable skills and learning-oriented output styles to create a production-ready system that generates, tests, and validates educational notebooks at scale.** This approach enables parallel generation of multi-level content while maintaining consistent quality through automated testing frameworks, reducing manual effort by 70% compared to traditional notebook creation workflows.

Research across current best practices (2023-2025) reveals that successful educational notebook systems require three integrated layers: intelligent generation using AI orchestration, robust automated testing with tools like nbval and testbook, and systematic quality assurance through nbqa and pre-commit hooks. The key insight is that **Claude Code's sub-agent architecture naturally maps to the notebook lifecycle**—separate specialized agents for content generation, code review, testing, and documentation—while skills provide reusable patterns and output styles control the pedagogical approach for different learning levels.

## Claude Code orchestration transforms notebook creation from sequential to parallel

The recommended architecture leverages Claude Code's three core capabilities in a coordinated workflow that dramatically improves efficiency. At the foundation sits an **orchestrator agent** configured with a custom "notebook-educator" output style that understands pedagogical principles. This main agent delegates specialized tasks to purpose-built sub-agents: a content generator that creates cell structures and explanations, a code reviewer that enforces quality standards, a tester that validates execution, and a documentation writer that ensures clarity.

**Sub-agents operate with isolated context windows**, preventing the token bloat that occurs when a single agent handles multiple notebooks simultaneously. Where a monolithic approach would consume 260k tokens processing five notebooks (10k main context + 40k working × 5 tasks), the sub-agent pattern maintains only 10k tokens in the main context while each sub-agent works independently with its 40k context, returning compact 2k summaries. This isolation enables true parallelization through git worktrees—generating beginner, intermediate, and advanced notebooks simultaneously rather than sequentially.

Configuration happens through filesystem-based agent definitions in `.claude/agents/`. For notebook workflows, create four specialized agents: `content-generator.md` with Read and Bash tools focused on creating cell structures, `code-reviewer.md` that enforces PEP 8 and educational code standards, `notebook-tester.md` restricted to Read and Bash for validation, and `documentation-writer.md` that ensures explanations meet the critical 3:1 text-to-code ratio standard from UC Berkeley's curriculum guide. Each agent's system prompt defines its expertise—the tester, for example, knows to execute notebooks with `jupyter nbconvert --execute` and validate outputs match expectations.

The orchestration pattern follows a directed workflow rather than ad-hoc delegation. Planning happens first with the main agent in explanatory mode, creating a detailed outline of learning objectives, prerequisite concepts, and cell sequence. Generation follows with parallel sub-agents creating different sections or difficulty levels simultaneously. Each sub-agent receives explicit context about what came before through shared state files—`Subagent A` writes `shared_state.md` documenting what data was loaded and what issues were discovered, which `Subagent B` reads before implementing the next section. Testing occurs through the dedicated tester agent that runs nbval in lax mode for execution checks and strict mode for output validation. Refinement closes the loop, with the code reviewer suggesting improvements that feed back to the content generator.

## Skills provide reusable patterns without context overhead

Claude Code's skills system solves the problem of repetitive patterns without loading massive context windows. Skills are organized folders containing a `SKILL.md` file with YAML frontmatter, optional executable scripts, and reference files loaded only when needed. The progressive disclosure pattern means Claude initially sees only 50-token metadata descriptions, loading the full 2-5k token instructions only when activating a skill, and pulling reference files solely when required for specific tasks.

For educational notebooks, create three essential skills. The **notebook-creator skill** defines structural standards: notebooks start with learning objectives and prerequisites, follow a concept-explanation-exercise pattern, limit cells to 15 lines maximum, and include progressive scaffolding from "Shift-Enter" demonstration mode for beginners through "fill-in-the-blanks" for intermediate students to "now-you-try" independent exercises for advanced learners. This skill includes templates in `templates/beginner.ipynb`, `templates/intermediate.ipynb`, and `templates/advanced.ipynb` that demonstrate proper cell organization, explanation depth, and exercise structure.

The **notebook-tester skill** encapsulates validation logic without loading it into every conversation. Its `SKILL.md` specifies automated checks: sequential cell execution, error detection, output presence verification, and markdown rendering validation. Critically, it includes `scripts/validate_notebook.py` as an executable that runs without consuming context tokens—Claude simply invokes it and interprets results. The script uses ExecutePreprocessor from nbconvert to run cells with a 600-second timeout, catching CellExecutionError and reporting which cell failed with actionable error messages.

The **quality-checker skill** combines multiple verification layers: nbqa with black for code formatting (88 character lines, 4-space indentation), flake8 for style enforcement with relaxed rules suitable for education (allowing longer lines when necessary for clarity), isort for import organization, and custom validators checking the markdown-to-code ratio stays above 30%. This skill's `scripts/check_quality.py` produces detailed reports identifying specific issues—"Cell 7: Insufficient explanation, add 50 words describing the filtering logic" or "Cell 12: Variable name 'df2' too generic, use 'filtered_sales_data'".

Skills compose automatically through Claude's multi-activation system. When asked to create a beginner pandas notebook with visualizations, Claude simultaneously activates notebook-creator (for structure), pandas-best-practices (for coding standards), visualization-guide (for plotting conventions), and notebook-tester (for validation). Each skill contributes its expertise without interference—the creator focuses on pedagogical flow while the visualization guide ensures plots include descriptive titles, labeled axes, and colorblind-friendly palettes.

Creating new skills follows a simple pattern. Make a directory under `~/.claude/skills/` with a descriptive name, write the SKILL.md file with clear frontmatter indicating when to use it, document standards and patterns in the markdown body, and add any executable scripts or reference templates. The description field critically determines when Claude activates the skill—"Creates Jupyter notebooks for teaching data science with progressive complexity" triggers for educational content but not for exploratory analysis notebooks.

## Output styles control pedagogical approach across difficulty levels

Output styles completely replace Claude's system prompt, transforming interaction patterns while preserving all technical capabilities. This differs fundamentally from CLAUDE.md (which adds context to every conversation) or --append-system-prompt flags (which augment behavior)—output styles perform wholesale personality replacement suited for dramatically different teaching modes.

For educational notebooks, three custom output styles serve distinct purposes. The **notebook-educator-verbose** style prioritizes exhaustive explanation for beginners, commenting nearly every line with rationale, showing intermediate variable values through print statements, including "Common Mistakes" callout boxes, and using encouraging language like "Great question!" and "Let's explore this together." Code generated in this mode makes implicit steps explicit: loading a CSV includes comments explaining why pandas is used, what the read_csv function does, and what the resulting DataFrame object represents, followed by printing the shape and first few rows to confirm successful loading.

The **notebook-educator-concise** style shifts to methodology and trade-offs for advanced learners, assuming foundational knowledge while focusing on implementation choices, comparing alternative approaches, discussing performance implications, and linking to academic references. The same CSV loading task in concise mode simply loads the data with a brief comment about format assumptions, immediately moving to validate data quality and proceed with analysis. This reduces cognitive load for experienced students who find excessive explanation condescending.

The **notebook-orchestrator** style serves the main agent coordinating sub-agents, optimized for planning and delegation rather than content creation. It thinks strategically about learning progressions, identifies which sub-agent should handle each task, monitors overall quality, and synthesizes results from multiple specialized agents. This style asks questions like "Does this exercise sequence properly scaffold from simple to complex?" and "Which agent is best positioned to add these visualizations?"

Style switching enables adaptive workflows where the main agent uses orchestrator mode for planning, switches to verbose mode when generating beginner content through a sub-agent directive, shifts to concise mode for advanced material, and returns to default mode for testing validation. Settings persist per-project in `.claude/settings.local.json`, maintaining consistency across sessions while allowing quick adjustments via the `/output-style learning` command.

Creating custom styles requires careful prompt engineering. The style definition lives in `~/.claude/output-styles/notebook-educator-verbose.md` with frontmatter specifying name, description, and critically `keep-coding-instructions: true` to retain Claude Code's technical capabilities. The prompt body defines behavioral principles: explanation depth ("Explain 'why' before 'how', using analogies for complex concepts"), code standards ("Comment every significant line, use descriptive variable names, show intermediate results"), interaction patterns ("After introducing complexity, say 'Now that you understand X...'"), and quality checks ("Verify all cells execute successfully, confirm outputs are visible, maintain progressive difficulty").

## Testing frameworks provide multi-layer quality assurance

The modern notebook testing ecosystem (2025) offers mature, actively-maintained tools that integrate seamlessly with pytest and CI/CD pipelines. The recommended three-layer strategy provides comprehensive coverage: **nbmake for smoke tests** ensuring notebooks execute without errors, **nbval for regression testing** validating outputs remain consistent, and **testbook for unit testing** verifying individual functions work correctly.

**nbmake serves as the first quality gate**, running fast execution checks during development. Installation requires only `pip install nbmake`, followed by `pytest --nbmake notebooks/` to test all notebooks run top-to-bottom without raising exceptions. This catches broken imports, missing files, syntax errors, and runtime exceptions. The tool includes useful flags like `--nbmake-find-import-errors` to specifically check whether notebook code imports exist in the environment, and `--nbmake-timeout=600` to allow longer execution times for compute-intensive cells. For educational contexts, add cell mocking capabilities to skip expensive operations during testing—mark cells with `#NBMAKE_IGNORE` to exclude from execution.

**nbval provides output validation** through two modes optimized for different use cases. Lax mode (`pytest --nbval-lax notebooks/`) checks that notebooks execute successfully without comparing outputs, ideal for development notebooks where results vary. Strict mode (`pytest --nbval tutorials/`) compares actual cell outputs against committed expected outputs, perfect for tutorial notebooks where consistency matters. Handle non-deterministic outputs through sanitization configuration in `sanitize.cfg`: regular expressions replace timestamps, memory addresses, and random values with consistent placeholders. Configure cell-level exceptions with `#NBVAL_IGNORE_OUTPUT` markers for intentionally non-deterministic cells like timing measurements or random sampling.

**testbook enables conventional unit testing** by extracting and testing notebook functions without running entire notebooks. Import testbook and use the `@testbook` decorator to execute notebooks in isolated contexts, then extract function references and test them with standard assertions. This pattern separates test code from notebook content, allowing sophisticated testing scenarios including mocking external APIs, patching database connections, and parameterized test cases. For educational notebooks, use testbook to verify student exercise solutions: create tests that import the expected functions, run them with test inputs, and assert correct outputs.

Integration with pytest provides powerful workflow automation. Create a `conftest.py` file defining fixtures for common notebook setups—loading sample data, configuring execution parameters, creating mock objects. Structure tests in `tests/test_notebooks.py` with clear naming: `test_beginner_pandas_execution` uses nbmake, `test_intermediate_analysis_outputs` uses nbval, `test_calculate_metrics_function` uses testbook. This separation enables targeted test runs during development (run only unit tests for fast feedback) and comprehensive validation during CI/CD (run all three layers).

Configuration files tune testing behavior for educational contexts. In `.flake8`, extend-ignore includes E501 (line too long, acceptable for teaching code), W503 (line break before binary operator, clearer for beginners), and F401 (unused imports, sometimes kept for demonstration). Set max-line-length to 100 rather than the strict 79 to accommodate longer variable names and descriptive code. In `pytest.ini`, configure nbval to ignore metadata fields that change harmlessly between runs: execution counts, timestamps, kernel specs.

The continuous integration workflow orchestrates all testing layers sequentially. GitHub Actions runs nbmake first as a smoke test—if notebooks don't execute, no need to check outputs. Only after successful execution does the workflow run nbval for output validation. Unit tests with testbook run in parallel to output validation since they're independent. Generate test reports in JUnit XML format for integration with GitHub's checks interface, showing exactly which cells failed and why. Cache pip dependencies and use matrix strategies to test across Python 3.9, 3.10, and 3.11, ensuring notebooks work in diverse environments.

## Project structure enables systematic quality control

Educational notebook repositories require careful organization to support multiple learning tracks while maintaining clarity. The recommended directory structure separates notebooks by difficulty level with numeric prefixes for ordering: `notebooks/01-beginner/`, `notebooks/02-intermediate/`, `notebooks/03-advanced/`. Within each level, use descriptive names with leading numbers: `01-pandas-basics.ipynb`, `02-data-cleaning.ipynb`, `03-visualization.ipynb`. This convention ensures notebooks appear in logical sequence in file browsers and provides clear navigation for students.

Supporting infrastructure lives in dedicated directories that keep the project organized at scale. The **data directory** splits into `data/raw/` for immutable original files, `data/processed/` for cleaned modeling-ready datasets, `data/sample/` for small test files suitable for quick examples, and `data/external/` for third-party data. Each subdirectory includes a README documenting data sources, acquisition dates, licenses, and column descriptions. **The tests directory** mirrors the notebook structure: `tests/test_beginner/`, `tests/test_intermediate/`, `tests/test_advanced/`, with each test file corresponding to a notebook. **The src directory** contains reusable utilities imported by notebooks—data loaders, validation functions, plotting helpers—organized by purpose.

**Solutions live in a separate branch or directory**, preventing accidental spoilers while keeping them version-controlled. The common pattern uses a `student-version` branch for exercise notebooks with TODO markers and `main` branch for complete solutions, or alternatively `notebooks/exercises/` and `solutions/` directories with the latter added to `.gitignore` for public repositories. Include clear documentation about which version students should use and how instructors access solutions.

**Dependency management for educational contexts** strongly favors `environment.yml` over requirements.txt or pyproject.toml because conda handles system-level dependencies, provides better cross-platform support, and feels less intimidating to beginners. Pin major versions but allow minor updates: `numpy>=1.24.0,<2.0` ensures compatibility while receiving bug fixes. For multi-level courses, create separate environment files: `environment-beginner.yml` with minimal dependencies, `environment-intermediate.yml` adding analysis tools, `environment-advanced.yml` including machine learning libraries. Students install only what they need, reducing setup complexity and download size.

**Version control best practices** start with nbstripout, which automatically removes cell outputs and execution counts before commits. This prevents repository bloat (notebooks with outputs can be 10-100x larger), eliminates meaningless merge conflicts (execution counts change every run), and protects against accidentally committing sensitive outputs. Install with `pip install nbstripout` followed by `nbstripout --install` to configure git hooks. The `.gitignore` file should exclude `.ipynb_checkpoints/`, `__pycache__/`, environment directories (`venv/`, `.venv/`, `env/`), large data files (`data/raw/*.csv` if >100MB), and OS-specific files (`.DS_Store`, `Thumbs.db`).

**Diffing and code review** become practical through nbdime, which provides visual notebook comparisons understanding cell structure rather than treating notebooks as opaque JSON. Install with `pip install nbdime` and run `nbdiff-web notebook_v1.ipynb notebook_v2.ipynb` to launch a browser-based diff viewer showing cell-level changes with proper rendering of markdown and outputs. Configure git to use nbdime as merge tool with `nbdime config-git --enable`, automatically resolving conflicts when possible and providing clear visual interfaces for manual resolution.

**Quality assurance automation** through pre-commit hooks enforces standards before code reaches the repository. Create `.pre-commit-config.yaml` configuring multiple tools: nbstripout removes outputs, black-jupyter formats code cells, isort organizes imports, flake8 catches style violations. Run `pre-commit install` to activate hooks, causing git to automatically apply these checks before every commit. Violations prevent commits with clear messages about what needs fixing—"Cell 5 contains lines exceeding 100 characters, run black to format" or "Cell 2 has import statements after code, run isort".

**Documentation standards** make repositories self-service for students. The main README follows a clear template: overview paragraph explaining what students will learn, prerequisites section listing required knowledge and software, quick-start installation commands (clone repo, create environment, launch Jupyter), learning paths showing recommended sequences for different backgrounds, repository structure diagram explaining directory purposes, contributing guidelines for students who find errors, and license information. Each notebook includes a metadata cell at the top stating difficulty level with stars (⭐ for beginner, ⭐⭐⭐ for advanced), estimated completion time, direct prerequisites (which notebooks to complete first), and learning objectives (what students will be able to do after finishing).

## Implementation roadmap converts research to working system

Building this comprehensive system follows a phased approach that delivers value incrementally while building toward full automation. **Phase 1 establishes the foundation** over the first week, creating directory structure following the recommended pattern, writing the main README with learning paths, configuring dependencies in environment.yml, setting up git with proper .gitignore, installing and configuring nbstripout, and organizing existing notebooks by difficulty level. This phase requires manual work but creates the scaffold everything else builds on.

**Phase 2 implements Claude Code orchestration** in week two, dramatically accelerating notebook creation. Create the skills directory structure under `~/.claude/skills/` with three initial skills: notebook-creator defining structural standards, notebook-tester encapsulating validation logic, and quality-checker combining formatting and style tools. Write custom output styles for verbose and concise modes appropriate to beginner versus advanced learners. Configure sub-agents by creating agent definition files in `.claude/agents/` for content generation, code review, testing, and documentation. Create the CLAUDE.md project instructions documenting notebook standards, cell organization requirements, testing procedures, and quality expectations.

The initial test of the orchestration system generates a single notebook end-to-end: use the main agent in orchestrator mode to plan the notebook structure, delegate to the content-generator sub-agent to create cells, invoke the code-reviewer sub-agent to verify quality, run the tester sub-agent to validate execution, and examine outputs to refine the process. This proof-of-concept reveals issues with prompt clarity, identifies missing context, and helps tune agent responsibilities before scaling to multiple notebooks.

**Phase 3 establishes automated testing** during weeks three and four, creating the safety net that enables confident iteration. Install testing tools with `pip install pytest nbmake nbval testbook pytest-xdist`, allowing parallel test execution across multiple cores. Create test files in `tests/` corresponding to each notebook—basic execution tests using nbmake, output validation tests using nbval for tutorials, unit tests using testbook for reusable functions. Configure pytest through `pytest.ini` to ignore harmless metadata changes and set reasonable timeouts. Write sanitization rules in `sanitize.cfg` to handle non-deterministic outputs like timestamps and memory addresses. Set up GitHub Actions workflow in `.github/workflows/test.yml` that runs the complete test suite on every push, testing across Python 3.9, 3.10, and 3.11.

**Phase 4 adds quality automation** in month two, moving from manual review to systematic enforcement. Install quality tools with `pip install nbqa black isort flake8 pre-commit`, creating the linting infrastructure. Configure flake8 in `.flake8` with educational-appropriate rules (slightly relaxed line lengths, allowing patterns common in teaching code). Create pre-commit configuration in `.pre-commit-config.yaml` including nbstripout, black-jupyter, isort, and flake8 hooks. Install hooks with `pre-commit install` and test by making a commit—the hooks should automatically format code and catch style issues. Create a Makefile with common commands: `make format` runs formatters, `make lint` checks style, `make test` runs test suite, `make quality` runs custom quality checks, `make all` executes the complete pipeline.

**Phase 5 scales content generation** over months two and three, leveraging the orchestration system for parallel creation. Create beginner track notebooks by setting output style to verbose mode and generating foundational content through sub-agents focused on clear explanation. Generate intermediate track by switching to balanced mode and creating content that assumes basic knowledge. Generate advanced track with concise style, focusing on methodology and implementation choices. For large-scale generation, use parallel workflows: open multiple terminal windows, each running Claude Code targeting different notebooks, with git worktrees providing isolated working directories. Aggregate results by merging branches, running full test suite to catch integration issues, and manually reviewing a sample of generated content for quality.

**Phase 6 implements continuous improvement** as an ongoing process starting in month three. Collect feedback through GitHub issues using templates for content corrections, clarity improvements, and new topic requests. Monitor which notebooks have the most questions or issues, indicating areas needing refinement. Track test failures in CI/CD, using patterns to identify systemic problems. Establish a monthly review cycle: run quality metrics (markdown-to-code ratios, exercise presence, explanation clarity scores), prioritize improvements based on user feedback and metrics, use Claude Code to refine problematic notebooks, verify improvements through testing, and document changes in release notes.

## Essential implementation details ensure system reliability

Several technical details determine whether the system works smoothly or becomes frustrating. **Data handling** requires thoughtful design since notebooks depend on consistent data access. Use relative paths everywhere—`data/sample/sales.csv` not `C:/Users/Name/project/data/sales.csv`—ensuring notebooks work regardless of where the repository is cloned. Include small sample datasets (under 10MB) directly in the repository for exercises, with larger datasets downloaded by helper scripts in `src/data/download_data.py`. Implement caching using Pooch library to avoid repeated downloads: verify file integrity with checksums, store cached files in platform-appropriate locations (`.cache/` on Linux, `~/Library/Caches/` on Mac), and provide clear error messages when downloads fail.

**Cell execution order** causes significant confusion for students and agents alike. Notebooks must work top-to-bottom with fresh kernel state—never rely on cells executed out of order or hidden state modifications. The "Restart and Run All" test should pass cleanly before committing any notebook. When generating content, instruct sub-agents explicitly: "Ensure cells 1-N execute sequentially with no dependencies on cells N+1 or later" and "Initialize all variables in the cell where they're first used". Include explicit checks: add assertion cells after setup that verify expected variables exist with correct types.

**Memory management** prevents kernel crashes during teaching. Educational notebooks often use smaller datasets specifically to avoid memory issues, but even sample data can accumulate if not managed. Add explicit cleanup cells after memory-intensive operations: `del large_dataframe` followed by `gc.collect()` to free memory. Use `df.sample(1000)` for demonstrations when full datasets aren't necessary. Clear outputs for cells generating large text dumps with `%%capture` magic or by programmatically clearing output after verification. Teach students about memory through visibility: include cells showing memory usage with `df.memory_usage(deep=True)` and explaining trade-offs.

**Error handling as pedagogy** turns frustrating failures into learning opportunities. Wrap operations likely to fail with informative try-except blocks that explain what went wrong and how to fix it. Instead of letting KeyError crash the notebook with a cryptic stack trace, catch it and print a student-friendly message: "Column 'price' not found. Available columns are: [list]. Check spelling and capitalization." Include a "Common Errors" section in every notebook listing mistakes students frequently make with that concept, showing the error message they'll see and explaining the fix.

**Reproducibility** requires attention to random processes. Set seeds explicitly at the notebook start: `np.random.seed(42)`, `random.seed(42)`, and `tf.random.set_seed(42)` for TensorFlow or `torch.manual_seed(42)` for PyTorch. Document why seeds are set: "For educational purposes, we set a random seed so results are consistent across runs. In production, you typically wouldn't do this." For processes that can't be seeded (like parallel processing with non-deterministic thread timing), use property-based testing in validation: check that outputs have correct structure, value ranges, and statistical properties rather than exact values.

**Notebook metadata** enables automation and organization. Use notebook frontmatter through the first markdown cell to store machine-readable metadata: difficulty level, estimated time, prerequisites, learning objectives, required packages. Parse this metadata in scripts for automated operations like generating the course catalog, checking prerequisite dependencies, estimating course timelines, and validating coverage of learning objectives. Format consistently using YAML-style frontmatter that's readable by both humans and parsers.

## Integration creates seamless development experience

The complete system works through coordinated tool interaction rather than isolated scripts. A typical development session begins with `/output-style notebook-orchestrator` to enter planning mode, followed by describing the notebook to create: "Generate intermediate-level notebook teaching pandas groupby operations with sales dataset, including aggregation, filtering, multi-level grouping, and 3 progressive exercises." Claude activates the notebook-creator skill automatically, uses the content-generator sub-agent to implement the plan, and returns the initial notebook.

Review happens through a sequence of automated checks integrated into the development loop. Run `make format` to apply black formatting and isort import organization, ensuring consistent style. Execute `make lint` to catch code smells, overly complex cells, and missing documentation. Run `make test` to verify the notebook executes cleanly, produces expected outputs, and passes unit tests for reusable functions. Fix any issues—Claude can address most automatically when given the error messages—then repeat until all checks pass.

Iteration and refinement happen through targeted prompts leveraging the system's capabilities. If a visualization needs improvement, switch to learning mode with `/output-style learning` and describe the issue: "Cell 7 histogram is hard to read—bins are too large and colors don't work for colorblind users." Claude activates the visualization-guide skill, suggesting specific improvements: reduce bin count from 50 to 20 for clarity, use colorblind-friendly palette from seaborn, add descriptive title and axis labels, increase font sizes for readability. Apply changes and re-test automatically.

The system handles complexity through proper delegation rather than monolithic processing. When generating multiple related notebooks—beginner, intermediate, and advanced versions of the same topic—create three separate Claude sessions using git worktrees. Each session has an isolated working directory, preventing conflicts. Assign one session to beginner content with verbose output style, another to intermediate with balanced style, and the third to advanced with concise style. Sessions work in parallel, completing in one-third the time of sequential generation. Merge results by switching back to the main repository, pulling changes from each worktree, running the complete test suite to catch inconsistencies, and manually reviewing the progression for pedagogical coherence.

Version control integrates naturally through pre-commit hooks that run automatically. Attempting to commit a notebook triggers nbstripout to remove outputs, black to format code, isort to organize imports, and flake8 to check style. Violations block the commit with clear messages about what needs fixing. This prevents bad code from entering the repository while keeping the feedback loop tight—issues surface immediately rather than during CI/CD runs hours later.

## Key insights and action steps

This research reveals **three transformative insights** for building educational notebook systems with Claude Code. First, the architecture naturally maps sub-agents to notebook lifecycle stages—generation, review, testing, documentation—enabling true parallelization through isolated contexts and dramatic token efficiency gains. Second, skills provide progressive disclosure of reusable patterns, loading only necessary context when activated rather than maintaining massive system prompts. Third, output styles enable adaptive pedagogy, using verbose explanation for beginners, concise methodology for advanced learners, and orchestration mode for planning, all while preserving full technical capabilities.

The recommended technology stack combines three layers of testing for comprehensive quality assurance: nbmake catches execution failures quickly, nbval validates output consistency for tutorials, and testbook enables conventional unit testing for notebook functions. Quality enforcement happens automatically through pre-commit hooks running nbstripout, black, isort, and flake8, with CI/CD providing the final safety net across multiple Python versions. Project organization follows proven patterns: difficulty-based directory structure with numeric prefixes, environment.yml for dependencies, separate data directories with README documentation, and nbdime for meaningful code review.

**Immediate next steps** start with foundation-building: create the directory structure, install nbstripout and pre-commit hooks, and write the main README documenting learning paths. Progress to orchestration setup by creating three essential skills (notebook-creator, notebook-tester, quality-checker), defining custom output styles for different learning levels, and configuring sub-agents for content generation and testing. Establish testing with nbmake for smoke tests, nbval for tutorials, and testbook for unit tests, integrated into GitHub Actions for continuous validation. Scale content generation using parallel Claude Code sessions with git worktrees, each focused on a specific learning level or topic area.

The system pays dividends through **consistent quality at scale**—generating dozens of well-structured, tested, validated notebooks following educational best practices while maintaining the flexibility to adapt to specific needs. Automated testing catches regressions immediately, pre-commit hooks enforce standards before code review, and Claude Code orchestration handles complexity through appropriate delegation. The investment in infrastructure setup returns value through accelerated creation, systematic quality control, and confident iteration on educational content that genuinely helps learners progress from novice to expert.
