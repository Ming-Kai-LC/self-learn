{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 00: Introduction to Feature Engineering\n",
    "\n",
    "**Difficulty**: ⭐ Beginner  \n",
    "**Estimated Time**: 45 minutes  \n",
    "**Prerequisites**: Basic ML understanding (Module 05.1), Pandas basics\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Explain what feature engineering is and why it's critical for ML success\n",
    "2. Understand the feature engineering workflow and when to apply it\n",
    "3. Identify different types of features and transformations\n",
    "4. Recognize common pitfalls (data leakage, overfitting)\n",
    "5. Demonstrate measurable impact of feature engineering on model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is Feature Engineering?\n",
    "\n",
    "**Feature Engineering** is the process of transforming raw data into features that better represent the underlying patterns to predictive models.\n",
    "\n",
    "### Why It Matters\n",
    "\n",
    "According to Andrew Ng:\n",
    "> \"Applied machine learning is basically feature engineering.\"\n",
    "\n",
    "**Real-world impact**:\n",
    "- Can improve model accuracy by 5-20% or more\n",
    "- Often more effective than algorithm selection\n",
    "- Essential for winning Kaggle competitions\n",
    "- Makes simpler models work as well as complex ones\n",
    "\n",
    "### Example: Predicting House Prices\n",
    "\n",
    "**Raw features**:\n",
    "- `length = 50 feet`\n",
    "- `width = 30 feet`\n",
    "- `price = $300,000`\n",
    "\n",
    "**Engineered feature**:\n",
    "- `area = length × width = 1,500 sq ft`\n",
    "\n",
    "The engineered `area` feature is much more predictive of price than length and width separately!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup\n",
    "\n",
    "Let's import libraries and create a simple dataset to demonstrate feature engineering impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Configuration\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 2)\n",
    "\n",
    "print(\"✓ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Demonstrating Feature Engineering Impact\n",
    "\n",
    "Let's create a synthetic dataset to show how feature engineering dramatically improves model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic house data\n",
    "n_samples = 500\n",
    "\n",
    "# Generate raw features\n",
    "house_data = pd.DataFrame({\n",
    "    'length_feet': np.random.uniform(20, 80, n_samples),\n",
    "    'width_feet': np.random.uniform(15, 60, n_samples),\n",
    "    'bedrooms': np.random.randint(1, 6, n_samples),\n",
    "    'age_years': np.random.randint(0, 50, n_samples),\n",
    "})\n",
    "\n",
    "# True price depends on area (length × width) and other factors\n",
    "# This simulates real-world where the underlying pattern involves interactions\n",
    "house_data['area_sqft'] = house_data['length_feet'] * house_data['width_feet']\n",
    "house_data['price'] = (\n",
    "    200 * house_data['area_sqft'] +  # $200 per sq ft\n",
    "    10000 * house_data['bedrooms'] +  # $10k per bedroom\n",
    "    -500 * house_data['age_years'] +   # Depreciation\n",
    "    np.random.normal(0, 50000, n_samples)  # Random noise\n",
    ")\n",
    "\n",
    "print(f\"Created dataset with {len(house_data)} houses\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "house_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance WITHOUT Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only raw features (length, width, bedrooms, age)\n",
    "# We're intentionally NOT using the area we calculated\n",
    "raw_features = ['length_feet', 'width_feet', 'bedrooms', 'age_years']\n",
    "X_raw = house_data[raw_features]\n",
    "y = house_data['price']\n",
    "\n",
    "# Split data\n",
    "X_raw_train, X_raw_test, y_train, y_test = train_test_split(\n",
    "    X_raw, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train model\n",
    "model_raw = LinearRegression()\n",
    "model_raw.fit(X_raw_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_raw = model_raw.predict(X_raw_test)\n",
    "rmse_raw = np.sqrt(mean_squared_error(y_test, y_pred_raw))\n",
    "r2_raw = r2_score(y_test, y_pred_raw)\n",
    "\n",
    "print(\"Model Performance WITHOUT Feature Engineering:\")\n",
    "print(f\"RMSE: ${rmse_raw:,.0f}\")\n",
    "print(f\"R² Score: {r2_raw:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance WITH Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now include our engineered feature: area\n",
    "# This better represents what actually affects house price\n",
    "engineered_features = ['area_sqft', 'bedrooms', 'age_years']\n",
    "X_engineered = house_data[engineered_features]\n",
    "\n",
    "# Split data (same random state for fair comparison)\n",
    "X_eng_train, X_eng_test, y_train, y_test = train_test_split(\n",
    "    X_engineered, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train model\n",
    "model_engineered = LinearRegression()\n",
    "model_engineered.fit(X_eng_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_eng = model_engineered.predict(X_eng_test)\n",
    "rmse_eng = np.sqrt(mean_squared_error(y_test, y_pred_eng))\n",
    "r2_eng = r2_score(y_test, y_pred_eng)\n",
    "\n",
    "print(\"Model Performance WITH Feature Engineering:\")\n",
    "print(f\"RMSE: ${rmse_eng:,.0f}\")\n",
    "print(f\"R² Score: {r2_eng:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate improvement\n",
    "rmse_improvement = (rmse_raw - rmse_eng) / rmse_raw * 100\n",
    "r2_improvement = (r2_eng - r2_raw) / r2_raw * 100\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FEATURE ENGINEERING IMPACT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nRMSE Reduction: {rmse_improvement:.1f}%\")\n",
    "print(f\"R² Improvement: {r2_improvement:.1f}%\")\n",
    "print(f\"\\nBy creating just ONE engineered feature (area),\")\n",
    "print(f\"we improved model performance by {rmse_improvement:.0f}%!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the difference\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Without feature engineering\n",
    "axes[0].scatter(y_test, y_pred_raw, alpha=0.5)\n",
    "axes[0].plot([y_test.min(), y_test.max()], \n",
    "             [y_test.min(), y_test.max()], \n",
    "             'r--', lw=2, label='Perfect Prediction')\n",
    "axes[0].set_xlabel('Actual Price')\n",
    "axes[0].set_ylabel('Predicted Price')\n",
    "axes[0].set_title(f'Without Feature Engineering\\nR² = {r2_raw:.3f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Plot 2: With feature engineering\n",
    "axes[1].scatter(y_test, y_pred_eng, alpha=0.5, color='green')\n",
    "axes[1].plot([y_test.min(), y_test.max()], \n",
    "             [y_test.min(), y_test.max()], \n",
    "             'r--', lw=2, label='Perfect Prediction')\n",
    "axes[1].set_xlabel('Actual Price')\n",
    "axes[1].set_ylabel('Predicted Price')\n",
    "axes[1].set_title(f'With Feature Engineering\\nR² = {r2_eng:.3f}')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice how predictions are much closer to the red line (perfect predictions)\")\n",
    "print(\"when we use the engineered 'area' feature!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Types of Feature Engineering\n",
    "\n",
    "Feature engineering encompasses many techniques:\n",
    "\n",
    "### 4.1 Handling Missing Data\n",
    "- **Imputation**: Fill missing values with mean, median, or advanced methods\n",
    "- **Indicator features**: Create `is_missing` flags\n",
    "- **Deletion**: Remove rows or columns (use cautiously)\n",
    "\n",
    "### 4.2 Encoding Categorical Variables\n",
    "- **One-hot encoding**: Convert categories to binary columns\n",
    "- **Ordinal encoding**: Map ordered categories to numbers\n",
    "- **Target encoding**: Use target statistics per category\n",
    "\n",
    "### 4.3 Feature Scaling\n",
    "- **Normalization**: Scale to [0, 1] range\n",
    "- **Standardization**: Transform to mean=0, std=1\n",
    "- **Robust scaling**: Handle outliers better\n",
    "\n",
    "### 4.4 Creating New Features\n",
    "- **Interactions**: Multiply features (like our `area` example)\n",
    "- **Polynomials**: Create squared, cubed terms\n",
    "- **Binning**: Convert continuous to categorical\n",
    "- **Domain-specific**: Use domain knowledge\n",
    "\n",
    "### 4.5 Feature Selection\n",
    "- **Filter methods**: Correlation, chi-square\n",
    "- **Wrapper methods**: Recursive feature elimination\n",
    "- **Embedded methods**: L1 regularization, tree importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. The Feature Engineering Workflow\n",
    "\n",
    "```\n",
    "1. Understand the Problem\n",
    "   ↓\n",
    "2. Exploratory Data Analysis (EDA)\n",
    "   ↓\n",
    "3. Handle Missing Data\n",
    "   ↓\n",
    "4. Encode Categorical Variables\n",
    "   ↓\n",
    "5. Create New Features\n",
    "   ↓\n",
    "6. Scale/Transform Features\n",
    "   ↓\n",
    "7. Select Important Features\n",
    "   ↓\n",
    "8. Build Pipeline & Evaluate\n",
    "   ↓\n",
    "9. Iterate (repeat steps 3-8)\n",
    "```\n",
    "\n",
    "**Critical Rules**:\n",
    "- ✅ **Always** split data BEFORE feature engineering\n",
    "- ✅ Fit transformations on training data only\n",
    "- ✅ Apply same transformations to test data\n",
    "- ❌ **Never** use test data statistics in feature engineering (causes data leakage!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Common Pitfalls and How to Avoid Them\n",
    "\n",
    "### 6.1 Data Leakage\n",
    "\n",
    "**What**: Information from test set leaks into training\n",
    "\n",
    "**Example of BAD practice**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ❌ BAD: Scaling before splitting\n",
    "# This causes data leakage!\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Don't do this:\n",
    "# X_scaled = StandardScaler().fit_transform(X)  # Uses ALL data including test\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_scaled, y)\n",
    "\n",
    "print(\"❌ Scaling before splitting causes data leakage!\")\n",
    "print(\"The scaler sees test data, giving unrealistic performance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example of GOOD practice**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ GOOD: Split first, then scale\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create sample data\n",
    "X_sample = house_data[['length_feet', 'width_feet', 'bedrooms']]\n",
    "y_sample = house_data['price']\n",
    "\n",
    "# 1. Split first\n",
    "X_train_sample, X_test_sample, y_train_sample, y_test_sample = train_test_split(\n",
    "    X_sample, y_sample, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 2. Fit scaler on training data only\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_sample)\n",
    "\n",
    "# 3. Apply same transformation to test data\n",
    "X_test_scaled = scaler.transform(X_test_sample)  # Note: transform, not fit_transform!\n",
    "\n",
    "print(\"✅ CORRECT: Split data, then fit on train, transform both\")\n",
    "print(f\"Training set shape: {X_train_scaled.shape}\")\n",
    "print(f\"Test set shape: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Target Leakage\n",
    "\n",
    "**What**: Features that wouldn't be available at prediction time\n",
    "\n",
    "**Example**: Predicting hospital readmission using \"number of days in hospital\"\n",
    "- You won't know this until AFTER the patient is admitted!\n",
    "\n",
    "### 6.3 Overfitting\n",
    "\n",
    "**What**: Creating too many features that memorize training data\n",
    "\n",
    "**Prevention**:\n",
    "- Use cross-validation\n",
    "- Apply regularization\n",
    "- Feature selection\n",
    "- Keep features interpretable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Exercise Section\n",
    "\n",
    "Let's practice what we learned!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Create Interaction Features\n",
    "\n",
    "Create a dataset of rectangles and predict their perimeter. Engineer a new feature that would help the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n",
    "\n",
    "# Create rectangle dataset\n",
    "rectangles = pd.DataFrame({\n",
    "    'length': np.random.uniform(1, 20, 200),\n",
    "    'width': np.random.uniform(1, 20, 200)\n",
    "})\n",
    "\n",
    "# True perimeter formula: 2 * (length + width)\n",
    "rectangles['perimeter'] = 2 * (rectangles['length'] + rectangles['width'])\n",
    "\n",
    "# TODO: Create an engineered feature that helps predict perimeter\n",
    "# Hint: What mathematical relationship exists between length, width, and perimeter?\n",
    "\n",
    "# Your code here:\n",
    "# rectangles['engineered_feature'] = ???\n",
    "\n",
    "# Test your feature\n",
    "# (We'll provide solution after you try!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to Exercise 1\n",
    "# (Try the exercise above first!)\n",
    "\n",
    "# The key insight: perimeter = 2*(length + width)\n",
    "# So a good engineered feature is the sum of length and width\n",
    "rectangles['length_plus_width'] = rectangles['length'] + rectangles['width']\n",
    "\n",
    "# Compare models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Without engineered feature\n",
    "X_basic = rectangles[['length', 'width']]\n",
    "y = rectangles['perimeter']\n",
    "model1 = LinearRegression().fit(X_basic, y)\n",
    "r2_basic = model1.score(X_basic, y)\n",
    "\n",
    "# With engineered feature\n",
    "X_engineered = rectangles[['length_plus_width']]\n",
    "model2 = LinearRegression().fit(X_engineered, y)\n",
    "r2_eng = model2.score(X_engineered, y)\n",
    "\n",
    "print(f\"R² without feature engineering: {r2_basic:.4f}\")\n",
    "print(f\"R² with feature engineering: {r2_eng:.4f}\")\n",
    "print(f\"\\nBoth should be perfect (1.0) because perimeter has a linear relationship!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Identify Data Leakage\n",
    "\n",
    "Which of the following would cause data leakage when predicting customer churn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Identify which features cause data leakage\n",
    "\n",
    "print(\"Scenario: Predicting if a customer will cancel their subscription (churn)\")\n",
    "print(\"\\nWhich features would cause data leakage?\\n\")\n",
    "\n",
    "features = {\n",
    "    'A': 'Number of customer service calls in last month',\n",
    "    'B': 'Cancellation date',\n",
    "    'C': 'Monthly subscription cost',\n",
    "    'D': 'Number of logins in last 7 days',\n",
    "    'E': 'Reason for cancellation',\n",
    "    'F': 'Customer age'\n",
    "}\n",
    "\n",
    "for key, feature in features.items():\n",
    "    print(f\"{key}. {feature}\")\n",
    "\n",
    "print(\"\\nYour answer: (list letters that cause leakage)\")\n",
    "# Write your answer here as a comment\n",
    "# answer = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to Exercise 2\n",
    "\n",
    "print(\"Features that cause data leakage:\")\n",
    "print(\"\\nB. Cancellation date - Only known AFTER churn happens!\")\n",
    "print(\"E. Reason for cancellation - Only known AFTER churn happens!\")\n",
    "print(\"\\nThese are examples of TARGET LEAKAGE - information only available\")\n",
    "print(\"after the event you're trying to predict.\")\n",
    "print(\"\\nFeatures A, C, D, F are all valid because they're known BEFORE churn.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Feature Engineering Ideas\n",
    "\n",
    "For each scenario, suggest 2-3 engineered features that might improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Brainstorm engineered features\n",
    "\n",
    "scenarios = {\n",
    "    'Scenario 1': {\n",
    "        'task': 'Predict credit card fraud',\n",
    "        'raw_features': ['transaction_amount', 'merchant_name', 'transaction_time', \n",
    "                         'card_holder_location', 'merchant_location'],\n",
    "        'your_ideas': [\n",
    "            # Write your ideas here\n",
    "            # Example: 'distance_from_home (using card_holder_location and merchant_location)'\n",
    "        ]\n",
    "    },\n",
    "    'Scenario 2': {\n",
    "        'task': 'Predict movie ratings',\n",
    "        'raw_features': ['user_id', 'movie_id', 'release_year', 'genres', 'runtime_minutes'],\n",
    "        'your_ideas': [\n",
    "            # Write your ideas here\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Think about:\")\n",
    "print(\"- Interactions between features\")\n",
    "print(\"- Domain knowledge\")\n",
    "print(\"- Time-based features\")\n",
    "print(\"- Statistical aggregations\\n\")\n",
    "\n",
    "for name, scenario in scenarios.items():\n",
    "    print(f\"{name}: {scenario['task']}\")\n",
    "    print(f\"Raw features: {scenario['raw_features']}\")\n",
    "    print(\"Your engineered features:\")\n",
    "    print(\"(Write your ideas in the code above)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to Exercise 3\n",
    "\n",
    "print(\"Suggested Engineered Features:\\n\")\n",
    "\n",
    "print(\"Scenario 1: Predict credit card fraud\")\n",
    "print(\"1. distance_between_locations - Geographic distance between card holder and merchant\")\n",
    "print(\"2. transaction_hour - Extract hour from transaction_time (frauds often at night)\")\n",
    "print(\"3. amount_vs_user_avg - How much larger than user's average transaction?\")\n",
    "print(\"4. is_foreign_country - Binary: merchant country != card holder country\")\n",
    "print(\"5. time_since_last_transaction - Rapid successive transactions = suspicious\\n\")\n",
    "\n",
    "print(\"Scenario 2: Predict movie ratings\")\n",
    "print(\"1. movie_age - Current year minus release_year\")\n",
    "print(\"2. genre_count - Number of genres (multi-genre might appeal to more people)\")\n",
    "print(\"3. is_long_movie - Binary: runtime > 150 minutes\")\n",
    "print(\"4. user_avg_rating - User's average rating of all movies (some rate higher)\")\n",
    "print(\"5. genre_match_user_preference - Does genre match user's most-watched genres?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Feature engineering often matters more than algorithm choice**\n",
    "   - Can improve performance by 5-20%+ \n",
    "   - Makes simple models competitive with complex ones\n",
    "\n",
    "2. **Core techniques** we'll cover in this module:\n",
    "   - Handling missing data (Module 01)\n",
    "   - Encoding categorical variables (Module 02)\n",
    "   - Feature scaling (Module 03)\n",
    "   - Creating interactions and polynomials (Module 04)\n",
    "   - Binning and discretization (Module 05)\n",
    "   - Date/time features (Module 06)\n",
    "   - Text features (Module 07)\n",
    "   - Feature selection (Module 08)\n",
    "   - Feature importance (Module 09)\n",
    "   - Automated feature engineering (Module 10)\n",
    "\n",
    "3. **Critical rules to prevent data leakage**:\n",
    "   - Always split data BEFORE feature engineering\n",
    "   - Fit transformations on training data only\n",
    "   - Never use information unavailable at prediction time\n",
    "\n",
    "4. **Feature engineering is iterative**:\n",
    "   - Try ideas, measure impact, keep what works\n",
    "   - Use domain knowledge\n",
    "   - Learn from competitions and real projects\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "**Module 01**: Handling Missing Data - Learn imputation strategies and when to use each\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [Kaggle Feature Engineering Course](https://www.kaggle.com/learn/feature-engineering)\n",
    "- \"Feature Engineering for Machine Learning\" by Alice Zheng\n",
    "- [Feature-engine library documentation](https://feature-engine.readthedocs.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Congratulations!** You've completed Module 00. You now understand:\n",
    "- What feature engineering is and why it's critical\n",
    "- The dramatic impact it can have on model performance\n",
    "- Different types of feature engineering techniques\n",
    "- How to avoid common pitfalls like data leakage\n",
    "\n",
    "Ready to dive deeper? Let's move to **Module 01: Handling Missing Data**!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
