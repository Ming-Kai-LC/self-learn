{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 03: Feature Scaling and Normalization\n",
    "\n",
    "**Difficulty**: ⭐ Beginner  \n",
    "**Estimated Time**: 50 minutes  \n",
    "**Prerequisites**: [Module 02: Encoding Categorical Variables](02_encoding_categorical_variables.ipynb)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Understand why feature scaling is critical for certain algorithms\n",
    "2. Apply Min-Max scaling to normalize features to [0, 1] range\n",
    "3. Use StandardScaler to standardize features (mean=0, std=1)\n",
    "4. Apply RobustScaler to handle outliers effectively\n",
    "5. Identify which algorithms require scaling and which don't\n",
    "6. Visualize the impact of scaling on model performance\n",
    "7. Avoid data leakage when scaling features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Feature Scaling Matters\n",
    "\n",
    "### The Problem\n",
    "\n",
    "Features often have vastly different scales:\n",
    "\n",
    "| Feature | Example Values | Range |\n",
    "|---------|---------------|-------|\n",
    "| Age | 25, 40, 65 | 0-100 |\n",
    "| Income | $30,000, $75,000, $150,000 | 0-500,000 |\n",
    "| Credit Score | 650, 720, 800 | 300-850 |\n",
    "\n",
    "**Problem**: Income dominates because its values are 1000x larger!\n",
    "\n",
    "### Which Algorithms Need Scaling?\n",
    "\n",
    "#### ✅ NEED Scaling (Distance-Based & Gradient-Based)\n",
    "- **K-Nearest Neighbors (KNN)**: Uses Euclidean distance\n",
    "- **Support Vector Machines (SVM)**: Depends on distances\n",
    "- **Linear/Logistic Regression**: Gradient descent converges faster\n",
    "- **Neural Networks**: Faster training, better convergence\n",
    "- **Principal Component Analysis (PCA)**: Variance-based\n",
    "- **K-Means Clustering**: Distance-based\n",
    "\n",
    "#### ❌ DON'T NEED Scaling (Tree-Based)\n",
    "- **Decision Trees**: Split on individual features\n",
    "- **Random Forests**: Ensemble of decision trees\n",
    "- **Gradient Boosting (XGBoost, LightGBM)**: Tree-based\n",
    "\n",
    "### Real-World Impact\n",
    "\n",
    "Without scaling, a KNN model might:\n",
    "- Find \"neighbors\" based mostly on income\n",
    "- Ignore age completely (scale too small)\n",
    "- Give poor predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scaling methods\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, MaxAbsScaler\n",
    "\n",
    "# Models for comparison\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Configuration\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 3)\n",
    "\n",
    "print(\"✓ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Dataset to Demonstrate Scaling\n",
    "\n",
    "Let's create a loan approval dataset with features on different scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "# Create features with very different scales\n",
    "loan_data = pd.DataFrame({\n",
    "    'age': np.random.randint(22, 70, n_samples),\n",
    "    'annual_income': np.random.normal(60000, 30000, n_samples).clip(20000, 300000),\n",
    "    'credit_score': np.random.randint(450, 850, n_samples),\n",
    "    'loan_amount': np.random.normal(150000, 80000, n_samples).clip(10000, 500000),\n",
    "    'employment_years': np.random.randint(0, 45, n_samples),\n",
    "    'existing_debts': np.random.normal(25000, 15000, n_samples).clip(0, 100000)\n",
    "})\n",
    "\n",
    "# Create target: loan approved based on multiple factors\n",
    "approval_score = (\n",
    "    0.3 * (loan_data['credit_score'] - 450) / 400 +\n",
    "    0.3 * (loan_data['annual_income'] - 20000) / 280000 +\n",
    "    0.2 * (loan_data['employment_years'] / 45) +\n",
    "    0.2 * (1 - loan_data['existing_debts'] / 100000) +\n",
    "    np.random.normal(0, 0.1, n_samples)\n",
    ")\n",
    "\n",
    "loan_data['approved'] = (approval_score > 0.5).astype(int)\n",
    "\n",
    "print(f\"Created dataset with {len(loan_data)} loan applications\")\n",
    "print(f\"\\nApproval rate: {loan_data['approved'].mean():.1%}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "loan_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature scales\n",
    "features = ['age', 'annual_income', 'credit_score', 'loan_amount', 'employment_years', 'existing_debts']\n",
    "\n",
    "print(\"Feature Statistics (showing different scales):\\n\")\n",
    "stats = loan_data[features].describe().loc[['min', 'max', 'mean', 'std']]\n",
    "print(stats)\n",
    "\n",
    "print(\"\\n⚠️  Notice the huge scale differences:\")\n",
    "print(f\"  - age: {loan_data['age'].min():.0f} to {loan_data['age'].max():.0f}\")\n",
    "print(f\"  - annual_income: ${loan_data['annual_income'].min():.0f} to ${loan_data['annual_income'].max():.0f}\")\n",
    "print(f\"  - credit_score: {loan_data['credit_score'].min():.0f} to {loan_data['credit_score'].max():.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize scale differences\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(features):\n",
    "    axes[idx].hist(loan_data[feature], bins=30, edgecolor='black', alpha=0.7)\n",
    "    axes[idx].set_title(f'{feature}\\n(Range: {loan_data[feature].min():.0f} - {loan_data[feature].max():.0f})')\n",
    "    axes[idx].set_xlabel('Value')\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice how different the x-axis scales are!\")\n",
    "print(\"This will cause problems for distance-based algorithms.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Method 1: Min-Max Scaling (Normalization)\n",
    "\n",
    "**Formula**: $X_{scaled} = \\frac{X - X_{min}}{X_{max} - X_{min}}$\n",
    "\n",
    "**Result**: All features scaled to [0, 1] range\n",
    "\n",
    "**When to use**:\n",
    "- When you need bounded values [0, 1]\n",
    "- For neural networks (activation functions work well in this range)\n",
    "- When data doesn't have outliers\n",
    "\n",
    "**Pros**: \n",
    "- Preserves zero values\n",
    "- Bounded range\n",
    "- Intuitive\n",
    "\n",
    "**Cons**: \n",
    "- Sensitive to outliers\n",
    "- Doesn't center data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for scaling\n",
    "X = loan_data[features]\n",
    "y = loan_data['approved']\n",
    "\n",
    "# IMPORTANT: Split BEFORE scaling!\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Min-Max Scaling\n",
    "minmax_scaler = MinMaxScaler()\n",
    "\n",
    "# Fit on training data only!\n",
    "X_train_minmax = minmax_scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform test data using training statistics\n",
    "X_test_minmax = minmax_scaler.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrame for visualization\n",
    "X_train_minmax_df = pd.DataFrame(X_train_minmax, columns=features, index=X_train.index)\n",
    "X_test_minmax_df = pd.DataFrame(X_test_minmax, columns=features, index=X_test.index)\n",
    "\n",
    "print(\"Min-Max Scaling Applied\\n\")\n",
    "print(\"Original training data (first 3 rows):\")\n",
    "print(X_train.head(3))\n",
    "print(\"\\nScaled training data (first 3 rows):\")\n",
    "print(X_train_minmax_df.head(3))\n",
    "\n",
    "print(\"\\nScaled data statistics:\")\n",
    "print(X_train_minmax_df.describe().loc[['min', 'max', 'mean']])\n",
    "print(\"\\n✓ All features now in [0, 1] range!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Method 2: Standardization (Z-Score Normalization)\n",
    "\n",
    "**Formula**: $X_{scaled} = \\frac{X - \\mu}{\\sigma}$\n",
    "\n",
    "where $\\mu$ = mean, $\\sigma$ = standard deviation\n",
    "\n",
    "**Result**: Mean = 0, Standard Deviation = 1\n",
    "\n",
    "**When to use**:\n",
    "- Most common scaling method\n",
    "- When features are approximately normally distributed\n",
    "- For algorithms assuming zero-centered data (SVM, Logistic Regression)\n",
    "\n",
    "**Pros**: \n",
    "- Less sensitive to outliers than Min-Max\n",
    "- Maintains shape of original distribution\n",
    "- Works well with PCA\n",
    "\n",
    "**Cons**: \n",
    "- No bounded range\n",
    "- Assumes data is roughly Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Standardization\n",
    "standard_scaler = StandardScaler()\n",
    "\n",
    "# Fit on training data\n",
    "X_train_standard = standard_scaler.fit_transform(X_train)\n",
    "X_test_standard = standard_scaler.transform(X_test)\n",
    "\n",
    "# Convert to DataFrame\n",
    "X_train_standard_df = pd.DataFrame(X_train_standard, columns=features, index=X_train.index)\n",
    "X_test_standard_df = pd.DataFrame(X_test_standard, columns=features, index=X_test.index)\n",
    "\n",
    "print(\"Standardization (Standard Scaling) Applied\\n\")\n",
    "print(\"Scaled training data (first 3 rows):\")\n",
    "print(X_train_standard_df.head(3))\n",
    "\n",
    "print(\"\\nScaled data statistics:\")\n",
    "print(X_train_standard_df.describe().loc[['mean', 'std', 'min', 'max']])\n",
    "print(\"\\n✓ Mean ≈ 0, Standard Deviation ≈ 1 for all features!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Method 3: Robust Scaling\n",
    "\n",
    "**Formula**: $X_{scaled} = \\frac{X - median}{IQR}$\n",
    "\n",
    "where IQR = Interquartile Range (75th percentile - 25th percentile)\n",
    "\n",
    "**When to use**:\n",
    "- When data has outliers\n",
    "- When outliers are meaningful (not errors)\n",
    "- More robust than StandardScaler\n",
    "\n",
    "**Pros**: \n",
    "- Not affected by outliers\n",
    "- Uses median instead of mean\n",
    "\n",
    "**Cons**: \n",
    "- Not as common\n",
    "- Doesn't guarantee bounded range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset WITH outliers to demonstrate\n",
    "X_with_outliers = X_train.copy()\n",
    "\n",
    "# Add some extreme outliers\n",
    "outlier_indices = np.random.choice(X_with_outliers.index, 20, replace=False)\n",
    "X_with_outliers.loc[outlier_indices, 'annual_income'] *= 5  # Extreme incomes\n",
    "X_with_outliers.loc[outlier_indices, 'existing_debts'] *= 10  # Extreme debts\n",
    "\n",
    "print(\"Dataset with outliers:\")\n",
    "print(X_with_outliers[['annual_income', 'existing_debts']].describe())\n",
    "print(\"\\nNotice the max values are now much larger!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare StandardScaler vs RobustScaler on data with outliers\n",
    "\n",
    "# StandardScaler (affected by outliers)\n",
    "standard_with_outliers = StandardScaler().fit_transform(X_with_outliers)\n",
    "standard_with_outliers_df = pd.DataFrame(standard_with_outliers, columns=features, index=X_with_outliers.index)\n",
    "\n",
    "# RobustScaler (robust to outliers)\n",
    "robust_scaler = RobustScaler()\n",
    "robust_scaled = robust_scaler.fit_transform(X_with_outliers)\n",
    "robust_scaled_df = pd.DataFrame(robust_scaled, columns=features, index=X_with_outliers.index)\n",
    "\n",
    "# Compare income scaling\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].hist(X_with_outliers['annual_income'], bins=30, edgecolor='black')\n",
    "axes[0].set_title('Original (with outliers)')\n",
    "axes[0].set_xlabel('Annual Income')\n",
    "\n",
    "axes[1].hist(standard_with_outliers_df['annual_income'], bins=30, edgecolor='black', color='orange')\n",
    "axes[1].set_title('StandardScaler\\n(outliers compress normal data)')\n",
    "axes[1].set_xlabel('Scaled Value')\n",
    "\n",
    "axes[2].hist(robust_scaled_df['annual_income'], bins=30, edgecolor='black', color='green')\n",
    "axes[2].set_title('RobustScaler\\n(outliers have less impact)')\n",
    "axes[2].set_xlabel('Scaled Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"RobustScaler is less affected by outliers because it uses median and IQR\")\n",
    "print(\"instead of mean and standard deviation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Impact on Model Performance\n",
    "\n",
    "Let's demonstrate how scaling affects different algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models with and without scaling\n",
    "\n",
    "# Prepare datasets\n",
    "datasets = {\n",
    "    'No Scaling': (X_train, X_test),\n",
    "    'Min-Max': (X_train_minmax, X_test_minmax),\n",
    "    'Standard': (X_train_standard, X_test_standard)\n",
    "}\n",
    "\n",
    "# Models to test\n",
    "models = {\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'SVM': SVC(kernel='rbf', random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10)\n",
    "}\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "for scaling_method, (X_tr, X_te) in datasets.items():\n",
    "    for model_name, model in models.items():\n",
    "        # Train\n",
    "        model.fit(X_tr, y_train)\n",
    "        \n",
    "        # Evaluate\n",
    "        y_pred = model.predict(X_te)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        results.append({\n",
    "            'Scaling': scaling_method,\n",
    "            'Model': model_name,\n",
    "            'Accuracy': accuracy\n",
    "        })\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_pivot = results_df.pivot(index='Model', columns='Scaling', values='Accuracy')\n",
    "\n",
    "print(\"Model Performance Comparison:\\n\")\n",
    "print(results_pivot)\n",
    "\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"1. KNN, Logistic Regression, and SVM benefit greatly from scaling\")\n",
    "print(\"2. Random Forest performs similarly with or without scaling\")\n",
    "print(\"3. Standard scaling often performs best for distance-based algorithms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the impact\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "results_pivot.plot(kind='bar', ax=ax, width=0.8)\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Impact of Feature Scaling on Different Algorithms')\n",
    "ax.set_xlabel('Algorithm')\n",
    "ax.legend(title='Scaling Method')\n",
    "ax.set_ylim([0.5, 1.0])\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt='%.3f', padding=3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualizing Scaled vs Unscaled Data\n",
    "\n",
    "Let's visualize how scaling affects the feature space for KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select two features for 2D visualization\n",
    "feature1 = 'age'\n",
    "feature2 = 'annual_income'\n",
    "\n",
    "# Get subset of data\n",
    "sample_indices = np.random.choice(X_train.index, 200, replace=False)\n",
    "X_sample = X_train.loc[sample_indices]\n",
    "y_sample = y_train.loc[sample_indices]\n",
    "\n",
    "# Scale the sample\n",
    "scaler = StandardScaler()\n",
    "X_sample_scaled = scaler.fit_transform(X_sample)\n",
    "X_sample_scaled_df = pd.DataFrame(X_sample_scaled, columns=features, index=sample_indices)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Unscaled\n",
    "axes[0].scatter(X_sample[feature1], X_sample[feature2], \n",
    "                c=y_sample, cmap='coolwarm', alpha=0.6, edgecolors='black')\n",
    "axes[0].set_xlabel(feature1)\n",
    "axes[0].set_ylabel(feature2)\n",
    "axes[0].set_title('Unscaled Features\\n(annual_income dominates the scale)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Scaled\n",
    "axes[1].scatter(X_sample_scaled_df[feature1], X_sample_scaled_df[feature2],\n",
    "                c=y_sample, cmap='coolwarm', alpha=0.6, edgecolors='black')\n",
    "axes[1].set_xlabel(f'{feature1} (scaled)')\n",
    "axes[1].set_ylabel(f'{feature2} (scaled)')\n",
    "axes[1].set_title('Scaled Features\\n(both features equally weighted)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice:\")\n",
    "print(\"- Left plot: annual_income has much larger range, dominates distance calculations\")\n",
    "print(\"- Right plot: both features have similar scale, both contribute equally to distance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. When to Use Which Scaler?\n",
    "\n",
    "| Scaler | When to Use | Pros | Cons |\n",
    "|--------|-------------|------|------|\n",
    "| **MinMaxScaler** | - Neural networks<br>- Algorithms needing bounded values<br>- No outliers | - Bounded [0,1]<br>- Preserves zero<br>- Intuitive | - Sensitive to outliers |\n",
    "| **StandardScaler** | - Most common choice<br>- SVM, Logistic Regression<br>- PCA<br>- Gaussian features | - Less sensitive to outliers than MinMax<br>- Centers data<br>- Maintains shape | - No bounded range<br>- Assumes Gaussian |\n",
    "| **RobustScaler** | - Data has outliers<br>- Outliers are meaningful | - Very robust to outliers<br>- Uses median/IQR | - Less common<br>- No bounded range |\n",
    "| **MaxAbsScaler** | - Sparse data<br>- Don't want to center | - Preserves sparsity<br>- Bounded [-1,1] | - Sensitive to outliers |\n",
    "| **None** | - Tree-based models<br>- Features already on same scale | - Simpler<br>- Faster | - Can't use with distance-based models |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Best Practices\n",
    "\n",
    "### ✅ DO:\n",
    "\n",
    "1. **Always split BEFORE scaling**\n",
    "   ```python\n",
    "   X_train, X_test = train_test_split(X, y)\n",
    "   scaler.fit(X_train)  # Fit on training only\n",
    "   X_train_scaled = scaler.transform(X_train)\n",
    "   X_test_scaled = scaler.transform(X_test)\n",
    "   ```\n",
    "\n",
    "2. **Scale features for**:\n",
    "   - KNN, SVM, Neural Networks\n",
    "   - Logistic/Linear Regression (faster convergence)\n",
    "   - PCA, K-Means\n",
    "\n",
    "3. **Use StandardScaler as default**\n",
    "   - Most common and works well\n",
    "   - Switch to RobustScaler if outliers\n",
    "   - Switch to MinMaxScaler for neural networks\n",
    "\n",
    "4. **Scale in production**\n",
    "   - Save fitted scaler\n",
    "   - Apply same transformation to new data\n",
    "\n",
    "### ❌ DON'T:\n",
    "\n",
    "1. **Don't fit scaler on test data** (data leakage!)\n",
    "2. **Don't scale tree-based models** (unnecessary)\n",
    "3. **Don't forget to scale new data** in production\n",
    "4. **Don't scale target variable** (usually)\n",
    "5. **Don't scale binary features** (0/1) - often not needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Exercise Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Identify Which Models Need Scaling\n",
    "\n",
    "For each algorithm, determine if scaling is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Which algorithms need scaling?\n",
    "\n",
    "algorithms = {\n",
    "    'A': 'K-Nearest Neighbors (KNN)',\n",
    "    'B': 'Decision Tree',\n",
    "    'C': 'Support Vector Machine (SVM)',\n",
    "    'D': 'Random Forest',\n",
    "    'E': 'Neural Network',\n",
    "    'F': 'Gradient Boosting (XGBoost)',\n",
    "    'G': 'Logistic Regression',\n",
    "    'H': 'Principal Component Analysis (PCA)'\n",
    "}\n",
    "\n",
    "print(\"Which algorithms NEED scaling?\\n\")\n",
    "for key, algo in algorithms.items():\n",
    "    print(f\"{key}. {algo}\")\n",
    "\n",
    "print(\"\\nYour answers (list letters that NEED scaling):\")\n",
    "# needs_scaling = []\n",
    "# doesnt_need = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to Exercise 1\n",
    "\n",
    "print(\"Solutions:\\n\")\n",
    "print(\"✅ NEEDS Scaling (distance or gradient-based):\")\n",
    "print(\"  A - KNN: Uses distance calculations\")\n",
    "print(\"  C - SVM: Distance-based decision boundary\")\n",
    "print(\"  E - Neural Network: Gradient descent converges faster\")\n",
    "print(\"  G - Logistic Regression: Gradient descent optimization\")\n",
    "print(\"  H - PCA: Variance-based, sensitive to scale\")\n",
    "\n",
    "print(\"\\n❌ DOESN'T NEED Scaling (tree-based):\")\n",
    "print(\"  B - Decision Tree: Splits on individual features\")\n",
    "print(\"  D - Random Forest: Ensemble of decision trees\")\n",
    "print(\"  F - Gradient Boosting: Tree-based algorithm\")\n",
    "\n",
    "print(\"\\nKey Rule: Distance-based and gradient-based → SCALE\")\n",
    "print(\"          Tree-based → NO SCALING NEEDED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Apply the Right Scaler\n",
    "\n",
    "Choose and apply the appropriate scaler for each scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Choose the right scaler\n",
    "\n",
    "# Scenario 1: House prices with outliers (some mansions worth $10M+)\n",
    "house_data = pd.DataFrame({\n",
    "    'sqft': [1000, 1500, 2000, 2500, 15000],  # Last one is a mansion\n",
    "    'bedrooms': [2, 3, 3, 4, 10],\n",
    "    'age': [10, 20, 5, 15, 50]\n",
    "})\n",
    "\n",
    "print(\"Scenario 1: House prices with outliers\")\n",
    "print(house_data)\n",
    "print(\"\\nWhich scaler should you use?\")\n",
    "print(\"A) MinMaxScaler\")\n",
    "print(\"B) StandardScaler\")\n",
    "print(\"C) RobustScaler\")\n",
    "print(\"\\nYour answer: ???\")\n",
    "\n",
    "# TODO: Apply the correct scaler\n",
    "# scaler = ???\n",
    "# house_scaled = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to Exercise 2\n",
    "\n",
    "print(\"Solution: C) RobustScaler\\n\")\n",
    "print(\"Reason: Data has a clear outlier (15,000 sqft mansion)\")\n",
    "print(\"RobustScaler uses median and IQR, which are robust to outliers\\n\")\n",
    "\n",
    "# Apply RobustScaler\n",
    "robust_scaler = RobustScaler()\n",
    "house_scaled = robust_scaler.fit_transform(house_data)\n",
    "house_scaled_df = pd.DataFrame(house_scaled, columns=house_data.columns)\n",
    "\n",
    "print(\"Original data:\")\n",
    "print(house_data)\n",
    "print(\"\\nScaled with RobustScaler:\")\n",
    "print(house_scaled_df)\n",
    "\n",
    "# Compare with StandardScaler\n",
    "standard_scaler = StandardScaler()\n",
    "house_standard = standard_scaler.fit_transform(house_data)\n",
    "house_standard_df = pd.DataFrame(house_standard, columns=house_data.columns)\n",
    "\n",
    "print(\"\\nScaled with StandardScaler (for comparison):\")\n",
    "print(house_standard_df)\n",
    "print(\"\\nNotice: StandardScaler is more affected by the outlier!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Detect Data Leakage in Scaling\n",
    "\n",
    "Find and fix the data leakage problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Fix the data leakage\n",
    "\n",
    "print(\"Problematic code:\")\n",
    "print('''\n",
    "# Load data\n",
    "X = data.drop('target', axis=1)\n",
    "y = data['target']\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y)\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train)\n",
    "''')\n",
    "\n",
    "print(\"\\nWhat's wrong with this code?\")\n",
    "print(\"How would you fix it?\")\n",
    "# Your answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to Exercise 3\n",
    "\n",
    "print(\"Problem: DATA LEAKAGE!\\n\")\n",
    "print(\"The scaler is fit on ALL data (including test set) before splitting.\")\n",
    "print(\"This means test set statistics (mean, std) leak into training.\\n\")\n",
    "\n",
    "print(\"Why this is bad:\")\n",
    "print(\"- Scaler uses mean/std from ENTIRE dataset\")\n",
    "print(\"- Test set influences the scaling\")\n",
    "print(\"- Model performance appears better than it actually is\")\n",
    "print(\"- Won't work correctly in production\\n\")\n",
    "\n",
    "print(\"Correct approach:\")\n",
    "print('''\n",
    "# Load data\n",
    "X = data.drop('target', axis=1)\n",
    "y = data['target']\n",
    "\n",
    "# Split FIRST!\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# Fit scaler on training data only\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Apply same transformation to test data\n",
    "X_test_scaled = scaler.transform(X_test)  # Note: transform, not fit_transform!\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train_scaled, y_train)\n",
    "''')\n",
    "\n",
    "print(\"\\n✓ Key: Split → Fit on train → Transform both train and test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Scaling Impact Experiment\n",
    "\n",
    "Demonstrate the impact of scaling on KNN performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Measure scaling impact on KNN\n",
    "\n",
    "# Create a simple dataset\n",
    "np.random.seed(42)\n",
    "n = 200\n",
    "\n",
    "exercise_data = pd.DataFrame({\n",
    "    'feature1': np.random.uniform(0, 10, n),      # Small scale\n",
    "    'feature2': np.random.uniform(0, 10000, n),   # Large scale\n",
    "    'feature3': np.random.uniform(0, 100, n)      # Medium scale\n",
    "})\n",
    "\n",
    "# Target depends equally on all three features\n",
    "exercise_data['target'] = (\n",
    "    (exercise_data['feature1'] > 5).astype(int) +\n",
    "    (exercise_data['feature2'] > 5000).astype(int) +\n",
    "    (exercise_data['feature3'] > 50).astype(int)\n",
    ") >= 2\n",
    "exercise_data['target'] = exercise_data['target'].astype(int)\n",
    "\n",
    "print(\"Dataset created with features on different scales:\")\n",
    "print(exercise_data.describe())\n",
    "\n",
    "# TODO: \n",
    "# 1. Train KNN on unscaled data\n",
    "# 2. Train KNN on scaled data (StandardScaler)\n",
    "# 3. Compare accuracy\n",
    "# 4. Which performs better and why?\n",
    "\n",
    "# Your code here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to Exercise 4\n",
    "\n",
    "X = exercise_data.drop('target', axis=1)\n",
    "y = exercise_data['target']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 1. KNN on unscaled data\n",
    "knn_unscaled = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_unscaled.fit(X_train, y_train)\n",
    "accuracy_unscaled = knn_unscaled.score(X_test, y_test)\n",
    "\n",
    "# 2. KNN on scaled data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_scaled.fit(X_train_scaled, y_train)\n",
    "accuracy_scaled = knn_scaled.score(X_test_scaled, y_test)\n",
    "\n",
    "# 3. Compare\n",
    "print(\"Results:\\n\")\n",
    "print(f\"KNN without scaling: {accuracy_unscaled:.3f}\")\n",
    "print(f\"KNN with scaling:    {accuracy_scaled:.3f}\")\n",
    "print(f\"\\nImprovement: {(accuracy_scaled - accuracy_unscaled):.3f} ({(accuracy_scaled - accuracy_unscaled)/accuracy_unscaled*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nWhy scaling helps:\")\n",
    "print(\"- feature2 (0-10,000) dominates distance calculations without scaling\")\n",
    "print(\"- feature1 (0-10) is essentially ignored\")\n",
    "print(\"- With scaling, all features contribute equally\")\n",
    "print(\"- KNN can now use all features to find true neighbors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Feature scaling is critical for distance-based algorithms**\n",
    "   - KNN, SVM, Neural Networks NEED scaling\n",
    "   - Tree-based models DON'T need scaling\n",
    "   - Can improve accuracy by 10-30%+\n",
    "\n",
    "2. **Three main scaling methods**:\n",
    "   - **MinMaxScaler**: [0, 1] range, good for neural networks\n",
    "   - **StandardScaler**: mean=0, std=1, most common choice\n",
    "   - **RobustScaler**: median/IQR, robust to outliers\n",
    "\n",
    "3. **Critical rule: Split BEFORE scaling**\n",
    "   - Fit scaler on training data only\n",
    "   - Transform both train and test with same scaler\n",
    "   - Prevents data leakage\n",
    "\n",
    "4. **Choosing the right scaler**:\n",
    "   - Default: StandardScaler\n",
    "   - Outliers: RobustScaler\n",
    "   - Neural networks: MinMaxScaler\n",
    "   - Tree models: No scaling needed\n",
    "\n",
    "5. **Production considerations**:\n",
    "   - Save fitted scaler with model\n",
    "   - Apply same transformation to new data\n",
    "   - Never refit scaler in production\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "**Module 04**: Polynomial Features and Interactions - Learn how to create new features by combining existing ones\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [Sklearn Preprocessing](https://scikit-learn.org/stable/modules/preprocessing.html)\n",
    "- [Feature Scaling Guide](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html)\n",
    "- \"Feature Engineering and Selection\" by Kuhn & Johnson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Congratulations!** You've completed Module 03. You now know:\n",
    "- Why feature scaling matters for ML algorithms\n",
    "- How to apply Min-Max, Standard, and Robust scaling\n",
    "- When to use each scaling method\n",
    "- How scaling dramatically improves distance-based algorithms\n",
    "- How to avoid data leakage during scaling\n",
    "\n",
    "Ready to create new features? Move to **Module 04: Polynomial Features and Interactions**!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
