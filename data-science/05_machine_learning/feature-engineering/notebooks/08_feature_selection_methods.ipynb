{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 08: Feature Selection Methods\n",
    "\n",
    "**Difficulty**: ⭐⭐ Intermediate  \n",
    "**Estimated Time**: 70 minutes  \n",
    "**Prerequisites**: Module 07 (Text Feature Engineering)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Understand why feature selection matters and the curse of dimensionality\n",
    "2. Apply filter methods (correlation, chi-square, mutual information)\n",
    "3. Implement wrapper methods (RFE, forward/backward selection)\n",
    "4. Use embedded methods (Lasso, tree-based feature importance)\n",
    "5. Compare feature selection methods and visualize performance vs. feature count\n",
    "6. Choose the appropriate feature selection method for different scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Feature Selection Matters\n",
    "\n",
    "**More features ≠ Better models!**\n",
    "\n",
    "**Problems with too many features**:\n",
    "- **Curse of dimensionality**: Models need exponentially more data\n",
    "- **Overfitting**: Model memorizes noise instead of learning patterns\n",
    "- **Slow training**: More features = more computation\n",
    "- **Poor interpretability**: Hard to understand which features matter\n",
    "- **Multicollinearity**: Redundant features confuse models\n",
    "\n",
    "**Benefits of feature selection**:\n",
    "- ✅ Better generalization (less overfitting)\n",
    "- ✅ Faster training and prediction\n",
    "- ✅ Improved model interpretability\n",
    "- ✅ Reduced storage and memory requirements\n",
    "\n",
    "**Three main approaches**:\n",
    "1. **Filter methods**: Statistical tests (fast, model-agnostic)\n",
    "2. **Wrapper methods**: Use model performance (slow, accurate)\n",
    "3. **Embedded methods**: Feature selection during training (balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification, load_breast_cancer\n",
    "\n",
    "# Feature selection methods\n",
    "from sklearn.feature_selection import (\n",
    "    SelectKBest, chi2, f_classif, mutual_info_classif,\n",
    "    RFE, SequentialFeatureSelector,\n",
    "    SelectFromModel\n",
    ")\n",
    "\n",
    "# Models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Lasso\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Configuration\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 3)\n",
    "\n",
    "print(\"✓ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Curse of Dimensionality\n",
    "\n",
    "Let's demonstrate why too many features can hurt model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate curse of dimensionality\n",
    "n_samples = 200\n",
    "feature_counts = [5, 10, 20, 50, 100, 200]\n",
    "results = []\n",
    "\n",
    "for n_features in feature_counts:\n",
    "    # Create dataset with mostly irrelevant features\n",
    "    X, y = make_classification(\n",
    "        n_samples=n_samples,\n",
    "        n_features=n_features,\n",
    "        n_informative=5,  # Only 5 features are actually useful!\n",
    "        n_redundant=0,\n",
    "        n_repeated=0,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Train model\n",
    "    model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    train_score = model.score(X_train, y_train)\n",
    "    test_score = model.score(X_test, y_test)\n",
    "    \n",
    "    results.append({\n",
    "        'Num Features': n_features,\n",
    "        'Train Accuracy': train_score,\n",
    "        'Test Accuracy': test_score,\n",
    "        'Overfitting': train_score - test_score\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"The Curse of Dimensionality:\")\n",
    "print(\"=\"*60)\n",
    "print(results_df)\n",
    "print(\"\\nNotice: More features → worse test performance!\")\n",
    "print(\"Only 5 features are informative, rest are noise.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize curse of dimensionality\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Performance vs number of features\n",
    "axes[0].plot(results_df['Num Features'], results_df['Train Accuracy'], \n",
    "            marker='o', label='Train Accuracy', linewidth=2)\n",
    "axes[0].plot(results_df['Num Features'], results_df['Test Accuracy'], \n",
    "            marker='s', label='Test Accuracy', linewidth=2)\n",
    "axes[0].set_xlabel('Number of Features')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('Curse of Dimensionality\\n(Only 5 features are informative)', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Overfitting measure\n",
    "axes[1].bar(results_df['Num Features'], results_df['Overfitting'], \n",
    "           color='coral', edgecolor='black')\n",
    "axes[1].set_xlabel('Number of Features')\n",
    "axes[1].set_ylabel('Overfitting (Train - Test Accuracy)')\n",
    "axes[1].set_title('Overfitting Increases with Irrelevant Features', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key insight: Adding irrelevant features hurts performance!\")\n",
    "print(\"Solution: Feature selection to remove noise and keep signal.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Dataset for Feature Selection Demo\n",
    "\n",
    "We'll use the Breast Cancer Wisconsin dataset - a classic binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"  - {X.shape[0]} samples\")\n",
    "print(f\"  - {X.shape[1]} features\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(f\"  - Malignant: {(y==0).sum()}\")\n",
    "print(f\"  - Benign: {(y==1).sum()}\")\n",
    "print(f\"\\nFeature names:\")\n",
    "print(list(X.columns[:10]), \"...\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Scale features (important for some methods)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Filter Methods: Statistical Tests\n",
    "\n",
    "**Filter methods** evaluate features independently using statistical tests:\n",
    "- Fast and scalable\n",
    "- Model-agnostic\n",
    "- Don't consider feature interactions\n",
    "\n",
    "**Common filter methods**:\n",
    "1. **Correlation**: Linear relationship with target\n",
    "2. **Chi-square (χ²)**: Independence test for categorical data\n",
    "3. **ANOVA F-statistic**: Variance between groups\n",
    "4. **Mutual Information**: Non-linear relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Correlation-Based Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation with target\n",
    "# Note: For classification, we can look at point-biserial correlation\n",
    "correlations = X_train.corrwith(pd.Series(y_train))\n",
    "correlations_abs = correlations.abs().sort_values(ascending=False)\n",
    "\n",
    "print(\"Top 10 features by absolute correlation with target:\")\n",
    "print(correlations_abs.head(10))\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 8))\n",
    "correlations_abs.plot(kind='barh', color='steelblue', edgecolor='black')\n",
    "plt.xlabel('Absolute Correlation with Target')\n",
    "plt.title('Feature Correlation with Target', fontsize=12, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Select top k features\n",
    "k = 10\n",
    "top_features_corr = correlations_abs.head(k).index.tolist()\n",
    "print(f\"\\nSelected top {k} features: {top_features_corr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 ANOVA F-test Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use ANOVA F-statistic for feature selection\n",
    "k = 10\n",
    "selector_f = SelectKBest(score_func=f_classif, k=k)\n",
    "selector_f.fit(X_train, y_train)\n",
    "\n",
    "# Get scores\n",
    "f_scores = pd.Series(selector_f.scores_, index=X.columns)\n",
    "f_scores_sorted = f_scores.sort_values(ascending=False)\n",
    "\n",
    "print(f\"Top {k} features by ANOVA F-statistic:\")\n",
    "print(f_scores_sorted.head(k))\n",
    "\n",
    "# Selected features\n",
    "selected_features_f = X.columns[selector_f.get_support()].tolist()\n",
    "print(f\"\\nSelected features: {selected_features_f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Mutual Information Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mutual Information - captures non-linear relationships\n",
    "k = 10\n",
    "selector_mi = SelectKBest(score_func=mutual_info_classif, k=k)\n",
    "selector_mi.fit(X_train, y_train)\n",
    "\n",
    "# Get scores\n",
    "mi_scores = pd.Series(selector_mi.scores_, index=X.columns)\n",
    "mi_scores_sorted = mi_scores.sort_values(ascending=False)\n",
    "\n",
    "print(f\"Top {k} features by Mutual Information:\")\n",
    "print(mi_scores_sorted.head(k))\n",
    "\n",
    "# Selected features\n",
    "selected_features_mi = X.columns[selector_mi.get_support()].tolist()\n",
    "print(f\"\\nSelected features: {selected_features_mi}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different filter methods\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Correlation\n",
    "top10_corr = correlations_abs.head(10)\n",
    "axes[0].barh(range(len(top10_corr)), top10_corr.values, color='skyblue', edgecolor='black')\n",
    "axes[0].set_yticks(range(len(top10_corr)))\n",
    "axes[0].set_yticklabels(top10_corr.index, fontsize=8)\n",
    "axes[0].set_xlabel('Absolute Correlation')\n",
    "axes[0].set_title('Top 10 by Correlation', fontweight='bold')\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# F-statistic\n",
    "top10_f = f_scores_sorted.head(10)\n",
    "axes[1].barh(range(len(top10_f)), top10_f.values, color='lightcoral', edgecolor='black')\n",
    "axes[1].set_yticks(range(len(top10_f)))\n",
    "axes[1].set_yticklabels(top10_f.index, fontsize=8)\n",
    "axes[1].set_xlabel('F-Statistic')\n",
    "axes[1].set_title('Top 10 by ANOVA F-test', fontweight='bold')\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "# Mutual Information\n",
    "top10_mi = mi_scores_sorted.head(10)\n",
    "axes[2].barh(range(len(top10_mi)), top10_mi.values, color='lightgreen', edgecolor='black')\n",
    "axes[2].set_yticks(range(len(top10_mi)))\n",
    "axes[2].set_yticklabels(top10_mi.index, fontsize=8)\n",
    "axes[2].set_xlabel('Mutual Information')\n",
    "axes[2].set_title('Top 10 by Mutual Information', fontweight='bold')\n",
    "axes[2].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice: Different methods may select different features!\")\n",
    "print(\"Correlation captures linear relationships, MI captures non-linear.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Wrapper Methods: Using Model Performance\n",
    "\n",
    "**Wrapper methods** use actual model performance to select features:\n",
    "- More accurate than filter methods\n",
    "- Computationally expensive\n",
    "- Consider feature interactions\n",
    "\n",
    "**Common wrapper methods**:\n",
    "1. **Recursive Feature Elimination (RFE)**: Iteratively remove least important features\n",
    "2. **Forward Selection**: Start with 0, add features one by one\n",
    "3. **Backward Selection**: Start with all, remove features one by one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Recursive Feature Elimination (RFE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFE with Logistic Regression\n",
    "k = 10\n",
    "estimator = LogisticRegression(max_iter=1000, random_state=42)\n",
    "rfe = RFE(estimator=estimator, n_features_to_select=k)\n",
    "\n",
    "# Fit RFE\n",
    "rfe.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get selected features\n",
    "selected_features_rfe = X.columns[rfe.support_].tolist()\n",
    "feature_ranking = pd.Series(rfe.ranking_, index=X.columns).sort_values()\n",
    "\n",
    "print(f\"RFE selected top {k} features:\")\n",
    "print(selected_features_rfe)\n",
    "print(f\"\\nFeature ranking (1 = selected):\")\n",
    "print(feature_ranking.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize RFE ranking\n",
    "plt.figure(figsize=(12, 8))\n",
    "colors = ['green' if r == 1 else 'gray' for r in feature_ranking.values]\n",
    "plt.barh(range(len(feature_ranking)), feature_ranking.values, color=colors, edgecolor='black')\n",
    "plt.yticks(range(len(feature_ranking)), feature_ranking.index, fontsize=8)\n",
    "plt.xlabel('Ranking (1 = Selected)')\n",
    "plt.title(f'RFE Feature Ranking (Top {k} in green)', fontsize=12, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Forward/Backward Sequential Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward Selection (starts with 0 features, adds one at a time)\n",
    "k = 10\n",
    "estimator = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Note: This can be slow, so we use a simpler estimator\n",
    "sfs_forward = SequentialFeatureSelector(\n",
    "    estimator, \n",
    "    n_features_to_select=k,\n",
    "    direction='forward',\n",
    "    cv=3  # Use cross-validation\n",
    ")\n",
    "\n",
    "print(\"Running forward selection (this may take a minute)...\")\n",
    "sfs_forward.fit(X_train_scaled, y_train)\n",
    "\n",
    "selected_features_forward = X.columns[sfs_forward.get_support()].tolist()\n",
    "print(f\"\\nForward Selection - selected {k} features:\")\n",
    "print(selected_features_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Embedded Methods: Built-in Feature Selection\n",
    "\n",
    "**Embedded methods** perform feature selection during model training:\n",
    "- Balance between filter and wrapper methods\n",
    "- Model-specific\n",
    "- Fast and accurate\n",
    "\n",
    "**Common embedded methods**:\n",
    "1. **Lasso (L1 Regularization)**: Shrinks coefficients to zero\n",
    "2. **Tree-based importance**: From Random Forest, XGBoost, etc.\n",
    "3. **Ridge (L2 Regularization)**: Shrinks but doesn't zero out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Lasso (L1 Regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso for feature selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# L1 regularization encourages sparsity\n",
    "lasso = LogisticRegression(penalty='l1', solver='liblinear', C=0.1, random_state=42)\n",
    "lasso.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get non-zero coefficients\n",
    "coefficients = pd.Series(lasso.coef_[0], index=X.columns)\n",
    "non_zero = coefficients[coefficients != 0]\n",
    "\n",
    "print(f\"Lasso selected {len(non_zero)} features (non-zero coefficients):\")\n",
    "print(non_zero.sort_values(key=abs, ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Lasso coefficients\n",
    "plt.figure(figsize=(12, 8))\n",
    "coefficients_sorted = coefficients.abs().sort_values(ascending=False)\n",
    "colors = ['green' if c != 0 else 'lightgray' for c in coefficients_sorted.values]\n",
    "plt.barh(range(len(coefficients_sorted)), coefficients_sorted.values, \n",
    "        color=colors, edgecolor='black')\n",
    "plt.yticks(range(len(coefficients_sorted)), coefficients_sorted.index, fontsize=8)\n",
    "plt.xlabel('Absolute Coefficient Value')\n",
    "plt.title('Lasso Coefficients (Green = Selected, Gray = Zero)', fontsize=12, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Lasso automatically selected {len(non_zero)} features by setting others to zero.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Tree-Based Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest feature importance\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "importances = pd.Series(rf.feature_importances_, index=X.columns)\n",
    "importances_sorted = importances.sort_values(ascending=False)\n",
    "\n",
    "print(\"Top 10 features by Random Forest importance:\")\n",
    "print(importances_sorted.head(10))\n",
    "\n",
    "# Select top k features\n",
    "k = 10\n",
    "selected_features_rf = importances_sorted.head(k).index.tolist()\n",
    "print(f\"\\nSelected top {k} features: {selected_features_rf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Random Forest feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "importances_sorted.plot(kind='barh', color='forestgreen', edgecolor='black')\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Random Forest Feature Importance', fontsize=12, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Compare All Feature Selection Methods\n",
    "\n",
    "Let's compare performance of different feature selection approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all methods\n",
    "k = 10\n",
    "\n",
    "feature_sets = {\n",
    "    'All Features': list(X.columns),\n",
    "    'Correlation (top 10)': top_features_corr,\n",
    "    'ANOVA F-test (top 10)': selected_features_f,\n",
    "    'Mutual Information (top 10)': selected_features_mi,\n",
    "    'RFE (top 10)': selected_features_rfe,\n",
    "    'Forward Selection (top 10)': selected_features_forward,\n",
    "    'Lasso (auto)': non_zero.index.tolist(),\n",
    "    'Random Forest (top 10)': selected_features_rf\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, features in feature_sets.items():\n",
    "    # Get feature indices\n",
    "    feature_indices = [X.columns.get_loc(f) for f in features]\n",
    "    \n",
    "    # Select features from scaled data\n",
    "    X_train_subset = X_train_scaled[:, feature_indices]\n",
    "    X_test_subset = X_test_scaled[:, feature_indices]\n",
    "    \n",
    "    # Train model\n",
    "    model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    model.fit(X_train_subset, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    train_acc = model.score(X_train_subset, y_train)\n",
    "    test_acc = model.score(X_test_subset, y_test)\n",
    "    \n",
    "    results.append({\n",
    "        'Method': name,\n",
    "        'Num Features': len(features),\n",
    "        'Train Accuracy': train_acc,\n",
    "        'Test Accuracy': test_acc\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nFeature Selection Method Comparison:\")\n",
    "print(\"=\"*80)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Test accuracy comparison\n",
    "results_df_sorted = results_df.sort_values('Test Accuracy', ascending=True)\n",
    "colors = ['red' if m == 'All Features' else 'steelblue' for m in results_df_sorted['Method']]\n",
    "axes[0].barh(results_df_sorted['Method'], results_df_sorted['Test Accuracy'], \n",
    "            color=colors, edgecolor='black')\n",
    "axes[0].set_xlabel('Test Accuracy')\n",
    "axes[0].set_title('Test Accuracy by Feature Selection Method', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlim([0.9, 1.0])\n",
    "axes[0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Feature count vs accuracy\n",
    "axes[1].scatter(results_df['Num Features'], results_df['Test Accuracy'], \n",
    "               s=100, alpha=0.6, edgecolor='black')\n",
    "for idx, row in results_df.iterrows():\n",
    "    axes[1].annotate(row['Method'], \n",
    "                    (row['Num Features'], row['Test Accuracy']),\n",
    "                    fontsize=7, ha='left')\n",
    "axes[1].set_xlabel('Number of Features')\n",
    "axes[1].set_ylabel('Test Accuracy')\n",
    "axes[1].set_title('Feature Count vs Performance', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey observations:\")\n",
    "print(\"- Using ALL features doesn't guarantee best performance\")\n",
    "print(\"- Good feature selection can match or beat using all features\")\n",
    "print(\"- Different methods select different features but achieve similar performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Performance vs. Number of Features\n",
    "\n",
    "Let's see how performance changes as we vary the number of selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different numbers of features using Mutual Information\n",
    "feature_counts = [1, 2, 5, 10, 15, 20, 30]\n",
    "performance_results = []\n",
    "\n",
    "for k in feature_counts:\n",
    "    # Select top k features using Mutual Information\n",
    "    selector = SelectKBest(score_func=mutual_info_classif, k=min(k, X.shape[1]))\n",
    "    selector.fit(X_train, y_train)\n",
    "    \n",
    "    X_train_selected = selector.transform(X_train)\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "    \n",
    "    # Train model\n",
    "    model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    model.fit(X_train_selected, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    train_acc = model.score(X_train_selected, y_train)\n",
    "    test_acc = model.score(X_test_selected, y_test)\n",
    "    \n",
    "    performance_results.append({\n",
    "        'Num Features': k,\n",
    "        'Train Accuracy': train_acc,\n",
    "        'Test Accuracy': test_acc\n",
    "    })\n",
    "\n",
    "perf_df = pd.DataFrame(performance_results)\n",
    "print(\"Performance vs Number of Features:\")\n",
    "print(perf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize performance curve\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(perf_df['Num Features'], perf_df['Train Accuracy'], \n",
    "        marker='o', label='Train Accuracy', linewidth=2)\n",
    "plt.plot(perf_df['Num Features'], perf_df['Test Accuracy'], \n",
    "        marker='s', label='Test Accuracy', linewidth=2)\n",
    "plt.xlabel('Number of Features Selected')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Performance vs Number of Features\\n(Using Mutual Information Selection)', \n",
    "         fontsize=12, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find optimal number of features\n",
    "optimal_k = perf_df.loc[perf_df['Test Accuracy'].idxmax(), 'Num Features']\n",
    "optimal_acc = perf_df['Test Accuracy'].max()\n",
    "\n",
    "print(f\"\\nOptimal number of features: {optimal_k}\")\n",
    "print(f\"Best test accuracy: {optimal_acc:.4f}\")\n",
    "print(f\"\\nDiminishing returns after ~{optimal_k} features!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Exercise Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Feature Selection on Synthetic Data\n",
    "\n",
    "Create a dataset with only 5 informative features out of 50 total. Apply feature selection and verify it identifies the correct features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Synthetic data with known informative features\n",
    "\n",
    "# Create dataset\n",
    "X_syn, y_syn = make_classification(\n",
    "    n_samples=500,\n",
    "    n_features=50,\n",
    "    n_informative=5,\n",
    "    n_redundant=0,\n",
    "    n_repeated=0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_syn_df = pd.DataFrame(X_syn, columns=[f'feature_{i}' for i in range(50)])\n",
    "\n",
    "# The first 5 features are informative by design\n",
    "true_informative = ['feature_0', 'feature_1', 'feature_2', 'feature_3', 'feature_4']\n",
    "\n",
    "print(f\"Dataset: {X_syn_df.shape}\")\n",
    "print(f\"True informative features: {true_informative}\")\n",
    "\n",
    "# TODO:\n",
    "# 1. Apply Mutual Information to select top 5 features\n",
    "# 2. Apply Random Forest to select top 5 features\n",
    "# 3. Compare with true informative features\n",
    "# 4. Calculate how many you got correct\n",
    "\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to Exercise 1\n",
    "\n",
    "# Split data\n",
    "X_syn_train, X_syn_test, y_syn_train, y_syn_test = train_test_split(\n",
    "    X_syn_df, y_syn, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# 1. Mutual Information\n",
    "selector_mi = SelectKBest(score_func=mutual_info_classif, k=5)\n",
    "selector_mi.fit(X_syn_train, y_syn_train)\n",
    "selected_mi = X_syn_df.columns[selector_mi.get_support()].tolist()\n",
    "\n",
    "# 2. Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_syn_train, y_syn_train)\n",
    "importances = pd.Series(rf.feature_importances_, index=X_syn_df.columns)\n",
    "selected_rf = importances.nlargest(5).index.tolist()\n",
    "\n",
    "# 3. Compare with true features\n",
    "print(\"True informative features:\")\n",
    "print(true_informative)\n",
    "print(\"\\nMutual Information selected:\")\n",
    "print(selected_mi)\n",
    "print(\"\\nRandom Forest selected:\")\n",
    "print(selected_rf)\n",
    "\n",
    "# 4. Calculate accuracy\n",
    "correct_mi = len(set(selected_mi) & set(true_informative))\n",
    "correct_rf = len(set(selected_rf) & set(true_informative))\n",
    "\n",
    "print(f\"\\nMutual Information correctly identified: {correct_mi}/5 features\")\n",
    "print(f\"Random Forest correctly identified: {correct_rf}/5 features\")\n",
    "\n",
    "print(\"\\nNote: With synthetic data, good feature selection should identify most/all informative features!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Compare Filter vs Wrapper Methods\n",
    "\n",
    "On the breast cancer dataset, compare the speed of filter methods vs wrapper methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Speed comparison\n",
    "\n",
    "import time\n",
    "\n",
    "# TODO: Measure execution time for:\n",
    "# 1. Mutual Information (filter method)\n",
    "# 2. RFE (wrapper method)\n",
    "# Compare both speed and accuracy\n",
    "\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to Exercise 2\n",
    "\n",
    "import time\n",
    "\n",
    "k = 10\n",
    "\n",
    "# 1. Mutual Information (Filter)\n",
    "start = time.time()\n",
    "selector_mi = SelectKBest(score_func=mutual_info_classif, k=k)\n",
    "selector_mi.fit(X_train, y_train)\n",
    "X_train_mi = selector_mi.transform(X_train)\n",
    "X_test_mi = selector_mi.transform(X_test)\n",
    "\n",
    "model_mi = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model_mi.fit(X_train_mi, y_train)\n",
    "acc_mi = model_mi.score(X_test_mi, y_test)\n",
    "time_mi = time.time() - start\n",
    "\n",
    "# 2. RFE (Wrapper)\n",
    "start = time.time()\n",
    "estimator = LogisticRegression(max_iter=1000, random_state=42)\n",
    "rfe = RFE(estimator=estimator, n_features_to_select=k)\n",
    "rfe.fit(X_train_scaled, y_train)\n",
    "X_train_rfe = rfe.transform(X_train_scaled)\n",
    "X_test_rfe = rfe.transform(X_test_scaled)\n",
    "\n",
    "model_rfe = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model_rfe.fit(X_train_rfe, y_train)\n",
    "acc_rfe = model_rfe.score(X_test_rfe, y_test)\n",
    "time_rfe = time.time() - start\n",
    "\n",
    "# Compare\n",
    "print(\"Speed and Accuracy Comparison:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"{'Method':<25} {'Time (s)':<12} {'Accuracy'}\")\n",
    "print(\"-\"*50)\n",
    "print(f\"{'Mutual Information':<25} {time_mi:<12.4f} {acc_mi:.4f}\")\n",
    "print(f\"{'RFE (Wrapper)':<25} {time_rfe:<12.4f} {acc_rfe:.4f}\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nRFE is {time_rfe/time_mi:.1f}x slower than Mutual Information\")\n",
    "print(f\"But accuracy difference is only {abs(acc_rfe - acc_mi):.4f}\")\n",
    "print(\"\\nTrade-off: Filter methods are faster, wrapper methods are more accurate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Optimal Feature Count\n",
    "\n",
    "Find the optimal number of features that maximizes test accuracy while minimizing feature count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Find optimal k\n",
    "\n",
    "# TODO:\n",
    "# 1. Test k values from 1 to 30\n",
    "# 2. For each k, use Random Forest feature importance to select features\n",
    "# 3. Plot test accuracy vs k\n",
    "# 4. Find the \"elbow point\" - where adding more features doesn't help much\n",
    "\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to Exercise 3\n",
    "\n",
    "# Train Random Forest once to get all importances\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "importances = pd.Series(rf.feature_importances_, index=X.columns)\n",
    "features_ranked = importances.sort_values(ascending=False).index\n",
    "\n",
    "# Test different k values\n",
    "k_values = range(1, 31)\n",
    "results = []\n",
    "\n",
    "for k in k_values:\n",
    "    # Select top k features\n",
    "    selected = features_ranked[:k].tolist()\n",
    "    \n",
    "    # Train model\n",
    "    X_train_k = X_train[selected]\n",
    "    X_test_k = X_test[selected]\n",
    "    \n",
    "    model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    model.fit(X_train_k, y_train)\n",
    "    \n",
    "    acc = model.score(X_test_k, y_test)\n",
    "    results.append({'k': k, 'accuracy': acc})\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(results_df['k'], results_df['accuracy'], marker='o', linewidth=2)\n",
    "plt.axvline(x=results_df.loc[results_df['accuracy'].idxmax(), 'k'], \n",
    "           color='red', linestyle='--', label='Best k')\n",
    "plt.xlabel('Number of Features (k)')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.title('Finding Optimal Number of Features', fontsize=12, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "best_k = results_df.loc[results_df['accuracy'].idxmax(), 'k']\n",
    "best_acc = results_df['accuracy'].max()\n",
    "\n",
    "print(f\"Optimal number of features: {best_k}\")\n",
    "print(f\"Best accuracy: {best_acc:.4f}\")\n",
    "print(f\"\\nDiminishing returns after k={best_k}\")\n",
    "print(f\"Using only {best_k}/{X.shape[1]} features ({best_k/X.shape[1]*100:.1f}% of total)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **More features ≠ better performance**\n",
    "   - Curse of dimensionality is real\n",
    "   - Irrelevant features add noise and cause overfitting\n",
    "   - Feature selection improves generalization\n",
    "\n",
    "2. **Three main approaches to feature selection**:\n",
    "   - **Filter methods**: Statistical tests (fast, model-agnostic)\n",
    "   - **Wrapper methods**: Use model performance (slow, accurate)\n",
    "   - **Embedded methods**: Built into training (balanced)\n",
    "\n",
    "3. **Filter methods** (Correlation, F-test, Mutual Information):\n",
    "   - ✅ Fast and scalable\n",
    "   - ✅ Model-agnostic\n",
    "   - ❌ Don't consider feature interactions\n",
    "   - ❌ May miss important feature combinations\n",
    "\n",
    "4. **Wrapper methods** (RFE, Forward/Backward Selection):\n",
    "   - ✅ More accurate\n",
    "   - ✅ Consider feature interactions\n",
    "   - ❌ Computationally expensive\n",
    "   - ❌ Risk of overfitting to validation set\n",
    "\n",
    "5. **Embedded methods** (Lasso, Tree importance):\n",
    "   - ✅ Good balance of speed and accuracy\n",
    "   - ✅ Integrated with training\n",
    "   - ❌ Model-specific\n",
    "   - ✅ Regularization prevents overfitting\n",
    "\n",
    "6. **Optimal feature count often much less than total**:\n",
    "   - Diminishing returns after certain point\n",
    "   - Plot performance vs feature count to find elbow\n",
    "   - Consider trade-off between accuracy and complexity\n",
    "\n",
    "### When to Use Each Method\n",
    "\n",
    "**Filter Methods**:\n",
    "- ✅ High-dimensional data (1000s of features)\n",
    "- ✅ Quick baseline\n",
    "- ✅ Preprocessing before wrapper methods\n",
    "- ✅ Exploratory analysis\n",
    "\n",
    "**Wrapper Methods**:\n",
    "- ✅ Small to medium feature sets (<100 features)\n",
    "- ✅ When accuracy is critical\n",
    "- ✅ Enough computational resources\n",
    "- ✅ Need optimal feature subset\n",
    "\n",
    "**Embedded Methods**:\n",
    "- ✅ Using regularized models (Lasso, Ridge)\n",
    "- ✅ Tree-based models (RF, XGBoost)\n",
    "- ✅ Production pipelines\n",
    "- ✅ Good default choice\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Always split data first**: Avoid data leakage\n",
    "2. **Start with filter methods**: Quick baseline\n",
    "3. **Use domain knowledge**: Don't blindly trust statistics\n",
    "4. **Plot performance curves**: Visualize feature count vs accuracy\n",
    "5. **Consider interpretability**: Fewer features = easier to explain\n",
    "6. **Cross-validate**: Ensure robust feature selection\n",
    "7. **Compare multiple methods**: Different methods may find different features\n",
    "8. **Monitor overfitting**: Train vs test performance gap\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "**Module 09**: Feature Importance and Interpretability - Learn to understand which features matter and why\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [Scikit-learn Feature Selection](https://scikit-learn.org/stable/modules/feature_selection.html)\n",
    "- [Feature Selection Guide](https://machinelearningmastery.com/feature-selection-machine-learning-python/)\n",
    "- [Curse of Dimensionality Explained](https://towardsdatascience.com/curse-of-dimensionality-2092410f3d27)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Congratulations!** You've completed Module 08. You now understand:\n",
    "- Why feature selection matters and the curse of dimensionality\n",
    "- Filter methods (correlation, F-test, mutual information)\n",
    "- Wrapper methods (RFE, sequential selection)\n",
    "- Embedded methods (Lasso, tree importance)\n",
    "- How to compare methods and find optimal feature count\n",
    "\n",
    "Ready to dive deeper into feature importance? Let's move to **Module 09: Feature Importance and Interpretability**!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
