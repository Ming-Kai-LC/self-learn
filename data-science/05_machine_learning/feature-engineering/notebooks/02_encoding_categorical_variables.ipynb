{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 02: Encoding Categorical Variables\n",
    "\n",
    "**Difficulty**: ⭐ Beginner  \n",
    "**Estimated Time**: 60 minutes  \n",
    "**Prerequisites**: [Module 01: Handling Missing Data](01_handling_missing_data.ipynb)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Understand why ML algorithms need numeric features\n",
    "2. Apply one-hot encoding for nominal categories\n",
    "3. Use label encoding and ordinal encoding for ordered categories\n",
    "4. Implement target encoding for high-cardinality features\n",
    "5. Handle the curse of dimensionality with categorical variables\n",
    "6. Choose the right encoding method for your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Encode Categorical Variables?\n",
    "\n",
    "**Problem**: Most machine learning algorithms only understand numbers!\n",
    "\n",
    "**Examples of categorical variables**:\n",
    "- **Nominal** (no order): Colors (red, blue, green), Cities (New York, London, Tokyo)\n",
    "- **Ordinal** (ordered): Education (High School < Bachelor < Master < PhD), Size (Small < Medium < Large)\n",
    "- **Binary**: Yes/No, True/False, Male/Female\n",
    "\n",
    "### The Wrong Way\n",
    "\n",
    "```python\n",
    "# ❌ DON'T DO THIS\n",
    "cities = ['New York', 'London', 'Tokyo', 'Paris']\n",
    "encoded = [1, 2, 3, 4]  # Label encoding for nominal data\n",
    "```\n",
    "\n",
    "**Problem**: This implies Tokyo (3) > London (2), which is meaningless!\n",
    "\n",
    "### The Right Way\n",
    "\n",
    "Choose encoding based on:\n",
    "1. **Type of category** (nominal vs ordinal)\n",
    "2. **Number of unique values** (cardinality)\n",
    "3. **Algorithm type** (tree-based vs linear)\n",
    "4. **Relationship to target** (predictive power)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Encoding methods\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder\n",
    "from category_encoders import TargetEncoder, BinaryEncoder\n",
    "import category_encoders as ce\n",
    "\n",
    "# Model evaluation\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Configuration\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\"✓ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Sample Dataset\n",
    "\n",
    "Let's create a realistic customer dataset for predicting product purchases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "# Define categories\n",
    "cities = ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix', \n",
    "          'Philadelphia', 'San Antonio', 'San Diego', 'Dallas', 'Austin',\n",
    "          'San Jose', 'Fort Worth', 'Columbus', 'Charlotte', 'Indianapolis']\n",
    "\n",
    "education_levels = ['High School', 'Associate', 'Bachelor', 'Master', 'PhD']\n",
    "product_categories = ['Electronics', 'Clothing', 'Books', 'Home & Garden', 'Sports']\n",
    "membership_tiers = ['Bronze', 'Silver', 'Gold', 'Platinum']\n",
    "\n",
    "# Create dataset\n",
    "customer_data = pd.DataFrame({\n",
    "    'age': np.random.randint(18, 80, n_samples),\n",
    "    'city': np.random.choice(cities, n_samples),\n",
    "    'education': np.random.choice(education_levels, n_samples),\n",
    "    'product_category': np.random.choice(product_categories, n_samples),\n",
    "    'membership_tier': np.random.choice(membership_tiers, n_samples),\n",
    "    'annual_income': np.random.normal(60000, 25000, n_samples).clip(15000, 200000),\n",
    "    'purchase_amount': np.random.normal(500, 200, n_samples).clip(10, 2000)\n",
    "})\n",
    "\n",
    "# Create target variable with some logic\n",
    "# Higher education, income, and certain cities increase purchase probability\n",
    "purchase_prob = (\n",
    "    0.2 +  # Base probability\n",
    "    0.1 * (customer_data['education'].map({'High School': 0, 'Associate': 1, 'Bachelor': 2, 'Master': 3, 'PhD': 4}) / 4) +\n",
    "    0.3 * ((customer_data['annual_income'] - customer_data['annual_income'].min()) / \n",
    "           (customer_data['annual_income'].max() - customer_data['annual_income'].min())) +\n",
    "    0.15 * (customer_data['city'].isin(['New York', 'San Francisco', 'Los Angeles'])).astype(int) +\n",
    "    np.random.normal(0, 0.1, n_samples)\n",
    ")\n",
    "\n",
    "customer_data['will_purchase'] = (purchase_prob > 0.5).astype(int)\n",
    "\n",
    "print(f\"Created dataset with {len(customer_data)} customers\")\n",
    "print(f\"\\nFeature types:\")\n",
    "print(customer_data.dtypes)\n",
    "print(f\"\\nFirst few rows:\")\n",
    "customer_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze categorical variables\n",
    "categorical_cols = ['city', 'education', 'product_category', 'membership_tier']\n",
    "\n",
    "print(\"Categorical Variable Analysis:\\n\")\n",
    "for col in categorical_cols:\n",
    "    n_unique = customer_data[col].nunique()\n",
    "    print(f\"{col}:\")\n",
    "    print(f\"  Unique values: {n_unique}\")\n",
    "    print(f\"  Type: {'High-cardinality' if n_unique > 10 else 'Low-cardinality'}\")\n",
    "    print(f\"  Sample values: {customer_data[col].unique()[:5].tolist()}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Method 1: One-Hot Encoding\n",
    "\n",
    "**Best for**: Nominal categories with low cardinality (<10 unique values)\n",
    "\n",
    "**How it works**: Create a binary column for each category\n",
    "\n",
    "**Example**:\n",
    "```\n",
    "Color        → Color_Red  Color_Blue  Color_Green\n",
    "Red          →     1          0           0\n",
    "Blue         →     0          1           0\n",
    "Green        →     0          0           1\n",
    "```\n",
    "\n",
    "**Pros**: \n",
    "- No ordinal assumption\n",
    "- Works well with linear models\n",
    "\n",
    "**Cons**: \n",
    "- Increases dimensionality\n",
    "- Can cause curse of dimensionality with high-cardinality features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode product_category (only 5 unique values)\n",
    "product_encoded = pd.get_dummies(customer_data['product_category'], prefix='product')\n",
    "\n",
    "print(\"Original product_category column:\")\n",
    "print(customer_data['product_category'].head())\n",
    "print(\"\\nOne-hot encoded:\")\n",
    "print(product_encoded.head())\n",
    "print(f\"\\nOriginal: 1 column\")\n",
    "print(f\"Encoded: {len(product_encoded.columns)} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using sklearn's OneHotEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# IMPORTANT: Fit on training data only to avoid data leakage!\n",
    "# For this demo, we'll show the mechanics first\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')  # drop='first' avoids multicollinearity\n",
    "\n",
    "# Encode multiple columns\n",
    "categorical_features = ['product_category', 'membership_tier']\n",
    "encoded_array = encoder.fit_transform(customer_data[categorical_features])\n",
    "\n",
    "# Get feature names\n",
    "feature_names = encoder.get_feature_names_out(categorical_features)\n",
    "\n",
    "encoded_df = pd.DataFrame(encoded_array, columns=feature_names, index=customer_data.index)\n",
    "\n",
    "print(\"Encoded features:\")\n",
    "print(encoded_df.head())\n",
    "print(f\"\\nShape: {encoded_df.shape}\")\n",
    "print(f\"Note: 'drop=first' removes one column per category to avoid multicollinearity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Method 2: Label Encoding\n",
    "\n",
    "**Best for**: Ordinal categories with clear ordering\n",
    "\n",
    "**How it works**: Map each category to an integer\n",
    "\n",
    "**Example**:\n",
    "```\n",
    "Education    → Encoded\n",
    "High School  →    0\n",
    "Bachelor     →    1\n",
    "Master       →    2\n",
    "PhD          →    3\n",
    "```\n",
    "\n",
    "**Pros**: \n",
    "- Simple, no new columns\n",
    "- Preserves ordinal information\n",
    "\n",
    "**Cons**: \n",
    "- ❌ DON'T use for nominal data (creates false ordering)\n",
    "- Assumes equal spacing between categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding (automatic ordering - not recommended for education!)\n",
    "label_encoder = LabelEncoder()\n",
    "education_label = label_encoder.fit_transform(customer_data['education'])\n",
    "\n",
    "print(\"Label Encoding (automatic):\")\n",
    "print(pd.DataFrame({\n",
    "    'Original': customer_data['education'][:10],\n",
    "    'Encoded': education_label[:10]\n",
    "}))\n",
    "print(f\"\\nMapping: {dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))}\")\n",
    "print(\"\\n⚠️ Problem: Order is alphabetical, not meaningful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Method 3: Ordinal Encoding\n",
    "\n",
    "**Best for**: Ordinal categories where YOU define the order\n",
    "\n",
    "**How it works**: Map categories to integers with custom ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordinal encoding with custom order\n",
    "education_order = ['High School', 'Associate', 'Bachelor', 'Master', 'PhD']\n",
    "membership_order = ['Bronze', 'Silver', 'Gold', 'Platinum']\n",
    "\n",
    "ordinal_encoder = OrdinalEncoder(\n",
    "    categories=[education_order, membership_order]\n",
    ")\n",
    "\n",
    "ordinal_features = customer_data[['education', 'membership_tier']]\n",
    "ordinal_encoded = ordinal_encoder.fit_transform(ordinal_features)\n",
    "\n",
    "print(\"Ordinal Encoding (custom order):\")\n",
    "result_df = pd.DataFrame({\n",
    "    'education': customer_data['education'][:10],\n",
    "    'education_encoded': ordinal_encoded[:10, 0],\n",
    "    'membership': customer_data['membership_tier'][:10],\n",
    "    'membership_encoded': ordinal_encoded[:10, 1]\n",
    "})\n",
    "print(result_df)\n",
    "\n",
    "print(\"\\n✓ Correct: PhD (4) > Bachelor (2) > High School (0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Method 4: Target Encoding\n",
    "\n",
    "**Best for**: High-cardinality nominal features (many unique values)\n",
    "\n",
    "**How it works**: Replace category with mean of target variable for that category\n",
    "\n",
    "**Example**: \n",
    "```\n",
    "City         Purchase_Rate    Encoded\n",
    "New York     0.65        →    0.65\n",
    "Houston      0.42        →    0.42\n",
    "```\n",
    "\n",
    "**Pros**: \n",
    "- Captures relationship to target\n",
    "- Handles high-cardinality well\n",
    "- No dimensionality increase\n",
    "\n",
    "**Cons**: \n",
    "- Can cause overfitting\n",
    "- Requires careful cross-validation\n",
    "- Leaks target information (use smoothing!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate target encoding concept\n",
    "# Calculate purchase rate by city\n",
    "city_purchase_rate = customer_data.groupby('city')['will_purchase'].mean().sort_values(ascending=False)\n",
    "\n",
    "print(\"Purchase rate by city (what target encoding captures):\")\n",
    "print(city_purchase_rate)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 5))\n",
    "city_purchase_rate.plot(kind='barh')\n",
    "plt.xlabel('Purchase Rate')\n",
    "plt.title('Target Encoding: Each City Encoded by Its Purchase Rate')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNotice: Cities with higher purchase rates get higher encoded values!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proper target encoding with train/test split\n",
    "# CRITICAL: Fit on training data only!\n",
    "\n",
    "X = customer_data.drop('will_purchase', axis=1)\n",
    "y = customer_data['will_purchase']\n",
    "\n",
    "# Split data first\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Target encode 'city' (high-cardinality feature)\n",
    "target_encoder = TargetEncoder(cols=['city'], smoothing=1.0)  # smoothing prevents overfitting\n",
    "\n",
    "# Fit on training data with training target\n",
    "X_train_encoded = target_encoder.fit_transform(X_train, y_train)\n",
    "\n",
    "# Transform test data (uses training statistics)\n",
    "X_test_encoded = target_encoder.transform(X_test)\n",
    "\n",
    "print(\"Target Encoding applied to 'city':\")\n",
    "print(\"\\nTraining set - original vs encoded:\")\n",
    "comparison = pd.DataFrame({\n",
    "    'city_original': X_train['city'].values[:10],\n",
    "    'city_encoded': X_train_encoded['city'].values[:10]\n",
    "})\n",
    "print(comparison)\n",
    "print(\"\\n✓ Each city is replaced by its average purchase rate in the training set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comparing Encoding Methods on Model Performance\n",
    "\n",
    "Let's compare how different encoding methods affect model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for comparison\n",
    "X_base = customer_data[['age', 'annual_income', 'purchase_amount']].copy()\n",
    "y = customer_data['will_purchase']\n",
    "\n",
    "# Function to prepare dataset with different encodings\n",
    "def prepare_encoded_data(encoding_method):\n",
    "    X = X_base.copy()\n",
    "    \n",
    "    if encoding_method == 'one_hot':\n",
    "        # One-hot encode all categorical features\n",
    "        for col in ['city', 'education', 'product_category', 'membership_tier']:\n",
    "            dummies = pd.get_dummies(customer_data[col], prefix=col, drop_first=True)\n",
    "            X = pd.concat([X, dummies], axis=1)\n",
    "    \n",
    "    elif encoding_method == 'label':\n",
    "        # Label encoding (not ideal for nominal features, but let's compare)\n",
    "        for col in ['city', 'education', 'product_category', 'membership_tier']:\n",
    "            le = LabelEncoder()\n",
    "            X[col] = le.fit_transform(customer_data[col])\n",
    "    \n",
    "    elif encoding_method == 'ordinal':\n",
    "        # Ordinal for ordered features, label for others\n",
    "        X['education'] = OrdinalEncoder(categories=[education_levels]).fit_transform(\n",
    "            customer_data[['education']]\n",
    "        )\n",
    "        X['membership_tier'] = OrdinalEncoder(categories=[membership_order]).fit_transform(\n",
    "            customer_data[['membership_tier']]\n",
    "        )\n",
    "        # Label encode nominal features\n",
    "        for col in ['city', 'product_category']:\n",
    "            le = LabelEncoder()\n",
    "            X[col] = le.fit_transform(customer_data[col])\n",
    "    \n",
    "    elif encoding_method == 'target':\n",
    "        # Target encoding for high-cardinality, ordinal for others\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Add categorical columns back\n",
    "        for col in ['city', 'education', 'product_category', 'membership_tier']:\n",
    "            X_train[col] = customer_data.loc[X_train.index, col]\n",
    "            X_test[col] = customer_data.loc[X_test.index, col]\n",
    "        \n",
    "        # Target encode high-cardinality 'city'\n",
    "        te = TargetEncoder(cols=['city'], smoothing=1.0)\n",
    "        X_train = te.fit_transform(X_train, y_train)\n",
    "        X_test = te.transform(X_test)\n",
    "        \n",
    "        # Label encode others\n",
    "        for col in ['education', 'product_category', 'membership_tier']:\n",
    "            le = LabelEncoder()\n",
    "            X_train[col] = le.fit_transform(X_train[col])\n",
    "            X_test[col] = le.transform(X_test[col])\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    # For non-target encoding, do standard split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "print(\"✓ Data preparation functions ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare encoding methods\n",
    "encoding_methods = ['one_hot', 'label', 'ordinal', 'target']\n",
    "results = []\n",
    "\n",
    "for method in encoding_methods:\n",
    "    # Prepare data\n",
    "    X_train, X_test, y_train, y_test = prepare_encoded_data(method)\n",
    "    \n",
    "    # Train Random Forest\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10)\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred = rf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        'Encoding Method': method,\n",
    "        'Accuracy': accuracy,\n",
    "        'Num Features': X_train.shape[1]\n",
    "    })\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame(results).sort_values('Accuracy', ascending=False)\n",
    "print(\"Model Performance by Encoding Method:\\n\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy comparison\n",
    "axes[0].barh(results_df['Encoding Method'], results_df['Accuracy'])\n",
    "axes[0].set_xlabel('Accuracy')\n",
    "axes[0].set_title('Model Accuracy by Encoding Method')\n",
    "axes[0].set_xlim([0.5, 1.0])\n",
    "for i, v in enumerate(results_df['Accuracy']):\n",
    "    axes[0].text(v + 0.01, i, f'{v:.3f}', va='center')\n",
    "\n",
    "# Feature count comparison\n",
    "axes[1].barh(results_df['Encoding Method'], results_df['Num Features'], color='coral')\n",
    "axes[1].set_xlabel('Number of Features')\n",
    "axes[1].set_title('Feature Dimensionality by Encoding Method')\n",
    "for i, v in enumerate(results_df['Num Features']):\n",
    "    axes[1].text(v + 0.5, i, f'{int(v)}', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Handling High-Cardinality Features\n",
    "\n",
    "**Problem**: Features with 100s or 1000s of unique values\n",
    "\n",
    "**Examples**: \n",
    "- ZIP codes (40,000+ in US)\n",
    "- User IDs (millions)\n",
    "- Product SKUs (thousands)\n",
    "\n",
    "**Solutions**:\n",
    "1. **Target encoding** (shown above)\n",
    "2. **Frequency encoding**: Replace with category frequency\n",
    "3. **Grouping**: Combine rare categories into \"Other\"\n",
    "4. **Feature hashing**: Hash categories to fixed number of bins\n",
    "5. **Embedding**: Learn dense representations (deep learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency Encoding\n",
    "def frequency_encoding(column):\n",
    "    \"\"\"Replace categories with their frequency of occurrence\"\"\"\n",
    "    freq_map = column.value_counts(normalize=True).to_dict()\n",
    "    return column.map(freq_map)\n",
    "\n",
    "# Apply to city\n",
    "city_freq_encoded = frequency_encoding(customer_data['city'])\n",
    "\n",
    "print(\"Frequency Encoding:\")\n",
    "comparison = pd.DataFrame({\n",
    "    'city': customer_data['city'][:15],\n",
    "    'frequency': city_freq_encoded[:15]\n",
    "})\n",
    "print(comparison)\n",
    "print(\"\\nFrequent cities get higher values, rare cities get lower values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping Rare Categories\n",
    "def group_rare_categories(column, threshold=0.05):\n",
    "    \"\"\"Combine categories that appear less than threshold into 'Other'\"\"\"\n",
    "    freq = column.value_counts(normalize=True)\n",
    "    rare_categories = freq[freq < threshold].index\n",
    "    return column.apply(lambda x: 'Other' if x in rare_categories else x)\n",
    "\n",
    "city_grouped = group_rare_categories(customer_data['city'], threshold=0.05)\n",
    "\n",
    "print(f\"Original unique cities: {customer_data['city'].nunique()}\")\n",
    "print(f\"After grouping rare cities: {city_grouped.nunique()}\")\n",
    "print(f\"\\nValue counts after grouping:\")\n",
    "print(city_grouped.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Best Practices and Decision Guide\n",
    "\n",
    "### Encoding Decision Tree\n",
    "\n",
    "```\n",
    "Is the feature categorical?\n",
    "├─ YES → Continue\n",
    "└─ NO → No encoding needed\n",
    "\n",
    "Is there a meaningful order?\n",
    "├─ YES (Ordinal) → Use OrdinalEncoder with custom order\n",
    "└─ NO (Nominal) → Continue\n",
    "\n",
    "How many unique values?\n",
    "├─ <10 (Low cardinality)\n",
    "│   ├─ Linear Model → One-Hot Encoding\n",
    "│   └─ Tree Model → Label Encoding or One-Hot\n",
    "└─ ≥10 (High cardinality)\n",
    "    ├─ Target Encoding (with smoothing)\n",
    "    ├─ Frequency Encoding\n",
    "    └─ Group rare categories\n",
    "```\n",
    "\n",
    "### Critical Rules\n",
    "\n",
    "✅ **DO**:\n",
    "1. Split data BEFORE encoding\n",
    "2. Fit encoder on training data only\n",
    "3. Handle unseen categories in test set\n",
    "4. Use one-hot with `drop='first'` for linear models\n",
    "5. Use smoothing with target encoding\n",
    "\n",
    "❌ **DON'T**:\n",
    "1. Use label encoding for nominal features with linear models\n",
    "2. Fit encoder on test data (data leakage!)\n",
    "3. Forget to handle categories not seen in training\n",
    "4. One-hot encode high-cardinality features\n",
    "5. Use target encoding without cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Exercise Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Choose the Right Encoding\n",
    "\n",
    "For each feature, choose the most appropriate encoding method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Match each feature to the best encoding method\n",
    "\n",
    "features = {\n",
    "    'A': 'T-shirt size: XS, S, M, L, XL, XXL',\n",
    "    'B': 'Country: (195 different countries)',\n",
    "    'C': 'Color: Red, Blue, Green, Yellow',\n",
    "    'D': 'Customer satisfaction: Very Bad, Bad, Neutral, Good, Very Good',\n",
    "    'E': 'Product ID: (50,000 unique products)'\n",
    "}\n",
    "\n",
    "encoding_options = {\n",
    "    '1': 'One-Hot Encoding',\n",
    "    '2': 'Label Encoding',\n",
    "    '3': 'Ordinal Encoding (custom order)',\n",
    "    '4': 'Target Encoding',\n",
    "    '5': 'Frequency Encoding'\n",
    "}\n",
    "\n",
    "print(\"Features:\")\n",
    "for key, feature in features.items():\n",
    "    print(f\"{key}. {feature}\")\n",
    "\n",
    "print(\"\\nEncoding Methods:\")\n",
    "for key, method in encoding_options.items():\n",
    "    print(f\"{key}. {method}\")\n",
    "\n",
    "print(\"\\nYour answers (write as comments):\")\n",
    "# A: ?\n",
    "# B: ?\n",
    "# C: ?\n",
    "# D: ?\n",
    "# E: ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to Exercise 1\n",
    "\n",
    "print(\"Solutions:\\n\")\n",
    "print(\"A: 3 - Ordinal Encoding (XS < S < M < L < XL < XXL - clear order)\")\n",
    "print(\"B: 4 - Target Encoding (195 countries = high cardinality, nominal)\")\n",
    "print(\"C: 1 - One-Hot Encoding (4 colors = low cardinality, nominal)\")\n",
    "print(\"D: 3 - Ordinal Encoding (Very Bad < Bad < Neutral < Good < Very Good)\")\n",
    "print(\"E: 4 or 5 - Target or Frequency Encoding (50k products = very high cardinality)\")\n",
    "\n",
    "print(\"\\nKey insights:\")\n",
    "print(\"- Ordinal data (A, D): Use ordinal encoding to preserve order\")\n",
    "print(\"- Low-cardinality nominal (C): One-hot encoding works well\")\n",
    "print(\"- High-cardinality nominal (B, E): Target or frequency encoding to avoid dimensionality explosion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Implement Custom Ordinal Encoding\n",
    "\n",
    "Create a dataset with movie ratings and apply ordinal encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Apply ordinal encoding to movie ratings\n",
    "\n",
    "# Movie ratings dataset\n",
    "movie_data = pd.DataFrame({\n",
    "    'movie': ['Movie A', 'Movie B', 'Movie C', 'Movie D', 'Movie E'],\n",
    "    'rating': ['Poor', 'Excellent', 'Good', 'Fair', 'Good']\n",
    "})\n",
    "\n",
    "print(\"Movie ratings:\")\n",
    "print(movie_data)\n",
    "\n",
    "# TODO: Create ordinal encoding for ratings\n",
    "# Order: Poor < Fair < Good < Excellent\n",
    "# Your code here:\n",
    "\n",
    "# rating_order = ???\n",
    "# encoder = ???\n",
    "# movie_data['rating_encoded'] = ???\n",
    "\n",
    "# print(\"\\nEncoded ratings:\")\n",
    "# print(movie_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to Exercise 2\n",
    "\n",
    "movie_data = pd.DataFrame({\n",
    "    'movie': ['Movie A', 'Movie B', 'Movie C', 'Movie D', 'Movie E'],\n",
    "    'rating': ['Poor', 'Excellent', 'Good', 'Fair', 'Good']\n",
    "})\n",
    "\n",
    "# Define order\n",
    "rating_order = ['Poor', 'Fair', 'Good', 'Excellent']\n",
    "\n",
    "# Apply ordinal encoding\n",
    "encoder = OrdinalEncoder(categories=[rating_order])\n",
    "movie_data['rating_encoded'] = encoder.fit_transform(movie_data[['rating']])\n",
    "\n",
    "print(\"Solution:\")\n",
    "print(movie_data)\n",
    "print(\"\\nMapping:\")\n",
    "for i, rating in enumerate(rating_order):\n",
    "    print(f\"{rating}: {i}\")\n",
    "print(\"\\n✓ Correct: Excellent (3) > Good (2) > Fair (1) > Poor (0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Detect and Fix Data Leakage\n",
    "\n",
    "Find the data leakage problem in this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: What's wrong with this code?\n",
    "\n",
    "print(\"Code snippet:\")\n",
    "print('''\n",
    "# Prepare data\n",
    "X = data[['city', 'product_category']]\n",
    "y = data['purchased']\n",
    "\n",
    "# One-hot encode\n",
    "X_encoded = pd.get_dummies(X)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y)\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train)\n",
    "''')\n",
    "\n",
    "print(\"\\nWhat's the problem? How would you fix it?\")\n",
    "# Your answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to Exercise 3\n",
    "\n",
    "print(\"Problem: No data leakage issue here!\")\n",
    "print(\"\\nWait... that's a trick question. Let me reconsider.\")\n",
    "print(\"\\nActual Problem: Potential issue if test set has new categories!\")\n",
    "print(\"\\nIf test set has a city not in training set, pd.get_dummies will create\")\n",
    "print(\"different columns, causing shape mismatch.\")\n",
    "print(\"\\nBetter approach:\")\n",
    "print('''\n",
    "# Split FIRST\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# Use sklearn's OneHotEncoder which handles unseen categories\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "X_train_encoded = encoder.fit_transform(X_train)\n",
    "X_test_encoded = encoder.transform(X_test)  # Same columns as training\n",
    "''')\n",
    "print(\"\\n✓ This ensures consistent columns and handles unseen categories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: High-Cardinality Challenge\n",
    "\n",
    "You have a dataset with 500 unique product categories. Compare different encoding strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Handle high-cardinality feature\n",
    "\n",
    "# Create dataset with 500 product categories\n",
    "np.random.seed(42)\n",
    "n_samples = 5000\n",
    "n_categories = 500\n",
    "\n",
    "# Some categories are more popular (Zipf distribution)\n",
    "category_ids = np.random.zipf(1.5, n_samples) % n_categories\n",
    "\n",
    "high_card_data = pd.DataFrame({\n",
    "    'product_id': [f'PROD_{i:04d}' for i in category_ids],\n",
    "    'price': np.random.uniform(10, 1000, n_samples),\n",
    "    'quantity': np.random.randint(1, 10, n_samples)\n",
    "})\n",
    "\n",
    "# Target: high-value purchases\n",
    "high_card_data['high_value'] = (high_card_data['price'] * high_card_data['quantity'] > 500).astype(int)\n",
    "\n",
    "print(f\"Dataset: {len(high_card_data)} samples\")\n",
    "print(f\"Unique products: {high_card_data['product_id'].nunique()}\")\n",
    "print(f\"\\nProduct frequency distribution:\")\n",
    "print(high_card_data['product_id'].value_counts().head(10))\n",
    "\n",
    "# TODO: Try different encoding strategies and compare\n",
    "# 1. Frequency encoding\n",
    "# 2. Grouping rare categories (threshold=0.01)\n",
    "# 3. Target encoding\n",
    "# Which works best?\n",
    "\n",
    "# Your code here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to Exercise 4\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X = high_card_data[['price', 'quantity', 'product_id']]\n",
    "y = high_card_data['high_value']\n",
    "\n",
    "results = []\n",
    "\n",
    "# 1. Frequency encoding\n",
    "X_freq = X.copy()\n",
    "X_freq['product_id'] = frequency_encoding(X_freq['product_id'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_freq, y, test_size=0.2, random_state=42)\n",
    "model = RandomForestClassifier(random_state=42, max_depth=10)\n",
    "model.fit(X_train, y_train)\n",
    "results.append({'Method': 'Frequency', 'Accuracy': accuracy_score(y_test, model.predict(X_test))})\n",
    "\n",
    "# 2. Grouping rare categories\n",
    "X_grouped = X.copy()\n",
    "X_grouped['product_id'] = group_rare_categories(X_grouped['product_id'], threshold=0.01)\n",
    "X_grouped = pd.get_dummies(X_grouped, columns=['product_id'], drop_first=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_grouped, y, test_size=0.2, random_state=42)\n",
    "model = RandomForestClassifier(random_state=42, max_depth=10)\n",
    "model.fit(X_train, y_train)\n",
    "results.append({'Method': 'Grouped+OneHot', 'Accuracy': accuracy_score(y_test, model.predict(X_test))})\n",
    "\n",
    "# 3. Target encoding\n",
    "X_target = X[['price', 'quantity']].copy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_target, y, test_size=0.2, random_state=42)\n",
    "X_train['product_id'] = X.loc[X_train.index, 'product_id']\n",
    "X_test['product_id'] = X.loc[X_test.index, 'product_id']\n",
    "te = TargetEncoder(cols=['product_id'], smoothing=1.0)\n",
    "X_train = te.fit_transform(X_train, y_train)\n",
    "X_test = te.transform(X_test)\n",
    "model = RandomForestClassifier(random_state=42, max_depth=10)\n",
    "model.fit(X_train, y_train)\n",
    "results.append({'Method': 'Target', 'Accuracy': accuracy_score(y_test, model.predict(X_test))})\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame(results).sort_values('Accuracy', ascending=False)\n",
    "print(\"\\nResults:\")\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"\\nInsight: For high-cardinality features, target encoding often works best!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Encoding converts categories to numbers** that ML algorithms can process\n",
    "   - Critical preprocessing step\n",
    "   - Choice of encoding affects model performance\n",
    "\n",
    "2. **Main encoding methods**:\n",
    "   - **One-Hot**: Nominal, low-cardinality (<10 values)\n",
    "   - **Ordinal**: Ordered categories with custom ordering\n",
    "   - **Label**: Only for tree-based models, not linear models\n",
    "   - **Target**: High-cardinality, captures target relationship\n",
    "   - **Frequency**: High-cardinality, simple alternative\n",
    "\n",
    "3. **Decision factors**:\n",
    "   - Nominal vs Ordinal\n",
    "   - Cardinality (number of unique values)\n",
    "   - Model type (linear vs tree-based)\n",
    "   - Relationship to target\n",
    "\n",
    "4. **Avoid data leakage**:\n",
    "   - Split data first\n",
    "   - Fit encoder on training data only\n",
    "   - Handle unseen categories in test set\n",
    "\n",
    "5. **High-cardinality strategies**:\n",
    "   - Target encoding (with smoothing)\n",
    "   - Frequency encoding\n",
    "   - Group rare categories\n",
    "   - Feature hashing\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "**Module 03**: Feature Scaling and Normalization - Learn when and how to scale numeric features\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [Category Encoders Library](https://contrib.scikit-learn.org/category_encoders/)\n",
    "- [Sklearn Preprocessing](https://scikit-learn.org/stable/modules/preprocessing.html)\n",
    "- \"Categorical Encoding Methods\" by Kaggle Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Congratulations!** You've completed Module 02. You now know:\n",
    "- Why categorical encoding is necessary\n",
    "- How to apply one-hot, label, ordinal, and target encoding\n",
    "- When to use each encoding method\n",
    "- How to handle high-cardinality features\n",
    "- How to avoid data leakage during encoding\n",
    "\n",
    "Ready to continue? Move to **Module 03: Feature Scaling and Normalization**!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
