{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 07: Text Feature Engineering\n",
    "\n",
    "**Difficulty**: ⭐⭐ Intermediate  \n",
    "**Estimated Time**: 70 minutes  \n",
    "**Prerequisites**: Module 06 (Datetime Feature Engineering)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Clean and preprocess text data for machine learning\n",
    "2. Convert text to numerical features using CountVectorizer (bag-of-words)\n",
    "3. Apply TF-IDF vectorization to weight terms by importance\n",
    "4. Create n-gram features to capture word sequences\n",
    "5. Understand basic word embeddings and when to use them\n",
    "6. Compare bag-of-words vs TF-IDF performance on sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Text Feature Engineering Matters\n",
    "\n",
    "**Text data is everywhere but models need numbers!**\n",
    "- Customer reviews and sentiment analysis\n",
    "- Email spam detection\n",
    "- Document classification\n",
    "- Chatbot intent recognition\n",
    "- Social media analysis\n",
    "\n",
    "**The challenge**: \n",
    "- Computers don't understand words like \"excellent\" or \"terrible\"\n",
    "- We need to convert text → numbers while preserving meaning\n",
    "\n",
    "**Common approaches**:\n",
    "1. **Bag-of-Words (CountVectorizer)**: Count word occurrences\n",
    "2. **TF-IDF**: Weight words by importance\n",
    "3. **N-grams**: Capture word sequences\n",
    "4. **Word Embeddings**: Dense vector representations\n",
    "\n",
    "In this module, we'll use **customer review sentiment analysis** to demonstrate these techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Text processing\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Configuration\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "pd.set_option('display.precision', 3)\n",
    "\n",
    "print(\"✓ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Customer Review Dataset\n",
    "\n",
    "We'll create a realistic dataset of product reviews with positive and negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive reviews\n",
    "positive_reviews = [\n",
    "    \"This product is amazing! Highly recommend it to everyone.\",\n",
    "    \"Excellent quality and fast shipping. Very satisfied with my purchase.\",\n",
    "    \"Love it! Works perfectly and exactly as described.\",\n",
    "    \"Outstanding product. Great value for money. Will buy again!\",\n",
    "    \"Fantastic! Exceeded my expectations. Five stars!\",\n",
    "    \"Best purchase I've made this year. Absolutely love it.\",\n",
    "    \"Incredible product quality. Customer service was also excellent.\",\n",
    "    \"Perfect! Just what I needed. Highly satisfied.\",\n",
    "    \"Wonderful experience. Product arrived quickly and works great.\",\n",
    "    \"Amazing quality! Worth every penny. Definitely recommend.\",\n",
    "    \"Superb product. Easy to use and very effective.\",\n",
    "    \"Brilliant! This has made my life so much easier.\",\n",
    "    \"Exceptional quality. Can't believe how good this is.\",\n",
    "    \"Perfect solution to my problem. Very happy with this.\",\n",
    "    \"Great product! Exactly what was advertised. No complaints.\"\n",
    "] * 20  # Repeat to get more data\n",
    "\n",
    "# Negative reviews\n",
    "negative_reviews = [\n",
    "    \"Terrible product. Broke after one day. Complete waste of money.\",\n",
    "    \"Very disappointed. Not as described. Requesting refund.\",\n",
    "    \"Poor quality. Would not recommend to anyone.\",\n",
    "    \"Awful! Stopped working after a week. Don't buy this.\",\n",
    "    \"Horrible experience. Product is defective and support is terrible.\",\n",
    "    \"Waste of money. Cheap quality and doesn't work properly.\",\n",
    "    \"Disappointed with this purchase. Not worth the price.\",\n",
    "    \"Bad product. Shipping took forever and item was damaged.\",\n",
    "    \"Poor craftsmanship. Fell apart immediately. Very unhappy.\",\n",
    "    \"Terrible! Nothing like the description. Total scam.\",\n",
    "    \"Not recommended. Broke on first use. Asking for refund.\",\n",
    "    \"Worst purchase ever. Cheap materials and poor design.\",\n",
    "    \"Useless product. Doesn't do what it claims. Very angry.\",\n",
    "    \"Horrible quality. Save your money and buy something else.\",\n",
    "    \"Completely unsatisfied. Product is garbage. Don't waste your time.\"\n",
    "] * 20  # Repeat to get more data\n",
    "\n",
    "# Create dataframe\n",
    "reviews_df = pd.DataFrame({\n",
    "    'review_text': positive_reviews + negative_reviews,\n",
    "    'sentiment': ['positive'] * len(positive_reviews) + ['negative'] * len(negative_reviews)\n",
    "})\n",
    "\n",
    "# Shuffle the data\n",
    "reviews_df = reviews_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Created dataset with {len(reviews_df)} reviews\")\n",
    "print(f\"\\nSentiment distribution:\")\n",
    "print(reviews_df['sentiment'].value_counts())\n",
    "print(f\"\\nSample reviews:\")\n",
    "reviews_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Text Preprocessing and Cleaning\n",
    "\n",
    "**Before converting text to features, we need to clean it**:\n",
    "- Convert to lowercase\n",
    "- Remove punctuation and special characters\n",
    "- Remove extra whitespace\n",
    "- (Optional) Remove stop words\n",
    "- (Optional) Stemming/Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean text data for feature extraction.\n",
    "    \n",
    "    Steps:\n",
    "    1. Convert to lowercase\n",
    "    2. Remove URLs\n",
    "    3. Remove special characters and digits\n",
    "    4. Remove extra whitespace\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    \n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # Keep only letters and spaces (remove punctuation and numbers)\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply cleaning\n",
    "reviews_df['review_clean'] = reviews_df['review_text'].apply(clean_text)\n",
    "\n",
    "print(\"Text cleaning examples:\")\n",
    "print(\"\\nOriginal vs Cleaned:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Original: {reviews_df.iloc[i]['review_text']}\")\n",
    "    print(f\"Cleaned:  {reviews_df.iloc[i]['review_clean']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze word frequency in positive vs negative reviews\n",
    "positive_text = ' '.join(reviews_df[reviews_df['sentiment']=='positive']['review_clean'])\n",
    "negative_text = ' '.join(reviews_df[reviews_df['sentiment']=='negative']['review_clean'])\n",
    "\n",
    "positive_words = Counter(positive_text.split())\n",
    "negative_words = Counter(negative_text.split())\n",
    "\n",
    "print(\"Most common words in POSITIVE reviews:\")\n",
    "for word, count in positive_words.most_common(10):\n",
    "    print(f\"  {word}: {count}\")\n",
    "\n",
    "print(\"\\nMost common words in NEGATIVE reviews:\")\n",
    "for word, count in negative_words.most_common(10):\n",
    "    print(f\"  {word}: {count}\")\n",
    "\n",
    "print(\"\\nNotice how sentiment is reflected in word choice!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Technique 1: Bag-of-Words (CountVectorizer)\n",
    "\n",
    "**Bag-of-Words approach**:\n",
    "- Count how many times each word appears in each document\n",
    "- Create a matrix where rows = documents, columns = words\n",
    "- Ignore word order (that's why it's a \"bag\")\n",
    "\n",
    "**Example**:\n",
    "```\n",
    "Doc 1: \"I love this product\"\n",
    "Doc 2: \"I hate this product\"\n",
    "\n",
    "Vocabulary: [I, love, hate, this, product]\n",
    "\n",
    "Vector for Doc 1: [1, 1, 0, 1, 1]  # Has \"love\", no \"hate\"\n",
    "Vector for Doc 2: [1, 0, 1, 1, 1]  # Has \"hate\", no \"love\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CountVectorizer\n",
    "count_vectorizer = CountVectorizer(\n",
    "    max_features=100,  # Keep only top 100 most frequent words\n",
    "    min_df=2,  # Word must appear in at least 2 documents\n",
    "    max_df=0.9,  # Ignore words in more than 90% of documents\n",
    "    stop_words='english'  # Remove common English stop words\n",
    ")\n",
    "\n",
    "# Fit and transform the cleaned text\n",
    "bow_features = count_vectorizer.fit_transform(reviews_df['review_clean'])\n",
    "\n",
    "print(f\"Bag-of-Words feature matrix shape: {bow_features.shape}\")\n",
    "print(f\"  - {bow_features.shape[0]} reviews\")\n",
    "print(f\"  - {bow_features.shape[1]} unique words (features)\")\n",
    "print(f\"\\nFeature sparsity: {(1 - bow_features.nnz / (bow_features.shape[0] * bow_features.shape[1])) * 100:.1f}%\")\n",
    "print(\"(Most entries are 0 because each review uses only a small subset of vocabulary)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the vocabulary\n",
    "vocabulary = count_vectorizer.get_feature_names_out()\n",
    "print(f\"Vocabulary (first 30 words):\")\n",
    "print(vocabulary[:30])\n",
    "\n",
    "# Show bag-of-words representation for a sample review\n",
    "sample_idx = 0\n",
    "sample_review = reviews_df.iloc[sample_idx]['review_clean']\n",
    "sample_vector = bow_features[sample_idx].toarray()[0]\n",
    "\n",
    "print(f\"\\nSample review: '{sample_review}'\")\n",
    "print(f\"\\nBag-of-Words representation (non-zero features only):\")\n",
    "for word, count in zip(vocabulary, sample_vector):\n",
    "    if count > 0:\n",
    "        print(f\"  {word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Technique 2: TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "\n",
    "**Problem with Bag-of-Words**: All words are treated equally!\n",
    "- \"the\", \"is\", \"and\" appear frequently but are not informative\n",
    "- \"excellent\" or \"terrible\" appear less but are very informative\n",
    "\n",
    "**TF-IDF solution**: Weight words by how unique/important they are\n",
    "\n",
    "**Formula**:\n",
    "```\n",
    "TF-IDF = (Term Frequency) × (Inverse Document Frequency)\n",
    "\n",
    "TF = (# times word appears in document) / (total words in document)\n",
    "IDF = log(total documents / documents containing word)\n",
    "```\n",
    "\n",
    "**Result**:\n",
    "- Common words (appear everywhere) → low score\n",
    "- Rare but informative words → high score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=100,\n",
    "    min_df=2,\n",
    "    max_df=0.9,\n",
    "    stop_words='english'\n",
    ")\n",
    "\n",
    "# Fit and transform\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(reviews_df['review_clean'])\n",
    "\n",
    "print(f\"TF-IDF feature matrix shape: {tfidf_features.shape}\")\n",
    "print(f\"  - {tfidf_features.shape[0]} reviews\")\n",
    "print(f\"  - {tfidf_features.shape[1]} unique words (features)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Bag-of-Words vs TF-IDF for same review\n",
    "sample_idx = 0\n",
    "sample_review = reviews_df.iloc[sample_idx]['review_clean']\n",
    "\n",
    "bow_vector = bow_features[sample_idx].toarray()[0]\n",
    "tfidf_vector = tfidf_features[sample_idx].toarray()[0]\n",
    "vocabulary_tfidf = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(f\"Sample review: '{sample_review}'\\n\")\n",
    "print(f\"{'Word':<20} {'Bag-of-Words':<15} {'TF-IDF':<15}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for word, bow_val, tfidf_val in zip(vocabulary_tfidf, bow_vector, tfidf_vector):\n",
    "    if bow_val > 0 or tfidf_val > 0:\n",
    "        print(f\"{word:<20} {bow_val:<15.0f} {tfidf_val:<15.4f}\")\n",
    "\n",
    "print(\"\\nNotice: TF-IDF gives different weights to words based on their importance!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top TF-IDF scores for positive vs negative reviews\n",
    "positive_indices = reviews_df[reviews_df['sentiment']=='positive'].index\n",
    "negative_indices = reviews_df[reviews_df['sentiment']=='negative'].index\n",
    "\n",
    "# Average TF-IDF scores across all positive/negative reviews\n",
    "positive_tfidf_mean = tfidf_features[positive_indices].mean(axis=0).A1\n",
    "negative_tfidf_mean = tfidf_features[negative_indices].mean(axis=0).A1\n",
    "\n",
    "# Get top words for each sentiment\n",
    "top_positive_idx = positive_tfidf_mean.argsort()[-10:][::-1]\n",
    "top_negative_idx = negative_tfidf_mean.argsort()[-10:][::-1]\n",
    "\n",
    "top_positive_words = [(vocabulary_tfidf[i], positive_tfidf_mean[i]) for i in top_positive_idx]\n",
    "top_negative_words = [(vocabulary_tfidf[i], negative_tfidf_mean[i]) for i in top_negative_idx]\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "words, scores = zip(*top_positive_words)\n",
    "axes[0].barh(words, scores, color='lightgreen', edgecolor='black')\n",
    "axes[0].set_xlabel('Average TF-IDF Score')\n",
    "axes[0].set_title('Top Words in Positive Reviews', fontsize=12, fontweight='bold')\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "words, scores = zip(*top_negative_words)\n",
    "axes[1].barh(words, scores, color='lightcoral', edgecolor='black')\n",
    "axes[1].set_xlabel('Average TF-IDF Score')\n",
    "axes[1].set_title('Top Words in Negative Reviews', fontsize=12, fontweight='bold')\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice how different words characterize positive vs negative sentiment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Technique 3: N-grams\n",
    "\n",
    "**Problem**: Bag-of-Words ignores word order!\n",
    "- \"not good\" is very different from \"good\"\n",
    "- \"highly recommend\" has different meaning than just \"highly\" or \"recommend\"\n",
    "\n",
    "**Solution: N-grams** = sequences of N consecutive words\n",
    "- **Unigrams** (1-gram): Single words [\"not\", \"good\"]\n",
    "- **Bigrams** (2-gram): Word pairs [\"not good\"]\n",
    "- **Trigrams** (3-gram): Word triplets [\"not very good\"]\n",
    "\n",
    "**Trade-off**: N-grams capture context but increase feature space dramatically!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF with bigrams\n",
    "tfidf_bigram = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),  # Use both unigrams and bigrams\n",
    "    max_features=200,  # More features because we have bigrams\n",
    "    min_df=2,\n",
    "    stop_words='english'\n",
    ")\n",
    "\n",
    "bigram_features = tfidf_bigram.fit_transform(reviews_df['review_clean'])\n",
    "\n",
    "print(f\"TF-IDF with bigrams feature matrix: {bigram_features.shape}\")\n",
    "print(f\"  - {bigram_features.shape[0]} reviews\")\n",
    "print(f\"  - {bigram_features.shape[1]} features (unigrams + bigrams)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show examples of bigrams captured\n",
    "vocabulary_bigram = tfidf_bigram.get_feature_names_out()\n",
    "\n",
    "# Find bigrams (contain space)\n",
    "bigrams_only = [word for word in vocabulary_bigram if ' ' in word]\n",
    "\n",
    "print(f\"Total bigrams captured: {len(bigrams_only)}\")\n",
    "print(f\"\\nSample bigrams:\")\n",
    "for bigram in bigrams_only[:30]:\n",
    "    print(f\"  '{bigram}'\")\n",
    "\n",
    "print(\"\\nNotice phrases like 'highly recommend', 'waste money', etc.\")\n",
    "print(\"These capture sentiment better than individual words!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find most important bigrams for each sentiment\n",
    "positive_indices = reviews_df[reviews_df['sentiment']=='positive'].index\n",
    "negative_indices = reviews_df[reviews_df['sentiment']=='negative'].index\n",
    "\n",
    "positive_bigram_mean = bigram_features[positive_indices].mean(axis=0).A1\n",
    "negative_bigram_mean = bigram_features[negative_indices].mean(axis=0).A1\n",
    "\n",
    "# Get top bigrams\n",
    "bigram_indices = [i for i, word in enumerate(vocabulary_bigram) if ' ' in word]\n",
    "\n",
    "top_positive_bigrams = sorted(\n",
    "    [(vocabulary_bigram[i], positive_bigram_mean[i]) for i in bigram_indices],\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True\n",
    ")[:10]\n",
    "\n",
    "top_negative_bigrams = sorted(\n",
    "    [(vocabulary_bigram[i], negative_bigram_mean[i]) for i in bigram_indices],\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True\n",
    ")[:10]\n",
    "\n",
    "print(\"Top bigrams in POSITIVE reviews:\")\n",
    "for bigram, score in top_positive_bigrams:\n",
    "    print(f\"  '{bigram}': {score:.4f}\")\n",
    "\n",
    "print(\"\\nTop bigrams in NEGATIVE reviews:\")\n",
    "for bigram, score in top_negative_bigrams:\n",
    "    print(f\"  '{bigram}': {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Performance Comparison\n",
    "\n",
    "Let's compare sentiment classification performance using different text features:\n",
    "1. **Bag-of-Words** (CountVectorizer)\n",
    "2. **TF-IDF** (unigrams only)\n",
    "3. **TF-IDF + Bigrams** (unigrams + bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare labels\n",
    "y = (reviews_df['sentiment'] == 'positive').astype(int)\n",
    "\n",
    "# Split data\n",
    "test_size = 0.25\n",
    "\n",
    "# Feature sets\n",
    "feature_sets = {\n",
    "    'Bag-of-Words': bow_features,\n",
    "    'TF-IDF (unigrams)': tfidf_features,\n",
    "    'TF-IDF (unigrams + bigrams)': bigram_features\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, features in feature_sets.items():\n",
    "    # Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features, y, test_size=test_size, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Train Logistic Regression\n",
    "    model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    train_acc = model.score(X_train, y_train)\n",
    "    test_acc = model.score(X_test, y_test)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(model, features, y, cv=5)\n",
    "    \n",
    "    results.append({\n",
    "        'Method': name,\n",
    "        'Num Features': features.shape[1],\n",
    "        'Train Accuracy': train_acc,\n",
    "        'Test Accuracy': test_acc,\n",
    "        'CV Accuracy (mean)': cv_scores.mean(),\n",
    "        'CV Accuracy (std)': cv_scores.std()\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nSentiment Classification Performance:\")\n",
    "print(\"=\"*90)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize performance comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Accuracy comparison\n",
    "x = np.arange(len(results_df))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x - width/2, results_df['Train Accuracy'], width, \n",
    "           label='Train Accuracy', color='skyblue', edgecolor='black')\n",
    "axes[0].bar(x + width/2, results_df['Test Accuracy'], width, \n",
    "           label='Test Accuracy', color='salmon', edgecolor='black')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('Model Accuracy by Feature Type', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(results_df['Method'], rotation=15, ha='right')\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim([0.8, 1.0])\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Cross-validation scores\n",
    "axes[1].bar(results_df['Method'], results_df['CV Accuracy (mean)'], \n",
    "           yerr=results_df['CV Accuracy (std)'],\n",
    "           color='lightgreen', edgecolor='black', capsize=5)\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Cross-Validation Accuracy (with std)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xticklabels(results_df['Method'], rotation=15, ha='right')\n",
    "axes[1].set_ylim([0.8, 1.0])\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey observations:\")\n",
    "print(\"- TF-IDF typically outperforms simple Bag-of-Words\")\n",
    "print(\"- Adding bigrams can improve performance by capturing context\")\n",
    "print(\"- Trade-off: More features = more complexity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report for best model\n",
    "best_method = results_df.loc[results_df['Test Accuracy'].idxmax(), 'Method']\n",
    "best_features = feature_sets[best_method]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    best_features, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(f\"Best performing method: {best_method}\\n\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Negative', 'Positive']))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Negative', 'Positive'],\n",
    "            yticklabels=['Negative', 'Positive'])\n",
    "plt.title(f'Confusion Matrix - {best_method}', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Introduction to Word Embeddings\n",
    "\n",
    "**Limitation of Bag-of-Words and TF-IDF**:\n",
    "- Each word is independent (no semantic similarity)\n",
    "- \"excellent\" and \"outstanding\" are treated as completely different\n",
    "- Very high-dimensional sparse vectors\n",
    "\n",
    "**Word Embeddings**: Dense vector representations that capture semantic meaning\n",
    "- Words with similar meanings have similar vectors\n",
    "- Typically 50-300 dimensions (vs 1000s for BoW)\n",
    "- Pre-trained models available (Word2Vec, GloVe, FastText)\n",
    "\n",
    "**Example**:\n",
    "```\n",
    "word2vec[\"excellent\"] ≈ word2vec[\"outstanding\"]\n",
    "word2vec[\"terrible\"] ≈ word2vec[\"horrible\"]\n",
    "```\n",
    "\n",
    "**When to use**:\n",
    "- ✅ Small datasets (leverage pre-trained knowledge)\n",
    "- ✅ Need semantic similarity\n",
    "- ✅ Deep learning models\n",
    "- ❌ Traditional ML often works fine with TF-IDF\n",
    "- ❌ Interpretability is important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple demonstration of embedding concept (using random embeddings)\n",
    "# In practice, you'd use pre-trained embeddings like Word2Vec or GloVe\n",
    "\n",
    "print(\"Conceptual comparison: Sparse BoW vs Dense Embeddings\\n\")\n",
    "\n",
    "# Bag-of-Words: Sparse, high-dimensional\n",
    "vocab_size = 10000\n",
    "bow_vector = np.zeros(vocab_size)\n",
    "bow_vector[[42, 156, 1523, 8932]] = 1  # Only 4 words present\n",
    "\n",
    "print(f\"Bag-of-Words representation:\")\n",
    "print(f\"  Dimension: {len(bow_vector)}\")\n",
    "print(f\"  Non-zero values: {np.count_nonzero(bow_vector)}\")\n",
    "print(f\"  Sparsity: {(1 - np.count_nonzero(bow_vector) / len(bow_vector)) * 100:.1f}%\")\n",
    "print(f\"  Sample values: {bow_vector[:20]}\\n\")\n",
    "\n",
    "# Word Embedding: Dense, low-dimensional\n",
    "embedding_dim = 100\n",
    "embedding_vector = np.random.randn(embedding_dim)  # Dense vector\n",
    "\n",
    "print(f\"Word Embedding representation:\")\n",
    "print(f\"  Dimension: {len(embedding_vector)}\")\n",
    "print(f\"  Non-zero values: {np.count_nonzero(embedding_vector)}\")\n",
    "print(f\"  Sparsity: {(1 - np.count_nonzero(embedding_vector) / len(embedding_vector)) * 100:.1f}%\")\n",
    "print(f\"  Sample values: {embedding_vector[:20]}\\n\")\n",
    "\n",
    "print(\"Key differences:\")\n",
    "print(\"- Embeddings are DENSE (most values non-zero)\")\n",
    "print(\"- Embeddings are LOWER dimensional (100 vs 10,000)\")\n",
    "print(\"- Embeddings capture SEMANTIC meaning\")\n",
    "print(\"\\nNote: This is just a conceptual demo. Real embeddings require training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Exercise Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Email Spam Classification\n",
    "\n",
    "Create a spam detection system using TF-IDF features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Spam detection dataset\n",
    "\n",
    "spam_emails = [\n",
    "    \"Congratulations! You've won $1000000. Click here to claim now!\",\n",
    "    \"FREE PRIZE! Limited time offer. Act now!\",\n",
    "    \"Earn money fast! Work from home opportunity.\",\n",
    "    \"Click here for amazing deals! Buy now!\",\n",
    "    \"You are a winner! Claim your prize today!\"\n",
    "] * 30\n",
    "\n",
    "legitimate_emails = [\n",
    "    \"Meeting scheduled for tomorrow at 2pm in conference room.\",\n",
    "    \"Please review the attached document and provide feedback.\",\n",
    "    \"Reminder: Project deadline is next Friday.\",\n",
    "    \"Thank you for your purchase. Your order will ship soon.\",\n",
    "    \"Team lunch on Thursday. Please confirm your attendance.\"\n",
    "] * 30\n",
    "\n",
    "email_data = pd.DataFrame({\n",
    "    'email_text': spam_emails + legitimate_emails,\n",
    "    'is_spam': [1] * len(spam_emails) + [0] * len(legitimate_emails)\n",
    "}).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Email dataset: {len(email_data)} emails\")\n",
    "print(f\"Spam: {email_data['is_spam'].sum()}, Legitimate: {(1-email_data['is_spam']).sum()}\")\n",
    "\n",
    "# TODO: \n",
    "# 1. Clean the email text\n",
    "# 2. Create TF-IDF features with bigrams\n",
    "# 3. Train a classifier (Naive Bayes works well for text)\n",
    "# 4. Evaluate accuracy\n",
    "\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to Exercise 1\n",
    "\n",
    "# 1. Clean email text\n",
    "email_data['email_clean'] = email_data['email_text'].apply(clean_text)\n",
    "\n",
    "# 2. Create TF-IDF features with bigrams\n",
    "tfidf = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),\n",
    "    max_features=100,\n",
    "    min_df=2,\n",
    "    stop_words='english'\n",
    ")\n",
    "X = tfidf.fit_transform(email_data['email_clean'])\n",
    "y = email_data['is_spam']\n",
    "\n",
    "# 3. Train classifier\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 4. Evaluate\n",
    "train_acc = model.score(X_train, y_train)\n",
    "test_acc = model.score(X_test, y_test)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(f\"Spam Detection Results:\")\n",
    "print(f\"Train Accuracy: {train_acc:.3f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.3f}\\n\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Legitimate', 'Spam']))\n",
    "\n",
    "# Show top spam indicators\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "top_spam_features = model.feature_log_prob_[1].argsort()[-10:][::-1]\n",
    "print(\"\\nTop spam indicators:\")\n",
    "for idx in top_spam_features:\n",
    "    print(f\"  '{feature_names[idx]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Compare Different N-gram Ranges\n",
    "\n",
    "Experiment with different n-gram settings and see which works best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: N-gram comparison\n",
    "\n",
    "# TODO: Test these n-gram ranges on the review sentiment data:\n",
    "# 1. (1, 1) - unigrams only\n",
    "# 2. (2, 2) - bigrams only\n",
    "# 3. (1, 2) - unigrams + bigrams\n",
    "# 4. (1, 3) - unigrams + bigrams + trigrams\n",
    "#\n",
    "# Compare their performance on sentiment classification\n",
    "\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to Exercise 2\n",
    "\n",
    "ngram_configs = [\n",
    "    (1, 1, 'Unigrams only'),\n",
    "    (2, 2, 'Bigrams only'),\n",
    "    (1, 2, 'Unigrams + Bigrams'),\n",
    "    (1, 3, 'Unigrams + Bigrams + Trigrams')\n",
    "]\n",
    "\n",
    "ngram_results = []\n",
    "\n",
    "for min_n, max_n, description in ngram_configs:\n",
    "    # Create vectorizer\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        ngram_range=(min_n, max_n),\n",
    "        max_features=200,\n",
    "        min_df=2,\n",
    "        stop_words='english'\n",
    "    )\n",
    "    \n",
    "    # Transform\n",
    "    X = vectorizer.fit_transform(reviews_df['review_clean'])\n",
    "    y = (reviews_df['sentiment'] == 'positive').astype(int)\n",
    "    \n",
    "    # Train and evaluate\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "    model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    ngram_results.append({\n",
    "        'N-gram Range': f'({min_n}, {max_n})',\n",
    "        'Description': description,\n",
    "        'Num Features': X.shape[1],\n",
    "        'Test Accuracy': model.score(X_test, y_test)\n",
    "    })\n",
    "\n",
    "ngram_results_df = pd.DataFrame(ngram_results)\n",
    "print(\"N-gram Range Comparison:\")\n",
    "print(\"=\"*70)\n",
    "print(ngram_results_df)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(ngram_results_df['Description'], ngram_results_df['Test Accuracy'], \n",
    "        color='steelblue', edgecolor='black')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.title('Performance vs N-gram Range', fontsize=12, fontweight='bold')\n",
    "plt.xticks(rotation=15, ha='right')\n",
    "plt.ylim([0.9, 1.0])\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(\"- Bigrams alone may underperform (lose single word info)\")\n",
    "print(\"- Combining unigrams + bigrams often works well\")\n",
    "print(\"- Trigrams add complexity with diminishing returns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Custom Text Cleaning\n",
    "\n",
    "Enhance the text cleaning function to handle more edge cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Advanced text cleaning\n",
    "\n",
    "messy_reviews = [\n",
    "    \"OMG!!! This is THE BEST product EVER!!! 5 stars ⭐⭐⭐⭐⭐\",\n",
    "    \"Sooooo disappointed :( Waste of $$$. Contact support@company.com\",\n",
    "    \"Check out my review at http://example.com/review123\",\n",
    "    \"Product received on 01/15/2024. Working perfectly!!!\"\n",
    "]\n",
    "\n",
    "# TODO: Create an enhanced clean_text function that:\n",
    "# 1. Handles repeated characters (\"sooooo\" → \"so\")\n",
    "# 2. Removes emojis\n",
    "# 3. Removes URLs\n",
    "# 4. Removes email addresses\n",
    "# 5. Removes numbers and dates\n",
    "\n",
    "def enhanced_clean_text(text):\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "# Test your function\n",
    "# for review in messy_reviews:\n",
    "#     print(f\"Original: {review}\")\n",
    "#     print(f\"Cleaned:  {enhanced_clean_text(review)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to Exercise 3\n",
    "\n",
    "def enhanced_clean_text(text):\n",
    "    \"\"\"\n",
    "    Enhanced text cleaning with additional preprocessing.\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    \n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # Remove dates (simple pattern)\n",
    "    text = re.sub(r'\\d{1,2}/\\d{1,2}/\\d{2,4}', '', text)\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Handle repeated characters (e.g., \"sooooo\" → \"soo\")\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
    "    \n",
    "    # Remove emojis and special unicode characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    \n",
    "    # Keep only letters and spaces\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "print(\"Enhanced text cleaning examples:\\n\")\n",
    "for review in messy_reviews:\n",
    "    print(f\"Original: {review}\")\n",
    "    print(f\"Cleaned:  {enhanced_clean_text(review)}\\n\")\n",
    "\n",
    "print(\"Improvements:\")\n",
    "print(\"✓ Removed URLs and emails\")\n",
    "print(\"✓ Normalized repeated characters\")\n",
    "print(\"✓ Removed emojis and special characters\")\n",
    "print(\"✓ Removed numbers and dates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Text must be converted to numbers** for machine learning\n",
    "   - Models can't understand words directly\n",
    "   - Feature engineering bridges the gap\n",
    "\n",
    "2. **Four core text vectorization techniques**:\n",
    "   - **Bag-of-Words (CountVectorizer)**: Simple word counts\n",
    "   - **TF-IDF**: Weight words by importance (usually better than BoW)\n",
    "   - **N-grams**: Capture word sequences and context\n",
    "   - **Word Embeddings**: Dense semantic representations\n",
    "\n",
    "3. **Text preprocessing is critical**:\n",
    "   - Lowercase conversion\n",
    "   - Remove punctuation, URLs, special characters\n",
    "   - Handle repeated characters\n",
    "   - Remove stop words (optional)\n",
    "\n",
    "4. **TF-IDF typically outperforms Bag-of-Words**:\n",
    "   - Down-weights common uninformative words\n",
    "   - Up-weights rare important words\n",
    "   - Good default choice for traditional ML\n",
    "\n",
    "5. **N-grams capture context but increase complexity**:\n",
    "   - Unigrams + bigrams often optimal\n",
    "   - Trigrams+ have diminishing returns\n",
    "   - Trade-off between performance and feature space size\n",
    "\n",
    "### When to Use Each Method\n",
    "\n",
    "**Bag-of-Words (CountVectorizer)**:\n",
    "- ✅ Simple baseline\n",
    "- ✅ Naive Bayes classifier\n",
    "- ✅ Document frequency matters more than term importance\n",
    "\n",
    "**TF-IDF**:\n",
    "- ✅ Most text classification tasks\n",
    "- ✅ Document similarity\n",
    "- ✅ Information retrieval\n",
    "- ✅ Works well with SVM, Logistic Regression\n",
    "\n",
    "**N-grams**:\n",
    "- ✅ Context matters (\"not good\" vs \"good\")\n",
    "- ✅ Sentiment analysis\n",
    "- ✅ Phrase detection\n",
    "\n",
    "**Word Embeddings**:\n",
    "- ✅ Small datasets (use pre-trained)\n",
    "- ✅ Need semantic similarity\n",
    "- ✅ Deep learning models\n",
    "- ✅ Multi-language tasks\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Always clean text first**: Garbage in = garbage out\n",
    "2. **Start with TF-IDF unigrams**: Good baseline\n",
    "3. **Add bigrams if context matters**: Sentiment, negation, phrases\n",
    "4. **Limit vocabulary size**: Use max_features to control complexity\n",
    "5. **Remove stop words**: Often helps, but test both ways\n",
    "6. **Consider min_df and max_df**: Filter rare and common words\n",
    "7. **Evaluate on held-out test set**: Avoid overfitting\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "**Module 08**: Feature Selection Methods - Learn to identify and keep only the most important features\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [Scikit-learn Text Feature Extraction](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)\n",
    "- [TF-IDF Explained](https://monkeylearn.com/blog/what-is-tf-idf/)\n",
    "- [Word Embeddings Guide](https://machinelearningmastery.com/what-are-word-embeddings/)\n",
    "- [NLTK Documentation](https://www.nltk.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Congratulations!** You've completed Module 07. You now understand:\n",
    "- How to clean and preprocess text data\n",
    "- Bag-of-Words and TF-IDF vectorization\n",
    "- N-grams for capturing word sequences\n",
    "- When to use each text feature engineering method\n",
    "- The performance impact of different approaches\n",
    "\n",
    "Ready to learn feature selection? Let's move to **Module 08: Feature Selection Methods**!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
