{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 04: Polynomial Features and Interactions\n",
    "\n",
    "**Difficulty**: ⭐⭐ Intermediate  \n",
    "**Estimated Time**: 60 minutes  \n",
    "**Prerequisites**: [Module 03: Feature Scaling and Normalization](03_feature_scaling_normalization.ipynb)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Understand when linear models need polynomial features\n",
    "2. Create polynomial features using sklearn's PolynomialFeatures\n",
    "3. Engineer interaction terms to capture feature relationships\n",
    "4. Measure the impact of polynomial features on model performance\n",
    "5. Recognize and avoid the curse of dimensionality\n",
    "6. Apply feature selection to manage polynomial feature explosion\n",
    "7. Decide the appropriate polynomial degree for your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Polynomial Features?\n",
    "\n",
    "### The Limitation of Linear Models\n",
    "\n",
    "Linear models assume a **straight-line relationship**:\n",
    "\n",
    "$$y = w_1x_1 + w_2x_2 + ... + w_nx_n + b$$\n",
    "\n",
    "**Problem**: Real-world relationships are often **non-linear**!\n",
    "\n",
    "### Example: House Prices\n",
    "\n",
    "**Linear model**: \n",
    "```\n",
    "price = w₁ × sqft + w₂ × bedrooms + b\n",
    "```\n",
    "\n",
    "**Better with polynomial**:\n",
    "```\n",
    "price = w₁ × sqft + w₂ × sqft² + w₃ × bedrooms + w₄ × (sqft × bedrooms) + b\n",
    "                      ^^^^^^^^                          ^^^^^^^^^^^^^^^^^^^\n",
    "                   Polynomial term                     Interaction term\n",
    "```\n",
    "\n",
    "### What We Create\n",
    "\n",
    "1. **Polynomial terms**: $x^2, x^3, ...$\n",
    "   - Capture non-linear relationships\n",
    "   - Example: Profit might increase exponentially with experience\n",
    "\n",
    "2. **Interaction terms**: $x_1 \\times x_2$\n",
    "   - Capture relationships between features\n",
    "   - Example: Large house with many bedrooms is worth MORE than the sum of parts\n",
    "\n",
    "### Visual Example\n",
    "\n",
    "```\n",
    "Linear:        Quadratic:        Cubic:\n",
    "  y              y                 y\n",
    "  |  /           |   _             |    __\n",
    "  | /            |  / \\            |   /  \\\n",
    "  |/             | /   \\           |  /    \\_\n",
    "  +----x         +------x          +--------x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Polynomial features\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Feature selection\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Configuration\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 3)\n",
    "\n",
    "print(\"✓ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Demonstrating the Need for Polynomial Features\n",
    "\n",
    "Let's create a dataset with a non-linear relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic data with quadratic relationship\n",
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "\n",
    "# Feature: years of experience\n",
    "experience = np.random.uniform(0, 10, n_samples)\n",
    "\n",
    "# Target: salary has a quadratic relationship with experience\n",
    "# Early years: rapid salary growth\n",
    "# Later years: growth slows down\n",
    "salary = 30000 + 8000 * experience + 500 * experience**2 + np.random.normal(0, 5000, n_samples)\n",
    "\n",
    "# Create DataFrame\n",
    "salary_data = pd.DataFrame({\n",
    "    'experience': experience,\n",
    "    'salary': salary\n",
    "})\n",
    "\n",
    "print(f\"Created dataset with {len(salary_data)} samples\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(salary_data.head())\n",
    "\n",
    "# Visualize the relationship\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(salary_data['experience'], salary_data['salary'], alpha=0.5, edgecolors='black')\n",
    "plt.xlabel('Years of Experience')\n",
    "plt.ylabel('Salary ($)')\n",
    "plt.title('Salary vs Experience (Non-Linear Relationship)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNotice: The relationship is curved, not a straight line!\")\n",
    "print(\"A linear model will struggle to capture this pattern.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Linear vs Polynomial Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X = salary_data[['experience']]\n",
    "y = salary_data['salary']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Model 1: Linear (degree=1)\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train, y_train)\n",
    "y_pred_linear = linear_model.predict(X_test)\n",
    "rmse_linear = np.sqrt(mean_squared_error(y_test, y_pred_linear))\n",
    "r2_linear = r2_score(y_test, y_pred_linear)\n",
    "\n",
    "# Model 2: Polynomial (degree=2)\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "\n",
    "poly_model = LinearRegression()\n",
    "poly_model.fit(X_train_poly, y_train)\n",
    "y_pred_poly = poly_model.predict(X_test_poly)\n",
    "rmse_poly = np.sqrt(mean_squared_error(y_test, y_pred_poly))\n",
    "r2_poly = r2_score(y_test, y_pred_poly)\n",
    "\n",
    "# Compare\n",
    "print(\"Model Performance Comparison:\\n\")\n",
    "print(f\"Linear Model (degree=1):\")\n",
    "print(f\"  RMSE: ${rmse_linear:,.2f}\")\n",
    "print(f\"  R²:   {r2_linear:.4f}\")\n",
    "print(f\"\\nPolynomial Model (degree=2):\")\n",
    "print(f\"  RMSE: ${rmse_poly:,.2f}\")\n",
    "print(f\"  R²:   {r2_poly:.4f}\")\n",
    "print(f\"\\nImprovement: {(rmse_linear - rmse_poly)/rmse_linear*100:.1f}% reduction in error!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize both models\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Generate smooth predictions for visualization\n",
    "X_plot = np.linspace(0, 10, 100).reshape(-1, 1)\n",
    "y_plot_linear = linear_model.predict(X_plot)\n",
    "y_plot_poly = poly_model.predict(poly.transform(X_plot))\n",
    "\n",
    "# Plot 1: Linear model\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_train, y_train, alpha=0.5, label='Training data')\n",
    "plt.plot(X_plot, y_plot_linear, 'r-', linewidth=2, label='Linear fit')\n",
    "plt.xlabel('Years of Experience')\n",
    "plt.ylabel('Salary ($)')\n",
    "plt.title(f'Linear Model\\nR² = {r2_linear:.4f}')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Polynomial model\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_train, y_train, alpha=0.5, label='Training data')\n",
    "plt.plot(X_plot, y_plot_poly, 'g-', linewidth=2, label='Polynomial fit (degree=2)')\n",
    "plt.xlabel('Years of Experience')\n",
    "plt.ylabel('Salary ($)')\n",
    "plt.title(f'Polynomial Model\\nR² = {r2_poly:.4f}')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice: Polynomial model captures the curved relationship much better!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Creating Polynomial Features with PolynomialFeatures\n",
    "\n",
    "Let's understand what PolynomialFeatures actually creates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple example with 2 features\n",
    "simple_data = pd.DataFrame({\n",
    "    'x1': [1, 2, 3],\n",
    "    'x2': [4, 5, 6]\n",
    "})\n",
    "\n",
    "print(\"Original features:\")\n",
    "print(simple_data)\n",
    "\n",
    "# Degree 2 polynomial\n",
    "poly2 = PolynomialFeatures(degree=2, include_bias=False)\n",
    "features_poly2 = poly2.fit_transform(simple_data)\n",
    "\n",
    "# Get feature names\n",
    "feature_names = poly2.get_feature_names_out(['x1', 'x2'])\n",
    "\n",
    "poly_df = pd.DataFrame(features_poly2, columns=feature_names)\n",
    "\n",
    "print(\"\\nPolynomial features (degree=2):\")\n",
    "print(poly_df)\n",
    "\n",
    "print(\"\\nWhat was created:\")\n",
    "print(\"- x1: Original feature\")\n",
    "print(\"- x2: Original feature\")\n",
    "print(\"- x1²: Polynomial term (x1 squared)\")\n",
    "print(\"- x1 × x2: Interaction term (product of x1 and x2)\")\n",
    "print(\"- x2²: Polynomial term (x2 squared)\")\n",
    "print(f\"\\nOriginal: {simple_data.shape[1]} features\")\n",
    "print(f\"After polynomial: {poly_df.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What happens with degree=3?\n",
    "poly3 = PolynomialFeatures(degree=3, include_bias=False)\n",
    "features_poly3 = poly3.fit_transform(simple_data)\n",
    "feature_names_3 = poly3.get_feature_names_out(['x1', 'x2'])\n",
    "\n",
    "print(\"Polynomial features (degree=3):\")\n",
    "print(f\"\\nFeature names: {list(feature_names_3)}\")\n",
    "print(f\"\\nNumber of features: {len(feature_names_3)}\")\n",
    "\n",
    "print(\"\\nIncludes:\")\n",
    "print(\"- Original: x1, x2\")\n",
    "print(\"- Degree 2: x1², x1×x2, x2²\")\n",
    "print(\"- Degree 3: x1³, x1²×x2, x1×x2², x2³\")\n",
    "\n",
    "print(\"\\n⚠️  Warning: Features grow rapidly with degree!\")\n",
    "print(f\"   2 features → {len(feature_names_3)} features with degree=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Interaction Features\n",
    "\n",
    "**Interaction terms** capture how features work together.\n",
    "\n",
    "### Real-World Examples:\n",
    "\n",
    "1. **House prices**: \n",
    "   - `sqft × location_quality`: Large house in good location = premium price\n",
    "   - Effect is multiplicative, not additive\n",
    "\n",
    "2. **Marketing**:\n",
    "   - `ad_spend × target_audience_size`: More impactful when both are high\n",
    "\n",
    "3. **Education**:\n",
    "   - `study_hours × teaching_quality`: Great teaching + effort = best outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset where interaction matters\n",
    "np.random.seed(42)\n",
    "n = 500\n",
    "\n",
    "house_features = pd.DataFrame({\n",
    "    'sqft': np.random.uniform(1000, 4000, n),\n",
    "    'bedrooms': np.random.randint(2, 6, n),\n",
    "    'age': np.random.randint(0, 50, n)\n",
    "})\n",
    "\n",
    "# Price depends on interaction: sqft × bedrooms\n",
    "# Large house with many bedrooms is worth MORE than sum of parts\n",
    "house_features['price'] = (\n",
    "    100 * house_features['sqft'] +\n",
    "    50000 * house_features['bedrooms'] +\n",
    "    30 * house_features['sqft'] * house_features['bedrooms'] +  # Interaction!\n",
    "    -1000 * house_features['age'] +\n",
    "    np.random.normal(0, 30000, n)\n",
    ")\n",
    "\n",
    "print(f\"Created house dataset with {len(house_features)} samples\")\n",
    "print(house_features.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model with and without interaction terms\n",
    "X = house_features[['sqft', 'bedrooms', 'age']]\n",
    "y = house_features['price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model 1: No interactions\n",
    "model_no_interaction = LinearRegression()\n",
    "model_no_interaction.fit(X_train, y_train)\n",
    "r2_no_interaction = model_no_interaction.score(X_test, y_test)\n",
    "\n",
    "# Model 2: With interactions (degree=2 includes interactions)\n",
    "poly_interaction = PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)\n",
    "X_train_inter = poly_interaction.fit_transform(X_train)\n",
    "X_test_inter = poly_interaction.transform(X_test)\n",
    "\n",
    "model_with_interaction = LinearRegression()\n",
    "model_with_interaction.fit(X_train_inter, y_train)\n",
    "r2_with_interaction = model_with_interaction.score(X_test_inter, y_test)\n",
    "\n",
    "print(\"Impact of Interaction Terms:\\n\")\n",
    "print(f\"Without interactions: R² = {r2_no_interaction:.4f}\")\n",
    "print(f\"With interactions:    R² = {r2_with_interaction:.4f}\")\n",
    "print(f\"\\nImprovement: {(r2_with_interaction - r2_no_interaction):.4f}\")\n",
    "\n",
    "print(\"\\nInteraction features created:\")\n",
    "print(poly_interaction.get_feature_names_out(['sqft', 'bedrooms', 'age']))\n",
    "print(\"\\nNote: interaction_only=True creates only interaction terms, not polynomial terms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. The Curse of Dimensionality\n",
    "\n",
    "**Problem**: Polynomial features grow **exponentially**!\n",
    "\n",
    "### Growth Rate\n",
    "\n",
    "Formula: $\\binom{n + d}{d}$ where n = features, d = degree\n",
    "\n",
    "| Original Features | Degree | Total Features |\n",
    "|------------------|--------|----------------|\n",
    "| 5 | 2 | 20 |\n",
    "| 5 | 3 | 56 |\n",
    "| 10 | 2 | 65 |\n",
    "| 10 | 3 | 285 |\n",
    "| 20 | 2 | 230 |\n",
    "| 20 | 3 | 1,771 |\n",
    "\n",
    "**Consequences**:\n",
    "1. ❌ Computational cost explodes\n",
    "2. ❌ Risk of overfitting increases\n",
    "3. ❌ Model becomes hard to interpret\n",
    "4. ❌ Training time increases dramatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate feature explosion\n",
    "def count_polynomial_features(n_features, degree):\n",
    "    \"\"\"Count features created by PolynomialFeatures\"\"\"\n",
    "    dummy_data = np.ones((1, n_features))\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    transformed = poly.fit_transform(dummy_data)\n",
    "    return transformed.shape[1]\n",
    "\n",
    "# Test different combinations\n",
    "results = []\n",
    "for n_features in [5, 10, 15, 20]:\n",
    "    for degree in [2, 3, 4]:\n",
    "        total = count_polynomial_features(n_features, degree)\n",
    "        results.append({\n",
    "            'Original Features': n_features,\n",
    "            'Degree': degree,\n",
    "            'Total Features': total,\n",
    "            'Multiplier': f'{total/n_features:.1f}x'\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"Feature Explosion with Polynomial Features:\\n\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n⚠️  WARNING: 20 features with degree=4 would create thousands of features!\")\n",
    "print(\"This leads to overfitting and slow training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature explosion\n",
    "pivot_df = results_df.pivot(index='Original Features', columns='Degree', values='Total Features')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "pivot_df.plot(kind='bar', width=0.8)\n",
    "plt.xlabel('Number of Original Features')\n",
    "plt.ylabel('Total Features After Transformation')\n",
    "plt.title('Polynomial Feature Explosion')\n",
    "plt.legend(title='Polynomial Degree')\n",
    "plt.yscale('log')  # Log scale to see the growth\n",
    "plt.grid(True, alpha=0.3, which='both')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice the exponential growth! (log scale used)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Managing Polynomial Features\n",
    "\n",
    "### Strategies to Avoid Curse of Dimensionality:\n",
    "\n",
    "1. **Limit degree**: Usually degree=2 or 3 is sufficient\n",
    "2. **Feature selection**: Select only important polynomial features\n",
    "3. **Regularization**: Use Ridge/Lasso to penalize complex models\n",
    "4. **Domain knowledge**: Only create interactions that make sense\n",
    "5. **interaction_only=True**: Skip polynomial terms, keep only interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 1: Use Regularization (Ridge/Lasso)\n",
    "# Regularization penalizes large coefficients, helping with many features\n",
    "\n",
    "# Create dataset with many features\n",
    "np.random.seed(42)\n",
    "n = 300\n",
    "X_many = pd.DataFrame({\n",
    "    f'feature_{i}': np.random.randn(n) for i in range(10)\n",
    "})\n",
    "\n",
    "# Target depends on only first 3 features\n",
    "y_many = (\n",
    "    5 * X_many['feature_0'] + \n",
    "    3 * X_many['feature_1']**2 + \n",
    "    2 * X_many['feature_0'] * X_many['feature_1'] +\n",
    "    np.random.normal(0, 1, n)\n",
    ")\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_many, y_many, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create polynomial features (degree=2)\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "\n",
    "print(f\"Original features: {X_train.shape[1]}\")\n",
    "print(f\"After polynomial (degree=2): {X_train_poly.shape[1]}\")\n",
    "\n",
    "# Compare models\n",
    "models = {\n",
    "    'Linear (no regularization)': LinearRegression(),\n",
    "    'Ridge (L2 regularization)': Ridge(alpha=1.0),\n",
    "    'Lasso (L1 regularization)': Lasso(alpha=0.1)\n",
    "}\n",
    "\n",
    "print(\"\\nModel Performance on Polynomial Features:\\n\")\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_poly, y_train)\n",
    "    r2 = model.score(X_test_poly, y_test)\n",
    "    print(f\"{name:30s} R² = {r2:.4f}\")\n",
    "\n",
    "print(\"\\nRegularization helps prevent overfitting with many polynomial features!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 2: Feature Selection\n",
    "# Keep only the most important polynomial features\n",
    "\n",
    "# Select top 20 features\n",
    "selector = SelectKBest(score_func=f_regression, k=20)\n",
    "X_train_selected = selector.fit_transform(X_train_poly, y_train)\n",
    "X_test_selected = selector.transform(X_test_poly)\n",
    "\n",
    "print(f\"Features before selection: {X_train_poly.shape[1]}\")\n",
    "print(f\"Features after selection: {X_train_selected.shape[1]}\")\n",
    "\n",
    "# Train model on selected features\n",
    "model_selected = LinearRegression()\n",
    "model_selected.fit(X_train_selected, y_train)\n",
    "r2_selected = model_selected.score(X_test_selected, y_test)\n",
    "\n",
    "print(f\"\\nR² with selected features: {r2_selected:.4f}\")\n",
    "print(\"\\nFeature selection reduces dimensionality while maintaining performance!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Choosing the Right Polynomial Degree\n",
    "\n",
    "Let's find the optimal degree by comparing performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different polynomial degrees\n",
    "degrees = range(1, 6)\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "# Use salary data from earlier\n",
    "X = salary_data[['experience']]\n",
    "y = salary_data['salary']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "for degree in degrees:\n",
    "    # Create polynomial features\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_test_poly = poly.transform(X_test)\n",
    "    \n",
    "    # Train model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_poly, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    train_score = model.score(X_train_poly, y_train)\n",
    "    test_score = model.score(X_test_poly, y_test)\n",
    "    \n",
    "    train_scores.append(train_score)\n",
    "    test_scores.append(test_score)\n",
    "    \n",
    "    print(f\"Degree {degree}: Train R² = {train_score:.4f}, Test R² = {test_score:.4f}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(degrees, train_scores, 'o-', label='Training Score', linewidth=2)\n",
    "plt.plot(degrees, test_scores, 's-', label='Test Score', linewidth=2)\n",
    "plt.xlabel('Polynomial Degree')\n",
    "plt.ylabel('R² Score')\n",
    "plt.title('Model Performance vs Polynomial Degree')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(degrees)\n",
    "plt.show()\n",
    "\n",
    "optimal_degree = degrees[np.argmax(test_scores)]\n",
    "print(f\"\\nOptimal degree: {optimal_degree}\")\n",
    "print(\"\\nNotice: After degree 2, test performance doesn't improve (may even decrease!)\")\n",
    "print(\"This is overfitting - model memorizes training data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Best Practices\n",
    "\n",
    "### ✅ DO:\n",
    "\n",
    "1. **Start with degree=2**\n",
    "   - Often sufficient for most problems\n",
    "   - Good balance of complexity and interpretability\n",
    "\n",
    "2. **Scale features BEFORE creating polynomials**\n",
    "   - Prevents numerical instability\n",
    "   - Squared large numbers get VERY large!\n",
    "\n",
    "3. **Use regularization (Ridge/Lasso)**\n",
    "   - Essential when creating many features\n",
    "   - Prevents overfitting\n",
    "\n",
    "4. **Validate on test set**\n",
    "   - Don't trust training accuracy\n",
    "   - Watch for overfitting\n",
    "\n",
    "5. **Consider domain knowledge**\n",
    "   - Create only meaningful interactions\n",
    "   - Not all feature pairs interact\n",
    "\n",
    "### ❌ DON'T:\n",
    "\n",
    "1. **Don't use degree > 3** (usually)\n",
    "2. **Don't create polynomials without scaling**\n",
    "3. **Don't ignore feature explosion**\n",
    "4. **Don't forget regularization with many features**\n",
    "5. **Don't use with tree-based models** (they handle non-linearity naturally)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Exercise Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Identify Non-Linear Relationships\n",
    "\n",
    "Which scenarios would benefit from polynomial features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Which scenarios need polynomial features?\n",
    "\n",
    "scenarios = {\n",
    "    'A': 'Predicting crop yield based on fertilizer amount (too little OR too much is bad)',\n",
    "    'B': 'Predicting salary based on job title (Junior, Senior, Manager)',\n",
    "    'C': 'Predicting breaking distance based on car speed (physics: d = v²)',\n",
    "    'D': 'Predicting house price based on number of bedrooms',\n",
    "    'E': 'Predicting student test score based on study hours (diminishing returns)'\n",
    "}\n",
    "\n",
    "print(\"Which scenarios would benefit from polynomial features?\\n\")\n",
    "for key, scenario in scenarios.items():\n",
    "    print(f\"{key}. {scenario}\")\n",
    "\n",
    "print(\"\\nYour answers (list letters that need polynomial features):\")\n",
    "# needs_polynomial = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to Exercise 1\n",
    "\n",
    "print(\"Solutions:\\n\")\n",
    "print(\"✅ NEEDS Polynomial Features:\")\n",
    "print(\"  A - Crop yield: Quadratic relationship (inverted U-shape)\")\n",
    "print(\"      Too little fertilizer = low yield, optimal amount = high, too much = low\")\n",
    "print(\"  C - Braking distance: Quadratic relationship (d ∝ v²)\")\n",
    "print(\"      Distance increases with square of speed\")\n",
    "print(\"  E - Test scores: Diminishing returns (logarithmic/quadratic)\")\n",
    "print(\"      First hours of study help a lot, additional hours help less\")\n",
    "\n",
    "print(\"\\n❌ DON'T NEED Polynomial Features:\")\n",
    "print(\"  B - Job title: Ordinal encoding is sufficient\")\n",
    "print(\"  D - Bedrooms: Likely linear relationship\")\n",
    "\n",
    "print(\"\\nKey insight: Look for relationships that are curved, not straight!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Create Interaction Features\n",
    "\n",
    "Create meaningful interaction features for a marketing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Create interaction features\n",
    "\n",
    "# Marketing campaign dataset\n",
    "marketing_data = pd.DataFrame({\n",
    "    'ad_spend': [1000, 5000, 10000, 2000, 8000],\n",
    "    'email_opens': [100, 500, 800, 200, 600],\n",
    "    'website_visits': [500, 2000, 4000, 1000, 3000]\n",
    "})\n",
    "\n",
    "print(\"Marketing dataset:\")\n",
    "print(marketing_data)\n",
    "\n",
    "# TODO: Create interaction features that make business sense\n",
    "# Which features might interact?\n",
    "# Hint: High ad spend + high email opens = ?\n",
    "\n",
    "# Your code here:\n",
    "# marketing_data['interaction1'] = ???\n",
    "# marketing_data['interaction2'] = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to Exercise 2\n",
    "\n",
    "marketing_data_solution = marketing_data.copy()\n",
    "\n",
    "# Meaningful interactions\n",
    "marketing_data_solution['spend_x_opens'] = (\n",
    "    marketing_data_solution['ad_spend'] * marketing_data_solution['email_opens']\n",
    ")\n",
    "\n",
    "marketing_data_solution['opens_x_visits'] = (\n",
    "    marketing_data_solution['email_opens'] * marketing_data_solution['website_visits']\n",
    ")\n",
    "\n",
    "marketing_data_solution['engagement_score'] = (\n",
    "    marketing_data_solution['ad_spend'] * \n",
    "    marketing_data_solution['email_opens'] * \n",
    "    marketing_data_solution['website_visits'] / 1000000  # Normalize\n",
    ")\n",
    "\n",
    "print(\"Solution:\")\n",
    "print(marketing_data_solution)\n",
    "\n",
    "print(\"\\nInteractions created:\")\n",
    "print(\"1. ad_spend × email_opens: Campaigns with both high spend AND high engagement\")\n",
    "print(\"2. email_opens × website_visits: Email effectiveness leading to site traffic\")\n",
    "print(\"3. Combined engagement score: Overall campaign effectiveness\")\n",
    "\n",
    "print(\"\\nThese interactions capture synergies between features!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Avoid Overfitting with Polynomial Features\n",
    "\n",
    "Demonstrate overfitting with high-degree polynomials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Show overfitting with high polynomial degree\n",
    "\n",
    "# Small dataset (prone to overfitting)\n",
    "np.random.seed(42)\n",
    "n_small = 20  # Very small!\n",
    "\n",
    "X_small = np.random.uniform(0, 10, n_small).reshape(-1, 1)\n",
    "y_small = 2 * X_small.ravel() + 5 + np.random.normal(0, 2, n_small)\n",
    "\n",
    "# TODO: \n",
    "# 1. Fit polynomial models with degrees 1, 5, and 10\n",
    "# 2. Plot the fitted curves\n",
    "# 3. Which degree overfits?\n",
    "\n",
    "# Your code here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to Exercise 3\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "degrees_test = [1, 5, 10]\n",
    "\n",
    "X_plot = np.linspace(0, 10, 100).reshape(-1, 1)\n",
    "\n",
    "for idx, degree in enumerate(degrees_test):\n",
    "    # Fit model\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    X_poly = poly.fit_transform(X_small)\n",
    "    X_plot_poly = poly.transform(X_plot)\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(X_poly, y_small)\n",
    "    y_plot = model.predict(X_plot_poly)\n",
    "    \n",
    "    # Plot\n",
    "    axes[idx].scatter(X_small, y_small, color='blue', s=50, alpha=0.7, label='Data')\n",
    "    axes[idx].plot(X_plot, y_plot, 'r-', linewidth=2, label=f'Degree {degree} fit')\n",
    "    axes[idx].set_xlabel('X')\n",
    "    axes[idx].set_ylabel('y')\n",
    "    axes[idx].set_title(f'Polynomial Degree = {degree}')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observations:\")\n",
    "print(\"- Degree 1 (left): Underfits - too simple\")\n",
    "print(\"- Degree 5 (middle): Starts to wiggle - early overfitting\")\n",
    "print(\"- Degree 10 (right): Severe overfitting - wild oscillations!\")\n",
    "print(\"\\nWith only 20 data points, high degrees memorize noise instead of learning patterns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Build Complete Pipeline\n",
    "\n",
    "Create a pipeline with scaling, polynomial features, and regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Build a complete pipeline\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Use house price data\n",
    "X = house_features[['sqft', 'bedrooms', 'age']]\n",
    "y = house_features['price']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# TODO: Create a pipeline with:\n",
    "# 1. StandardScaler\n",
    "# 2. PolynomialFeatures (degree=2)\n",
    "# 3. Ridge regression (alpha=1.0)\n",
    "\n",
    "# Your code here:\n",
    "# pipeline = Pipeline([\n",
    "#     ('scaler', ???),\n",
    "#     ('poly', ???),\n",
    "#     ('model', ???)\n",
    "# ])\n",
    "\n",
    "# pipeline.fit(X_train, y_train)\n",
    "# score = pipeline.score(X_test, y_test)\n",
    "# print(f\"Pipeline R² score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to Exercise 4\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    ('model', Ridge(alpha=1.0))\n",
    "])\n",
    "\n",
    "# Fit pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "train_score = pipeline.score(X_train, y_train)\n",
    "test_score = pipeline.score(X_test, y_test)\n",
    "\n",
    "print(\"Pipeline Results:\")\n",
    "print(f\"Training R²: {train_score:.4f}\")\n",
    "print(f\"Test R²:     {test_score:.4f}\")\n",
    "\n",
    "print(\"\\nBenefits of using a Pipeline:\")\n",
    "print(\"✓ Ensures proper order: scale → polynomial → model\")\n",
    "print(\"✓ Prevents data leakage (all steps fit on train only)\")\n",
    "print(\"✓ Easy to deploy (save entire pipeline)\")\n",
    "print(\"✓ Cleaner code, less error-prone\")\n",
    "\n",
    "# Make predictions\n",
    "new_house = pd.DataFrame({\n",
    "    'sqft': [2500],\n",
    "    'bedrooms': [4],\n",
    "    'age': [10]\n",
    "})\n",
    "\n",
    "predicted_price = pipeline.predict(new_house)[0]\n",
    "print(f\"\\nPrediction for new house: ${predicted_price:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Polynomial features enable linear models to capture non-linearity**\n",
    "   - Transform straight lines into curves\n",
    "   - Critical when relationships are quadratic, cubic, etc.\n",
    "\n",
    "2. **Two types of features created**:\n",
    "   - **Polynomial terms**: x², x³ (capture curved relationships)\n",
    "   - **Interaction terms**: x₁×x₂ (capture feature synergies)\n",
    "\n",
    "3. **Curse of dimensionality is real**:\n",
    "   - Features grow exponentially with degree\n",
    "   - 10 features, degree 3 → 285 features!\n",
    "   - Leads to overfitting and slow training\n",
    "\n",
    "4. **Best practices**:\n",
    "   - Start with degree=2 (usually sufficient)\n",
    "   - Scale features BEFORE creating polynomials\n",
    "   - Use regularization (Ridge/Lasso)\n",
    "   - Validate on test set to detect overfitting\n",
    "   - Consider feature selection\n",
    "\n",
    "5. **When to use**:\n",
    "   - ✅ Linear models on non-linear data\n",
    "   - ✅ When domain knowledge suggests interactions\n",
    "   - ✅ When you have enough data (avoid overfitting)\n",
    "   - ❌ Tree-based models (they handle non-linearity naturally)\n",
    "   - ❌ When you have very little data\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "**Module 05**: Binning and Discretization - Learn how to convert continuous features into categorical bins\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [Sklearn PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html)\n",
    "- [Feature Engineering for Machine Learning](https://www.oreilly.com/library/view/feature-engineering-for/9781491953235/)\n",
    "- [Regularization Techniques](https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Congratulations!** You've completed Module 04. You now know:\n",
    "- How to create polynomial and interaction features\n",
    "- When polynomial features improve model performance\n",
    "- How to avoid the curse of dimensionality\n",
    "- How to use regularization with polynomial features\n",
    "- How to build complete pipelines with scaling and polynomials\n",
    "\n",
    "Ready to learn discretization? Move to **Module 05: Binning and Discretization**!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
