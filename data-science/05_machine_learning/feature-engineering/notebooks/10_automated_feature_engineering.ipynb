{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 10: Automated Feature Engineering\n",
    "\n",
    "**Difficulty**: ‚≠ê‚≠ê‚≠ê Advanced  \n",
    "**Estimated Time**: 70 minutes  \n",
    "**Prerequisites**: Module 09 (Feature Importance and Interpretability)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Understand the concept and benefits of automated feature engineering\n",
    "2. Use polynomial features and interaction generators\n",
    "3. Apply Deep Feature Synthesis concepts\n",
    "4. Create custom automated transformation pipelines\n",
    "5. Compare manual vs automated feature engineering\n",
    "6. Know when automation helps and when it hurts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Automate Feature Engineering?\n",
    "\n",
    "**Manual feature engineering challenges**:\n",
    "- Time-consuming and labor-intensive\n",
    "- Requires deep domain knowledge\n",
    "- May miss complex interactions\n",
    "- Not scalable to many datasets\n",
    "\n",
    "**Automated feature engineering can**:\n",
    "- ‚úÖ Generate hundreds of features automatically\n",
    "- ‚úÖ Discover non-obvious patterns\n",
    "- ‚úÖ Save time in exploration phase\n",
    "- ‚úÖ Provide good baseline features\n",
    "\n",
    "**But automation also**:\n",
    "- ‚ùå Can create too many irrelevant features\n",
    "- ‚ùå Loses interpretability\n",
    "- ‚ùå Increases overfitting risk\n",
    "- ‚ùå Still needs feature selection\n",
    "\n",
    "**Best approach**: Combine manual domain knowledge with automated exploration!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from itertools import combinations\n",
    "\n",
    "# Feature engineering tools\n",
    "from sklearn.preprocessing import PolynomialFeatures, FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Models\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# Configuration\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 3)\n",
    "\n",
    "print(\"‚úì Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Synthetic Dataset\n",
    "\n",
    "We'll create a dataset where important features come from interactions and transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic dataset where target depends on feature interactions\n",
    "n_samples = 1000\n",
    "\n",
    "# Generate base features\n",
    "np.random.seed(42)\n",
    "X_raw = pd.DataFrame({\n",
    "    'feature_1': np.random.uniform(1, 10, n_samples),\n",
    "    'feature_2': np.random.uniform(1, 10, n_samples),\n",
    "    'feature_3': np.random.uniform(1, 10, n_samples),\n",
    "    'feature_4': np.random.uniform(1, 10, n_samples),\n",
    "    'feature_5': np.random.uniform(1, 10, n_samples),\n",
    "})\n",
    "\n",
    "# Target depends on ENGINEERED features (not raw ones!)\n",
    "y = (\n",
    "    5 * (X_raw['feature_1'] * X_raw['feature_2']) +  # Interaction\n",
    "    3 * (X_raw['feature_3'] ** 2) +  # Polynomial\n",
    "    2 * np.sqrt(X_raw['feature_4']) +  # Non-linear transform\n",
    "    1 * (X_raw['feature_1'] + X_raw['feature_5']) +  # Sum\n",
    "    np.random.normal(0, 10, n_samples)  # Noise\n",
    ")\n",
    "\n",
    "print(f\"Dataset: {X_raw.shape}\")\n",
    "print(f\"\\nTrue data generating process:\")\n",
    "print(\"  y = 5*(f1*f2) + 3*(f3¬≤) + 2*‚àöf4 + (f1+f5) + noise\")\n",
    "print(\"\\nChallenge: Can automated feature engineering discover these patterns?\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "X_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Baseline: No Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_raw, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train model with raw features only\n",
    "baseline_model = Ridge(alpha=1.0)\n",
    "baseline_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_baseline = baseline_model.predict(X_test)\n",
    "rmse_baseline = np.sqrt(mean_squared_error(y_test, y_pred_baseline))\n",
    "r2_baseline = r2_score(y_test, y_pred_baseline)\n",
    "\n",
    "print(\"Baseline (Raw Features Only):\")\n",
    "print(f\"  RMSE: {rmse_baseline:.2f}\")\n",
    "print(f\"  R¬≤ Score: {r2_baseline:.3f}\")\n",
    "print(\"\\nNote: Poor performance because model can't discover interactions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Method 1: Polynomial Features\n",
    "\n",
    "**PolynomialFeatures** automatically generates:\n",
    "- Powers of features (x¬≤, x¬≥)\n",
    "- Interactions (x‚ÇÅ √ó x‚ÇÇ, x‚ÇÅ √ó x‚ÇÇ √ó x‚ÇÉ)\n",
    "\n",
    "**Example**: [a, b] with degree=2 ‚Üí [1, a, b, a¬≤, ab, b¬≤]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create polynomial features (degree 2 = include interactions and squares)\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "\n",
    "# Get feature names\n",
    "poly_feature_names = poly.get_feature_names_out(X_raw.columns)\n",
    "\n",
    "print(f\"Original features: {X_train.shape[1]}\")\n",
    "print(f\"After polynomial expansion: {X_train_poly.shape[1]}\")\n",
    "print(f\"\\nGenerated features include:\")\n",
    "print(list(poly_feature_names[:10]), \"...\")\n",
    "print(\"\\nNotice features like 'feature_1 feature_2' (interaction!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model with polynomial features\n",
    "poly_model = Ridge(alpha=1.0)  # Regularization important with many features!\n",
    "poly_model.fit(X_train_poly, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_poly = poly_model.predict(X_test_poly)\n",
    "rmse_poly = np.sqrt(mean_squared_error(y_test, y_pred_poly))\n",
    "r2_poly = r2_score(y_test, y_pred_poly)\n",
    "\n",
    "print(\"With Polynomial Features:\")\n",
    "print(f\"  RMSE: {rmse_poly:.2f} (baseline: {rmse_baseline:.2f})\")\n",
    "print(f\"  R¬≤ Score: {r2_poly:.3f} (baseline: {r2_baseline:.3f})\")\n",
    "print(f\"\\nImprovement: {((rmse_baseline - rmse_poly) / rmse_baseline * 100):.1f}% error reduction!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine which generated features are most important\n",
    "coefficients = pd.Series(poly_model.coef_, index=poly_feature_names)\n",
    "top_features = coefficients.abs().nlargest(10)\n",
    "\n",
    "print(\"Top 10 most important polynomial features:\")\n",
    "print(top_features)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_features.plot(kind='barh', color='steelblue', edgecolor='black')\n",
    "plt.xlabel('Absolute Coefficient Value')\n",
    "plt.title('Most Important Polynomial Features', fontsize=12, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNotice: Model discovered the important interactions!\")\n",
    "print(\"  - 'feature_1 feature_2' (true interaction)\")\n",
    "print(\"  - 'feature_3¬≤' (true polynomial)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Method 2: Custom Automated Transformations\n",
    "\n",
    "Create functions that automatically generate domain-inspired features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mathematical_features(X):\n",
    "    \"\"\"\n",
    "    Automatically create mathematical transformations.\n",
    "    \"\"\"\n",
    "    X_new = X.copy()\n",
    "    \n",
    "    for col in X.columns:\n",
    "        # Non-linear transformations\n",
    "        X_new[f'{col}_squared'] = X[col] ** 2\n",
    "        X_new[f'{col}_sqrt'] = np.sqrt(X[col])\n",
    "        X_new[f'{col}_log'] = np.log(X[col] + 1)  # +1 to avoid log(0)\n",
    "    \n",
    "    return X_new\n",
    "\n",
    "def create_interaction_features(X, max_combinations=2):\n",
    "    \"\"\"\n",
    "    Automatically create interaction features.\n",
    "    \"\"\"\n",
    "    X_new = X.copy()\n",
    "    \n",
    "    # Create pairwise interactions\n",
    "    for col1, col2 in combinations(X.columns, 2):\n",
    "        X_new[f'{col1}_times_{col2}'] = X[col1] * X[col2]\n",
    "        X_new[f'{col1}_div_{col2}'] = X[col1] / (X[col2] + 1e-5)  # Avoid div by 0\n",
    "    \n",
    "    return X_new\n",
    "\n",
    "def create_aggregate_features(X):\n",
    "    \"\"\"\n",
    "    Create aggregate features across all columns.\n",
    "    \"\"\"\n",
    "    X_new = X.copy()\n",
    "    \n",
    "    X_new['sum_all'] = X.sum(axis=1)\n",
    "    X_new['mean_all'] = X.mean(axis=1)\n",
    "    X_new['max_all'] = X.max(axis=1)\n",
    "    X_new['min_all'] = X.min(axis=1)\n",
    "    X_new['std_all'] = X.std(axis=1)\n",
    "    \n",
    "    return X_new\n",
    "\n",
    "# Apply all transformations\n",
    "X_train_auto = X_train.copy()\n",
    "X_test_auto = X_test.copy()\n",
    "\n",
    "# Mathematical transformations\n",
    "X_train_auto = create_mathematical_features(X_train_auto)\n",
    "X_test_auto = create_mathematical_features(X_test_auto)\n",
    "\n",
    "# Interactions\n",
    "X_train_auto = create_interaction_features(X_train_auto)\n",
    "X_test_auto = create_interaction_features(X_test_auto)\n",
    "\n",
    "# Aggregates\n",
    "X_train_auto = create_aggregate_features(X_train_auto)\n",
    "X_test_auto = create_aggregate_features(X_test_auto)\n",
    "\n",
    "print(f\"Original features: {X_train.shape[1]}\")\n",
    "print(f\"After automated engineering: {X_train_auto.shape[1]}\")\n",
    "print(f\"\\nGenerated {X_train_auto.shape[1] - X_train.shape[1]} new features!\")\n",
    "print(f\"\\nSample feature names:\")\n",
    "print(list(X_train_auto.columns[-10:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with automated features\n",
    "auto_model = Ridge(alpha=1.0)\n",
    "auto_model.fit(X_train_auto, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_auto = auto_model.predict(X_test_auto)\n",
    "rmse_auto = np.sqrt(mean_squared_error(y_test, y_pred_auto))\n",
    "r2_auto = r2_score(y_test, y_pred_auto)\n",
    "\n",
    "print(\"With Automated Features:\")\n",
    "print(f\"  RMSE: {rmse_auto:.2f}\")\n",
    "print(f\"  R¬≤ Score: {r2_auto:.3f}\")\n",
    "\n",
    "# Check top features\n",
    "coefficients = pd.Series(auto_model.coef_, index=X_train_auto.columns)\n",
    "top_features = coefficients.abs().nlargest(10)\n",
    "\n",
    "print(f\"\\nTop 10 discovered features:\")\n",
    "for feature, coef in top_features.items():\n",
    "    print(f\"  {feature}: {coef:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Selection After Automation\n",
    "\n",
    "**Problem**: Automated methods create MANY features!\n",
    "**Solution**: Use feature selection to keep only the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top k features using statistical test\n",
    "k = 20\n",
    "selector = SelectKBest(score_func=f_regression, k=k)\n",
    "X_train_selected = selector.fit_transform(X_train_auto, y_train)\n",
    "X_test_selected = selector.transform(X_test_auto)\n",
    "\n",
    "# Get selected feature names\n",
    "selected_features = X_train_auto.columns[selector.get_support()].tolist()\n",
    "\n",
    "print(f\"Selected top {k} features from {X_train_auto.shape[1]} total:\")\n",
    "print(selected_features)\n",
    "\n",
    "# Train with selected features\n",
    "selected_model = Ridge(alpha=1.0)\n",
    "selected_model.fit(X_train_selected, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_selected = selected_model.predict(X_test_selected)\n",
    "rmse_selected = np.sqrt(mean_squared_error(y_test, y_pred_selected))\n",
    "r2_selected = r2_score(y_test, y_pred_selected)\n",
    "\n",
    "print(f\"\\nWith Feature Selection:\")\n",
    "print(f\"  RMSE: {rmse_selected:.2f}\")\n",
    "print(f\"  R¬≤ Score: {r2_selected:.3f}\")\n",
    "print(f\"\\nUsing only {k}/{X_train_auto.shape[1]} features ({k/X_train_auto.shape[1]*100:.1f}%)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Compare All Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive comparison\n",
    "results = pd.DataFrame({\n",
    "    'Method': [\n",
    "        'Baseline (Raw Features)',\n",
    "        'Polynomial Features',\n",
    "        'Automated Features (All)',\n",
    "        'Automated + Selection'\n",
    "    ],\n",
    "    'Num Features': [\n",
    "        X_train.shape[1],\n",
    "        X_train_poly.shape[1],\n",
    "        X_train_auto.shape[1],\n",
    "        k\n",
    "    ],\n",
    "    'RMSE': [rmse_baseline, rmse_poly, rmse_auto, rmse_selected],\n",
    "    'R¬≤ Score': [r2_baseline, r2_poly, r2_auto, r2_selected]\n",
    "})\n",
    "\n",
    "print(\"\\nPerformance Comparison:\")\n",
    "print(\"=\"*70)\n",
    "print(results.to_string(index=False))\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# RMSE comparison\n",
    "axes[0].barh(results['Method'], results['RMSE'], color='coral', edgecolor='black')\n",
    "axes[0].set_xlabel('RMSE (Lower is Better)')\n",
    "axes[0].set_title('Model Error by Method', fontsize=12, fontweight='bold')\n",
    "axes[0].invert_yaxis()\n",
    "axes[0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Feature count vs R¬≤\n",
    "axes[1].scatter(results['Num Features'], results['R¬≤ Score'], \n",
    "               s=200, alpha=0.6, edgecolor='black')\n",
    "for idx, row in results.iterrows():\n",
    "    axes[1].annotate(row['Method'], \n",
    "                    (row['Num Features'], row['R¬≤ Score']),\n",
    "                    fontsize=8, ha='left', va='bottom')\n",
    "axes[1].set_xlabel('Number of Features')\n",
    "axes[1].set_ylabel('R¬≤ Score (Higher is Better)')\n",
    "axes[1].set_title('Performance vs Feature Count', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey insights:\")\n",
    "print(\"1. Automated features dramatically improve performance\")\n",
    "print(\"2. Feature selection maintains performance with fewer features\")\n",
    "print(\"3. More features ‚â† always better (need selection!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. When Automation Helps vs Hurts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate curse of dimensionality with too many features\n",
    "print(\"Demonstrating when automation can HURT:\\n\")\n",
    "\n",
    "# Create dataset where automation creates mostly noise\n",
    "X_simple = pd.DataFrame({\n",
    "    'x1': np.random.randn(200),\n",
    "    'x2': np.random.randn(200),\n",
    "    'x3': np.random.randn(200),\n",
    "})\n",
    "\n",
    "# Simple linear relationship\n",
    "y_simple = 2*X_simple['x1'] + 3*X_simple['x2'] + np.random.randn(200)*0.1\n",
    "\n",
    "X_train_s, X_test_s, y_train_s, y_test_s = train_test_split(\n",
    "    X_simple, y_simple, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Baseline\n",
    "model1 = Ridge(alpha=0.1)\n",
    "model1.fit(X_train_s, y_train_s)\n",
    "r2_simple = r2_score(y_test_s, model1.predict(X_test_s))\n",
    "\n",
    "# With polynomial features (generates many irrelevant features!)\n",
    "poly_extreme = PolynomialFeatures(degree=3, include_bias=False)\n",
    "X_train_poly_s = poly_extreme.fit_transform(X_train_s)\n",
    "X_test_poly_s = poly_extreme.transform(X_test_s)\n",
    "\n",
    "model2 = Ridge(alpha=0.1)  # Same regularization\n",
    "model2.fit(X_train_poly_s, y_train_s)\n",
    "r2_poly_s = r2_score(y_test_s, model2.predict(X_test_poly_s))\n",
    "\n",
    "print(f\"Simple linear data:\")\n",
    "print(f\"  Raw features (3): R¬≤ = {r2_simple:.3f}\")\n",
    "print(f\"  Polynomial features ({X_train_poly_s.shape[1]}): R¬≤ = {r2_poly_s:.3f}\")\n",
    "print(f\"\\nAutomation HURT performance by {(r2_simple - r2_poly_s)*100:.1f}%!\")\n",
    "print(f\"\\nWhy? Generated {X_train_poly_s.shape[1]-3} mostly irrelevant features.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary: When to use automated feature engineering\n",
    "\n",
    "summary = \"\"\"\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë           WHEN AUTOMATED FEATURE ENGINEERING HELPS vs HURTS          ‚ïë\n",
    "‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\n",
    "‚ïë                                                                       ‚ïë\n",
    "‚ïë  ‚úÖ AUTOMATION HELPS WHEN:                                           ‚ïë\n",
    "‚ïë  ‚Ä¢ Target has non-linear relationships                               ‚ïë\n",
    "‚ïë  ‚Ä¢ Feature interactions are important                                ‚ïë\n",
    "‚ïë  ‚Ä¢ You have enough data (>1000 samples)                              ‚ïë\n",
    "‚ïë  ‚Ä¢ Domain knowledge is limited (exploratory phase)                   ‚ïë\n",
    "‚ïë  ‚Ä¢ Using regularization + feature selection                          ‚ïë\n",
    "‚ïë  ‚Ä¢ Tree-based models (handle many features well)                     ‚ïë\n",
    "‚ïë                                                                       ‚ïë\n",
    "‚ïë  ‚ùå AUTOMATION HURTS WHEN:                                           ‚ïë\n",
    "‚ïë  ‚Ä¢ Simple linear relationships                                       ‚ïë\n",
    "‚ïë  ‚Ä¢ Small datasets (<500 samples)                                     ‚ïë\n",
    "‚ïë  ‚Ä¢ Interpretability is critical                                      ‚ïë\n",
    "‚ïë  ‚Ä¢ No feature selection applied                                      ‚ïë\n",
    "‚ïë  ‚Ä¢ Low regularization                                                ‚ïë\n",
    "‚ïë  ‚Ä¢ Production constraints (too many features)                        ‚ïë\n",
    "‚ïë                                                                       ‚ïë\n",
    "‚ïë  üí° BEST PRACTICE:                                                   ‚ïë\n",
    "‚ïë  1. Start with domain-driven manual features                         ‚ïë\n",
    "‚ïë  2. Add automated features for exploration                           ‚ïë\n",
    "‚ïë  3. Apply feature selection aggressively                             ‚ïë\n",
    "‚ïë  4. Use cross-validation to prevent overfitting                      ‚ïë\n",
    "‚ïë  5. Compare with baseline (raw features)                             ‚ïë\n",
    "‚ïë                                                                       ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\"\"\"\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Exercise Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Polynomial Features with Different Degrees\n",
    "\n",
    "Compare polynomial features with degrees 1, 2, and 3. Find the optimal degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Test different polynomial degrees\n",
    "\n",
    "# TODO:\n",
    "# 1. Create polynomial features with degree 1, 2, and 3\n",
    "# 2. Train Ridge models for each\n",
    "# 3. Compare test R¬≤ scores\n",
    "# 4. Plot performance vs degree\n",
    "#\n",
    "# Which degree works best and why?\n",
    "\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to Exercise 1\n",
    "\n",
    "degrees = [1, 2, 3]\n",
    "results = []\n",
    "\n",
    "for degree in degrees:\n",
    "    # Create polynomial features\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    X_train_p = poly.fit_transform(X_train)\n",
    "    X_test_p = poly.transform(X_test)\n",
    "    \n",
    "    # Train model\n",
    "    model = Ridge(alpha=1.0)\n",
    "    model.fit(X_train_p, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    r2 = r2_score(y_test, model.predict(X_test_p))\n",
    "    \n",
    "    results.append({\n",
    "        'Degree': degree,\n",
    "        'Num Features': X_train_p.shape[1],\n",
    "        'R¬≤ Score': r2\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"Polynomial Degree Comparison:\")\n",
    "print(results_df)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(results_df['Degree'], results_df['R¬≤ Score'], marker='o', linewidth=2)\n",
    "plt.xlabel('Polynomial Degree')\n",
    "plt.ylabel('R¬≤ Score')\n",
    "plt.title('Performance vs Polynomial Degree', fontsize=12, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(degrees)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "best_degree = results_df.loc[results_df['R¬≤ Score'].idxmax(), 'Degree']\n",
    "print(f\"\\nOptimal degree: {best_degree}\")\n",
    "print(\"Degree 2 captures interactions and squares (matches our data generating process!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Custom Feature Generator\n",
    "\n",
    "Create a function that generates ratio features (feature_i / feature_j for all pairs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Create ratio features\n",
    "\n",
    "# TODO: Write a function that:\n",
    "# 1. Takes a dataframe X\n",
    "# 2. Creates ratio features for all column pairs\n",
    "# 3. Returns expanded dataframe\n",
    "#\n",
    "# Hint: Use itertools.combinations\n",
    "\n",
    "def create_ratio_features(X):\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "# Test your function\n",
    "# X_with_ratios = create_ratio_features(X_train)\n",
    "# print(f\"Original: {X_train.shape[1]} features\")\n",
    "# print(f\"With ratios: {X_with_ratios.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to Exercise 2\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "def create_ratio_features(X):\n",
    "    \"\"\"\n",
    "    Create ratio features for all column pairs.\n",
    "    \"\"\"\n",
    "    X_new = X.copy()\n",
    "    \n",
    "    # Generate all pairwise ratios\n",
    "    for col1, col2 in combinations(X.columns, 2):\n",
    "        # Avoid division by zero\n",
    "        X_new[f'{col1}_div_{col2}'] = X[col1] / (X[col2] + 1e-5)\n",
    "        X_new[f'{col2}_div_{col1}'] = X[col2] / (X[col1] + 1e-5)\n",
    "    \n",
    "    return X_new\n",
    "\n",
    "# Test\n",
    "X_train_ratios = create_ratio_features(X_train)\n",
    "X_test_ratios = create_ratio_features(X_test)\n",
    "\n",
    "print(f\"Original: {X_train.shape[1]} features\")\n",
    "print(f\"With ratios: {X_train_ratios.shape[1]} features\")\n",
    "print(f\"\\nAdded {X_train_ratios.shape[1] - X_train.shape[1]} ratio features\")\n",
    "print(f\"\\nSample ratio features:\")\n",
    "print(list(X_train_ratios.columns[-10:]))\n",
    "\n",
    "# Train and evaluate\n",
    "model = Ridge(alpha=1.0)\n",
    "model.fit(X_train_ratios, y_train)\n",
    "r2 = r2_score(y_test, model.predict(X_test_ratios))\n",
    "\n",
    "print(f\"\\nR¬≤ with ratio features: {r2:.3f}\")\n",
    "print(f\"Baseline R¬≤: {r2_baseline:.3f}\")\n",
    "print(f\"Improvement: {(r2 - r2_baseline)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Feature Selection Importance\n",
    "\n",
    "Compare automated features WITH and WITHOUT feature selection on a small dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Demonstrate overfitting without selection\n",
    "\n",
    "# Create small dataset (overfitting risk!)\n",
    "X_small = X_raw.sample(100, random_state=42)\n",
    "y_small = y.loc[X_small.index]\n",
    "\n",
    "X_train_small, X_test_small, y_train_small, y_test_small = train_test_split(\n",
    "    X_small, y_small, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Small dataset: {len(X_train_small)} training samples\\n\")\n",
    "\n",
    "# TODO:\n",
    "# 1. Create polynomial features (degree 2)\n",
    "# 2. Train WITHOUT feature selection\n",
    "# 3. Train WITH feature selection (select top 10)\n",
    "# 4. Compare train vs test R¬≤ (check overfitting!)\n",
    "\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to Exercise 3\n",
    "\n",
    "# Create polynomial features\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train_poly_small = poly.fit_transform(X_train_small)\n",
    "X_test_poly_small = poly.transform(X_test_small)\n",
    "\n",
    "print(f\"Features: {X_train_poly_small.shape[1]}\")\n",
    "print(f\"Samples: {X_train_poly_small.shape[0]}\")\n",
    "print(f\"Features > Samples? {X_train_poly_small.shape[1] > X_train_poly_small.shape[0]}\\n\")\n",
    "\n",
    "# 1. WITHOUT feature selection\n",
    "model_no_sel = Ridge(alpha=1.0)\n",
    "model_no_sel.fit(X_train_poly_small, y_train_small)\n",
    "\n",
    "train_r2_no_sel = r2_score(y_train_small, model_no_sel.predict(X_train_poly_small))\n",
    "test_r2_no_sel = r2_score(y_test_small, model_no_sel.predict(X_test_poly_small))\n",
    "\n",
    "# 2. WITH feature selection\n",
    "selector = SelectKBest(score_func=f_regression, k=10)\n",
    "X_train_selected_small = selector.fit_transform(X_train_poly_small, y_train_small)\n",
    "X_test_selected_small = selector.transform(X_test_poly_small)\n",
    "\n",
    "model_with_sel = Ridge(alpha=1.0)\n",
    "model_with_sel.fit(X_train_selected_small, y_train_small)\n",
    "\n",
    "train_r2_with_sel = r2_score(y_train_small, model_with_sel.predict(X_train_selected_small))\n",
    "test_r2_with_sel = r2_score(y_test_small, model_with_sel.predict(X_test_selected_small))\n",
    "\n",
    "# Compare\n",
    "print(\"Performance Comparison (Small Dataset):\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Method':<30} {'Train R¬≤':<12} {'Test R¬≤':<12} {'Overfit'}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Without Selection (all features)':<30} {train_r2_no_sel:<12.3f} {test_r2_no_sel:<12.3f} {train_r2_no_sel - test_r2_no_sel:.3f}\")\n",
    "print(f\"{'With Selection (top 10)':<30} {train_r2_with_sel:<12.3f} {test_r2_with_sel:<12.3f} {train_r2_with_sel - test_r2_with_sel:.3f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nKey observation:\")\n",
    "print(\"Feature selection REDUCES overfitting (smaller train-test gap)\")\n",
    "print(\"This is CRITICAL with small datasets and many features!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Automated feature engineering can discover patterns humans miss**\n",
    "   - Polynomial features find interactions and non-linearities\n",
    "   - Custom transformations apply domain knowledge at scale\n",
    "   - Can dramatically improve model performance\n",
    "\n",
    "2. **Automation creates many features - need feature selection!**\n",
    "   - More features ‚â† better performance\n",
    "   - Without selection ‚Üí overfitting risk\n",
    "   - Select top k features or use regularization\n",
    "\n",
    "3. **When automation helps**:\n",
    "   - Complex non-linear relationships\n",
    "   - Feature interactions important\n",
    "   - Sufficient data (>1000 samples)\n",
    "   - Exploratory phase\n",
    "\n",
    "4. **When automation hurts**:\n",
    "   - Simple linear relationships\n",
    "   - Small datasets\n",
    "   - Need interpretability\n",
    "   - No feature selection applied\n",
    "\n",
    "5. **Best approach: Hybrid strategy**\n",
    "   - Start with manual domain features\n",
    "   - Add automated features for exploration\n",
    "   - Apply aggressive feature selection\n",
    "   - Compare with baseline\n",
    "\n",
    "### Common Automated Techniques\n",
    "\n",
    "**Mathematical Transformations**:\n",
    "- Polynomial (x¬≤, x¬≥)\n",
    "- Square root, log, exponential\n",
    "- Trigonometric (sin, cos)\n",
    "\n",
    "**Interaction Features**:\n",
    "- Multiplication (x‚ÇÅ √ó x‚ÇÇ)\n",
    "- Division (x‚ÇÅ / x‚ÇÇ)\n",
    "- Min/Max combinations\n",
    "\n",
    "**Aggregate Features**:\n",
    "- Sum, mean, std across features\n",
    "- Count of values above/below threshold\n",
    "- Percentiles\n",
    "\n",
    "### Tools and Libraries\n",
    "\n",
    "**Scikit-learn**:\n",
    "- `PolynomialFeatures`: Automatic interactions\n",
    "- `FunctionTransformer`: Custom transformations\n",
    "\n",
    "**Advanced (not covered in detail)**:\n",
    "- `featuretools`: Deep Feature Synthesis\n",
    "- `tsfresh`: Time-series features\n",
    "- `category_encoders`: Advanced categorical encoding\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "**Module 11**: Final Project Pipeline - Build a complete end-to-end feature engineering pipeline combining all techniques\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [Featuretools Documentation](https://docs.featuretools.com/)\n",
    "- [Feature Engineering for Machine Learning](https://www.oreilly.com/library/view/feature-engineering-for/9781491953235/)\n",
    "- [Sklearn Preprocessing](https://scikit-learn.org/stable/modules/preprocessing.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Congratulations!** You've completed Module 10. You now understand:\n",
    "- How to automate feature generation with polynomial features\n",
    "- How to create custom automated transformation pipelines\n",
    "- Why feature selection is critical after automation\n",
    "- When automation helps vs when it hurts\n",
    "- How to combine manual and automated approaches\n",
    "\n",
    "Ready for the final challenge? Let's move to **Module 11: Final Project Pipeline** to put it all together!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
