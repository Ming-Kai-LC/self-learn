# Machine Learning

Core machine learning algorithms and techniques.

## Overview

According to the DataScience_SelfLearnPath.md: **"Andrew Ng's Machine Learning Specialization on Coursera is the universally recommended starting point"** combined with practical implementation using scikit-learn.

This domain covers supervised and unsupervised learning, feature engineering, and ensemble methods - the core skills for 77% of AI-related jobs.

## Projects in This Domain

1. **machine-learning-fundamentals** (80-100 hours) ðŸš§ *Placeholder*
   - Supervised learning (regression, classification)
   - Unsupervised learning (clustering, dimensionality reduction)
   - Model evaluation and selection
   - Hyperparameter tuning
   - Bias-variance tradeoff

2. **feature-engineering** (30-40 hours) ðŸš§ *Placeholder*
   - Handling missing data
   - Encoding categorical variables
   - Feature scaling and normalization
   - Creating interaction features
   - Feature selection methods

3. **ensemble-methods** (40-50 hours) ðŸš§ *Placeholder*
   - XGBoost mastery
   - LightGBM optimization
   - CatBoost for categorical data
   - Stacking and blending
   - Competition-level techniques

## Learning Path

**Recommended order:**
1. machine-learning-fundamentals (core algorithms)
2. feature-engineering (improving performance)
3. ensemble-methods (advanced techniques)

**Time investment:** 6-9 months (Intermediate to Advanced phase)

## Prerequisites

- **mathematics-for-data-science** (statistics, linear algebra)
- **data-science-fundamentals** (NumPy, Pandas)
- **data-visualization-fundamentals** (for result analysis)

## What You'll Achieve

After completing these projects, you will:
- Implement ML algorithms from scratch
- Choose appropriate algorithms for different problems
- Engineer features that improve model performance
- Tune hyperparameters systematically
- Build ensemble models for production
- Win Kaggle competitions (Bronze/Silver medals)
- Understand when deep learning is unnecessary

## Key Algorithms

**Regression**: Linear, Ridge, Lasso, ElasticNet, SVR
**Classification**: Logistic, SVM, Decision Trees, Random Forest, XGBoost
**Clustering**: K-means, DBSCAN, Hierarchical
**Dimensionality Reduction**: PCA, t-SNE, UMAP
**Ensemble**: Bagging, Boosting, Stacking

## Next Steps

Proceed to:
- **06_deep_learning** - Neural networks and deep learning
- **11_portfolio_projects** - Apply ML to real problems
- **09_mlops_deployment** - Deploy ML models

## Roadmap Alignment

**DataScience_SelfLearnPath.md**: Intermediate to Advanced (Months 4-12)
- **Months 4-6**: ML fundamentals and Andrew Ng's course
- **Months 4-6**: Feature engineering (often improves performance more than algorithms)
- **Months 10-12**: Ensemble methods (Kaggle competition winners)

## Industry Relevance

- **77%** of AI jobs require traditional ML (not just deep learning)
- XGBoost/LightGBM dominate tabular data competitions
- Feature engineering separates junior from senior data scientists
- Required skills for data scientist roles at all levels

---

**Status**: ðŸš§ All three projects are placeholders - need development
