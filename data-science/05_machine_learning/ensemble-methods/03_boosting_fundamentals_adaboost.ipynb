{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 03: Boosting Fundamentals and AdaBoost\n",
    "\n",
    "**Difficulty**: ⭐⭐⭐ Advanced\n",
    "**Estimated Time**: 80 minutes\n",
    "**Prerequisites**: \n",
    "- Module 00: Introduction to Ensemble Learning\n",
    "- Module 01: Bagging and Bootstrap\n",
    "- Module 02: Random Forests\n",
    "- Understanding of bias-variance tradeoff\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Explain how boosting differs from bagging (sequential vs parallel)\n",
    "2. Understand the AdaBoost algorithm and adaptive weighting mechanism\n",
    "3. Implement AdaBoost from scratch to understand the mechanics\n",
    "4. Compare bias-variance properties of boosting vs bagging\n",
    "5. Analyze the impact of learning rate on boosting performance\n",
    "6. Identify when boosting is sensitive to outliers and noisy data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.datasets import make_classification, make_moons, load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, learning_curve\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Configuration\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 4)\n",
    "\n",
    "print(\"Setup complete! All libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sequential Learning: The Boosting Paradigm\n",
    "\n",
    "### Bagging vs Boosting: A Fundamental Difference\n",
    "\n",
    "#### Bagging (Parallel Ensemble)\n",
    "```\n",
    "Tree 1 ←┐\n",
    "Tree 2 ←├─ Bootstrap samples (independent)\n",
    "Tree 3 ←┘\n",
    "     ↓\n",
    "  Average\n",
    "```\n",
    "- All trees trained independently in parallel\n",
    "- Each sees different random sample\n",
    "- **Goal**: Reduce variance\n",
    "\n",
    "#### Boosting (Sequential Ensemble)\n",
    "```\n",
    "Tree 1 → identify errors → Tree 2 → identify errors → Tree 3\n",
    "  ↓           ↓                ↓          ↓             ↓\n",
    "pred 1   +  pred 2       +   pred 3   = Final prediction\n",
    "```\n",
    "- Trees trained sequentially, each learning from previous mistakes\n",
    "- Later trees focus on examples that earlier trees got wrong\n",
    "- **Goal**: Reduce bias\n",
    "\n",
    "### Why Sequential Learning Works\n",
    "\n",
    "**Key insight**: Complex patterns can be learned by combining many simple patterns\n",
    "\n",
    "- Start with weak learner (slightly better than random)\n",
    "- Each new learner focuses on \"hard\" examples\n",
    "- Gradually build up complex decision boundary\n",
    "- Final model: weighted combination of all learners\n",
    "\n",
    "**Analogy**: Like building expertise\n",
    "- First, learn basics (easy examples)\n",
    "- Then, focus on edge cases (hard examples)\n",
    "- Eventually master the entire domain\n",
    "\n",
    "### Bias-Variance Perspective\n",
    "\n",
    "**Bagging**:\n",
    "- Uses strong base learners (deep trees)\n",
    "- High variance → Averaging reduces variance\n",
    "- Bias stays roughly the same\n",
    "\n",
    "**Boosting**:\n",
    "- Uses weak base learners (shallow trees, stumps)\n",
    "- High bias → Sequential learning reduces bias\n",
    "- Can increase variance if overtrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual demonstration: Bagging vs Boosting on same dataset\n",
    "# Create non-linear decision boundary\n",
    "X, y = make_moons(n_samples=300, noise=0.3, random_state=RANDOM_STATE)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Train both ensembles with weak base learners\n",
    "base_estimator = DecisionTreeClassifier(max_depth=1, random_state=RANDOM_STATE)  # Stump\n",
    "\n",
    "bagging_model = BaggingClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=1, random_state=RANDOM_STATE),\n",
    "    n_estimators=50,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "boosting_model = AdaBoostClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=1, random_state=RANDOM_STATE),\n",
    "    n_estimators=50,\n",
    "    random_state=RANDOM_STATE,\n",
    "    algorithm='SAMME'\n",
    ")\n",
    "\n",
    "bagging_model.fit(X_train, y_train)\n",
    "boosting_model.fit(X_train, y_train)\n",
    "\n",
    "# Create mesh for decision boundary visualization\n",
    "h = 0.02  # Step size\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "# Plot decision boundaries\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Single weak learner\n",
    "single_tree = DecisionTreeClassifier(max_depth=1, random_state=RANDOM_STATE)\n",
    "single_tree.fit(X_train, y_train)\n",
    "Z_single = single_tree.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "axes[0].contourf(xx, yy, Z_single, alpha=0.4, cmap='RdYlBu')\n",
    "axes[0].scatter(X_train[:, 0], X_train[:, 1], c=y_train, \n",
    "                cmap='RdYlBu', edgecolor='black', alpha=0.7)\n",
    "axes[0].set_title(f'Single Weak Learner (Stump)\\nAccuracy: {single_tree.score(X_test, y_test):.3f}',\n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Feature 1')\n",
    "axes[0].set_ylabel('Feature 2')\n",
    "\n",
    "# Bagging\n",
    "Z_bagging = bagging_model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "axes[1].contourf(xx, yy, Z_bagging, alpha=0.4, cmap='RdYlBu')\n",
    "axes[1].scatter(X_train[:, 0], X_train[:, 1], c=y_train, \n",
    "                cmap='RdYlBu', edgecolor='black', alpha=0.7)\n",
    "axes[1].set_title(f'Bagging (50 stumps)\\nAccuracy: {bagging_model.score(X_test, y_test):.3f}',\n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Feature 1')\n",
    "\n",
    "# Boosting\n",
    "Z_boosting = boosting_model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "axes[2].contourf(xx, yy, Z_boosting, alpha=0.4, cmap='RdYlBu')\n",
    "axes[2].scatter(X_train[:, 0], X_train[:, 1], c=y_train, \n",
    "                cmap='RdYlBu', edgecolor='black', alpha=0.7)\n",
    "axes[2].set_title(f'Boosting (50 stumps)\\nAccuracy: {boosting_model.score(X_test, y_test):.3f}',\n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[2].set_xlabel('Feature 1')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Observation:\")\n",
    "print(\"Boosting creates a much more complex decision boundary from weak learners!\")\n",
    "print(\"Sequential learning allows combining simple patterns into complex ones.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. AdaBoost Algorithm\n",
    "\n",
    "### Adaptive Boosting (AdaBoost)\n",
    "\n",
    "Developed by Freund and Schapire (1996), AdaBoost was the first practical boosting algorithm.\n",
    "\n",
    "### The Algorithm\n",
    "\n",
    "**Initialize**: All samples have equal weight $w_i = 1/N$\n",
    "\n",
    "**For each iteration t = 1 to T**:\n",
    "\n",
    "1. **Train weak learner** on weighted samples\n",
    "   - Samples with higher weights are more important\n",
    "   \n",
    "2. **Calculate weighted error**:\n",
    "   $$\\epsilon_t = \\sum_{i: h_t(x_i) \\neq y_i} w_i$$\n",
    "   - Sum of weights for misclassified samples\n",
    "   \n",
    "3. **Calculate learner weight** (importance of this learner):\n",
    "   $$\\alpha_t = \\frac{1}{2} \\ln\\left(\\frac{1 - \\epsilon_t}{\\epsilon_t}\\right)$$\n",
    "   - Better learners get higher weight\n",
    "   - If $\\epsilon_t = 0.5$ (random), then $\\alpha_t = 0$ (ignored)\n",
    "   - If $\\epsilon_t < 0.5$ (better than random), then $\\alpha_t > 0$\n",
    "   \n",
    "4. **Update sample weights**:\n",
    "   $$w_i \\leftarrow w_i \\times e^{\\alpha_t \\times \\mathbb{1}[h_t(x_i) \\neq y_i]}$$\n",
    "   - Increase weight for misclassified samples\n",
    "   - Decrease weight for correctly classified samples\n",
    "   \n",
    "5. **Normalize weights**: $w_i \\leftarrow w_i / \\sum_j w_j$\n",
    "\n",
    "**Final prediction**:\n",
    "$$H(x) = \\text{sign}\\left(\\sum_{t=1}^T \\alpha_t h_t(x)\\right)$$\n",
    "- Weighted vote of all learners\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "1. **Adaptive weights**: Algorithm focuses on hard examples\n",
    "2. **Exponential loss**: Misclassified samples get exponentially higher weight\n",
    "3. **Weak learner assumption**: Only requires $\\epsilon_t < 0.5$ (better than random)\n",
    "4. **Theoretical guarantee**: Training error decreases exponentially fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate AdaBoost weight evolution\n",
    "# Create simple 1D dataset for visualization\n",
    "np.random.seed(RANDOM_STATE)\n",
    "X_simple = np.random.randn(100, 1)\n",
    "y_simple = (X_simple[:, 0] > 0).astype(int)\n",
    "\n",
    "# Add some noise to make it interesting\n",
    "noise_idx = np.random.choice(100, 10, replace=False)\n",
    "y_simple[noise_idx] = 1 - y_simple[noise_idx]\n",
    "\n",
    "# Train AdaBoost with just 5 estimators to track weight evolution\n",
    "ada = AdaBoostClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=5,\n",
    "    random_state=RANDOM_STATE,\n",
    "    algorithm='SAMME'\n",
    ")\n",
    "\n",
    "# Manually track weight evolution\n",
    "weights_history = []\n",
    "n_samples = len(X_simple)\n",
    "sample_weights = np.ones(n_samples) / n_samples\n",
    "\n",
    "# Store initial weights\n",
    "weights_history.append(sample_weights.copy())\n",
    "\n",
    "# Train and track\n",
    "ada.fit(X_simple, y_simple)\n",
    "\n",
    "# Get estimator weights (alpha values)\n",
    "estimator_weights = ada.estimator_weights_\n",
    "estimator_errors = ada.estimator_errors_\n",
    "\n",
    "# Display information\n",
    "print(\"AdaBoost Training Progress:\")\n",
    "print(\"=\" * 70)\n",
    "for i, (weight, error) in enumerate(zip(estimator_weights, estimator_errors)):\n",
    "    print(f\"Estimator {i+1}:\")\n",
    "    print(f\"  Weighted Error: {error:.4f}\")\n",
    "    print(f\"  Alpha (weight): {weight:.4f}\")\n",
    "    if error < 0.5:\n",
    "        print(f\"  → Better than random, positive contribution\")\n",
    "    elif error > 0.5:\n",
    "        print(f\"  → Worse than random, negative contribution\")\n",
    "    else:\n",
    "        print(f\"  → Random guessing, zero contribution\")\n",
    "    print()\n",
    "\n",
    "# Visualize how alpha relates to error\n",
    "errors = np.linspace(0.01, 0.99, 100)\n",
    "alphas = 0.5 * np.log((1 - errors) / errors)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(errors, alphas, linewidth=2, color='steelblue')\n",
    "plt.axhline(0, color='red', linestyle='--', linewidth=1, label='Zero weight')\n",
    "plt.axvline(0.5, color='green', linestyle='--', linewidth=1, label='Random guessing')\n",
    "plt.scatter(estimator_errors, estimator_weights, color='orange', s=100, \n",
    "            zorder=5, edgecolor='black', label='Our estimators')\n",
    "plt.xlabel('Weighted Error Rate', fontsize=12)\n",
    "plt.ylabel('Estimator Weight (Alpha)', fontsize=12)\n",
    "plt.title('AdaBoost: How Error Affects Estimator Weight', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Insight:\")\n",
    "print(\"- Error < 0.5 → Positive alpha (learner helps)\")\n",
    "print(\"- Error = 0.5 → Zero alpha (learner ignored)\")\n",
    "print(\"- Error > 0.5 → Negative alpha (learner hurts, flip predictions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementing AdaBoost from Scratch\n",
    "\n",
    "Let's build a simple AdaBoost classifier to understand the mechanics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAdaBoost:\n",
    "    \"\"\"\n",
    "    Simple AdaBoost implementation for binary classification.\n",
    "    \n",
    "    This implementation shows the core AdaBoost algorithm:\n",
    "    - Adaptive sample weighting\n",
    "    - Estimator weight calculation\n",
    "    - Weighted voting\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_estimators=50, random_state=None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.random_state = random_state\n",
    "        self.estimators = []\n",
    "        self.estimator_weights = []\n",
    "        self.estimator_errors = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train AdaBoost ensemble.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "        y : array-like, shape (n_samples,)\n",
    "            Labels must be -1 or +1\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        # Initialize sample weights uniformly\n",
    "        sample_weights = np.ones(n_samples) / n_samples\n",
    "        \n",
    "        # Convert labels to -1, +1 if needed\n",
    "        y_encoded = np.where(y == 0, -1, y)\n",
    "        \n",
    "        for i in range(self.n_estimators):\n",
    "            # Train weak learner with weighted samples\n",
    "            estimator = DecisionTreeClassifier(\n",
    "                max_depth=1,  # Decision stump\n",
    "                random_state=self.random_state + i if self.random_state else None\n",
    "            )\n",
    "            estimator.fit(X, y_encoded, sample_weight=sample_weights)\n",
    "            \n",
    "            # Make predictions\n",
    "            predictions = estimator.predict(X)\n",
    "            \n",
    "            # Calculate weighted error\n",
    "            incorrect = (predictions != y_encoded)\n",
    "            weighted_error = np.sum(sample_weights[incorrect]) / np.sum(sample_weights)\n",
    "            \n",
    "            # Avoid division by zero and numerical issues\n",
    "            weighted_error = np.clip(weighted_error, 1e-10, 1 - 1e-10)\n",
    "            \n",
    "            # Calculate estimator weight (alpha)\n",
    "            estimator_weight = 0.5 * np.log((1 - weighted_error) / weighted_error)\n",
    "            \n",
    "            # Update sample weights\n",
    "            # Increase weights for misclassified samples\n",
    "            sample_weights *= np.exp(estimator_weight * incorrect * \n",
    "                                    ((predictions != y_encoded) * 2 - 1))\n",
    "            \n",
    "            # Normalize weights\n",
    "            sample_weights /= np.sum(sample_weights)\n",
    "            \n",
    "            # Store estimator and its weight\n",
    "            self.estimators.append(estimator)\n",
    "            self.estimator_weights.append(estimator_weight)\n",
    "            self.estimator_errors.append(weighted_error)\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions using weighted voting.\n",
    "        \"\"\"\n",
    "        # Get predictions from all estimators\n",
    "        estimator_predictions = np.array([\n",
    "            estimator.predict(X) for estimator in self.estimators\n",
    "        ])\n",
    "        \n",
    "        # Weight predictions by estimator weights\n",
    "        weighted_predictions = np.dot(self.estimator_weights, estimator_predictions)\n",
    "        \n",
    "        # Return sign of weighted sum\n",
    "        # Convert back to 0/1 labels\n",
    "        return np.where(weighted_predictions >= 0, 1, 0)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"Calculate accuracy.\"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions == y)\n",
    "\n",
    "print(\"SimpleAdaBoost class defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test our implementation\n",
    "X, y = make_classification(\n",
    "    n_samples=500,\n",
    "    n_features=10,\n",
    "    n_informative=8,\n",
    "    n_redundant=2,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Train our implementation\n",
    "our_ada = SimpleAdaBoost(n_estimators=50, random_state=RANDOM_STATE)\n",
    "our_ada.fit(X_train, y_train)\n",
    "our_score = our_ada.score(X_test, y_test)\n",
    "\n",
    "# Train sklearn implementation for comparison\n",
    "sklearn_ada = AdaBoostClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=50,\n",
    "    random_state=RANDOM_STATE,\n",
    "    algorithm='SAMME'\n",
    ")\n",
    "sklearn_ada.fit(X_train, y_train)\n",
    "sklearn_score = sklearn_ada.score(X_test, y_test)\n",
    "\n",
    "print(\"Implementation Comparison:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Our AdaBoost:     {our_score:.4f}\")\n",
    "print(f\"Sklearn AdaBoost: {sklearn_score:.4f}\")\n",
    "print(f\"Difference:       {abs(our_score - sklearn_score):.4f}\")\n",
    "print(\"\\nOur implementation works! Small differences due to numerical precision.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize error evolution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Estimator errors over iterations\n",
    "axes[0].plot(range(1, len(our_ada.estimator_errors) + 1), \n",
    "             our_ada.estimator_errors, marker='o', linewidth=2)\n",
    "axes[0].axhline(0.5, color='red', linestyle='--', linewidth=1, label='Random guessing')\n",
    "axes[0].set_xlabel('Estimator Number', fontsize=11)\n",
    "axes[0].set_ylabel('Weighted Error Rate', fontsize=11)\n",
    "axes[0].set_title('Weighted Error for Each Weak Learner', fontsize=12, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Estimator weights\n",
    "axes[1].bar(range(1, len(our_ada.estimator_weights) + 1), \n",
    "            our_ada.estimator_weights, alpha=0.7, edgecolor='black')\n",
    "axes[1].set_xlabel('Estimator Number', fontsize=11)\n",
    "axes[1].set_ylabel('Alpha (Estimator Weight)', fontsize=11)\n",
    "axes[1].set_title('Weight (Importance) of Each Weak Learner', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservation:\")\n",
    "print(\"- Later estimators often have higher error (harder examples)\")\n",
    "print(\"- But they still contribute (error < 0.5)\")\n",
    "print(\"- Estimator weights vary based on performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Bias-Variance Analysis: Boosting vs Bagging\n",
    "\n",
    "Let's empirically compare how boosting and bagging affect bias and variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset with known complexity\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=20,\n",
    "    n_informative=15,\n",
    "    n_redundant=5,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Compare weak learner, bagging, and boosting\n",
    "n_estimators_range = [1, 5, 10, 25, 50, 100, 200]\n",
    "\n",
    "# Single weak learner baseline\n",
    "weak_learner = DecisionTreeClassifier(max_depth=2, random_state=RANDOM_STATE)\n",
    "weak_learner.fit(X_train, y_train)\n",
    "weak_score = weak_learner.score(X_test, y_test)\n",
    "\n",
    "# Test bagging and boosting with increasing ensemble size\n",
    "bagging_scores = []\n",
    "boosting_scores = []\n",
    "\n",
    "for n_est in n_estimators_range:\n",
    "    # Bagging\n",
    "    bagging = BaggingClassifier(\n",
    "        estimator=DecisionTreeClassifier(max_depth=2, random_state=RANDOM_STATE),\n",
    "        n_estimators=n_est,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    bagging.fit(X_train, y_train)\n",
    "    bagging_scores.append(bagging.score(X_test, y_test))\n",
    "    \n",
    "    # Boosting\n",
    "    boosting = AdaBoostClassifier(\n",
    "        estimator=DecisionTreeClassifier(max_depth=2, random_state=RANDOM_STATE),\n",
    "        n_estimators=n_est,\n",
    "        random_state=RANDOM_STATE,\n",
    "        algorithm='SAMME'\n",
    "    )\n",
    "    boosting.fit(X_train, y_train)\n",
    "    boosting_scores.append(boosting.score(X_test, y_test))\n",
    "\n",
    "# Visualize comparison\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.axhline(weak_score, color='gray', linestyle='--', linewidth=2, \n",
    "            label=f'Single Weak Learner: {weak_score:.3f}')\n",
    "plt.plot(n_estimators_range, bagging_scores, marker='o', linewidth=2, \n",
    "         markersize=8, label='Bagging')\n",
    "plt.plot(n_estimators_range, boosting_scores, marker='s', linewidth=2, \n",
    "         markersize=8, label='Boosting (AdaBoost)')\n",
    "plt.xlabel('Number of Estimators', fontsize=12)\n",
    "plt.ylabel('Test Accuracy', fontsize=12)\n",
    "plt.title('Bias-Variance: Bagging vs Boosting with Weak Learners', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Insights:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Single weak learner:     {weak_score:.4f} (high bias)\")\n",
    "print(f\"Bagging (200 learners):  {bagging_scores[-1]:.4f} (modest improvement)\")\n",
    "print(f\"Boosting (200 learners): {boosting_scores[-1]:.4f} (large improvement)\")\n",
    "print(\"\\nWhy?\")\n",
    "print(\"- Bagging: Weak learners have high bias, averaging doesn't fix bias\")\n",
    "print(\"- Boosting: Sequentially reduces bias by focusing on errors\")\n",
    "print(\"- Boosting is MORE effective with weak learners!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Learning Rate Impact\n",
    "\n",
    "The learning rate (also called shrinkage) controls how much each estimator contributes to the final prediction.\n",
    "\n",
    "**Formula**: Instead of using full $\\alpha_t$, use $\\nu \\times \\alpha_t$ where $0 < \\nu \\leq 1$\n",
    "\n",
    "**Trade-off**:\n",
    "- **Higher learning rate** (close to 1): Faster learning, fewer estimators needed, risk of overfitting\n",
    "- **Lower learning rate** (e.g., 0.1): Slower learning, more estimators needed, better generalization\n",
    "\n",
    "**General rule**: Lower learning rate + more estimators = better performance (but longer training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different learning rates\n",
    "learning_rates = [0.01, 0.1, 0.5, 1.0, 2.0]\n",
    "n_estimators = 200\n",
    "\n",
    "# Use breast cancer dataset\n",
    "cancer_data = load_breast_cancer()\n",
    "X, y = cancer_data.data, cancer_data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    ada = AdaBoostClassifier(\n",
    "        estimator=DecisionTreeClassifier(max_depth=1),\n",
    "        n_estimators=n_estimators,\n",
    "        learning_rate=lr,\n",
    "        random_state=RANDOM_STATE,\n",
    "        algorithm='SAMME'\n",
    "    )\n",
    "    ada.fit(X_train, y_train)\n",
    "    \n",
    "    train_score = ada.score(X_train, y_train)\n",
    "    test_score = ada.score(X_test, y_test)\n",
    "    \n",
    "    results.append({\n",
    "        'Learning Rate': lr,\n",
    "        'Train Accuracy': train_score,\n",
    "        'Test Accuracy': test_score,\n",
    "        'Overfit Gap': train_score - test_score\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"Learning Rate Impact:\")\n",
    "print(\"=\" * 70)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Train vs Test accuracy\n",
    "axes[0].plot(learning_rates, results_df['Train Accuracy'], \n",
    "             marker='o', linewidth=2, label='Train Accuracy')\n",
    "axes[0].plot(learning_rates, results_df['Test Accuracy'], \n",
    "             marker='s', linewidth=2, label='Test Accuracy')\n",
    "axes[0].set_xlabel('Learning Rate', fontsize=11)\n",
    "axes[0].set_ylabel('Accuracy', fontsize=11)\n",
    "axes[0].set_title('Learning Rate vs Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xscale('log')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Overfitting gap\n",
    "axes[1].bar(range(len(learning_rates)), results_df['Overfit Gap'], \n",
    "            alpha=0.7, edgecolor='black')\n",
    "axes[1].set_xticks(range(len(learning_rates)))\n",
    "axes[1].set_xticklabels(learning_rates)\n",
    "axes[1].set_xlabel('Learning Rate', fontsize=11)\n",
    "axes[1].set_ylabel('Train - Test Gap', fontsize=11)\n",
    "axes[1].set_title('Overfitting vs Learning Rate', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nRecommendation:\")\n",
    "best_idx = results_df['Test Accuracy'].idxmax()\n",
    "best_lr = results_df.loc[best_idx, 'Learning Rate']\n",
    "print(f\"Best learning rate for this dataset: {best_lr}\")\n",
    "print(\"General advice: Start with 0.1 or 1.0, tune based on validation performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Sensitivity to Outliers and Noise\n",
    "\n",
    "### A Critical Weakness of Boosting\n",
    "\n",
    "**Problem**: Boosting increases weights for misclassified samples\n",
    "- Outliers and mislabeled data get very high weights\n",
    "- Later estimators focus heavily on these noisy samples\n",
    "- Can lead to overfitting on noise\n",
    "\n",
    "**Comparison with Bagging**:\n",
    "- Bagging treats all samples equally (with bootstrap sampling)\n",
    "- Outliers don't get special attention\n",
    "- More robust to noisy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate sensitivity to label noise\n",
    "# Create clean dataset\n",
    "X_clean, y_clean = make_classification(\n",
    "    n_samples=500,\n",
    "    n_features=20,\n",
    "    n_informative=15,\n",
    "    n_redundant=5,\n",
    "    flip_y=0,  # No label noise\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Add increasing amounts of label noise\n",
    "noise_levels = [0.0, 0.05, 0.10, 0.15, 0.20, 0.30]\n",
    "bagging_performance = []\n",
    "boosting_performance = []\n",
    "rf_performance = []\n",
    "\n",
    "for noise in noise_levels:\n",
    "    # Add label noise\n",
    "    y_noisy = y_clean.copy()\n",
    "    n_flip = int(len(y_noisy) * noise)\n",
    "    flip_idx = np.random.choice(len(y_noisy), n_flip, replace=False)\n",
    "    y_noisy[flip_idx] = 1 - y_noisy[flip_idx]\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_clean, y_noisy, test_size=0.3, random_state=RANDOM_STATE\n",
    "    )\n",
    "    \n",
    "    # Train models\n",
    "    bagging = BaggingClassifier(\n",
    "        estimator=DecisionTreeClassifier(max_depth=5, random_state=RANDOM_STATE),\n",
    "        n_estimators=50,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    bagging.fit(X_train, y_train)\n",
    "    bagging_performance.append(bagging.score(X_test, y_test))\n",
    "    \n",
    "    boosting = AdaBoostClassifier(\n",
    "        estimator=DecisionTreeClassifier(max_depth=5, random_state=RANDOM_STATE),\n",
    "        n_estimators=50,\n",
    "        random_state=RANDOM_STATE,\n",
    "        algorithm='SAMME'\n",
    "    )\n",
    "    boosting.fit(X_train, y_train)\n",
    "    boosting_performance.append(boosting.score(X_test, y_test))\n",
    "    \n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=50,\n",
    "        max_depth=5,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    rf.fit(X_train, y_train)\n",
    "    rf_performance.append(rf.score(X_test, y_test))\n",
    "\n",
    "# Visualize robustness to noise\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.plot(noise_levels, bagging_performance, marker='o', linewidth=2, \n",
    "         markersize=8, label='Bagging')\n",
    "plt.plot(noise_levels, boosting_performance, marker='s', linewidth=2, \n",
    "         markersize=8, label='AdaBoost')\n",
    "plt.plot(noise_levels, rf_performance, marker='^', linewidth=2, \n",
    "         markersize=8, label='Random Forest')\n",
    "plt.xlabel('Label Noise Rate', fontsize=12)\n",
    "plt.ylabel('Test Accuracy', fontsize=12)\n",
    "plt.title('Robustness to Label Noise', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate degradation\n",
    "bagging_deg = bagging_performance[0] - bagging_performance[-1]\n",
    "boosting_deg = boosting_performance[0] - boosting_performance[-1]\n",
    "rf_deg = rf_performance[0] - rf_performance[-1]\n",
    "\n",
    "print(\"\\nPerformance Degradation with 30% Label Noise:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Bagging:       {bagging_deg:.4f} drop\")\n",
    "print(f\"AdaBoost:      {boosting_deg:.4f} drop  ← Most affected!\")\n",
    "print(f\"Random Forest: {rf_deg:.4f} drop\")\n",
    "print(\"\\nConclusion:\")\n",
    "print(\"AdaBoost is most sensitive to label noise!\")\n",
    "print(\"Bagging methods (including Random Forest) are more robust.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Weak Learner Strength Analysis\n",
    "\n",
    "Compare AdaBoost performance with different weak learner complexities:\n",
    "- Decision stumps (max_depth=1)\n",
    "- Shallow trees (max_depth=2, 3, 5)\n",
    "- Deeper trees (max_depth=10)\n",
    "\n",
    "For each:\n",
    "1. Train AdaBoost with 100 estimators\n",
    "2. Compare training and test accuracy\n",
    "3. Measure training time\n",
    "4. Analyze the trade-off between weak learner complexity and ensemble size\n",
    "\n",
    "**Question**: What's the optimal weak learner complexity for AdaBoost?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Early Stopping for AdaBoost\n",
    "\n",
    "Implement early stopping for AdaBoost:\n",
    "1. Split training data into train/validation\n",
    "2. Train AdaBoost with many estimators (e.g., 500)\n",
    "3. Track validation accuracy after each estimator is added\n",
    "4. Plot learning curves (train vs validation)\n",
    "5. Identify the optimal stopping point\n",
    "\n",
    "**Bonus**: Implement patience-based early stopping (stop if no improvement for N iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Outlier Removal Strategy\n",
    "\n",
    "Since AdaBoost is sensitive to outliers, implement and test an outlier removal strategy:\n",
    "\n",
    "1. Create a dataset with synthetic outliers\n",
    "2. Train AdaBoost on full data (with outliers)\n",
    "3. Identify potential outliers using:\n",
    "   - Isolation Forest\n",
    "   - Or samples with highest final weights in AdaBoost\n",
    "4. Remove suspected outliers and retrain\n",
    "5. Compare performance\n",
    "\n",
    "**Does outlier removal improve AdaBoost performance?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Learning Rate vs Number of Estimators\n",
    "\n",
    "Explore the relationship between learning rate and ensemble size:\n",
    "\n",
    "1. Create a grid of (learning_rate, n_estimators) combinations\n",
    "2. For each combination, train AdaBoost and record CV accuracy\n",
    "3. Create a heatmap showing performance across the grid\n",
    "4. Find Pareto-optimal combinations (good accuracy with fewer estimators)\n",
    "\n",
    "**Goal**: Understand the trade-off between learning rate and ensemble size for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Boosting Philosophy**:\n",
    "   - Sequential learning: each model corrects previous errors\n",
    "   - Weak learners combine to form strong learner\n",
    "   - Focus on hard examples through adaptive weighting\n",
    "   - Reduces bias, can increase variance if overtrained\n",
    "\n",
    "2. **AdaBoost Algorithm**:\n",
    "   - Initialize equal sample weights\n",
    "   - For each iteration:\n",
    "     - Train weak learner on weighted data\n",
    "     - Calculate weighted error\n",
    "     - Compute learner weight (alpha)\n",
    "     - Increase weights for misclassified samples\n",
    "   - Final prediction: weighted vote\n",
    "\n",
    "3. **Key Parameters**:\n",
    "   - `n_estimators`: More estimators → better performance (until overfitting)\n",
    "   - `learning_rate`: Lower rates need more estimators but generalize better\n",
    "   - `base_estimator`: Weak learners (stumps or shallow trees) work best\n",
    "\n",
    "4. **Strengths**:\n",
    "   - Excellent performance with weak learners\n",
    "   - Theoretically grounded (provable convergence)\n",
    "   - Often achieves better accuracy than bagging\n",
    "   - Automatic feature selection (focuses on informative features)\n",
    "\n",
    "5. **Weaknesses**:\n",
    "   - **Sensitive to outliers and noise** (critical limitation)\n",
    "   - Sequential training (can't parallelize like bagging)\n",
    "   - Risk of overfitting with too many estimators\n",
    "   - Less stable than bagging methods\n",
    "\n",
    "### Bagging vs Boosting Comparison\n",
    "\n",
    "| Aspect | Bagging | Boosting |\n",
    "|--------|---------|----------|\n",
    "| Training | Parallel | Sequential |\n",
    "| Reduces | Variance | Bias |\n",
    "| Base learner | Strong (deep trees) | Weak (stumps) |\n",
    "| Sample weighting | Equal | Adaptive |\n",
    "| Noise sensitivity | Robust | Sensitive |\n",
    "| Overfitting risk | Low | Moderate |\n",
    "| Parallelization | Easy | Hard |\n",
    "\n",
    "### When to Use AdaBoost\n",
    "\n",
    "**Good for**:\n",
    "- Clean datasets with low noise\n",
    "- Binary classification problems\n",
    "- When you need high accuracy and can afford sequential training\n",
    "- Baseline boosting algorithm before trying modern variants\n",
    "\n",
    "**Avoid when**:\n",
    "- Data has many outliers or label noise\n",
    "- Need fast training (use Random Forest)\n",
    "- Need robustness over peak accuracy\n",
    "- Working with very large datasets (use LightGBM/XGBoost instead)\n",
    "\n",
    "### Historical Impact\n",
    "\n",
    "AdaBoost (1996) was groundbreaking:\n",
    "- First practical boosting algorithm\n",
    "- Inspired modern gradient boosting methods\n",
    "- Freund & Schapire won Gödel Prize (2003)\n",
    "- Foundation for XGBoost, LightGBM, CatBoost\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In **Module 04: Gradient Boosting Machines**, we'll explore:\n",
    "- Gradient boosting framework (more general than AdaBoost)\n",
    "- How boosting fits in function space\n",
    "- Different loss functions\n",
    "- Regularization techniques\n",
    "- Connection to gradient descent\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- **Original Paper**: \"A Decision-Theoretic Generalization of On-Line Learning\" (Freund & Schapire, 1997)\n",
    "- **Tutorial**: \"Boosting Algorithms as Gradient Descent\" (Mason et al., 1999)\n",
    "- **Book**: \"Boosting: Foundations and Algorithms\" by Schapire & Freund\n",
    "- **Sklearn Docs**: [AdaBoost Classifier](https://scikit-learn.org/stable/modules/ensemble.html#adaboost)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
