{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 05: XGBoost - Extreme Gradient Boosting\n",
    "\n",
    "**Difficulty**: ⭐⭐\n",
    "**Estimated Time**: 50 minutes\n",
    "**Prerequisites**: \n",
    "- Module 00: Introduction to Ensemble Methods\n",
    "- Module 04: Gradient Boosting Machines\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Understand what makes XGBoost different from standard gradient boosting\n",
    "2. Install and use the XGBoost library for classification and regression\n",
    "3. Explain XGBoost's key innovations: regularization, tree pruning, and parallel processing\n",
    "4. Tune important hyperparameters: max_depth, eta, subsample, colsample_bytree\n",
    "5. Implement early stopping to optimize training\n",
    "6. Analyze and interpret feature importance scores\n",
    "7. Handle missing values effectively with XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to XGBoost\n",
    "\n",
    "### What is XGBoost?\n",
    "\n",
    "**XGBoost** (Extreme Gradient Boosting) is an optimized distributed gradient boosting library designed to be highly efficient, flexible, and portable. Created by Tianqi Chen in 2014, it has become the **most popular machine learning algorithm** for structured data and has won numerous Kaggle competitions.\n",
    "\n",
    "### Why XGBoost is Better than Standard Gradient Boosting:\n",
    "\n",
    "1. **Regularization**: \n",
    "   - Adds L1 (Lasso) and L2 (Ridge) regularization to prevent overfitting\n",
    "   - Standard GB doesn't have built-in regularization\n",
    "\n",
    "2. **Smarter Tree Pruning**:\n",
    "   - Uses \"max_depth\" parameter and then prunes trees backward\n",
    "   - Removes splits that don't provide enough gain (controlled by gamma)\n",
    "   - More efficient than standard GB's greedy approach\n",
    "\n",
    "3. **Built-in Cross-Validation**:\n",
    "   - Can perform CV during training\n",
    "   - Makes hyperparameter tuning easier\n",
    "\n",
    "4. **Handling Missing Values**:\n",
    "   - Automatically learns the best direction for missing values\n",
    "   - No need to impute before training\n",
    "\n",
    "5. **Parallel Processing**:\n",
    "   - While trees are built sequentially, XGBoost parallelizes the construction of each tree\n",
    "   - Much faster than standard GB\n",
    "\n",
    "6. **Tree Pruning Using Depth-First Approach**:\n",
    "   - More efficient memory usage\n",
    "   - Faster training\n",
    "\n",
    "7. **Hardware Optimization**:\n",
    "   - Cache-aware access patterns\n",
    "   - Out-of-core computing for datasets that don't fit in memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install XGBoost if not already installed\n",
    "# Uncomment the line below if you need to install\n",
    "# !pip install xgboost\n",
    "\n",
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from time import time\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.datasets import make_classification, make_regression, load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    mean_squared_error, r2_score, mean_absolute_error\n",
    ")\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Configuration\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check XGBoost version\n",
    "print(f\"XGBoost version: {xgb.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. XGBoost for Classification\n",
    "\n",
    "Let's start with a classification example using the breast cancer dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the breast cancer dataset\n",
    "cancer_data = load_breast_cancer()\n",
    "X = cancer_data.data\n",
    "y = cancer_data.target\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"\\nFeatures: {cancer_data.feature_names[:5]}... (30 total)\")\n",
    "print(f\"Classes: {cancer_data.target_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a basic XGBoost classifier\n",
    "xgb_clf = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss'  # Suppress warning about default metric\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_train = xgb_clf.predict(X_train)\n",
    "y_pred_test = xgb_clf.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "train_acc = accuracy_score(y_train, y_pred_train)\n",
    "test_acc = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"XGBoost Classifier Performance:\")\n",
    "print(f\"Train Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_test, target_names=cancer_data.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=cancer_data.target_names,\n",
    "            yticklabels=cancer_data.target_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('XGBoost Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with Standard Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train sklearn's GradientBoostingClassifier for comparison\n",
    "start_time = time()\n",
    "gb_clf = GradientBoostingClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=42\n",
    ")\n",
    "gb_clf.fit(X_train, y_train)\n",
    "gb_time = time() - start_time\n",
    "gb_acc = gb_clf.score(X_test, y_test)\n",
    "\n",
    "# Train XGBoost again and time it\n",
    "start_time = time()\n",
    "xgb_clf_timed = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "xgb_clf_timed.fit(X_train, y_train)\n",
    "xgb_time = time() - start_time\n",
    "xgb_acc = xgb_clf_timed.score(X_test, y_test)\n",
    "\n",
    "# Compare\n",
    "print(\"Performance Comparison:\")\n",
    "print(f\"\\nStandard Gradient Boosting:\")\n",
    "print(f\"  Accuracy: {gb_acc:.4f}\")\n",
    "print(f\"  Training Time: {gb_time:.3f} seconds\")\n",
    "print(f\"\\nXGBoost:\")\n",
    "print(f\"  Accuracy: {xgb_acc:.4f}\")\n",
    "print(f\"  Training Time: {xgb_time:.3f} seconds\")\n",
    "print(f\"\\nSpeedup: {gb_time/xgb_time:.2f}x faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Key XGBoost Hyperparameters\n",
    "\n",
    "XGBoost has many hyperparameters. Here are the most important ones:\n",
    "\n",
    "### Tree-Specific Parameters:\n",
    "\n",
    "1. **max_depth** (default=6):\n",
    "   - Maximum depth of trees\n",
    "   - Higher values → more complex models, risk of overfitting\n",
    "   - Typical range: 3-10\n",
    "\n",
    "2. **min_child_weight** (default=1):\n",
    "   - Minimum sum of instance weight needed in a child\n",
    "   - Higher values → more conservative, prevents overfitting\n",
    "   - Typical range: 1-10\n",
    "\n",
    "3. **gamma** (default=0):\n",
    "   - Minimum loss reduction required to make a split\n",
    "   - Higher values → more conservative tree pruning\n",
    "   - Typical range: 0-5\n",
    "\n",
    "### Boosting Parameters:\n",
    "\n",
    "4. **eta** (learning_rate, default=0.3):\n",
    "   - Step size shrinkage to prevent overfitting\n",
    "   - Lower values → need more trees but better generalization\n",
    "   - Typical range: 0.01-0.3\n",
    "\n",
    "5. **n_estimators** (default=100):\n",
    "   - Number of boosting rounds (trees)\n",
    "   - More trees → better performance up to a point\n",
    "\n",
    "### Randomness Parameters:\n",
    "\n",
    "6. **subsample** (default=1):\n",
    "   - Fraction of samples used for each tree\n",
    "   - Values < 1.0 prevent overfitting\n",
    "   - Typical range: 0.5-1.0\n",
    "\n",
    "7. **colsample_bytree** (default=1):\n",
    "   - Fraction of features used for each tree\n",
    "   - Similar to Random Forest's max_features\n",
    "   - Typical range: 0.3-1.0\n",
    "\n",
    "8. **colsample_bylevel** (default=1):\n",
    "   - Fraction of features used at each level\n",
    "   - Adds more randomness\n",
    "\n",
    "### Regularization Parameters:\n",
    "\n",
    "9. **reg_alpha** (default=0):\n",
    "   - L1 regularization term on weights\n",
    "   - Higher values → more regularization\n",
    "\n",
    "10. **reg_lambda** (default=1):\n",
    "    - L2 regularization term on weights\n",
    "    - Higher values → more regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of Learning Rate (eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different learning rates\n",
    "learning_rates = [0.01, 0.05, 0.1, 0.3]\n",
    "results = []\n",
    "\n",
    "for eta in learning_rates:\n",
    "    model = XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=eta,\n",
    "        max_depth=3,\n",
    "        random_state=42,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    train_acc = model.score(X_train, y_train)\n",
    "    test_acc = model.score(X_test, y_test)\n",
    "    \n",
    "    results.append({\n",
    "        'learning_rate': eta,\n",
    "        'train_acc': train_acc,\n",
    "        'test_acc': test_acc\n",
    "    })\n",
    "    \n",
    "    print(f\"Learning Rate: {eta}\")\n",
    "    print(f\"  Train: {train_acc:.4f}, Test: {test_acc:.4f}\")\n",
    "\n",
    "# Visualize\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x = np.arange(len(learning_rates))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, results_df['train_acc'], width, label='Train', alpha=0.8)\n",
    "ax.bar(x + width/2, results_df['test_acc'], width, label='Test', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Learning Rate')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Effect of Learning Rate on Performance')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(learning_rates)\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of Max Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different max depths\n",
    "max_depths = [2, 3, 4, 5, 7, 10]\n",
    "depth_results = []\n",
    "\n",
    "for depth in max_depths:\n",
    "    model = XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=depth,\n",
    "        random_state=42,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    train_acc = model.score(X_train, y_train)\n",
    "    test_acc = model.score(X_test, y_test)\n",
    "    \n",
    "    depth_results.append({\n",
    "        'max_depth': depth,\n",
    "        'train_acc': train_acc,\n",
    "        'test_acc': test_acc,\n",
    "        'gap': train_acc - test_acc\n",
    "    })\n",
    "\n",
    "depth_df = pd.DataFrame(depth_results)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Accuracy plot\n",
    "axes[0].plot(depth_df['max_depth'], depth_df['train_acc'], 'o-', \n",
    "             label='Train', linewidth=2, markersize=8)\n",
    "axes[0].plot(depth_df['max_depth'], depth_df['test_acc'], 's-', \n",
    "             label='Test', linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('Max Depth')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('Accuracy vs Max Depth')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Overfitting gap plot\n",
    "axes[1].plot(depth_df['max_depth'], depth_df['gap'], 'ro-', linewidth=2, markersize=8)\n",
    "axes[1].set_xlabel('Max Depth')\n",
    "axes[1].set_ylabel('Train-Test Gap')\n",
    "axes[1].set_title('Overfitting Gap vs Max Depth')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nBest max_depth based on test accuracy:\")\n",
    "best_idx = depth_df['test_acc'].idxmax()\n",
    "print(f\"Max Depth: {depth_df.loc[best_idx, 'max_depth']}\")\n",
    "print(f\"Test Accuracy: {depth_df.loc[best_idx, 'test_acc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of Subsample and Colsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different subsample and colsample_bytree values\n",
    "sample_params = [\n",
    "    (1.0, 1.0),\n",
    "    (0.8, 1.0),\n",
    "    (1.0, 0.8),\n",
    "    (0.8, 0.8),\n",
    "    (0.7, 0.7),\n",
    "    (0.5, 0.5)\n",
    "]\n",
    "\n",
    "sample_results = []\n",
    "\n",
    "for subsample, colsample in sample_params:\n",
    "    model = XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=3,\n",
    "        subsample=subsample,\n",
    "        colsample_bytree=colsample,\n",
    "        random_state=42,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    train_acc = model.score(X_train, y_train)\n",
    "    test_acc = model.score(X_test, y_test)\n",
    "    \n",
    "    sample_results.append({\n",
    "        'subsample': subsample,\n",
    "        'colsample': colsample,\n",
    "        'params': f\"{subsample}/{colsample}\",\n",
    "        'train_acc': train_acc,\n",
    "        'test_acc': test_acc\n",
    "    })\n",
    "    \n",
    "    print(f\"Subsample: {subsample}, Colsample: {colsample}\")\n",
    "    print(f\"  Train: {train_acc:.4f}, Test: {test_acc:.4f}\")\n",
    "\n",
    "# Visualize\n",
    "sample_df = pd.DataFrame(sample_results)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = np.arange(len(sample_params))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, sample_df['train_acc'], width, label='Train', alpha=0.8)\n",
    "ax.bar(x + width/2, sample_df['test_acc'], width, label='Test', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Subsample / Colsample_bytree')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Effect of Subsample and Colsample on Performance')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(sample_df['params'])\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Insight**: Using subsample and colsample_bytree < 1.0 often reduces overfitting (smaller train-test gap) while maintaining good test performance. This adds randomness similar to Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Early Stopping\n",
    "\n",
    "XGBoost has excellent built-in early stopping support. It monitors a validation set and stops training when performance stops improving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a validation set\n",
    "X_train_sub, X_val, y_train_sub, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "# Train with early stopping\n",
    "xgb_early = XGBClassifier(\n",
    "    n_estimators=1000,  # Set high, early stopping will determine actual number\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    early_stopping_rounds=10,  # Stop if no improvement for 10 rounds\n",
    "    random_state=42,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "# Fit with validation set\n",
    "xgb_early.fit(\n",
    "    X_train_sub, y_train_sub,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    verbose=False  # Set to True to see training progress\n",
    ")\n",
    "\n",
    "print(f\"Best iteration: {xgb_early.best_iteration}\")\n",
    "print(f\"Best score: {xgb_early.best_score:.4f}\")\n",
    "print(f\"Total boosting rounds: {xgb_early.n_estimators}\")\n",
    "print(f\"\\nTest Accuracy: {xgb_early.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training progress with eval_set results\n",
    "# Retrain with verbose output to capture evaluation results\n",
    "xgb_verbose = XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "# Fit with both training and validation sets tracked\n",
    "xgb_verbose.fit(\n",
    "    X_train_sub, y_train_sub,\n",
    "    eval_set=[(X_train_sub, y_train_sub), (X_val, y_val)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Get evaluation results\n",
    "results = xgb_verbose.evals_result()\n",
    "train_logloss = results['validation_0']['logloss']\n",
    "val_logloss = results['validation_1']['logloss']\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_logloss, label='Train', linewidth=2)\n",
    "plt.plot(val_logloss, label='Validation', linewidth=2)\n",
    "plt.xlabel('Boosting Round')\n",
    "plt.ylabel('Log Loss')\n",
    "plt.title('XGBoost Training Progress')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Find best iteration\n",
    "best_iter = np.argmin(val_logloss)\n",
    "print(f\"Best iteration: {best_iter}\")\n",
    "print(f\"Best validation log loss: {val_logloss[best_iter]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Importance\n",
    "\n",
    "XGBoost provides multiple types of feature importance:\n",
    "\n",
    "1. **'weight'**: Number of times a feature appears in trees\n",
    "2. **'gain'**: Average gain when the feature is used for splitting (default)\n",
    "3. **'cover'**: Average coverage of the feature when used in trees\n",
    "4. **'total_gain'**: Total gain of the feature\n",
    "5. **'total_cover'**: Total coverage of the feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a model for feature importance analysis\n",
    "xgb_fi = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "xgb_fi.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importance using different metrics\n",
    "importance_types = ['weight', 'gain', 'cover']\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, imp_type in enumerate(importance_types):\n",
    "    # Get importance scores\n",
    "    importance_dict = xgb_fi.get_booster().get_score(importance_type=imp_type)\n",
    "    \n",
    "    # Convert to DataFrame for easier plotting\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': [f\"f{i}\" for i in range(len(cancer_data.feature_names))],\n",
    "        'feature_name': cancer_data.feature_names,\n",
    "        'importance': [importance_dict.get(f\"f{i}\", 0) for i in range(len(cancer_data.feature_names))]\n",
    "    }).sort_values('importance', ascending=False).head(10)\n",
    "    \n",
    "    # Plot\n",
    "    axes[idx].barh(range(len(importance_df)), importance_df['importance'])\n",
    "    axes[idx].set_yticks(range(len(importance_df)))\n",
    "    axes[idx].set_yticklabels(importance_df['feature_name'], fontsize=9)\n",
    "    axes[idx].set_xlabel('Importance Score')\n",
    "    axes[idx].set_title(f'Feature Importance ({imp_type})')\n",
    "    axes[idx].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in XGBoost plot_importance function\n",
    "from xgboost import plot_importance\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "plot_importance(xgb_fi, ax=ax, max_num_features=15, importance_type='gain')\n",
    "plt.title('Top 15 Feature Importances (by Gain)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Handling Missing Values\n",
    "\n",
    "One of XGBoost's powerful features is its ability to handle missing values automatically. It learns the optimal direction to send missing values during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset with missing values\n",
    "np.random.seed(42)\n",
    "X_missing = X_train.copy()\n",
    "\n",
    "# Randomly set 10% of values to NaN\n",
    "mask = np.random.rand(*X_missing.shape) < 0.1\n",
    "X_missing[mask] = np.nan\n",
    "\n",
    "print(f\"Original training data shape: {X_train.shape}\")\n",
    "print(f\"Number of missing values: {np.isnan(X_missing).sum()}\")\n",
    "print(f\"Percentage of missing values: {np.isnan(X_missing).sum() / X_missing.size * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost can handle missing values directly - no imputation needed!\n",
    "xgb_missing = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "# Train on data with missing values\n",
    "xgb_missing.fit(X_missing, y_train)\n",
    "\n",
    "# Evaluate on test set (no missing values)\n",
    "y_pred_missing = xgb_missing.predict(X_test)\n",
    "acc_missing = accuracy_score(y_test, y_pred_missing)\n",
    "\n",
    "# Compare with model trained on complete data\n",
    "xgb_complete = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "xgb_complete.fit(X_train, y_train)\n",
    "acc_complete = xgb_complete.score(X_test, y_test)\n",
    "\n",
    "print(\"Performance Comparison:\")\n",
    "print(f\"\\nModel trained on complete data:\")\n",
    "print(f\"  Test Accuracy: {acc_complete:.4f}\")\n",
    "print(f\"\\nModel trained on data with 10% missing values:\")\n",
    "print(f\"  Test Accuracy: {acc_missing:.4f}\")\n",
    "print(f\"\\nAccuracy difference: {abs(acc_complete - acc_missing):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Insight**: XGBoost handles missing values gracefully without requiring imputation. It learns the optimal default direction for missing values during training, which can actually be more effective than simple imputation strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. XGBoost for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a regression dataset\n",
    "X_reg, y_reg = make_regression(\n",
    "    n_samples=1000,\n",
    "    n_features=10,\n",
    "    n_informative=8,\n",
    "    noise=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Split the data\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train XGBoost Regressor\n",
    "xgb_reg = XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb_reg.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train_reg = xgb_reg.predict(X_train_reg)\n",
    "y_pred_test_reg = xgb_reg.predict(X_test_reg)\n",
    "\n",
    "# Evaluate\n",
    "train_r2 = r2_score(y_train_reg, y_pred_train_reg)\n",
    "test_r2 = r2_score(y_test_reg, y_pred_test_reg)\n",
    "train_mse = mean_squared_error(y_train_reg, y_pred_train_reg)\n",
    "test_mse = mean_squared_error(y_test_reg, y_pred_test_reg)\n",
    "\n",
    "print(\"XGBoost Regressor Performance:\")\n",
    "print(f\"Train R²: {train_r2:.4f}\")\n",
    "print(f\"Test R²: {test_r2:.4f}\")\n",
    "print(f\"Train MSE: {train_mse:.2f}\")\n",
    "print(f\"Test MSE: {test_mse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Training set\n",
    "axes[0].scatter(y_train_reg, y_pred_train_reg, alpha=0.5)\n",
    "axes[0].plot([y_train_reg.min(), y_train_reg.max()],\n",
    "             [y_train_reg.min(), y_train_reg.max()],\n",
    "             'r--', linewidth=2, label='Perfect prediction')\n",
    "axes[0].set_xlabel('Actual values')\n",
    "axes[0].set_ylabel('Predicted values')\n",
    "axes[0].set_title(f'Training Set (R² = {train_r2:.4f})')\n",
    "axes[0].legend()\n",
    "\n",
    "# Test set\n",
    "axes[1].scatter(y_test_reg, y_pred_test_reg, alpha=0.5)\n",
    "axes[1].plot([y_test_reg.min(), y_test_reg.max()],\n",
    "             [y_test_reg.min(), y_test_reg.max()],\n",
    "             'r--', linewidth=2, label='Perfect prediction')\n",
    "axes[1].set_xlabel('Actual values')\n",
    "axes[1].set_ylabel('Predicted values')\n",
    "axes[1].set_title(f'Test Set (R² = {test_r2:.4f})')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Hyperparameter Tuning with Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.3],\n",
    "    'n_estimators': [50, 100],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Create base model\n",
    "xgb_base = XGBClassifier(\n",
    "    random_state=42,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "# Grid search with cross-validation\n",
    "# Note: This can take a while! Using a smaller grid for demonstration\n",
    "grid_search = GridSearchCV(\n",
    "    xgb_base,\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Starting grid search...\")\n",
    "print(f\"Total combinations to test: {len(param_grid['max_depth']) * len(param_grid['learning_rate']) * len(param_grid['n_estimators']) * len(param_grid['subsample']) * len(param_grid['colsample_bytree'])}\")\n",
    "print(\"This may take a few minutes...\\n\")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nBest parameters:\")\n",
    "print(grid_search.best_params_)\n",
    "print(f\"\\nBest cross-validation score: {grid_search.best_score_:.4f}\")\n",
    "print(f\"Test set score: {grid_search.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze grid search results\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Show top 10 parameter combinations\n",
    "print(\"Top 10 parameter combinations:\")\n",
    "print(results_df[['params', 'mean_test_score', 'std_test_score']]\n",
    "      .sort_values('mean_test_score', ascending=False)\n",
    "      .head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Exercises\n",
    "\n",
    "Now it's your turn to practice! Complete the following exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Basic XGBoost Training\n",
    "\n",
    "Create a synthetic classification dataset with 800 samples and 20 features. Train an XGBoost classifier with:\n",
    "- 100 estimators\n",
    "- Learning rate of 0.15\n",
    "- Max depth of 5\n",
    "\n",
    "Calculate and compare training vs test accuracy. Is there overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Step 1: Create dataset using make_classification\n",
    "# Step 2: Split into train/test sets\n",
    "# Step 3: Train XGBClassifier\n",
    "# Step 4: Evaluate and analyze overfitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Regularization Effect\n",
    "\n",
    "Using the breast cancer dataset:\n",
    "1. Train three XGBoost models with different regularization:\n",
    "   - No regularization: reg_alpha=0, reg_lambda=0\n",
    "   - L2 only: reg_alpha=0, reg_lambda=1\n",
    "   - Both L1 and L2: reg_alpha=1, reg_lambda=1\n",
    "2. Compare their training and test accuracies\n",
    "3. Which regularization setting works best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Step 1: Train models with different regularization parameters\n",
    "# Step 2: Evaluate each model\n",
    "# Step 3: Compare results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Early Stopping Analysis\n",
    "\n",
    "Train an XGBoost classifier on the breast cancer data with:\n",
    "- n_estimators=500\n",
    "- Early stopping enabled (early_stopping_rounds=15)\n",
    "- Use a validation set\n",
    "\n",
    "Questions:\n",
    "1. At what iteration does training stop?\n",
    "2. How much computational time is saved compared to training all 500 trees?\n",
    "3. What is the test accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Step 1: Split data into train, validation, and test sets\n",
    "# Step 2: Time the training with early stopping\n",
    "# Step 3: Train without early stopping for comparison\n",
    "# Step 4: Compare iterations and timing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Feature Selection with Importance\n",
    "\n",
    "Using the breast cancer dataset:\n",
    "1. Train an XGBoost model and get feature importances (use 'gain')\n",
    "2. Create a new model using only the top 15 most important features\n",
    "3. Compare the performance of the full model vs the reduced model\n",
    "4. How much does performance degrade with fewer features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Step 1: Train model and extract feature importances\n",
    "# Step 2: Select top 15 features\n",
    "# Step 3: Train new model with reduced features\n",
    "# Step 4: Compare performances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Missing Value Experiment\n",
    "\n",
    "Create an experiment to test XGBoost's missing value handling:\n",
    "1. Create a copy of the breast cancer training data\n",
    "2. Randomly set 20% of values to NaN\n",
    "3. Train two models:\n",
    "   - XGBoost on data with missing values (no imputation)\n",
    "   - XGBoost on data with mean imputation\n",
    "4. Compare test set performance\n",
    "5. Which approach works better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Step 1: Create data with missing values\n",
    "# Step 2: Create imputed version using SimpleImputer\n",
    "# Step 3: Train both models\n",
    "# Step 4: Compare performances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary\n",
    "\n",
    "In this notebook, you learned about XGBoost, the most popular gradient boosting implementation:\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "1. **XGBoost Innovations**:\n",
    "   - L1/L2 regularization prevents overfitting\n",
    "   - Smart tree pruning using gamma parameter\n",
    "   - Parallel processing for faster training\n",
    "   - Native missing value handling\n",
    "   - Built-in cross-validation\n",
    "\n",
    "2. **Important Hyperparameters**:\n",
    "   - **max_depth**: Tree complexity (3-10)\n",
    "   - **eta (learning_rate)**: Step size shrinkage (0.01-0.3)\n",
    "   - **subsample**: Row sampling ratio (0.5-1.0)\n",
    "   - **colsample_bytree**: Column sampling ratio (0.3-1.0)\n",
    "   - **gamma**: Minimum loss reduction for splits\n",
    "   - **reg_alpha, reg_lambda**: Regularization terms\n",
    "\n",
    "3. **Feature Importance Types**:\n",
    "   - 'weight': Frequency of feature usage\n",
    "   - 'gain': Average gain from feature splits (most informative)\n",
    "   - 'cover': Coverage of feature splits\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "1. **Start with conservative defaults**:\n",
    "   ```python\n",
    "   XGBClassifier(\n",
    "       n_estimators=100,\n",
    "       learning_rate=0.1,\n",
    "       max_depth=3,\n",
    "       subsample=0.8,\n",
    "       colsample_bytree=0.8\n",
    "   )\n",
    "   ```\n",
    "\n",
    "2. **Use early stopping** to find optimal number of trees\n",
    "3. **Lower learning rate + more trees** often gives better results\n",
    "4. **Add randomness** via subsample and colsample to reduce overfitting\n",
    "5. **Use regularization** (gamma, reg_alpha, reg_lambda) when overfitting\n",
    "6. **Monitor train/validation curves** to detect overfitting\n",
    "\n",
    "### Advantages:\n",
    "\n",
    "- **State-of-the-art performance** on structured/tabular data\n",
    "- **Fast training** due to parallelization and optimizations\n",
    "- **Handles missing values** automatically\n",
    "- **Built-in regularization** prevents overfitting\n",
    "- **Flexible** - supports custom objectives and metrics\n",
    "- **Feature importance** for interpretability\n",
    "- **Cross-platform** - works on CPU and GPU\n",
    "\n",
    "### Disadvantages:\n",
    "\n",
    "- **Many hyperparameters** to tune\n",
    "- **Can overfit** if not carefully configured\n",
    "- **Sequential training** (though each tree is parallelized)\n",
    "- **Memory intensive** for large datasets\n",
    "- **Less interpretable** than simple models\n",
    "\n",
    "### When to Use XGBoost:\n",
    "\n",
    "✅ **Use XGBoost when:**\n",
    "- Working with structured/tabular data\n",
    "- Need state-of-the-art performance\n",
    "- Have missing values in data\n",
    "- Dataset is small to medium sized (< 10M rows)\n",
    "- Participating in ML competitions\n",
    "\n",
    "❌ **Consider alternatives when:**\n",
    "- Working with images, text, or sequences (use deep learning)\n",
    "- Need real-time predictions with strict latency requirements\n",
    "- Interpretability is critical (use linear models or decision trees)\n",
    "- Dataset is extremely large (consider LightGBM)\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In the next module, we'll explore **LightGBM** (Light Gradient Boosting Machine), Microsoft's gradient boosting framework that's optimized for:\n",
    "- **Faster training speed** on large datasets\n",
    "- **Lower memory usage**\n",
    "- **Better accuracy** in some scenarios\n",
    "- **Native categorical feature** support\n",
    "\n",
    "We'll compare LightGBM with XGBoost and learn when to choose each one!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "- [XGBoost Documentation](https://xgboost.readthedocs.io/)\n",
    "- [XGBoost Parameters Explained](https://xgboost.readthedocs.io/en/latest/parameter.html)\n",
    "- [Original XGBoost Paper (Chen & Guestrin, 2016)](https://arxiv.org/abs/1603.02754)\n",
    "- [XGBoost Python Tutorial](https://xgboost.readthedocs.io/en/latest/python/python_intro.html)\n",
    "- [Complete Guide to Parameter Tuning in XGBoost](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
