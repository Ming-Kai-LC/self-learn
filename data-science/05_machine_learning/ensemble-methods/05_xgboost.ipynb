{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 05: XGBoost\n",
    "\n",
    "**Difficulty**: ⭐⭐⭐ Advanced\n",
    "**Estimated Time**: 90 minutes\n",
    "**Prerequisites**: \n",
    "- Module 00: Introduction to Ensemble Learning\n",
    "- Module 03: Boosting Fundamentals\n",
    "- Module 04: Gradient Boosting Machines\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Understand XGBoost's algorithmic innovations and advantages\n",
    "2. Install and use the XGBoost library effectively\n",
    "3. Tune critical XGBoost hyperparameters for optimal performance\n",
    "4. Apply regularization techniques (L1, L2, gamma) to prevent overfitting\n",
    "5. Use early stopping with validation sets\n",
    "6. Extract and interpret feature importance (gain, cover, frequency)\n",
    "7. Use SHAP values for model interpretation\n",
    "8. Leverage XGBoost's built-in cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.datasets import load_breast_cancer, fetch_california_housing, load_wine\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_auc_score, roc_curve, log_loss,\n",
    "    mean_squared_error, r2_score\n",
    ")\n",
    "\n",
    "# XGBoost\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    print(f\"XGBoost version: {xgb.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"Installing XGBoost...\")\n",
    "    !pip install xgboost -q\n",
    "    import xgboost as xgb\n",
    "    print(f\"XGBoost version: {xgb.__version__}\")\n",
    "\n",
    "# SHAP for interpretability (optional but recommended)\n",
    "try:\n",
    "    import shap\n",
    "    print(f\"SHAP version: {shap.__version__}\")\n",
    "    SHAP_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"SHAP not available. Install with: pip install shap\")\n",
    "    SHAP_AVAILABLE = False\n",
    "\n",
    "# Configuration\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 4)\n",
    "\n",
    "print(\"\\nSetup complete! All libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What Makes XGBoost Special?\n",
    "\n",
    "### XGBoost = eXtreme Gradient Boosting\n",
    "\n",
    "Created by Tianqi Chen (2014), XGBoost dominated Kaggle competitions and became the go-to algorithm for tabular data.\n",
    "\n",
    "### Key Innovations\n",
    "\n",
    "#### 1.1 Regularized Learning Objective\n",
    "\n",
    "Standard gradient boosting minimizes:\n",
    "$$L = \\sum_{i} l(y_i, \\hat{y}_i)$$\n",
    "\n",
    "XGBoost adds regularization:\n",
    "$$L = \\sum_{i} l(y_i, \\hat{y}_i) + \\sum_{k} \\Omega(f_k)$$\n",
    "\n",
    "where $\\Omega(f) = \\gamma T + \\frac{1}{2}\\lambda \\|w\\|^2 + \\alpha \\|w\\|_1$\n",
    "- $T$: number of leaves\n",
    "- $\\gamma$: minimum loss reduction to split (like min_impurity_decrease)\n",
    "- $\\lambda$: L2 regularization on leaf weights\n",
    "- $\\alpha$: L1 regularization on leaf weights\n",
    "\n",
    "#### 1.2 Sparsity-Aware Split Finding\n",
    "- Automatically handles missing values\n",
    "- Learns optimal direction for missing values during training\n",
    "- No need for imputation!\n",
    "\n",
    "#### 1.3 Weighted Quantile Sketch\n",
    "- Efficient algorithm for finding split points\n",
    "- Handles weighted data properly\n",
    "- Much faster than exact greedy search\n",
    "\n",
    "#### 1.4 System Optimizations\n",
    "- Cache-aware access patterns\n",
    "- Out-of-core computing (handles data larger than RAM)\n",
    "- Parallel tree construction\n",
    "- GPU acceleration support\n",
    "- Distributed computing support\n",
    "\n",
    "### Why XGBoost Wins Competitions\n",
    "\n",
    "1. **Performance**: Usually best accuracy on structured data\n",
    "2. **Speed**: 10x+ faster than sklearn GradientBoosting\n",
    "3. **Flexibility**: Many tuning options\n",
    "4. **Robustness**: Handles missing data, large datasets\n",
    "5. **Built-in tools**: CV, early stopping, feature importance\n",
    "\n",
    "### XGBoost vs Sklearn GradientBoosting\n",
    "\n",
    "| Feature | Sklearn GB | XGBoost |\n",
    "|---------|-----------|----------|\n",
    "| Speed | Baseline | 10-50x faster |\n",
    "| Missing values | Need imputation | Automatic |\n",
    "| Regularization | Limited | L1, L2, gamma |\n",
    "| Parallel training | No | Yes |\n",
    "| GPU support | No | Yes |\n",
    "| Built-in CV | No | Yes |\n",
    "| Memory efficiency | Good | Excellent |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick comparison: XGBoost vs Sklearn GradientBoosting\n",
    "# Load dataset\n",
    "cancer_data = load_breast_cancer()\n",
    "X, y = cancer_data.data, cancer_data.target\n",
    "feature_names = cancer_data.feature_names\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"Dataset: {len(X_train)} training samples, {X.shape[1]} features\")\n",
    "print(f\"Classes: {np.unique(y)}, Distribution: {np.bincount(y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train both models with similar parameters\n",
    "n_estimators = 100\n",
    "\n",
    "# Sklearn GradientBoosting\n",
    "print(\"Training Sklearn GradientBoosting...\")\n",
    "start = time.time()\n",
    "gb_sklearn = GradientBoostingClassifier(\n",
    "    n_estimators=n_estimators,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "gb_sklearn.fit(X_train, y_train)\n",
    "sklearn_time = time.time() - start\n",
    "sklearn_acc = gb_sklearn.score(X_test, y_test)\n",
    "\n",
    "# XGBoost\n",
    "print(\"Training XGBoost...\")\n",
    "start = time.time()\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=n_estimators,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=RANDOM_STATE,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_time = time.time() - start\n",
    "xgb_acc = xgb_model.score(X_test, y_test)\n",
    "\n",
    "# Compare\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Comparison Results:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nSklearn GradientBoosting:\")\n",
    "print(f\"  Training time: {sklearn_time:.3f} seconds\")\n",
    "print(f\"  Test accuracy: {sklearn_acc:.4f}\")\n",
    "print(f\"\\nXGBoost:\")\n",
    "print(f\"  Training time: {xgb_time:.3f} seconds\")\n",
    "print(f\"  Test accuracy: {xgb_acc:.4f}\")\n",
    "print(f\"\\nSpeed improvement: {sklearn_time / xgb_time:.1f}x faster\")\n",
    "print(f\"Accuracy difference: {(xgb_acc - sklearn_acc) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. XGBoost APIs\n",
    "\n",
    "XGBoost provides multiple APIs:\n",
    "\n",
    "### 2.1 Scikit-learn API (Easiest)\n",
    "```python\n",
    "xgb.XGBClassifier() / xgb.XGBRegressor()\n",
    "```\n",
    "- Familiar sklearn interface\n",
    "- Easy integration with sklearn pipelines\n",
    "- Good for most use cases\n",
    "\n",
    "### 2.2 Native API (Most Powerful)\n",
    "```python\n",
    "xgb.DMatrix()  # Optimized data structure\n",
    "xgb.train()    # Training function\n",
    "```\n",
    "- More control and features\n",
    "- Slightly faster\n",
    "- Required for advanced features\n",
    "\n",
    "We'll use both in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate native API with DMatrix\n",
    "# DMatrix is XGBoost's optimized data structure\n",
    "\n",
    "# Create DMatrix objects\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train, feature_names=feature_names)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test, feature_names=feature_names)\n",
    "\n",
    "print(\"DMatrix created:\")\n",
    "print(f\"Training samples: {dtrain.num_row()}\")\n",
    "print(f\"Features: {dtrain.num_col()}\")\n",
    "print(f\"\\nDMatrix is more memory efficient and faster for XGBoost!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Critical Hyperparameters\n",
    "\n",
    "XGBoost has many parameters. Here are the most important ones:\n",
    "\n",
    "### 3.1 Tree Parameters\n",
    "\n",
    "**`max_depth`** (default=6): Maximum tree depth\n",
    "- Higher → more complex model, risk of overfitting\n",
    "- Typical: 3-10\n",
    "\n",
    "**`min_child_weight`** (default=1): Minimum sum of instance weight in a leaf\n",
    "- Higher → more conservative\n",
    "- Helps prevent overfitting\n",
    "- Typical: 1-10\n",
    "\n",
    "**`gamma`** (default=0): Minimum loss reduction required to split\n",
    "- Higher → more conservative\n",
    "- Makes algorithm more conservative\n",
    "- Typical: 0-5\n",
    "\n",
    "### 3.2 Boosting Parameters\n",
    "\n",
    "**`learning_rate`** (eta, default=0.3): Step size shrinkage\n",
    "- Lower → need more trees but better generalization\n",
    "- Typical: 0.01-0.3\n",
    "\n",
    "**`n_estimators`** (default=100): Number of trees\n",
    "- Use with early stopping\n",
    "- Typical: 100-1000\n",
    "\n",
    "### 3.3 Sampling Parameters\n",
    "\n",
    "**`subsample`** (default=1): Fraction of samples for each tree\n",
    "- < 1.0 → stochastic gradient boosting\n",
    "- Prevents overfitting\n",
    "- Typical: 0.6-1.0\n",
    "\n",
    "**`colsample_bytree`** (default=1): Fraction of features per tree\n",
    "- Like Random Forest feature randomness\n",
    "- Typical: 0.6-1.0\n",
    "\n",
    "**`colsample_bylevel`**: Fraction of features per level\n",
    "**`colsample_bynode`**: Fraction of features per split\n",
    "\n",
    "### 3.4 Regularization Parameters\n",
    "\n",
    "**`reg_alpha`** (default=0): L1 regularization\n",
    "- Higher → simpler model\n",
    "- Encourages sparsity\n",
    "- Typical: 0-1\n",
    "\n",
    "**`reg_lambda`** (default=1): L2 regularization\n",
    "- Higher → simpler model\n",
    "- Smooths leaf weights\n",
    "- Typical: 1-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of key hyperparameters\n",
    "# We'll test max_depth systematically\n",
    "\n",
    "max_depths = [2, 3, 4, 5, 6, 7, 8, 10]\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for depth in max_depths:\n",
    "    model = xgb.XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=depth,\n",
    "        learning_rate=0.1,\n",
    "        random_state=RANDOM_STATE,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    train_scores.append(model.score(X_train, y_train))\n",
    "    test_scores.append(model.score(X_test, y_test))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(max_depths, train_scores, marker='o', linewidth=2, markersize=8, label='Train Accuracy')\n",
    "plt.plot(max_depths, test_scores, marker='s', linewidth=2, markersize=8, label='Test Accuracy')\n",
    "plt.axvline(6, color='red', linestyle='--', alpha=0.5, label='Default (6)')\n",
    "plt.xlabel('max_depth', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('Effect of Tree Depth', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find optimal\n",
    "best_depth = max_depths[np.argmax(test_scores)]\n",
    "print(f\"\\nOptimal max_depth: {best_depth}\")\n",
    "print(f\"Test accuracy: {max(test_scores):.4f}\")\n",
    "print(f\"Overfitting at depth={max_depths[-1]}: {train_scores[-1] - test_scores[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of regularization (L1 and L2)\n",
    "reg_values = [0, 0.1, 1, 10, 100]\n",
    "\n",
    "# Test L1 regularization (reg_alpha)\n",
    "l1_train = []\n",
    "l1_test = []\n",
    "\n",
    "for alpha in reg_values:\n",
    "    model = xgb.XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=4,\n",
    "        reg_alpha=alpha,\n",
    "        random_state=RANDOM_STATE,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    l1_train.append(model.score(X_train, y_train))\n",
    "    l1_test.append(model.score(X_test, y_test))\n",
    "\n",
    "# Test L2 regularization (reg_lambda)\n",
    "l2_train = []\n",
    "l2_test = []\n",
    "\n",
    "for lambda_val in reg_values:\n",
    "    model = xgb.XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=4,\n",
    "        reg_lambda=lambda_val,\n",
    "        random_state=RANDOM_STATE,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    l2_train.append(model.score(X_train, y_train))\n",
    "    l2_test.append(model.score(X_test, y_test))\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# L1 regularization\n",
    "axes[0].plot(reg_values, l1_train, marker='o', label='Train', linewidth=2)\n",
    "axes[0].plot(reg_values, l1_test, marker='s', label='Test', linewidth=2)\n",
    "axes[0].set_xlabel('L1 Regularization (alpha)', fontsize=11)\n",
    "axes[0].set_ylabel('Accuracy', fontsize=11)\n",
    "axes[0].set_title('L1 Regularization Effect', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xscale('symlog')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# L2 regularization\n",
    "axes[1].plot(reg_values, l2_train, marker='o', label='Train', linewidth=2)\n",
    "axes[1].plot(reg_values, l2_test, marker='s', label='Test', linewidth=2)\n",
    "axes[1].set_xlabel('L2 Regularization (lambda)', fontsize=11)\n",
    "axes[1].set_ylabel('Accuracy', fontsize=11)\n",
    "axes[1].set_title('L2 Regularization Effect', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xscale('symlog')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nRegularization helps reduce overfitting!\")\n",
    "print(\"L1 (alpha): Encourages sparsity, some features ignored\")\n",
    "print(\"L2 (lambda): Smooths weights, reduces magnitude\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Handling Missing Values\n",
    "\n",
    "XGBoost's killer feature: automatic handling of missing values.\n",
    "\n",
    "### How it works:\n",
    "1. During training, XGBoost tries both directions for missing values at each split\n",
    "2. Chooses the direction that improves the loss most\n",
    "3. Remembers this default direction\n",
    "4. At prediction time, missing values go to the learned default direction\n",
    "\n",
    "**No imputation needed!** XGBoost learns the optimal treatment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate missing value handling\n",
    "# Artificially introduce missing values\n",
    "X_missing = X_train.copy()\n",
    "X_test_missing = X_test.copy()\n",
    "\n",
    "# Randomly set 10% of values to NaN\n",
    "np.random.seed(RANDOM_STATE)\n",
    "mask_train = np.random.random(X_missing.shape) < 0.1\n",
    "mask_test = np.random.random(X_test_missing.shape) < 0.1\n",
    "\n",
    "X_missing[mask_train] = np.nan\n",
    "X_test_missing[mask_test] = np.nan\n",
    "\n",
    "print(f\"Training data: {np.isnan(X_missing).sum()} missing values ({np.isnan(X_missing).sum() / X_missing.size * 100:.1f}%)\")\n",
    "print(f\"Test data: {np.isnan(X_test_missing).sum()} missing values ({np.isnan(X_test_missing).sum() / X_test_missing.size * 100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost on data with missing values (no imputation!)\n",
    "xgb_missing = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.1,\n",
    "    random_state=RANDOM_STATE,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "xgb_missing.fit(X_missing, y_train)\n",
    "missing_score = xgb_missing.score(X_test_missing, y_test)\n",
    "\n",
    "# Compare with model trained on complete data\n",
    "xgb_complete = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.1,\n",
    "    random_state=RANDOM_STATE,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "xgb_complete.fit(X_train, y_train)\n",
    "complete_score = xgb_complete.score(X_test, y_test)\n",
    "\n",
    "print(\"\\nPerformance Comparison:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Complete data:      {complete_score:.4f}\")\n",
    "print(f\"10% missing values: {missing_score:.4f}\")\n",
    "print(f\"Accuracy drop:      {(complete_score - missing_score) * 100:.2f}%\")\n",
    "print(\"\\nXGBoost handles missing values gracefully!\")\n",
    "print(\"No imputation needed - it learns optimal treatment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Early Stopping with Validation Set\n",
    "\n",
    "XGBoost makes early stopping easy with the `eval_set` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data: train, validation, test\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Train with early stopping\n",
    "xgb_es = xgb.XGBClassifier(\n",
    "    n_estimators=500,  # Set high, early stopping will find optimal\n",
    "    max_depth=4,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=RANDOM_STATE,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "# Fit with validation monitoring and early stopping\n",
    "xgb_es.fit(\n",
    "    X_train_split, y_train_split,\n",
    "    eval_set=[(X_train_split, y_train_split), (X_val, y_val)],\n",
    "    early_stopping_rounds=20,  # Stop if no improvement for 20 rounds\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"Early Stopping Results:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Best iteration: {xgb_es.best_iteration}\")\n",
    "print(f\"Best validation score: {xgb_es.best_score:.4f}\")\n",
    "print(f\"\\nStopped at {xgb_es.best_iteration} instead of 500!\")\n",
    "print(f\"Saved training {500 - xgb_es.best_iteration} unnecessary iterations.\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_acc = xgb_es.score(X_test, y_test)\n",
    "print(f\"\\nFinal test accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize learning curves\n",
    "results = xgb_es.evals_result()\n",
    "\n",
    "train_logloss = results['validation_0']['logloss']\n",
    "val_logloss = results['validation_1']['logloss']\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train_logloss, label='Train', linewidth=2)\n",
    "plt.plot(val_logloss, label='Validation', linewidth=2)\n",
    "plt.axvline(xgb_es.best_iteration, color='red', linestyle='--', linewidth=2,\n",
    "            label=f'Best iteration ({xgb_es.best_iteration})')\n",
    "plt.xlabel('Boosting Round', fontsize=12)\n",
    "plt.ylabel('Log Loss', fontsize=12)\n",
    "plt.title('Training Progress with Early Stopping', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Observation:\")\n",
    "print(\"Validation loss stops improving, triggering early stopping.\")\n",
    "print(\"This prevents overfitting and saves computation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Importance\n",
    "\n",
    "XGBoost provides three types of feature importance:\n",
    "\n",
    "### 6.1 Weight (Frequency)\n",
    "- Number of times feature is used in splits\n",
    "- Simple count\n",
    "\n",
    "### 6.2 Gain (Most Important!)\n",
    "- Average gain when feature is used\n",
    "- Measures improvement in loss\n",
    "- **Most informative** for feature importance\n",
    "\n",
    "### 6.3 Cover\n",
    "- Average number of samples affected when feature is used\n",
    "- Indicates breadth of impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model for feature importance analysis\n",
    "xgb_importance = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.1,\n",
    "    random_state=RANDOM_STATE,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "xgb_importance.fit(X_train, y_train)\n",
    "\n",
    "# Get different importance types\n",
    "importance_weight = xgb_importance.get_booster().get_score(importance_type='weight')\n",
    "importance_gain = xgb_importance.get_booster().get_score(importance_type='gain')\n",
    "importance_cover = xgb_importance.get_booster().get_score(importance_type='cover')\n",
    "\n",
    "# Convert to DataFrame for easier handling\n",
    "def importance_to_df(importance_dict, feature_names):\n",
    "    # XGBoost uses f0, f1, etc. as feature names\n",
    "    importance_df = pd.DataFrame([\n",
    "        {'Feature': feature_names[int(k[1:])], 'Importance': v}\n",
    "        for k, v in importance_dict.items()\n",
    "    ])\n",
    "    return importance_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "df_weight = importance_to_df(importance_weight, feature_names)\n",
    "df_gain = importance_to_df(importance_gain, feature_names)\n",
    "df_cover = importance_to_df(importance_cover, feature_names)\n",
    "\n",
    "print(\"Top 10 Features by Gain (Most Important):\")\n",
    "print(\"=\" * 70)\n",
    "print(df_gain.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize all three importance types\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot top 10 for each type\n",
    "for ax, df, title in zip(axes, [df_weight, df_gain, df_cover], \n",
    "                          ['Weight (Frequency)', 'Gain (Average Improvement)', 'Cover (Sample Count)']):\n",
    "    top_features = df.head(10)\n",
    "    ax.barh(range(len(top_features)), top_features['Importance'], color='steelblue', edgecolor='black')\n",
    "    ax.set_yticks(range(len(top_features)))\n",
    "    ax.set_yticklabels(top_features['Feature'], fontsize=9)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_xlabel('Importance Score', fontsize=10)\n",
    "    ax.set_title(title, fontsize=11, fontweight='bold')\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Weight: How often feature is used\")\n",
    "print(\"- Gain: How much improvement when used (BEST for interpretation)\")\n",
    "print(\"- Cover: How many samples affected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. SHAP Values for Interpretability\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) provides the most principled way to interpret model predictions.\n",
    "\n",
    "### Why SHAP?\n",
    "- Based on game theory (Shapley values)\n",
    "- Shows contribution of each feature to each prediction\n",
    "- Handles feature interactions properly\n",
    "- Can explain individual predictions or global patterns\n",
    "\n",
    "### SHAP for XGBoost\n",
    "- Fast implementation using TreeSHAP\n",
    "- Industry standard for model interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHAP_AVAILABLE:\n",
    "    # Create SHAP explainer\n",
    "    explainer = shap.TreeExplainer(xgb_importance)\n",
    "    \n",
    "    # Calculate SHAP values for test set (use subset for speed)\n",
    "    X_test_sample = X_test[:100]\n",
    "    shap_values = explainer.shap_values(X_test_sample)\n",
    "    \n",
    "    print(\"SHAP values computed!\")\n",
    "    print(f\"Shape: {shap_values.shape}\")\n",
    "    print(f\"One SHAP value per feature per sample\")\n",
    "    print(f\"\\nPositive SHAP → pushes prediction toward class 1\")\n",
    "    print(f\"Negative SHAP → pushes prediction toward class 0\")\n",
    "else:\n",
    "    print(\"SHAP not installed. Install with: pip install shap\")\n",
    "    print(\"Skipping SHAP analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHAP_AVAILABLE:\n",
    "    # Global feature importance (average absolute SHAP)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    shap.summary_plot(shap_values, X_test_sample, feature_names=feature_names, \n",
    "                     plot_type='bar', show=False)\n",
    "    plt.title('Global Feature Importance (SHAP)', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nThis shows which features are most important overall.\")\n",
    "    print(\"Based on average absolute SHAP values across all predictions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHAP_AVAILABLE:\n",
    "    # Detailed SHAP summary plot\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    shap.summary_plot(shap_values, X_test_sample, feature_names=feature_names, show=False)\n",
    "    plt.title('SHAP Summary Plot', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nHow to read this plot:\")\n",
    "    print(\"- Each dot is one prediction\")\n",
    "    print(\"- X-axis: SHAP value (impact on prediction)\")\n",
    "    print(\"- Color: Feature value (red=high, blue=low)\")\n",
    "    print(\"- Example: High 'worst perimeter' → high SHAP → predicts malignant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHAP_AVAILABLE:\n",
    "    # Explain individual prediction\n",
    "    sample_idx = 0\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    shap.waterfall_plot(\n",
    "        shap.Explanation(\n",
    "            values=shap_values[sample_idx],\n",
    "            base_values=explainer.expected_value,\n",
    "            data=X_test_sample[sample_idx],\n",
    "            feature_names=feature_names\n",
    "        ),\n",
    "        show=False\n",
    "    )\n",
    "    plt.title(f'SHAP Explanation for Sample {sample_idx}', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    actual = y_test[sample_idx]\n",
    "    predicted = xgb_importance.predict(X_test_sample[sample_idx:sample_idx+1])[0]\n",
    "    \n",
    "    print(f\"\\nSample {sample_idx}:\")\n",
    "    print(f\"Actual class: {actual}\")\n",
    "    print(f\"Predicted class: {predicted}\")\n",
    "    print(f\"\\nWaterfall shows how each feature contributes to final prediction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Built-in Cross-Validation\n",
    "\n",
    "XGBoost has efficient built-in cross-validation with `xgb.cv()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data in DMatrix format for native API\n",
    "dtrain_full = xgb.DMatrix(X_train, label=y_train)\n",
    "\n",
    "# Set parameters\n",
    "params = {\n",
    "    'max_depth': 4,\n",
    "    'eta': 0.1,  # learning rate\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'seed': RANDOM_STATE\n",
    "}\n",
    "\n",
    "# Run cross-validation\n",
    "print(\"Running 5-fold cross-validation...\")\n",
    "cv_results = xgb.cv(\n",
    "    params=params,\n",
    "    dtrain=dtrain_full,\n",
    "    num_boost_round=200,\n",
    "    nfold=5,\n",
    "    stratified=True,\n",
    "    early_stopping_rounds=20,\n",
    "    verbose_eval=False,\n",
    "    seed=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(\"\\nCross-Validation Results:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Best iteration: {len(cv_results)}\")\n",
    "print(f\"Best train logloss: {cv_results['train-logloss-mean'].iloc[-1]:.4f}\")\n",
    "print(f\"Best test logloss:  {cv_results['test-logloss-mean'].iloc[-1]:.4f}\")\n",
    "print(f\"Std deviation:      {cv_results['test-logloss-std'].iloc[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize CV results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(cv_results['train-logloss-mean'], label='Train (mean)', linewidth=2)\n",
    "plt.plot(cv_results['test-logloss-mean'], label='CV (mean)', linewidth=2)\n",
    "plt.fill_between(\n",
    "    range(len(cv_results)),\n",
    "    cv_results['test-logloss-mean'] - cv_results['test-logloss-std'],\n",
    "    cv_results['test-logloss-mean'] + cv_results['test-logloss-std'],\n",
    "    alpha=0.2,\n",
    "    label='CV std'\n",
    ")\n",
    "plt.xlabel('Boosting Round', fontsize=12)\n",
    "plt.ylabel('Log Loss', fontsize=12)\n",
    "plt.title('Cross-Validation Learning Curves', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nBuilt-in CV is fast and convenient!\")\n",
    "print(\"Use for hyperparameter tuning without extra code.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Comprehensive Hyperparameter Tuning\n",
    "\n",
    "Use RandomizedSearchCV or GridSearchCV to tune XGBoost on a regression dataset:\n",
    "\n",
    "Parameters to tune:\n",
    "- `max_depth`: [3, 5, 7]\n",
    "- `learning_rate`: [0.01, 0.05, 0.1]\n",
    "- `n_estimators`: [100, 200, 500]\n",
    "- `subsample`: [0.6, 0.8, 1.0]\n",
    "- `colsample_bytree`: [0.6, 0.8, 1.0]\n",
    "- `reg_alpha`: [0, 0.1, 1]\n",
    "- `reg_lambda`: [1, 10, 100]\n",
    "\n",
    "1. Find best parameters\n",
    "2. Analyze which parameters matter most\n",
    "3. Compare with default parameters\n",
    "4. Measure training time vs performance trade-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Learning Rate Schedule Experiment\n",
    "\n",
    "Test the hypothesis: \"Lower learning rate + more estimators = better performance\"\n",
    "\n",
    "Train models with:\n",
    "1. lr=0.3, n_estimators=50\n",
    "2. lr=0.1, n_estimators=150\n",
    "3. lr=0.03, n_estimators=500\n",
    "4. lr=0.01, n_estimators=1500\n",
    "\n",
    "For each:\n",
    "- Use early stopping\n",
    "- Track train/validation performance\n",
    "- Measure training time\n",
    "- Plot learning curves\n",
    "\n",
    "**Is there a clear winner? What's the sweet spot?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Feature Engineering Impact\n",
    "\n",
    "On a dataset of your choice:\n",
    "\n",
    "1. Train baseline XGBoost on raw features\n",
    "2. Engineer new features:\n",
    "   - Polynomial features (degree 2)\n",
    "   - Interaction terms\n",
    "   - Binned/discretized features\n",
    "3. Train XGBoost on engineered features\n",
    "4. Use SHAP to see which engineered features are important\n",
    "5. Compare performance\n",
    "\n",
    "**Does XGBoost benefit from feature engineering, or does it capture interactions automatically?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Imbalanced Classification Challenge\n",
    "\n",
    "Create or load a highly imbalanced dataset (90:10 or worse). Apply multiple strategies:\n",
    "\n",
    "1. **No adjustment**: Standard XGBoost\n",
    "2. **Scale_pos_weight**: Set to (negative samples / positive samples)\n",
    "3. **Custom eval_metric**: Use AUC instead of logloss\n",
    "4. **Threshold tuning**: Adjust decision threshold on probabilities\n",
    "5. **Focal loss**: Implement custom objective (advanced)\n",
    "\n",
    "Compare using:\n",
    "- ROC-AUC\n",
    "- Precision-Recall curve\n",
    "- F1 score\n",
    "- Confusion matrix\n",
    "\n",
    "**Which approach works best?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Monotonic Constraints\n",
    "\n",
    "XGBoost supports monotonic constraints: force feature relationships to be monotonic.\n",
    "\n",
    "Use a dataset where you know the relationship (e.g., house prices):\n",
    "- Square footage should increase price (monotonic increasing)\n",
    "- Age might decrease price (monotonic decreasing)\n",
    "\n",
    "1. Train without constraints\n",
    "2. Train with monotonic constraints\n",
    "3. Compare predictions and feature relationships\n",
    "4. Visualize how constraints affect decision boundaries\n",
    "\n",
    "**Hint**: Use `monotone_constraints` parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **XGBoost Innovations**:\n",
    "   - Regularized objective (L1, L2, gamma)\n",
    "   - Sparsity-aware algorithm (automatic missing value handling)\n",
    "   - Weighted quantile sketch (efficient split finding)\n",
    "   - System optimizations (parallel, cache-aware, out-of-core)\n",
    "\n",
    "2. **Critical Hyperparameters**:\n",
    "   - `max_depth`: Tree complexity (3-10)\n",
    "   - `learning_rate`: Shrinkage (0.01-0.3)\n",
    "   - `n_estimators`: Number of trees (use early stopping)\n",
    "   - `subsample`, `colsample_bytree`: Randomness\n",
    "   - `reg_alpha`, `reg_lambda`: Regularization\n",
    "   - `gamma`: Minimum split loss reduction\n",
    "\n",
    "3. **Best Practices**:\n",
    "   - Use early stopping with validation set\n",
    "   - Start with defaults, tune systematically\n",
    "   - Lower learning rate + more trees = better generalization\n",
    "   - Use regularization to prevent overfitting\n",
    "   - Leverage built-in CV for hyperparameter search\n",
    "\n",
    "4. **Feature Importance**:\n",
    "   - **Gain**: Most informative (average improvement)\n",
    "   - Weight: Frequency of use\n",
    "   - Cover: Sample count\n",
    "   - Use SHAP for principled interpretation\n",
    "\n",
    "5. **Advantages Over Sklearn GB**:\n",
    "   - 10-50x faster\n",
    "   - Handles missing values automatically\n",
    "   - More regularization options\n",
    "   - Parallel training\n",
    "   - Built-in CV and early stopping\n",
    "   - Production-ready features\n",
    "\n",
    "### Strengths\n",
    "\n",
    "- State-of-the-art performance on tabular data\n",
    "- Fast training (parallelized)\n",
    "- Handles missing values elegantly\n",
    "- Extensive regularization options\n",
    "- Production-ready (used at scale)\n",
    "- Great documentation and community\n",
    "- GPU support for massive datasets\n",
    "\n",
    "### Weaknesses\n",
    "\n",
    "- Many hyperparameters (can be overwhelming)\n",
    "- Requires careful tuning for optimal performance\n",
    "- Slower than LightGBM on very large datasets\n",
    "- Not ideal for extrapolation\n",
    "- Can overfit with poor hyperparameters\n",
    "\n",
    "### When to Use XGBoost\n",
    "\n",
    "**Ideal for**:\n",
    "- Kaggle competitions (proven winner)\n",
    "- Production ML systems (fast + accurate)\n",
    "- Tabular data with complex patterns\n",
    "- When you need interpretability (SHAP)\n",
    "- Data with missing values\n",
    "\n",
    "**Consider alternatives**:\n",
    "- LightGBM: Even faster, especially on large data\n",
    "- CatBoost: Better for categorical features\n",
    "- Neural nets: Images, text, audio\n",
    "- Linear models: Need interpretability, linear relationships\n",
    "\n",
    "### Tuning Strategy\n",
    "\n",
    "**Phase 1: Tree structure**\n",
    "1. Fix `learning_rate=0.1`, `n_estimators=100`\n",
    "2. Tune `max_depth` (3-10)\n",
    "3. Tune `min_child_weight` (1-10)\n",
    "4. Tune `gamma` (0-5)\n",
    "\n",
    "**Phase 2: Randomness**\n",
    "5. Tune `subsample` (0.6-1.0)\n",
    "6. Tune `colsample_bytree` (0.6-1.0)\n",
    "\n",
    "**Phase 3: Regularization**\n",
    "7. Tune `reg_alpha` (0-10)\n",
    "8. Tune `reg_lambda` (1-100)\n",
    "\n",
    "**Phase 4: Learning rate**\n",
    "9. Lower `learning_rate` (0.01-0.05)\n",
    "10. Increase `n_estimators` (500-1000+)\n",
    "11. Use early stopping\n",
    "\n",
    "### Production Tips\n",
    "\n",
    "1. **Model serialization**: Use `model.save_model()` and `load_model()`\n",
    "2. **Version control**: Save hyperparameters with model\n",
    "3. **Monitoring**: Track feature importance changes over time\n",
    "4. **Retraining**: Set up pipeline for regular retraining\n",
    "5. **A/B testing**: Compare model versions in production\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In **Module 06: LightGBM**, we'll explore:\n",
    "- Microsoft's gradient boosting implementation\n",
    "- GOSS and EFB algorithms for speed\n",
    "- Leaf-wise vs level-wise growth\n",
    "- Histogram-based learning\n",
    "- Comparison with XGBoost\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- **Paper**: \"XGBoost: A Scalable Tree Boosting System\" (Chen & Guestrin, 2016)\n",
    "- **Documentation**: [XGBoost Official Docs](https://xgboost.readthedocs.io/)\n",
    "- **Tutorial**: [Complete Guide to Parameter Tuning](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/)\n",
    "- **SHAP**: [Official SHAP documentation](https://shap.readthedocs.io/)\n",
    "- **Book**: \"Hands-On Gradient Boosting with XGBoost and scikit-learn\" by Corey Wade"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
