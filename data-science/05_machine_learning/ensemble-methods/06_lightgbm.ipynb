{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 06: LightGBM - Light Gradient Boosting Machine\n",
    "\n",
    "**Difficulty**: ⭐⭐\n",
    "**Estimated Time**: 50 minutes\n",
    "**Prerequisites**: \n",
    "- Module 04: Gradient Boosting Machines\n",
    "- Module 05: XGBoost\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Understand what makes LightGBM unique: GOSS, EFB, and leaf-wise growth\n",
    "2. Install and use LightGBM for classification and regression tasks\n",
    "3. Compare LightGBM's speed and accuracy with XGBoost\n",
    "4. Tune LightGBM-specific hyperparameters effectively\n",
    "5. Handle categorical features natively without encoding\n",
    "6. Optimize LightGBM for large datasets\n",
    "7. Understand when to choose LightGBM over XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to LightGBM\n",
    "\n",
    "### What is LightGBM?\n",
    "\n",
    "**LightGBM** (Light Gradient Boosting Machine) is a gradient boosting framework developed by Microsoft in 2017. It's designed to be:\n",
    "- **Faster** than XGBoost\n",
    "- **More memory efficient**\n",
    "- **Better on large datasets** (millions of rows)\n",
    "- **Accurate** with lower computational cost\n",
    "\n",
    "### Key Innovations:\n",
    "\n",
    "#### 1. **GOSS (Gradient-based One-Side Sampling)**\n",
    "\n",
    "Instead of using all data points:\n",
    "- Keeps all instances with **large gradients** (poorly predicted samples)\n",
    "- Randomly samples instances with **small gradients** (well-predicted samples)\n",
    "- **Why?** Instances with large gradients contribute more to information gain\n",
    "- **Result:** Faster training without significant accuracy loss\n",
    "\n",
    "#### 2. **EFB (Exclusive Feature Bundling)**\n",
    "\n",
    "- Bundles mutually exclusive features (features that rarely take non-zero values simultaneously)\n",
    "- Reduces the number of features without losing information\n",
    "- **Why?** Many real-world datasets have sparse features (e.g., one-hot encoded)\n",
    "- **Result:** Lower dimensionality, faster training\n",
    "\n",
    "#### 3. **Leaf-wise (Best-first) Tree Growth**\n",
    "\n",
    "- **XGBoost:** Grows trees level-wise (depth-wise) - splits all nodes at each level\n",
    "- **LightGBM:** Grows trees leaf-wise - splits the leaf with maximum loss reduction\n",
    "- **Advantage:** More accurate models with fewer leaves\n",
    "- **Caution:** Can overfit on small datasets if max_depth is not set\n",
    "\n",
    "```\n",
    "Level-wise (XGBoost):          Leaf-wise (LightGBM):\n",
    "        Root                           Root\n",
    "       /    \\                         /    \\\n",
    "      A      B                       A      B*\n",
    "     / \\    / \\                     / \\      \\\n",
    "    C   D  E   F                   C   D      E\n",
    "                                              \\\n",
    "                                               F\n",
    "    \n",
    "* = Best leaf to split next\n",
    "```\n",
    "\n",
    "### LightGBM vs XGBoost:\n",
    "\n",
    "| Feature | XGBoost | LightGBM |\n",
    "|---------|---------|----------|\n",
    "| Speed | Fast | **Faster** |\n",
    "| Memory | Moderate | **Lower** |\n",
    "| Tree Growth | Level-wise | **Leaf-wise** |\n",
    "| Large Datasets | Good | **Better** |\n",
    "| Small Datasets | Better | Can overfit |\n",
    "| Categorical Features | Need encoding | **Native support** |\n",
    "| GPU Support | Yes | **Yes (faster)** |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install LightGBM if not already installed\n",
    "# Uncomment the line below if needed\n",
    "# !pip install lightgbm\n",
    "\n",
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from time import time\n",
    "\n",
    "# LightGBM\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "\n",
    "# XGBoost for comparison\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.datasets import make_classification, make_regression, load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    mean_squared_error, r2_score, log_loss\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check version\n",
    "print(f\"LightGBM version: {lgb.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LightGBM for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load breast cancer dataset\n",
    "cancer_data = load_breast_cancer()\n",
    "X = cancer_data.data\n",
    "y = cancer_data.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"Features: {len(cancer_data.feature_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a basic LightGBM classifier\n",
    "lgbm_clf = LGBMClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=42,\n",
    "    verbose=-1  # Suppress warnings\n",
    ")\n",
    "\n",
    "# Time the training\n",
    "start_time = time()\n",
    "lgbm_clf.fit(X_train, y_train)\n",
    "lgbm_time = time() - start_time\n",
    "\n",
    "# Predictions\n",
    "y_pred_train = lgbm_clf.predict(X_train)\n",
    "y_pred_test = lgbm_clf.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "train_acc = accuracy_score(y_train, y_pred_train)\n",
    "test_acc = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"LightGBM Classifier Performance:\")\n",
    "print(f\"Training time: {lgbm_time:.4f} seconds\")\n",
    "print(f\"Train Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_test, target_names=cancer_data.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens',\n",
    "            xticklabels=cancer_data.target_names,\n",
    "            yticklabels=cancer_data.target_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('LightGBM Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Speed Comparison: LightGBM vs XGBoost\n",
    "\n",
    "Let's create a larger dataset to see the speed difference more clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a larger dataset for speed comparison\n",
    "X_large, y_large = make_classification(\n",
    "    n_samples=50000,\n",
    "    n_features=50,\n",
    "    n_informative=30,\n",
    "    n_redundant=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train_large, X_test_large, y_train_large, y_test_large = train_test_split(\n",
    "    X_large, y_large, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Large dataset - Training: {X_train_large.shape}\")\n",
    "print(f\"Large dataset - Test: {X_test_large.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LightGBM\n",
    "print(\"Training LightGBM...\")\n",
    "start_time = time()\n",
    "lgbm_large = LGBMClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "lgbm_large.fit(X_train_large, y_train_large)\n",
    "lgbm_large_time = time() - start_time\n",
    "lgbm_large_acc = lgbm_large.score(X_test_large, y_test_large)\n",
    "\n",
    "print(f\"LightGBM - Time: {lgbm_large_time:.2f}s, Accuracy: {lgbm_large_acc:.4f}\")\n",
    "\n",
    "# Train XGBoost\n",
    "print(\"\\nTraining XGBoost...\")\n",
    "start_time = time()\n",
    "xgb_large = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss',\n",
    "    verbosity=0\n",
    ")\n",
    "xgb_large.fit(X_train_large, y_train_large)\n",
    "xgb_large_time = time() - start_time\n",
    "xgb_large_acc = xgb_large.score(X_test_large, y_test_large)\n",
    "\n",
    "print(f\"XGBoost - Time: {xgb_large_time:.2f}s, Accuracy: {xgb_large_acc:.4f}\")\n",
    "\n",
    "# Compare\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"Comparison:\")\n",
    "print(f\"Speedup: LightGBM is {xgb_large_time/lgbm_large_time:.2f}x faster\")\n",
    "print(f\"Accuracy difference: {abs(lgbm_large_acc - xgb_large_acc):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Training time comparison\n",
    "models = ['LightGBM', 'XGBoost']\n",
    "times = [lgbm_large_time, xgb_large_time]\n",
    "accuracies = [lgbm_large_acc, xgb_large_acc]\n",
    "\n",
    "axes[0].bar(models, times, color=['green', 'orange'], alpha=0.7)\n",
    "axes[0].set_ylabel('Training Time (seconds)')\n",
    "axes[0].set_title('Training Speed Comparison')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "for i, (model, t) in enumerate(zip(models, times)):\n",
    "    axes[0].text(i, t + 0.1, f'{t:.2f}s', ha='center', va='bottom')\n",
    "\n",
    "# Accuracy comparison\n",
    "axes[1].bar(models, accuracies, color=['green', 'orange'], alpha=0.7)\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Accuracy Comparison')\n",
    "axes[1].set_ylim([min(accuracies) - 0.01, 1.0])\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "for i, (model, acc) in enumerate(zip(models, accuracies)):\n",
    "    axes[1].text(i, acc + 0.001, f'{acc:.4f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. LightGBM Hyperparameters\n",
    "\n",
    "LightGBM has similar parameters to XGBoost but with some differences:\n",
    "\n",
    "### Core Parameters:\n",
    "\n",
    "1. **num_leaves** (default=31):\n",
    "   - Maximum number of leaves in one tree\n",
    "   - **Key difference from XGBoost:** Controls tree complexity instead of max_depth\n",
    "   - Rule of thumb: num_leaves < 2^max_depth\n",
    "   - Typical range: 20-100\n",
    "\n",
    "2. **max_depth** (default=-1, unlimited):\n",
    "   - Still important to prevent overfitting\n",
    "   - Set to limit tree depth even with leaf-wise growth\n",
    "   - Typical range: 3-12\n",
    "\n",
    "3. **learning_rate** (default=0.1):\n",
    "   - Same as XGBoost\n",
    "   - Typical range: 0.01-0.3\n",
    "\n",
    "4. **n_estimators** (default=100):\n",
    "   - Number of boosting rounds\n",
    "\n",
    "### LightGBM-Specific Parameters:\n",
    "\n",
    "5. **min_child_samples** (default=20):\n",
    "   - Minimum number of data points in a leaf\n",
    "   - Prevents overfitting\n",
    "   - Higher values → more conservative\n",
    "\n",
    "6. **subsample** (bagging_fraction, default=1.0):\n",
    "   - Fraction of data to use for each iteration\n",
    "   - Need to set bagging_freq > 0 to enable\n",
    "\n",
    "7. **colsample_bytree** (feature_fraction, default=1.0):\n",
    "   - Fraction of features to use for each tree\n",
    "\n",
    "8. **reg_alpha** (lambda_l1, default=0):\n",
    "   - L1 regularization\n",
    "\n",
    "9. **reg_lambda** (lambda_l2, default=0):\n",
    "   - L2 regularization\n",
    "\n",
    "10. **min_split_gain** (min_gain_to_split, default=0):\n",
    "    - Minimum gain to make a split\n",
    "    - Similar to gamma in XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of num_leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different num_leaves values\n",
    "num_leaves_values = [7, 15, 31, 63, 127]\n",
    "leaves_results = []\n",
    "\n",
    "for num_leaves in num_leaves_values:\n",
    "    model = LGBMClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        num_leaves=num_leaves,\n",
    "        random_state=42,\n",
    "        verbose=-1\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    train_acc = model.score(X_train, y_train)\n",
    "    test_acc = model.score(X_test, y_test)\n",
    "    \n",
    "    leaves_results.append({\n",
    "        'num_leaves': num_leaves,\n",
    "        'train_acc': train_acc,\n",
    "        'test_acc': test_acc,\n",
    "        'gap': train_acc - test_acc\n",
    "    })\n",
    "    \n",
    "    print(f\"num_leaves={num_leaves}: Train={train_acc:.4f}, Test={test_acc:.4f}\")\n",
    "\n",
    "leaves_df = pd.DataFrame(leaves_results)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Accuracy plot\n",
    "axes[0].plot(leaves_df['num_leaves'], leaves_df['train_acc'], 'o-', \n",
    "             label='Train', linewidth=2, markersize=8)\n",
    "axes[0].plot(leaves_df['num_leaves'], leaves_df['test_acc'], 's-', \n",
    "             label='Test', linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('Number of Leaves')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('Accuracy vs Number of Leaves')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Overfitting gap\n",
    "axes[1].plot(leaves_df['num_leaves'], leaves_df['gap'], 'ro-', \n",
    "             linewidth=2, markersize=8)\n",
    "axes[1].set_xlabel('Number of Leaves')\n",
    "axes[1].set_ylabel('Train-Test Gap')\n",
    "axes[1].set_title('Overfitting Gap vs Number of Leaves')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nBest num_leaves based on test accuracy:\")\n",
    "best_idx = leaves_df['test_acc'].idxmax()\n",
    "print(f\"num_leaves: {leaves_df.loc[best_idx, 'num_leaves']}\")\n",
    "print(f\"Test Accuracy: {leaves_df.loc[best_idx, 'test_acc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of min_child_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different min_child_samples values\n",
    "min_samples_values = [5, 10, 20, 50, 100]\n",
    "samples_results = []\n",
    "\n",
    "for min_samples in min_samples_values:\n",
    "    model = LGBMClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        num_leaves=31,\n",
    "        min_child_samples=min_samples,\n",
    "        random_state=42,\n",
    "        verbose=-1\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    train_acc = model.score(X_train, y_train)\n",
    "    test_acc = model.score(X_test, y_test)\n",
    "    \n",
    "    samples_results.append({\n",
    "        'min_child_samples': min_samples,\n",
    "        'train_acc': train_acc,\n",
    "        'test_acc': test_acc\n",
    "    })\n",
    "    \n",
    "    print(f\"min_child_samples={min_samples}: Train={train_acc:.4f}, Test={test_acc:.4f}\")\n",
    "\n",
    "samples_df = pd.DataFrame(samples_results)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(samples_df['min_child_samples'], samples_df['train_acc'], 'o-', \n",
    "         label='Train', linewidth=2, markersize=8)\n",
    "plt.plot(samples_df['min_child_samples'], samples_df['test_acc'], 's-', \n",
    "         label='Test', linewidth=2, markersize=8)\n",
    "plt.xlabel('min_child_samples')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Effect of min_child_samples on Performance')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Categorical Features Support\n",
    "\n",
    "One of LightGBM's most powerful features is **native categorical feature support**. You don't need to one-hot encode categorical variables!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset with categorical features\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "# Numerical features\n",
    "X_num = np.random.randn(n_samples, 5)\n",
    "\n",
    "# Categorical features\n",
    "cat_feature1 = np.random.choice(['A', 'B', 'C', 'D'], n_samples)\n",
    "cat_feature2 = np.random.choice(['Low', 'Medium', 'High'], n_samples)\n",
    "cat_feature3 = np.random.choice(range(10), n_samples)  # Integer categories\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(X_num, columns=[f'num_{i}' for i in range(5)])\n",
    "df['category1'] = cat_feature1\n",
    "df['category2'] = cat_feature2\n",
    "df['category3'] = cat_feature3\n",
    "\n",
    "# Create target based on features (including categoricals)\n",
    "y_cat = (X_num[:, 0] + \n",
    "         (df['category1'] == 'A').astype(int) * 2 + \n",
    "         (df['category2'] == 'High').astype(int) * 1.5 +\n",
    "         np.random.randn(n_samples) * 0.5) > 1\n",
    "y_cat = y_cat.astype(int)\n",
    "\n",
    "print(\"Dataset with categorical features:\")\n",
    "print(df.head())\n",
    "print(f\"\\nShape: {df.shape}\")\n",
    "print(f\"Target distribution: {pd.Series(y_cat).value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for LightGBM\n",
    "# Convert categorical columns to 'category' dtype\n",
    "for col in ['category1', 'category2', 'category3']:\n",
    "    df[col] = df[col].astype('category')\n",
    "\n",
    "# Split data\n",
    "X_cat = df\n",
    "X_train_cat, X_test_cat, y_train_cat, y_test_cat = train_test_split(\n",
    "    X_cat, y_cat, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Get categorical feature indices\n",
    "categorical_features = ['category1', 'category2', 'category3']\n",
    "\n",
    "print(\"Categorical features identified:\")\n",
    "print(categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LightGBM with categorical features\n",
    "lgbm_cat = LGBMClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    num_leaves=31,\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "# Specify categorical features\n",
    "lgbm_cat.fit(\n",
    "    X_train_cat, y_train_cat,\n",
    "    categorical_feature=categorical_features\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "train_acc_cat = lgbm_cat.score(X_train_cat, y_train_cat)\n",
    "test_acc_cat = lgbm_cat.score(X_test_cat, y_test_cat)\n",
    "\n",
    "print(\"LightGBM with Native Categorical Support:\")\n",
    "print(f\"Train Accuracy: {train_acc_cat:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc_cat:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with one-hot encoding approach\n",
    "X_encoded = pd.get_dummies(X_cat, columns=['category1', 'category2'], drop_first=True)\n",
    "# Convert category3 to numeric if not already\n",
    "X_encoded['category3'] = X_encoded['category3'].astype(int)\n",
    "\n",
    "X_train_enc, X_test_enc, y_train_enc, y_test_enc = train_test_split(\n",
    "    X_encoded, y_cat, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train with one-hot encoded features\n",
    "start_time = time()\n",
    "lgbm_encoded = LGBMClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    num_leaves=31,\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "lgbm_encoded.fit(X_train_enc, y_train_enc)\n",
    "encoded_time = time() - start_time\n",
    "\n",
    "# Train with categorical features\n",
    "start_time = time()\n",
    "lgbm_cat_timed = LGBMClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    num_leaves=31,\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "lgbm_cat_timed.fit(X_train_cat, y_train_cat, categorical_feature=categorical_features)\n",
    "cat_time = time() - start_time\n",
    "\n",
    "# Compare\n",
    "print(\"Comparison: One-Hot Encoding vs Native Categorical:\")\n",
    "print(f\"\\nOne-Hot Encoded:\")\n",
    "print(f\"  Number of features: {X_encoded.shape[1]}\")\n",
    "print(f\"  Training time: {encoded_time:.4f}s\")\n",
    "print(f\"  Test Accuracy: {lgbm_encoded.score(X_test_enc, y_test_enc):.4f}\")\n",
    "print(f\"\\nNative Categorical:\")\n",
    "print(f\"  Number of features: {X_cat.shape[1]}\")\n",
    "print(f\"  Training time: {cat_time:.4f}s\")\n",
    "print(f\"  Test Accuracy: {lgbm_cat_timed.score(X_test_cat, y_test_cat):.4f}\")\n",
    "print(f\"\\nSpeedup: {encoded_time/cat_time:.2f}x faster with native categorical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Insight**: LightGBM's native categorical support is not only more convenient (no need for encoding) but also often faster and can lead to better performance by finding optimal splits directly on categorical values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Early Stopping and Model Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create validation set\n",
    "X_train_sub, X_val, y_train_sub, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "# Train with early stopping and callbacks\n",
    "lgbm_early = LGBMClassifier(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=31,\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "# Fit with early stopping\n",
    "lgbm_early.fit(\n",
    "    X_train_sub, y_train_sub,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_metric='binary_logloss',\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=20, verbose=False)]\n",
    ")\n",
    "\n",
    "print(f\"Best iteration: {lgbm_early.best_iteration_}\")\n",
    "print(f\"Best score: {lgbm_early.best_score_['valid_0']['binary_logloss']:.4f}\")\n",
    "print(f\"Test Accuracy: {lgbm_early.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training progress\n",
    "# Train another model to capture full training history\n",
    "lgbm_history = LGBMClassifier(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=31,\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "# Use callbacks to log evaluation results\n",
    "evals_result = {}\n",
    "lgbm_history.fit(\n",
    "    X_train_sub, y_train_sub,\n",
    "    eval_set=[(X_train_sub, y_train_sub), (X_val, y_val)],\n",
    "    eval_metric='binary_logloss',\n",
    "    callbacks=[lgb.record_evaluation(evals_result)]\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "train_logloss = evals_result['training']['binary_logloss']\n",
    "val_logloss = evals_result['valid_1']['binary_logloss']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_logloss, label='Training', linewidth=2)\n",
    "plt.plot(val_logloss, label='Validation', linewidth=2)\n",
    "plt.xlabel('Boosting Round')\n",
    "plt.ylabel('Log Loss')\n",
    "plt.title('LightGBM Training Progress')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Find best iteration\n",
    "best_iter = np.argmin(val_logloss)\n",
    "print(f\"Optimal iteration: {best_iter}\")\n",
    "print(f\"Best validation log loss: {val_logloss[best_iter]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "feature_importance = lgbm_clf.feature_importances_\n",
    "feature_names = cancer_data.feature_names\n",
    "\n",
    "# Create DataFrame\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importance\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"Top 10 Most Important Features:\")\n",
    "print(importance_df.head(10))\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_features = importance_df.head(15)\n",
    "plt.barh(range(len(top_features)), top_features['Importance'], color='green', alpha=0.7)\n",
    "plt.yticks(range(len(top_features)), top_features['Feature'])\n",
    "plt.xlabel('Feature Importance (Gain)')\n",
    "plt.title('Top 15 Feature Importances - LightGBM')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM also supports split-based and gain-based importance\n",
    "# Get importance using the booster object\n",
    "lgb.plot_importance(lgbm_clf, max_num_features=15, importance_type='gain', \n",
    "                   figsize=(10, 8), title='Feature Importance (Gain)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "lgb.plot_importance(lgbm_clf, max_num_features=15, importance_type='split',\n",
    "                   figsize=(10, 8), title='Feature Importance (Split Count)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. LightGBM for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create regression dataset\n",
    "X_reg, y_reg = make_regression(\n",
    "    n_samples=1000,\n",
    "    n_features=10,\n",
    "    n_informative=8,\n",
    "    noise=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train LightGBM Regressor\n",
    "lgbm_reg = LGBMRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    num_leaves=31,\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "lgbm_reg.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train_reg = lgbm_reg.predict(X_train_reg)\n",
    "y_pred_test_reg = lgbm_reg.predict(X_test_reg)\n",
    "\n",
    "# Evaluate\n",
    "train_r2 = r2_score(y_train_reg, y_pred_train_reg)\n",
    "test_r2 = r2_score(y_test_reg, y_pred_test_reg)\n",
    "train_mse = mean_squared_error(y_train_reg, y_pred_train_reg)\n",
    "test_mse = mean_squared_error(y_test_reg, y_pred_test_reg)\n",
    "\n",
    "print(\"LightGBM Regressor Performance:\")\n",
    "print(f\"Train R²: {train_r2:.4f}\")\n",
    "print(f\"Test R²: {test_r2:.4f}\")\n",
    "print(f\"Train MSE: {train_mse:.2f}\")\n",
    "print(f\"Test MSE: {test_mse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axes[0].scatter(y_train_reg, y_pred_train_reg, alpha=0.5, color='green')\n",
    "axes[0].plot([y_train_reg.min(), y_train_reg.max()],\n",
    "             [y_train_reg.min(), y_train_reg.max()],\n",
    "             'r--', linewidth=2, label='Perfect prediction')\n",
    "axes[0].set_xlabel('Actual values')\n",
    "axes[0].set_ylabel('Predicted values')\n",
    "axes[0].set_title(f'Training Set (R² = {train_r2:.4f})')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].scatter(y_test_reg, y_pred_test_reg, alpha=0.5, color='green')\n",
    "axes[1].plot([y_test_reg.min(), y_test_reg.max()],\n",
    "             [y_test_reg.min(), y_test_reg.max()],\n",
    "             'r--', linewidth=2, label='Perfect prediction')\n",
    "axes[1].set_xlabel('Actual values')\n",
    "axes[1].set_ylabel('Predicted values')\n",
    "axes[1].set_title(f'Test Set (R² = {test_r2:.4f})')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Basic LightGBM Training\n",
    "\n",
    "Create a synthetic classification dataset with 1000 samples and 25 features. Train a LightGBM classifier with:\n",
    "- 150 estimators\n",
    "- Learning rate of 0.05\n",
    "- num_leaves of 50\n",
    "\n",
    "Compare training and test accuracy. Is there overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Speed Comparison on Large Data\n",
    "\n",
    "Create a large dataset with 100,000 samples and 100 features. Train both LightGBM and XGBoost with identical parameters:\n",
    "- 100 estimators\n",
    "- Learning rate 0.1\n",
    "- Max depth 5\n",
    "\n",
    "Compare:\n",
    "1. Training time\n",
    "2. Test accuracy\n",
    "3. Which is faster and by how much?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Categorical Feature Handling\n",
    "\n",
    "Create a dataset with:\n",
    "- 5 numerical features\n",
    "- 3 categorical features (with 5, 10, and 20 categories respectively)\n",
    "- 2000 samples\n",
    "\n",
    "Train two models:\n",
    "1. LightGBM with native categorical support\n",
    "2. LightGBM with one-hot encoded features\n",
    "\n",
    "Compare performance and training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Hyperparameter Grid Search\n",
    "\n",
    "Using the breast cancer dataset, perform a grid search over:\n",
    "- num_leaves: [15, 31, 63]\n",
    "- learning_rate: [0.01, 0.05, 0.1]\n",
    "- min_child_samples: [10, 20, 50]\n",
    "\n",
    "Find the best combination and report the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Early Stopping Optimization\n",
    "\n",
    "Train a LightGBM model on the breast cancer data with:\n",
    "- n_estimators=1000\n",
    "- Early stopping enabled with 30 rounds patience\n",
    "- Learning rate of 0.03\n",
    "\n",
    "Questions:\n",
    "1. At what iteration does it stop?\n",
    "2. Plot the training and validation loss curves\n",
    "3. What's the final test accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary\n",
    "\n",
    "In this notebook, you learned about LightGBM, Microsoft's high-performance gradient boosting framework:\n",
    "\n",
    "### Key Innovations:\n",
    "\n",
    "1. **GOSS (Gradient-based One-Side Sampling)**:\n",
    "   - Keeps samples with large gradients\n",
    "   - Randomly samples those with small gradients\n",
    "   - Result: Faster training with minimal accuracy loss\n",
    "\n",
    "2. **EFB (Exclusive Feature Bundling)**:\n",
    "   - Bundles mutually exclusive features\n",
    "   - Reduces dimensionality without information loss\n",
    "   - Particularly effective for sparse data\n",
    "\n",
    "3. **Leaf-wise Tree Growth**:\n",
    "   - Splits the best leaf (highest loss reduction)\n",
    "   - More accurate than level-wise growth\n",
    "   - Requires max_depth to prevent overfitting\n",
    "\n",
    "### Important Hyperparameters:\n",
    "\n",
    "- **num_leaves**: Controls tree complexity (key parameter!)\n",
    "- **max_depth**: Limits tree depth to prevent overfitting\n",
    "- **min_child_samples**: Minimum data in leaf\n",
    "- **learning_rate**: Step size shrinkage\n",
    "- **bagging_fraction/feature_fraction**: Add randomness\n",
    "- **min_split_gain**: Minimum gain for splits\n",
    "- **reg_alpha/reg_lambda**: Regularization\n",
    "\n",
    "### Advantages:\n",
    "\n",
    "✅ **Faster training** than XGBoost, especially on large datasets\n",
    "✅ **Lower memory usage**\n",
    "✅ **Native categorical feature support** (no encoding needed)\n",
    "✅ **Better accuracy** in many scenarios\n",
    "✅ **Handles large datasets** efficiently\n",
    "✅ **GPU support** with excellent performance\n",
    "✅ **Network training** for distributed learning\n",
    "\n",
    "### Disadvantages:\n",
    "\n",
    "❌ Can **overfit on small datasets** (< 10,000 samples)\n",
    "❌ Leaf-wise growth requires **careful tuning** of max_depth\n",
    "❌ More **sensitive to parameters** than XGBoost\n",
    "❌ Less **documentation and community** than XGBoost (though growing)\n",
    "\n",
    "### LightGBM vs XGBoost - When to Use Which:\n",
    "\n",
    "**Choose LightGBM when:**\n",
    "- Dataset is **large** (> 100K samples)\n",
    "- Training **speed is critical**\n",
    "- Working with **categorical features**\n",
    "- **Memory is limited**\n",
    "- Need **GPU acceleration**\n",
    "\n",
    "**Choose XGBoost when:**\n",
    "- Dataset is **small** (< 10K samples)\n",
    "- Need more **stable/conservative** performance\n",
    "- Want **extensive documentation** and examples\n",
    "- Prefer **safer defaults**\n",
    "- **Regularization** is critical\n",
    "\n",
    "**Both work well for:**\n",
    "- Medium-sized datasets (10K - 100K samples)\n",
    "- Most tabular data problems\n",
    "- Kaggle competitions (winners use both!)\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "1. **Start with conservative parameters**:\n",
    "   ```python\n",
    "   LGBMClassifier(\n",
    "       num_leaves=31,\n",
    "       max_depth=7,  # Set explicitly to prevent overfitting\n",
    "       learning_rate=0.05,\n",
    "       n_estimators=100,\n",
    "       min_child_samples=20\n",
    "   )\n",
    "   ```\n",
    "\n",
    "2. **Always set max_depth** to prevent overfitting\n",
    "3. **Use categorical features natively** when possible\n",
    "4. **Enable early stopping** for optimal tree count\n",
    "5. **Monitor train/val curves** to detect overfitting\n",
    "6. **Use bagging/feature fraction** for regularization\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In the next module, we'll explore **CatBoost**, Yandex's gradient boosting library that:\n",
    "- Handles **categorical features even better** than LightGBM\n",
    "- Uses **ordered boosting** to reduce overfitting\n",
    "- Employs **symmetric trees** for faster prediction\n",
    "- Often achieves **best accuracy** out-of-the-box\n",
    "\n",
    "We'll compare all three libraries (XGBoost, LightGBM, CatBoost) to help you choose the right one!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "- [LightGBM Documentation](https://lightgbm.readthedocs.io/)\n",
    "- [LightGBM Parameters](https://lightgbm.readthedocs.io/en/latest/Parameters.html)\n",
    "- [Original LightGBM Paper (Ke et al., 2017)](https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree)\n",
    "- [LightGBM Python Quick Start](https://lightgbm.readthedocs.io/en/latest/Python-Intro.html)\n",
    "- [Parameter Tuning Guide](https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
