{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 06: LightGBM\n",
    "\n",
    "**Difficulty**: ⭐⭐⭐ Advanced\n",
    "**Estimated Time**: 90 minutes\n",
    "**Prerequisites**: \n",
    "- Module 00: Introduction to Ensemble Learning\n",
    "- Module 04: Gradient Boosting Machines\n",
    "- Module 05: XGBoost\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Understand LightGBM's unique algorithmic innovations (GOSS, EFB)\n",
    "2. Explain histogram-based learning and leaf-wise tree growth\n",
    "3. Leverage LightGBM's native categorical feature handling\n",
    "4. Optimize critical LightGBM hyperparameters for speed and accuracy\n",
    "5. Benchmark LightGBM speed against XGBoost\n",
    "6. Use both sklearn API and native API effectively\n",
    "7. Demonstrate memory efficiency on large datasets\n",
    "8. Apply SHAP for model interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.datasets import load_breast_cancer, make_classification\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_auc_score, roc_curve, log_loss\n",
    ")\n",
    "\n",
    "# XGBoost for comparison\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    XGB_AVAILABLE = False\n",
    "    print(\"XGBoost not available for comparison\")\n",
    "\n",
    "# LightGBM\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    print(f\"LightGBM version: {lgb.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"Installing LightGBM...\")\n",
    "    !pip install lightgbm -q\n",
    "    import lightgbm as lgb\n",
    "    print(f\"LightGBM version: {lgb.__version__}\")\n",
    "\n",
    "# SHAP for interpretability\n",
    "try:\n",
    "    import shap\n",
    "    SHAP_AVAILABLE = True\n",
    "    print(f\"SHAP version: {shap.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"SHAP not available. Install with: pip install shap\")\n",
    "    SHAP_AVAILABLE = False\n",
    "\n",
    "# Configuration\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 4)\n",
    "\n",
    "print(\"\\nSetup complete! All libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What Makes LightGBM Special?\n",
    "\n",
    "### LightGBM = Light Gradient Boosting Machine\n",
    "\n",
    "Developed by Microsoft Research (2017), LightGBM is designed for:\n",
    "- **Speed**: Faster training than XGBoost\n",
    "- **Memory efficiency**: Uses less RAM\n",
    "- **Large datasets**: Handles millions of samples\n",
    "- **Accuracy**: Competitive or better performance\n",
    "\n",
    "### Key Innovations\n",
    "\n",
    "#### 1.1 Histogram-Based Learning\n",
    "\n",
    "**Traditional gradient boosting**:\n",
    "- Considers all possible split points\n",
    "- Slow for continuous features\n",
    "\n",
    "**LightGBM's histogram approach**:\n",
    "- Bins continuous values into discrete bins (default: 255)\n",
    "- Only considers bin boundaries as split points\n",
    "- Much faster, slight accuracy trade-off\n",
    "\n",
    "**Benefits**:\n",
    "- Reduced computation: O(data × bins) vs O(data × data)\n",
    "- Reduced memory: Store bin values, not raw values\n",
    "- Better cache performance\n",
    "\n",
    "#### 1.2 Leaf-wise (Best-first) Tree Growth\n",
    "\n",
    "**Level-wise** (XGBoost, sklearn):\n",
    "```\n",
    "        Root\n",
    "       /    \\\n",
    "     A        B      ← Split all nodes at level 1\n",
    "    / \\      / \\\n",
    "   C   D    E   F    ← Split all nodes at level 2\n",
    "```\n",
    "- Balanced trees\n",
    "- May split nodes that don't benefit much\n",
    "\n",
    "**Leaf-wise** (LightGBM):\n",
    "```\n",
    "        Root\n",
    "       /    \\\n",
    "     A        B\n",
    "    / \\\n",
    "   C   D           ← Only split node A (best gain)\n",
    "  / \\\n",
    " E   F             ← Only split node C (best gain)\n",
    "```\n",
    "- Unbalanced trees\n",
    "- Always splits node with best gain\n",
    "- **Faster convergence**, better accuracy\n",
    "- **Risk**: Can overfit easily (needs regularization)\n",
    "\n",
    "#### 1.3 GOSS (Gradient-based One-Side Sampling)\n",
    "\n",
    "**Observation**: Samples with small gradients are well-fitted, large gradients need more work.\n",
    "\n",
    "**GOSS algorithm**:\n",
    "1. Sort samples by gradient magnitude\n",
    "2. Keep all samples with top-k largest gradients (e.g., top 20%)\n",
    "3. Randomly sample from remaining samples (e.g., 10%)\n",
    "4. Amplify small-gradient samples to maintain distribution\n",
    "\n",
    "**Result**: Use fewer samples, maintain accuracy, faster training!\n",
    "\n",
    "#### 1.4 EFB (Exclusive Feature Bundling)\n",
    "\n",
    "**Observation**: Sparse features rarely take non-zero values simultaneously.\n",
    "\n",
    "**EFB algorithm**:\n",
    "- Bundle mutually exclusive features into single feature\n",
    "- Reduces feature dimension\n",
    "- Critical for high-dimensional sparse data\n",
    "\n",
    "**Example**: One-hot encoded categories can be bundled back!\n",
    "\n",
    "#### 1.5 Native Categorical Support\n",
    "\n",
    "Unlike XGBoost:\n",
    "- No need to one-hot encode\n",
    "- No need for target encoding\n",
    "- Direct categorical split finding\n",
    "- More efficient, often better accuracy\n",
    "\n",
    "### Comparison: LightGBM vs XGBoost\n",
    "\n",
    "| Feature | XGBoost | LightGBM |\n",
    "|---------|---------|----------|\n",
    "| Tree growth | Level-wise | Leaf-wise |\n",
    "| Split finding | Exact/Approx | Histogram |\n",
    "| Speed | Fast | Faster (2-10x) |\n",
    "| Memory | Moderate | Lower |\n",
    "| Categorical features | Manual encoding | Native support |\n",
    "| Large datasets | Good | Excellent |\n",
    "| Overfitting risk | Moderate | Higher (leaf-wise) |\n",
    "| Default params | Generally good | Need tuning |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset with many features\n",
    "# Create synthetic dataset to demonstrate speed\n",
    "print(\"Creating large synthetic dataset...\")\n",
    "X, y = make_classification(\n",
    "    n_samples=50000,\n",
    "    n_features=100,\n",
    "    n_informative=60,\n",
    "    n_redundant=20,\n",
    "    n_clusters_per_class=3,\n",
    "    class_sep=0.8,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset created:\")\n",
    "print(f\"Training samples: {len(X_train):,}\")\n",
    "print(f\"Test samples: {len(X_test):,}\")\n",
    "print(f\"Features: {X.shape[1]}\")\n",
    "print(f\"Classes: {np.unique(y)}\")\n",
    "print(f\"Class distribution: {np.bincount(y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Speed Benchmark: LightGBM vs XGBoost\n",
    "\n",
    "Let's test the claim that LightGBM is faster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common parameters for fair comparison\n",
    "n_estimators = 100\n",
    "max_depth = 7\n",
    "learning_rate = 0.1\n",
    "\n",
    "results = []\n",
    "\n",
    "# Train LightGBM\n",
    "print(\"Training LightGBM...\")\n",
    "start = time.time()\n",
    "lgb_model = lgb.LGBMClassifier(\n",
    "    n_estimators=n_estimators,\n",
    "    max_depth=max_depth,\n",
    "    learning_rate=learning_rate,\n",
    "    num_leaves=31,  # LightGBM-specific\n",
    "    random_state=RANDOM_STATE,\n",
    "    verbose=-1\n",
    ")\n",
    "lgb_model.fit(X_train, y_train)\n",
    "lgb_train_time = time.time() - start\n",
    "lgb_acc = lgb_model.score(X_test, y_test)\n",
    "results.append({'Model': 'LightGBM', 'Train Time': lgb_train_time, 'Accuracy': lgb_acc})\n",
    "\n",
    "# Train XGBoost (if available)\n",
    "if XGB_AVAILABLE:\n",
    "    print(\"Training XGBoost...\")\n",
    "    start = time.time()\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        learning_rate=learning_rate,\n",
    "        random_state=RANDOM_STATE,\n",
    "        eval_metric='logloss',\n",
    "        verbosity=0\n",
    "    )\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    xgb_train_time = time.time() - start\n",
    "    xgb_acc = xgb_model.score(X_test, y_test)\n",
    "    results.append({'Model': 'XGBoost', 'Train Time': xgb_train_time, 'Accuracy': xgb_acc})\n",
    "\n",
    "# Display results\n",
    "df_results = pd.DataFrame(results)\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Speed Benchmark Results\")\n",
    "print(\"=\" * 70)\n",
    "print(df_results.to_string(index=False))\n",
    "\n",
    "if XGB_AVAILABLE:\n",
    "    speedup = xgb_train_time / lgb_train_time\n",
    "    print(f\"\\nLightGBM is {speedup:.2f}x faster!\")\n",
    "    print(f\"Accuracy difference: {(lgb_acc - xgb_acc) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "if XGB_AVAILABLE:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Training time\n",
    "    axes[0].bar(df_results['Model'], df_results['Train Time'], \n",
    "                color=['#2ecc71', '#e74c3c'], edgecolor='black', linewidth=1.5)\n",
    "    axes[0].set_ylabel('Training Time (seconds)', fontsize=12)\n",
    "    axes[0].set_title('Training Speed Comparison', fontsize=13, fontweight='bold')\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Accuracy\n",
    "    axes[1].bar(df_results['Model'], df_results['Accuracy'], \n",
    "                color=['#2ecc71', '#e74c3c'], edgecolor='black', linewidth=1.5)\n",
    "    axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "    axes[1].set_title('Accuracy Comparison', fontsize=13, fontweight='bold')\n",
    "    axes[1].set_ylim([0.8, 1.0])\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nKey Observation:\")\n",
    "print(\"LightGBM achieves similar accuracy with significantly faster training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LightGBM Hyperparameters\n",
    "\n",
    "### 3.1 Core Parameters\n",
    "\n",
    "**`num_leaves`** (default=31): Maximum number of leaves per tree\n",
    "- Most important parameter!\n",
    "- Controls model complexity\n",
    "- Should be < 2^max_depth (leaf-wise growth)\n",
    "- Typical: 20-100\n",
    "\n",
    "**`max_depth`** (default=-1): Maximum tree depth\n",
    "- -1 means no limit\n",
    "- Use to prevent overfitting\n",
    "- Typical: 5-15\n",
    "\n",
    "**`learning_rate`** (default=0.1): Shrinkage rate\n",
    "- Lower = need more trees\n",
    "- Typical: 0.01-0.3\n",
    "\n",
    "**`n_estimators`** (default=100): Number of boosting rounds\n",
    "- Use early stopping\n",
    "- Typical: 100-1000\n",
    "\n",
    "### 3.2 Data Sampling\n",
    "\n",
    "**`bagging_fraction`** (subsample, default=1.0): Fraction of data for each tree\n",
    "- < 1.0 enables bagging\n",
    "- Typical: 0.7-1.0\n",
    "\n",
    "**`bagging_freq`** (default=0): Bagging frequency\n",
    "- 0 = disabled\n",
    "- k = use bagging every k iterations\n",
    "\n",
    "**`feature_fraction`** (colsample_bytree, default=1.0): Fraction of features per tree\n",
    "- Typical: 0.6-1.0\n",
    "\n",
    "### 3.3 Regularization\n",
    "\n",
    "**`min_data_in_leaf`** (min_child_samples, default=20): Minimum samples per leaf\n",
    "- Higher = more conservative\n",
    "- Critical for preventing overfitting\n",
    "- Typical: 10-100\n",
    "\n",
    "**`lambda_l1`** (reg_alpha): L1 regularization\n",
    "**`lambda_l2`** (reg_lambda): L2 regularization\n",
    "**`min_gain_to_split`**: Minimum gain to perform split\n",
    "\n",
    "### 3.4 Speed vs Accuracy\n",
    "\n",
    "**`max_bin`** (default=255): Number of histogram bins\n",
    "- Lower = faster but less accurate\n",
    "- Higher = slower but more accurate\n",
    "- Typical: 63, 127, 255, 511"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of num_leaves\n",
    "num_leaves_values = [7, 15, 31, 63, 127, 255]\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "train_times = []\n",
    "\n",
    "for num_leaves in num_leaves_values:\n",
    "    start = time.time()\n",
    "    model = lgb.LGBMClassifier(\n",
    "        n_estimators=100,\n",
    "        num_leaves=num_leaves,\n",
    "        learning_rate=0.1,\n",
    "        random_state=RANDOM_STATE,\n",
    "        verbose=-1\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    train_times.append(time.time() - start)\n",
    "    train_accs.append(model.score(X_train, y_train))\n",
    "    test_accs.append(model.score(X_test, y_test))\n",
    "\n",
    "# Plot results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy\n",
    "axes[0].plot(num_leaves_values, train_accs, marker='o', linewidth=2, \n",
    "             markersize=8, label='Train Accuracy')\n",
    "axes[0].plot(num_leaves_values, test_accs, marker='s', linewidth=2, \n",
    "             markersize=8, label='Test Accuracy')\n",
    "axes[0].axvline(31, color='red', linestyle='--', alpha=0.5, label='Default (31)')\n",
    "axes[0].set_xlabel('num_leaves', fontsize=12)\n",
    "axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[0].set_title('Effect of num_leaves on Accuracy', fontsize=13, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_xscale('log', base=2)\n",
    "\n",
    "# Training time\n",
    "axes[1].bar(range(len(num_leaves_values)), train_times, color='steelblue', \n",
    "            edgecolor='black', linewidth=1.5)\n",
    "axes[1].set_xticks(range(len(num_leaves_values)))\n",
    "axes[1].set_xticklabels(num_leaves_values)\n",
    "axes[1].set_xlabel('num_leaves', fontsize=12)\n",
    "axes[1].set_ylabel('Training Time (seconds)', fontsize=12)\n",
    "axes[1].set_title('Effect of num_leaves on Speed', fontsize=13, fontweight='bold')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "best_idx = np.argmax(test_accs)\n",
    "print(f\"\\nOptimal num_leaves: {num_leaves_values[best_idx]}\")\n",
    "print(f\"Test accuracy: {test_accs[best_idx]:.4f}\")\n",
    "print(f\"Training time: {train_times[best_idx]:.3f} seconds\")\n",
    "print(\"\\nKey insight: Higher num_leaves = more complex model, risk of overfitting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of min_data_in_leaf (regularization)\n",
    "min_data_values = [5, 10, 20, 50, 100, 200]\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "\n",
    "for min_data in min_data_values:\n",
    "    model = lgb.LGBMClassifier(\n",
    "        n_estimators=100,\n",
    "        num_leaves=63,\n",
    "        min_data_in_leaf=min_data,\n",
    "        learning_rate=0.1,\n",
    "        random_state=RANDOM_STATE,\n",
    "        verbose=-1\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    train_accs.append(model.score(X_train, y_train))\n",
    "    test_accs.append(model.score(X_test, y_test))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(min_data_values, train_accs, marker='o', linewidth=2, \n",
    "         markersize=8, label='Train Accuracy')\n",
    "plt.plot(min_data_values, test_accs, marker='s', linewidth=2, \n",
    "         markersize=8, label='Test Accuracy')\n",
    "plt.axvline(20, color='red', linestyle='--', alpha=0.5, label='Default (20)')\n",
    "plt.xlabel('min_data_in_leaf', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('Effect of min_data_in_leaf (Regularization)', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservation:\")\n",
    "print(\"Higher min_data_in_leaf = stronger regularization = less overfitting\")\n",
    "print(\"Find sweet spot where test accuracy is maximized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Native Categorical Feature Support\n",
    "\n",
    "LightGBM's killer feature: no need to encode categorical variables!\n",
    "\n",
    "### How it works:\n",
    "1. Mark columns as categorical using `categorical_feature` parameter\n",
    "2. LightGBM uses optimal split finding for categories\n",
    "3. Handles high-cardinality categories efficiently\n",
    "4. Often better than one-hot or target encoding\n",
    "\n",
    "### Benefits:\n",
    "- No manual encoding needed\n",
    "- Better memory efficiency (no one-hot explosion)\n",
    "- Better accuracy (captures category relationships)\n",
    "- Handles unseen categories gracefully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset with categorical features\n",
    "print(\"Creating dataset with categorical features...\")\n",
    "\n",
    "# Start with numeric features\n",
    "n_samples = 10000\n",
    "X_numeric = np.random.randn(n_samples, 5)\n",
    "\n",
    "# Add categorical features\n",
    "categories_a = np.random.choice(['A', 'B', 'C', 'D'], size=n_samples)\n",
    "categories_b = np.random.choice(['Low', 'Medium', 'High'], size=n_samples)\n",
    "categories_c = np.random.choice([f'Cat_{i}' for i in range(20)], size=n_samples)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(X_numeric, columns=[f'num_{i}' for i in range(5)])\n",
    "df['cat_a'] = categories_a\n",
    "df['cat_b'] = categories_b\n",
    "df['cat_c'] = categories_c\n",
    "\n",
    "# Create target based on features (including categorical influence)\n",
    "target_numeric = X_numeric.sum(axis=1)\n",
    "target_cat_a = (categories_a == 'A').astype(int) * 2\n",
    "target_cat_b = (categories_b == 'High').astype(int) * 1.5\n",
    "y_continuous = target_numeric + target_cat_a + target_cat_b + np.random.randn(n_samples) * 0.5\n",
    "y_cat = (y_continuous > y_continuous.median()).astype(int)\n",
    "\n",
    "print(f\"\\nDataset created:\")\n",
    "print(f\"Samples: {len(df)}\")\n",
    "print(f\"Numeric features: 5\")\n",
    "print(f\"Categorical features: 3\")\n",
    "print(f\"\\nSample data:\")\n",
    "print(df.head())\n",
    "print(f\"\\nCategorical column dtypes:\")\n",
    "print(df[['cat_a', 'cat_b', 'cat_c']].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_cat = df.copy()\n",
    "X_train_cat, X_test_cat, y_train_cat, y_test_cat = train_test_split(\n",
    "    X_cat, y_cat, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Method 1: LightGBM with categorical features (native)\n",
    "print(\"Method 1: Native categorical support...\")\n",
    "categorical_features = ['cat_a', 'cat_b', 'cat_c']\n",
    "\n",
    "lgb_cat = lgb.LGBMClassifier(\n",
    "    n_estimators=100,\n",
    "    num_leaves=31,\n",
    "    learning_rate=0.1,\n",
    "    random_state=RANDOM_STATE,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "lgb_cat.fit(\n",
    "    X_train_cat, y_train_cat,\n",
    "    categorical_feature=categorical_features\n",
    ")\n",
    "native_time = time.time() - start\n",
    "native_acc = lgb_cat.score(X_test_cat, y_test_cat)\n",
    "\n",
    "print(f\"  Training time: {native_time:.3f}s\")\n",
    "print(f\"  Test accuracy: {native_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: One-hot encoding (traditional)\n",
    "print(\"\\nMethod 2: One-hot encoding...\")\n",
    "X_train_ohe = pd.get_dummies(X_train_cat, columns=categorical_features)\n",
    "X_test_ohe = pd.get_dummies(X_test_cat, columns=categorical_features)\n",
    "\n",
    "# Align columns (in case test has different categories)\n",
    "X_train_ohe, X_test_ohe = X_train_ohe.align(X_test_ohe, join='left', axis=1, fill_value=0)\n",
    "\n",
    "lgb_ohe = lgb.LGBMClassifier(\n",
    "    n_estimators=100,\n",
    "    num_leaves=31,\n",
    "    learning_rate=0.1,\n",
    "    random_state=RANDOM_STATE,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "lgb_ohe.fit(X_train_ohe, y_train_cat)\n",
    "ohe_time = time.time() - start\n",
    "ohe_acc = lgb_ohe.score(X_test_ohe, y_test_cat)\n",
    "\n",
    "print(f\"  Features after encoding: {X_train_ohe.shape[1]}\")\n",
    "print(f\"  Training time: {ohe_time:.3f}s\")\n",
    "print(f\"  Test accuracy: {ohe_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison\n",
    "comparison_df = pd.DataFrame([\n",
    "    {\n",
    "        'Method': 'Native Categorical',\n",
    "        'Features': X_train_cat.shape[1],\n",
    "        'Training Time': native_time,\n",
    "        'Accuracy': native_acc\n",
    "    },\n",
    "    {\n",
    "        'Method': 'One-Hot Encoding',\n",
    "        'Features': X_train_ohe.shape[1],\n",
    "        'Training Time': ohe_time,\n",
    "        'Accuracy': ohe_acc\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Categorical Feature Handling Comparison\")\n",
    "print(\"=\" * 70)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n✅ Native categorical support is:\")\n",
    "print(f\"   - Simpler (no encoding code)\")\n",
    "print(f\"   - Faster ({ohe_time / native_time:.2f}x speedup)\")\n",
    "print(f\"   - More memory efficient ({X_train_ohe.shape[1] / X_train_cat.shape[1]:.1f}x fewer features)\")\n",
    "if native_acc > ohe_acc:\n",
    "    print(f\"   - More accurate (+{(native_acc - ohe_acc) * 100:.2f}%)\")\n",
    "else:\n",
    "    print(f\"   - Similar accuracy ({(native_acc - ohe_acc) * 100:+.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. LightGBM Native API\n",
    "\n",
    "For maximum control and performance, use LightGBM's native API with `Dataset` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use breast cancer dataset for native API demo\n",
    "cancer_data = load_breast_cancer()\n",
    "X_cancer, y_cancer = cancer_data.data, cancer_data.target\n",
    "feature_names_cancer = cancer_data.feature_names\n",
    "\n",
    "X_train_bc, X_test_bc, y_train_bc, y_test_bc = train_test_split(\n",
    "    X_cancer, y_cancer, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Further split for validation\n",
    "X_train_bc, X_val_bc, y_train_bc, y_val_bc = train_test_split(\n",
    "    X_train_bc, y_train_bc, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(X_train_bc)}\")\n",
    "print(f\"Validation samples: {len(X_val_bc)}\")\n",
    "print(f\"Test samples: {len(X_test_bc)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LightGBM Datasets\n",
    "train_data = lgb.Dataset(\n",
    "    X_train_bc, \n",
    "    label=y_train_bc,\n",
    "    feature_name=list(feature_names_cancer)\n",
    ")\n",
    "\n",
    "val_data = lgb.Dataset(\n",
    "    X_val_bc,\n",
    "    label=y_val_bc,\n",
    "    reference=train_data,  # Use same binning as training\n",
    "    feature_name=list(feature_names_cancer)\n",
    ")\n",
    "\n",
    "print(\"LightGBM Datasets created!\")\n",
    "print(f\"\\nDataset is optimized for LightGBM training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': -1,\n",
    "    'seed': RANDOM_STATE\n",
    "}\n",
    "\n",
    "# Train with early stopping\n",
    "print(\"Training with native API and early stopping...\")\n",
    "evals_result = {}\n",
    "lgb_native = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    num_boost_round=500,\n",
    "    valid_sets=[train_data, val_data],\n",
    "    valid_names=['train', 'valid'],\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=20),\n",
    "        lgb.log_evaluation(period=0),\n",
    "        lgb.record_evaluation(evals_result)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"\\nBest iteration: {lgb_native.best_iteration}\")\n",
    "print(f\"Best validation score: {lgb_native.best_score['valid']['binary_logloss']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training progress\n",
    "train_loss = evals_result['train']['binary_logloss']\n",
    "val_loss = evals_result['valid']['binary_logloss']\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train_loss, label='Train', linewidth=2)\n",
    "plt.plot(val_loss, label='Validation', linewidth=2)\n",
    "plt.axvline(lgb_native.best_iteration, color='red', linestyle='--', \n",
    "            linewidth=2, label=f'Best iteration ({lgb_native.best_iteration})')\n",
    "plt.xlabel('Boosting Round', fontsize=12)\n",
    "plt.ylabel('Binary Log Loss', fontsize=12)\n",
    "plt.title('LightGBM Training Progress', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Test set evaluation\n",
    "y_pred = lgb_native.predict(X_test_bc)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "test_acc = accuracy_score(y_test_bc, y_pred_binary)\n",
    "test_auc = roc_auc_score(y_test_bc, y_pred)\n",
    "\n",
    "print(f\"\\nTest Set Performance:\")\n",
    "print(f\"Accuracy: {test_acc:.4f}\")\n",
    "print(f\"AUC-ROC: {test_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Memory Efficiency Demonstration\n",
    "\n",
    "LightGBM is designed for large datasets. Let's demonstrate memory efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create progressively larger datasets and measure memory usage\n",
    "import sys\n",
    "\n",
    "dataset_sizes = [10000, 50000, 100000, 200000]\n",
    "memory_usage = []\n",
    "training_times = []\n",
    "\n",
    "for size in dataset_sizes:\n",
    "    print(f\"\\nTesting with {size:,} samples...\")\n",
    "    \n",
    "    # Create dataset\n",
    "    X_large, y_large = make_classification(\n",
    "        n_samples=size,\n",
    "        n_features=50,\n",
    "        n_informative=30,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    \n",
    "    # Create LightGBM Dataset (efficient representation)\n",
    "    lgb_dataset = lgb.Dataset(X_large, label=y_large)\n",
    "    \n",
    "    # Estimate memory (rough approximation)\n",
    "    data_memory = sys.getsizeof(X_large) / (1024 ** 2)  # MB\n",
    "    memory_usage.append(data_memory)\n",
    "    \n",
    "    # Train and time\n",
    "    start = time.time()\n",
    "    params = {'objective': 'binary', 'metric': 'binary_logloss', 'verbose': -1}\n",
    "    lgb.train(params, lgb_dataset, num_boost_round=50)\n",
    "    training_times.append(time.time() - start)\n",
    "    \n",
    "    print(f\"  Memory: ~{data_memory:.1f} MB\")\n",
    "    print(f\"  Training time: {training_times[-1]:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize scalability\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Training time vs dataset size\n",
    "axes[0].plot(dataset_sizes, training_times, marker='o', linewidth=2, markersize=10, color='#e74c3c')\n",
    "axes[0].set_xlabel('Number of Samples', fontsize=12)\n",
    "axes[0].set_ylabel('Training Time (seconds)', fontsize=12)\n",
    "axes[0].set_title('LightGBM Scalability', fontsize=13, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Memory usage\n",
    "axes[1].plot(dataset_sizes, memory_usage, marker='s', linewidth=2, markersize=10, color='#3498db')\n",
    "axes[1].set_xlabel('Number of Samples', fontsize=12)\n",
    "axes[1].set_ylabel('Memory Usage (MB)', fontsize=12)\n",
    "axes[1].set_title('Memory Efficiency', fontsize=13, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nLightGBM handles large datasets efficiently!\")\n",
    "print(\"Near-linear scaling in both time and memory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Importance and SHAP\n",
    "\n",
    "LightGBM provides similar feature importance metrics as XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model on breast cancer for feature importance\n",
    "lgb_fi = lgb.LGBMClassifier(\n",
    "    n_estimators=100,\n",
    "    num_leaves=31,\n",
    "    learning_rate=0.1,\n",
    "    random_state=RANDOM_STATE,\n",
    "    verbose=-1\n",
    ")\n",
    "lgb_fi.fit(X_train_bc, y_train_bc)\n",
    "\n",
    "# Get feature importance (split, gain)\n",
    "importance_split = lgb_fi.feature_importances_  # Default is split\n",
    "importance_gain = lgb_fi.booster_.feature_importance(importance_type='gain')\n",
    "\n",
    "# Create DataFrame\n",
    "fi_df = pd.DataFrame({\n",
    "    'Feature': feature_names_cancer,\n",
    "    'Split': importance_split,\n",
    "    'Gain': importance_gain\n",
    "}).sort_values('Gain', ascending=False)\n",
    "\n",
    "print(\"Top 10 Features by Gain:\")\n",
    "print(\"=\" * 70)\n",
    "print(fi_df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top features\n",
    "top_features = fi_df.head(15)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(range(len(top_features)), top_features['Gain'], color='steelblue', edgecolor='black')\n",
    "plt.yticks(range(len(top_features)), top_features['Feature'])\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xlabel('Importance (Gain)', fontsize=12)\n",
    "plt.title('Top 15 Features by Gain', fontsize=14, fontweight='bold')\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP analysis\n",
    "if SHAP_AVAILABLE:\n",
    "    print(\"Computing SHAP values...\")\n",
    "    explainer = shap.TreeExplainer(lgb_fi)\n",
    "    X_test_sample = X_test_bc[:100]\n",
    "    shap_values = explainer.shap_values(X_test_sample)\n",
    "    \n",
    "    # For binary classification, shap_values is a list\n",
    "    if isinstance(shap_values, list):\n",
    "        shap_values = shap_values[1]  # Use positive class\n",
    "    \n",
    "    print(\"SHAP values computed!\")\n",
    "    print(f\"Shape: {shap_values.shape}\")\n",
    "else:\n",
    "    print(\"SHAP not available. Install with: pip install shap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHAP_AVAILABLE:\n",
    "    # Global feature importance\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    shap.summary_plot(shap_values, X_test_sample, \n",
    "                     feature_names=feature_names_cancer, \n",
    "                     plot_type='bar', show=False)\n",
    "    plt.title('Global Feature Importance (SHAP)', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHAP_AVAILABLE:\n",
    "    # Detailed SHAP summary\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    shap.summary_plot(shap_values, X_test_sample, \n",
    "                     feature_names=feature_names_cancer, show=False)\n",
    "    plt.title('SHAP Summary Plot', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nInterpretation:\")\n",
    "    print(\"- Red dots = high feature value\")\n",
    "    print(\"- Blue dots = low feature value\")\n",
    "    print(\"- Position = impact on prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Hyperparameter Optimization\n",
    "\n",
    "Use GridSearchCV or RandomizedSearchCV to find optimal LightGBM parameters:\n",
    "\n",
    "Parameters to tune:\n",
    "- `num_leaves`: [15, 31, 63, 127]\n",
    "- `max_depth`: [5, 7, 10, -1]\n",
    "- `learning_rate`: [0.01, 0.05, 0.1]\n",
    "- `n_estimators`: [100, 200, 500]\n",
    "- `min_data_in_leaf`: [10, 20, 50]\n",
    "- `feature_fraction`: [0.6, 0.8, 1.0]\n",
    "- `bagging_fraction`: [0.6, 0.8, 1.0]\n",
    "\n",
    "1. Perform hyperparameter search\n",
    "2. Identify best parameters\n",
    "3. Analyze which parameters matter most\n",
    "4. Compare with default parameters\n",
    "5. Plot validation curves for key parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: LightGBM vs XGBoost Speed Comparison\n",
    "\n",
    "Conduct comprehensive speed benchmarks:\n",
    "\n",
    "1. Create datasets of varying sizes: 10K, 50K, 100K, 500K samples\n",
    "2. For each dataset:\n",
    "   - Train LightGBM with optimal parameters\n",
    "   - Train XGBoost with equivalent parameters\n",
    "   - Measure training time\n",
    "   - Measure prediction time\n",
    "   - Compare accuracy\n",
    "3. Plot results:\n",
    "   - Training time vs dataset size\n",
    "   - Speedup ratio\n",
    "   - Accuracy comparison\n",
    "4. Determine at what dataset size LightGBM's advantage is most pronounced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Categorical Feature Experiments\n",
    "\n",
    "Test different categorical encoding strategies:\n",
    "\n",
    "1. Create dataset with varying cardinality categories:\n",
    "   - Low cardinality (3-5 values)\n",
    "   - Medium cardinality (10-20 values)\n",
    "   - High cardinality (100+ values)\n",
    "2. Compare encoding methods:\n",
    "   - LightGBM native categorical\n",
    "   - One-hot encoding\n",
    "   - Label encoding\n",
    "   - Target encoding\n",
    "3. Measure for each:\n",
    "   - Training time\n",
    "   - Memory usage\n",
    "   - Accuracy\n",
    "   - Number of features\n",
    "4. Determine when native categorical support provides the biggest advantage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Preventing Overfitting with Leaf-wise Growth\n",
    "\n",
    "LightGBM's leaf-wise growth can overfit. Test regularization strategies:\n",
    "\n",
    "1. Create a small dataset prone to overfitting (1000 samples, 50 features)\n",
    "2. Train baseline model with aggressive settings:\n",
    "   - `num_leaves=255`\n",
    "   - `max_depth=-1`\n",
    "   - `min_data_in_leaf=1`\n",
    "3. Apply regularization techniques:\n",
    "   - Reduce `num_leaves`\n",
    "   - Set `max_depth` limit\n",
    "   - Increase `min_data_in_leaf`\n",
    "   - Add L1/L2 regularization\n",
    "   - Use `feature_fraction` and `bagging_fraction`\n",
    "4. Compare train vs test performance\n",
    "5. Find optimal regularization combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Custom Objective and Metric\n",
    "\n",
    "Implement a custom objective function and evaluation metric:\n",
    "\n",
    "1. Create an imbalanced classification problem (90:10 ratio)\n",
    "2. Implement custom focal loss objective:\n",
    "   - Focal loss focuses on hard examples\n",
    "   - Formula: FL = -α(1-p)^γ log(p)\n",
    "3. Implement custom F1 evaluation metric\n",
    "4. Train LightGBM with:\n",
    "   - Custom focal loss objective\n",
    "   - Custom F1 metric\n",
    "   - Early stopping based on F1\n",
    "5. Compare with standard binary logloss\n",
    "6. Analyze improvements on minority class\n",
    "\n",
    "**Hint**: Use `fobj` and `feval` parameters in `lgb.train()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **LightGBM Innovations**:\n",
    "   - **Histogram-based learning**: Bins continuous values (faster, memory efficient)\n",
    "   - **Leaf-wise growth**: Best-first tree building (faster convergence, risk of overfitting)\n",
    "   - **GOSS**: Gradient-based One-Side Sampling (fewer samples, maintained accuracy)\n",
    "   - **EFB**: Exclusive Feature Bundling (reduces dimension in sparse data)\n",
    "   - **Native categorical support**: No encoding needed!\n",
    "\n",
    "2. **Critical Hyperparameters**:\n",
    "   - `num_leaves`: Most important! Controls complexity (20-100)\n",
    "   - `max_depth`: Limits tree depth, prevents overfitting (5-15 or -1)\n",
    "   - `min_data_in_leaf`: Minimum samples per leaf, strong regularization (10-100)\n",
    "   - `learning_rate`: Shrinkage rate (0.01-0.3)\n",
    "   - `feature_fraction`, `bagging_fraction`: Randomness for regularization (0.6-1.0)\n",
    "   - `max_bin`: Histogram bins, speed/accuracy trade-off (63-511)\n",
    "\n",
    "3. **Best Practices**:\n",
    "   - Start with `num_leaves=31`, tune up/down\n",
    "   - Use `min_data_in_leaf` to prevent overfitting\n",
    "   - Leverage categorical features natively (don't one-hot encode)\n",
    "   - Use early stopping with validation set\n",
    "   - Lower `learning_rate` + more `n_estimators` = better generalization\n",
    "   - Monitor train/validation gap\n",
    "\n",
    "4. **Advantages Over XGBoost**:\n",
    "   - **Faster**: 2-10x speedup, especially on large data\n",
    "   - **Memory efficient**: Lower RAM usage\n",
    "   - **Categorical features**: Native support, no preprocessing\n",
    "   - **Large datasets**: Designed for millions of samples\n",
    "   - **Sparse data**: EFB handles sparsity efficiently\n",
    "\n",
    "5. **When to Use LightGBM**:\n",
    "   - ✅ Large datasets (100K+ samples)\n",
    "   - ✅ Many categorical features\n",
    "   - ✅ High-dimensional sparse data\n",
    "   - ✅ Need fast training\n",
    "   - ✅ Limited memory\n",
    "   - ⚠️ Small datasets: May overfit (use strong regularization)\n",
    "   - ⚠️ Need deterministic results: Leaf-wise can vary slightly\n",
    "\n",
    "### Strengths\n",
    "\n",
    "- Extremely fast training\n",
    "- Excellent memory efficiency\n",
    "- Handles categorical features natively\n",
    "- Great for large-scale production systems\n",
    "- Competitive or better accuracy than XGBoost\n",
    "- Active development and community\n",
    "\n",
    "### Weaknesses\n",
    "\n",
    "- Leaf-wise growth can overfit on small data\n",
    "- Requires careful hyperparameter tuning\n",
    "- Default parameters may not be optimal\n",
    "- Less stable than level-wise growth\n",
    "- Can be sensitive to noise\n",
    "\n",
    "### LightGBM vs XGBoost: Which to Choose?\n",
    "\n",
    "**Choose LightGBM when**:\n",
    "- Dataset is large (100K+ samples)\n",
    "- Many categorical features\n",
    "- Speed is critical\n",
    "- Memory is limited\n",
    "- Features are sparse\n",
    "\n",
    "**Choose XGBoost when**:\n",
    "- Dataset is small (<10K samples)\n",
    "- Need maximum stability\n",
    "- Default parameters matter\n",
    "- Already familiar with XGBoost\n",
    "\n",
    "**Truth**: Both are excellent! Try both and use what works best for your problem.\n",
    "\n",
    "### Tuning Strategy\n",
    "\n",
    "**Phase 1: Prevent overfitting**\n",
    "1. Start with `num_leaves=31`\n",
    "2. Increase `min_data_in_leaf` if overfitting (20 → 50 → 100)\n",
    "3. Set `max_depth` if needed (7-10)\n",
    "\n",
    "**Phase 2: Improve accuracy**\n",
    "4. Tune `num_leaves` (increase if underfitting, decrease if overfitting)\n",
    "5. Add randomness: `feature_fraction=0.8`, `bagging_fraction=0.8`\n",
    "\n",
    "**Phase 3: Optimize learning**\n",
    "6. Lower `learning_rate` to 0.05 or 0.01\n",
    "7. Increase `n_estimators` accordingly\n",
    "8. Use early stopping\n",
    "\n",
    "**Phase 4: Fine-tune**\n",
    "9. Adjust `max_bin` for speed/accuracy trade-off\n",
    "10. Add L1/L2 regularization if still overfitting\n",
    "\n",
    "### Production Tips\n",
    "\n",
    "1. **Model persistence**: Use `model.save_model()` / `Booster.save_model()`\n",
    "2. **Categorical features**: Save category mappings with model\n",
    "3. **Monitoring**: Track feature importance changes\n",
    "4. **Retraining**: Schedule regular retraining\n",
    "5. **A/B testing**: Compare model versions\n",
    "6. **GPU acceleration**: Use `device='gpu'` for huge speedups (if available)\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In **Module 07: CatBoost**, we'll explore:\n",
    "- Yandex's gradient boosting implementation\n",
    "- Ordered boosting to prevent target leakage\n",
    "- Symmetric trees for faster prediction\n",
    "- Even better categorical feature handling\n",
    "- Three-way comparison: XGBoost vs LightGBM vs CatBoost\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- **Paper**: \"LightGBM: A Highly Efficient Gradient Boosting Decision Tree\" (Ke et al., 2017)\n",
    "- **Documentation**: [LightGBM Official Docs](https://lightgbm.readthedocs.io/)\n",
    "- **Parameters Guide**: [LightGBM Parameters](https://lightgbm.readthedocs.io/en/latest/Parameters.html)\n",
    "- **Tutorial**: [LightGBM Parameter Tuning](https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html)\n",
    "- **GitHub**: [Microsoft LightGBM](https://github.com/microsoft/LightGBM)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
