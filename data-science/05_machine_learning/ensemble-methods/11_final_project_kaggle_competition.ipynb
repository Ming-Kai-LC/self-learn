{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 11: Final Project - Kaggle Competition (Titanic Survival Prediction)\n",
    "\n",
    "**Difficulty**: ‚≠ê‚≠ê‚≠ê Advanced\n",
    "**Estimated Time**: 150 minutes\n",
    "**Prerequisites**: \n",
    "- All modules 00-10 (complete ensemble methods series)\n",
    "- Feature engineering fundamentals\n",
    "- Model evaluation and optimization\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Execute a complete end-to-end machine learning competition workflow\n",
    "2. Perform comprehensive exploratory data analysis and feature engineering\n",
    "3. Compare and optimize multiple ensemble methods systematically\n",
    "4. Use advanced techniques: Optuna optimization, SHAP interpretation, stacking\n",
    "5. Create production-ready prediction pipelines\n",
    "6. Generate competition-ready submission files\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "**Challenge**: Predict survival on the Titanic\n",
    "\n",
    "**Dataset**: Titanic passenger data (train + test)\n",
    "- Features: Age, Sex, Class, Fare, Family, Embarked location, etc.\n",
    "- Target: Survived (0 = No, 1 = Yes)\n",
    "- Evaluation: Accuracy\n",
    "\n",
    "**Workflow**:\n",
    "1. Setup and Data Loading\n",
    "2. Exploratory Data Analysis\n",
    "3. Data Preprocessing\n",
    "4. Feature Engineering\n",
    "5. Baseline Models\n",
    "6. Ensemble Methods\n",
    "7. Hyperparameter Optimization\n",
    "8. Model Stacking\n",
    "9. Model Interpretation\n",
    "10. Final Submission\n",
    "\n",
    "Let's build a competition-winning solution! üö¢"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Setup and Data Loading (10 min)\n",
    "\n",
    "Import libraries and load the Titanic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Machine learning basics\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix, \n",
    "    roc_auc_score, roc_curve\n",
    ")\n",
    "\n",
    "# Base models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Ensemble methods\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, \n",
    "    GradientBoostingClassifier,\n",
    "    AdaBoostClassifier,\n",
    "    VotingClassifier,\n",
    "    StackingClassifier\n",
    ")\n",
    "\n",
    "# Advanced ensemble libraries\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Hyperparameter optimization\n",
    "import optuna\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances\n",
    "\n",
    "# Model interpretation\n",
    "import shap\n",
    "\n",
    "# Configuration\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "warnings.filterwarnings('ignore')\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 4)\n",
    "\n",
    "print(\"‚úì All libraries imported successfully!\")\n",
    "print(f\"‚úì Random seed: {RANDOM_STATE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Titanic dataset\n",
    "# Option 1: From seaborn (easier, but combined train+test)\n",
    "# Option 2: Download from Kaggle or load from CSV\n",
    "\n",
    "# We'll use seaborn's titanic dataset and create our own train/test split\n",
    "titanic_full = sns.load_dataset('titanic')\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"Total samples: {len(titanic_full)}\")\n",
    "print(f\"Features: {titanic_full.shape[1]}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "titanic_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial data inspection\n",
    "print(\"Dataset Information:\")\n",
    "print(\"=\" * 80)\n",
    "titanic_full.info()\n",
    "\n",
    "print(\"\\n\\nBasic Statistics:\")\n",
    "print(\"=\" * 80)\n",
    "titanic_full.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify target and features\n",
    "print(\"Target Variable: survived\")\n",
    "print(f\"Classes: 0 (Did not survive), 1 (Survived)\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(titanic_full['survived'].value_counts())\n",
    "print(f\"\\nSurvival rate: {titanic_full['survived'].mean():.2%}\")\n",
    "\n",
    "print(\"\\n\\nAvailable Features:\")\n",
    "for col in titanic_full.columns:\n",
    "    print(f\"  ‚Ä¢ {col}: {titanic_full[col].dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Exploratory Data Analysis (20 min)\n",
    "\n",
    "Understand the data through visualization and statistical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing value analysis\n",
    "print(\"Missing Values Analysis:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "missing_data = pd.DataFrame({\n",
    "    'Column': titanic_full.columns,\n",
    "    'Missing Count': titanic_full.isnull().sum(),\n",
    "    'Missing Percentage': (titanic_full.isnull().sum() / len(titanic_full) * 100).round(2)\n",
    "})\n",
    "missing_data = missing_data[missing_data['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
    "\n",
    "print(missing_data.to_string(index=False))\n",
    "\n",
    "# Visualize missing values\n",
    "plt.figure(figsize=(12, 6))\n",
    "missing_counts = titanic_full.isnull().sum()\n",
    "missing_counts = missing_counts[missing_counts > 0].sort_values(ascending=True)\n",
    "\n",
    "plt.barh(missing_counts.index, missing_counts.values, color='coral', edgecolor='black')\n",
    "plt.xlabel('Number of Missing Values', fontweight='bold')\n",
    "plt.title('Missing Values by Feature', fontsize=14, fontweight='bold')\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add percentage labels\n",
    "for i, (idx, val) in enumerate(missing_counts.items()):\n",
    "    pct = (val / len(titanic_full)) * 100\n",
    "    plt.text(val + 5, i, f'{val} ({pct:.1f}%)', va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Observations:\")\n",
    "print(\"  ‚Ä¢ 'deck' has ~77% missing - may need to drop or create 'Unknown' category\")\n",
    "print(\"  ‚Ä¢ 'age' has ~20% missing - imputation needed\")\n",
    "print(\"  ‚Ä¢ 'embarked' and 'embark_town' have minimal missing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Survival rate by key features\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# 1. Survival by Sex\n",
    "ax = axes[0, 0]\n",
    "survival_by_sex = titanic_full.groupby('sex')['survived'].agg(['mean', 'count'])\n",
    "bars = ax.bar(survival_by_sex.index, survival_by_sex['mean'], \n",
    "             color=['lightblue', 'lightcoral'], edgecolor='black', linewidth=1.5)\n",
    "ax.set_ylabel('Survival Rate', fontweight='bold')\n",
    "ax.set_title('Survival Rate by Sex', fontweight='bold', fontsize=12)\n",
    "ax.set_ylim(0, 1)\n",
    "for bar, val, count in zip(bars, survival_by_sex['mean'], survival_by_sex['count']):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, val + 0.02, \n",
    "           f'{val:.1%}\\n(n={count})', ha='center', fontweight='bold')\n",
    "\n",
    "# 2. Survival by Class\n",
    "ax = axes[0, 1]\n",
    "survival_by_class = titanic_full.groupby('pclass')['survived'].agg(['mean', 'count'])\n",
    "bars = ax.bar(survival_by_class.index.astype(str), survival_by_class['mean'], \n",
    "             color=plt.cm.viridis([0.3, 0.6, 0.9]), edgecolor='black', linewidth=1.5)\n",
    "ax.set_xlabel('Class', fontweight='bold')\n",
    "ax.set_ylabel('Survival Rate', fontweight='bold')\n",
    "ax.set_title('Survival Rate by Passenger Class', fontweight='bold', fontsize=12)\n",
    "ax.set_ylim(0, 1)\n",
    "for bar, val, count in zip(bars, survival_by_class['mean'], survival_by_class['count']):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, val + 0.02, \n",
    "           f'{val:.1%}\\n(n={count})', ha='center', fontweight='bold')\n",
    "\n",
    "# 3. Survival by Embarkation Port\n",
    "ax = axes[0, 2]\n",
    "survival_by_embarked = titanic_full.groupby('embarked')['survived'].agg(['mean', 'count'])\n",
    "bars = ax.bar(survival_by_embarked.index, survival_by_embarked['mean'], \n",
    "             color=['gold', 'lightgreen', 'skyblue'], edgecolor='black', linewidth=1.5)\n",
    "ax.set_xlabel('Embarked Port', fontweight='bold')\n",
    "ax.set_ylabel('Survival Rate', fontweight='bold')\n",
    "ax.set_title('Survival Rate by Embarkation Port', fontweight='bold', fontsize=12)\n",
    "ax.set_ylim(0, 1)\n",
    "for bar, val, count in zip(bars, survival_by_embarked['mean'], survival_by_embarked['count']):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, val + 0.02, \n",
    "           f'{val:.1%}\\n(n={count})', ha='center', fontweight='bold', fontsize=9)\n",
    "\n",
    "# 4. Age distribution by survival\n",
    "ax = axes[1, 0]\n",
    "survived_ages = titanic_full[titanic_full['survived'] == 1]['age'].dropna()\n",
    "died_ages = titanic_full[titanic_full['survived'] == 0]['age'].dropna()\n",
    "ax.hist([died_ages, survived_ages], bins=20, label=['Died', 'Survived'], \n",
    "       color=['coral', 'lightgreen'], alpha=0.7, edgecolor='black')\n",
    "ax.set_xlabel('Age', fontweight='bold')\n",
    "ax.set_ylabel('Count', fontweight='bold')\n",
    "ax.set_title('Age Distribution by Survival', fontweight='bold', fontsize=12)\n",
    "ax.legend()\n",
    "\n",
    "# 5. Fare distribution by survival\n",
    "ax = axes[1, 1]\n",
    "survived_fare = titanic_full[titanic_full['survived'] == 1]['fare'].dropna()\n",
    "died_fare = titanic_full[titanic_full['survived'] == 0]['fare'].dropna()\n",
    "ax.hist([died_fare, survived_fare], bins=30, label=['Died', 'Survived'], \n",
    "       color=['coral', 'lightgreen'], alpha=0.7, edgecolor='black', range=(0, 200))\n",
    "ax.set_xlabel('Fare', fontweight='bold')\n",
    "ax.set_ylabel('Count', fontweight='bold')\n",
    "ax.set_title('Fare Distribution by Survival (capped at 200)', fontweight='bold', fontsize=12)\n",
    "ax.legend()\n",
    "\n",
    "# 6. Family size (SibSp + Parch) by survival\n",
    "ax = axes[1, 2]\n",
    "titanic_full['family_size'] = titanic_full['sibsp'] + titanic_full['parch'] + 1\n",
    "survival_by_family = titanic_full.groupby('family_size')['survived'].mean()\n",
    "counts = titanic_full['family_size'].value_counts().sort_index()\n",
    "bars = ax.bar(survival_by_family.index, survival_by_family.values, \n",
    "             color=plt.cm.coolwarm(survival_by_family.values), edgecolor='black', linewidth=1.5)\n",
    "ax.set_xlabel('Family Size', fontweight='bold')\n",
    "ax.set_ylabel('Survival Rate', fontweight='bold')\n",
    "ax.set_title('Survival Rate by Family Size', fontweight='bold', fontsize=12)\n",
    "ax.set_ylim(0, 1)\n",
    "for bar, idx in zip(bars, survival_by_family.index):\n",
    "    val = survival_by_family[idx]\n",
    "    count = counts[idx]\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, val + 0.02, \n",
    "           f'{val:.1%}\\n(n={count})', ha='center', fontweight='bold', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('titanic_eda_survival_analysis.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Insights:\")\n",
    "print(\"  ‚Ä¢ Sex: Women had ~74% survival vs ~19% for men (strong predictor!)\")\n",
    "print(\"  ‚Ä¢ Class: 1st class ~63% survival, 3rd class ~24% (socioeconomic factor)\")\n",
    "print(\"  ‚Ä¢ Age: Children appear to have higher survival rates\")\n",
    "print(\"  ‚Ä¢ Fare: Higher fares correlate with survival (proxy for class)\")\n",
    "print(\"  ‚Ä¢ Family: Solo travelers and very large families had lower survival\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis (numerical features only)\n",
    "numerical_features = titanic_full.select_dtypes(include=[np.number]).columns.tolist()\n",
    "corr_matrix = titanic_full[numerical_features].corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)\n",
    "sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "           center=0, square=True, linewidths=1, cbar_kws={'label': 'Correlation'})\n",
    "plt.title('Feature Correlation Matrix\\n(Lower triangle only)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('titanic_correlation_matrix.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Correlation with target\n",
    "target_corr = corr_matrix['survived'].drop('survived').sort_values(ascending=False)\n",
    "print(\"\\nCorrelation with Survival (target):\")\n",
    "print(\"=\" * 50)\n",
    "for feat, corr in target_corr.items():\n",
    "    print(f\"  {feat:15s}: {corr:+.3f}\")\n",
    "\n",
    "print(\"\\nüí° Strongest Predictors:\")\n",
    "print(\"  ‚Ä¢ Fare (+0.26): Higher fare ‚Üí higher survival\")\n",
    "print(\"  ‚Ä¢ Pclass (-0.34): Lower class number (1st) ‚Üí higher survival\")\n",
    "print(\"  ‚Ä¢ Age has weak negative correlation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3: Data Preprocessing (20 min)\n",
    "\n",
    "Clean and prepare data for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for preprocessing\n",
    "df = titanic_full.copy()\n",
    "\n",
    "print(\"Starting Preprocessing...\")\n",
    "print(f\"Initial shape: {df.shape}\")\n",
    "print(f\"Initial missing values: {df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "\n",
    "# 1. Age: Impute with median by class and sex (more sophisticated than overall median)\n",
    "print(\"\\nImputing Age...\")\n",
    "df['age_original'] = df['age'].copy()  # Keep original for analysis\n",
    "\n",
    "# Calculate median age by class and sex\n",
    "age_medians = df.groupby(['pclass', 'sex'])['age'].median()\n",
    "print(\"Age medians by class and sex:\")\n",
    "print(age_medians)\n",
    "\n",
    "# Impute missing ages\n",
    "for pclass in df['pclass'].unique():\n",
    "    for sex in df['sex'].unique():\n",
    "        mask = (df['pclass'] == pclass) & (df['sex'] == sex) & (df['age'].isnull())\n",
    "        median_age = age_medians.loc[(pclass, sex)]\n",
    "        df.loc[mask, 'age'] = median_age\n",
    "        n_imputed = mask.sum()\n",
    "        if n_imputed > 0:\n",
    "            print(f\"  Imputed {n_imputed} ages for class {pclass}, {sex} with {median_age}\")\n",
    "\n",
    "# 2. Embarked: Impute with mode (most common)\n",
    "print(\"\\nImputing Embarked...\")\n",
    "mode_embarked = df['embarked'].mode()[0]\n",
    "n_missing_embarked = df['embarked'].isnull().sum()\n",
    "df['embarked'].fillna(mode_embarked, inplace=True)\n",
    "print(f\"  Imputed {n_missing_embarked} missing values with mode: {mode_embarked}\")\n",
    "\n",
    "# 3. Embark_town: Fill with corresponding value\n",
    "df['embark_town'].fillna(df['embarked'].map({'C': 'Cherbourg', 'Q': 'Queenstown', 'S': 'Southampton'}), inplace=True)\n",
    "\n",
    "# 4. Deck: Too many missing (77%), create 'Unknown' category\n",
    "print(\"\\nHandling Deck...\")\n",
    "df['deck'].fillna('Unknown', inplace=True)\n",
    "print(f\"  Created 'Unknown' category for {(df['deck'] == 'Unknown').sum()} missing deck values\")\n",
    "\n",
    "# Verify no missing values in key features\n",
    "print(\"\\nMissing values after imputation:\")\n",
    "print(df[['age', 'embarked', 'embark_town', 'deck']].isnull().sum())\n",
    "\n",
    "# Drop remaining columns with too many missing or not useful\n",
    "df = df.drop(columns=['age_original'])  # We already used this for imputation\n",
    "\n",
    "print(f\"\\nShape after preprocessing: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "\n",
    "print(\"Encoding categorical variables...\")\n",
    "\n",
    "# Sex: Binary encoding (0 = female, 1 = male)\n",
    "df['sex_male'] = (df['sex'] == 'male').astype(int)\n",
    "print(f\"  ‚úì Encoded 'sex' as 'sex_male' (0=female, 1=male)\")\n",
    "\n",
    "# Embarked: One-hot encoding\n",
    "embarked_dummies = pd.get_dummies(df['embarked'], prefix='embarked', drop_first=False)\n",
    "df = pd.concat([df, embarked_dummies], axis=1)\n",
    "print(f\"  ‚úì One-hot encoded 'embarked': {embarked_dummies.columns.tolist()}\")\n",
    "\n",
    "# Class: Already numerical (1, 2, 3) but create dummies for flexibility\n",
    "class_dummies = pd.get_dummies(df['pclass'], prefix='class', drop_first=False)\n",
    "df = pd.concat([df, class_dummies], axis=1)\n",
    "print(f\"  ‚úì One-hot encoded 'pclass': {class_dummies.columns.tolist()}\")\n",
    "\n",
    "# Deck: One-hot encoding (including Unknown)\n",
    "deck_dummies = pd.get_dummies(df['deck'], prefix='deck', drop_first=False)\n",
    "df = pd.concat([df, deck_dummies], axis=1)\n",
    "print(f\"  ‚úì One-hot encoded 'deck': {len(deck_dummies.columns)} categories\")\n",
    "\n",
    "# Who: Encode man/woman/child\n",
    "who_dummies = pd.get_dummies(df['who'], prefix='who', drop_first=False)\n",
    "df = pd.concat([df, who_dummies], axis=1)\n",
    "print(f\"  ‚úì One-hot encoded 'who': {who_dummies.columns.tolist()}\")\n",
    "\n",
    "# Alone: Already binary\n",
    "df['alone_flag'] = df['alone'].astype(int)\n",
    "print(f\"  ‚úì Converted 'alone' to integer flag\")\n",
    "\n",
    "print(f\"\\nShape after encoding: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "\n",
    "# Original categorical columns (already encoded)\n",
    "cols_to_drop = ['sex', 'embarked', 'embark_town', 'class', 'who', 'adult_male', \n",
    "               'deck', 'alive', 'alone']  # 'alive' is target leakage!\n",
    "\n",
    "df = df.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "print(\"Dropped unnecessary columns:\")\n",
    "for col in cols_to_drop:\n",
    "    print(f\"  ‚Ä¢ {col}\")\n",
    "\n",
    "print(f\"\\nFinal shape: {df.shape}\")\n",
    "print(f\"\\nRemaining columns:\")\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"  {i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 4: Feature Engineering Deep Dive (15 min)\n",
    "\n",
    "Create advanced features to improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "\n",
    "print(\"Creating engineered features...\\n\")\n",
    "\n",
    "# 1. Family size (already calculated in EDA)\n",
    "df['family_size'] = df['sibsp'] + df['parch'] + 1\n",
    "print(f\"‚úì Created 'family_size': {df['family_size'].min()}-{df['family_size'].max()}\")\n",
    "\n",
    "# 2. Is alone (redundant with alone_flag but keep for clarity)\n",
    "df['is_alone'] = (df['family_size'] == 1).astype(int)\n",
    "print(f\"‚úì Created 'is_alone': {df['is_alone'].sum()} passengers traveled alone\")\n",
    "\n",
    "# 3. Age groups (bin continuous age into categories)\n",
    "age_bins = [0, 12, 18, 35, 60, 100]\n",
    "age_labels = ['Child', 'Teen', 'Adult', 'Middle-Aged', 'Senior']\n",
    "df['age_group'] = pd.cut(df['age'], bins=age_bins, labels=age_labels)\n",
    "\n",
    "# One-hot encode age groups\n",
    "age_group_dummies = pd.get_dummies(df['age_group'], prefix='age_group')\n",
    "df = pd.concat([df, age_group_dummies], axis=1)\n",
    "print(f\"‚úì Created age groups: {age_labels}\")\n",
    "\n",
    "# 4. Fare bins (categorize fares)\n",
    "fare_bins = [0, 10, 30, 100, 600]\n",
    "fare_labels = ['Low', 'Medium', 'High', 'Very High']\n",
    "df['fare_group'] = pd.cut(df['fare'], bins=fare_bins, labels=fare_labels)\n",
    "\n",
    "# One-hot encode fare groups\n",
    "fare_group_dummies = pd.get_dummies(df['fare_group'], prefix='fare_group')\n",
    "df = pd.concat([df, fare_group_dummies], axis=1)\n",
    "print(f\"‚úì Created fare groups: {fare_labels}\")\n",
    "\n",
    "# 5. Fare per person (fare divided by family size)\n",
    "df['fare_per_person'] = df['fare'] / df['family_size']\n",
    "print(f\"‚úì Created 'fare_per_person': mean = {df['fare_per_person'].mean():.2f}\")\n",
    "\n",
    "# 6. Age * Class interaction (socioeconomic + age effect)\n",
    "df['age_class_interaction'] = df['age'] * df['pclass']\n",
    "print(f\"‚úì Created 'age_class_interaction'\")\n",
    "\n",
    "# 7. Title extraction from name (if 'name' column exists)\n",
    "# Note: seaborn titanic doesn't have 'name', but Kaggle version does\n",
    "# We'll skip this for seaborn dataset\n",
    "\n",
    "# 8. Cabin number extraction (from deck)\n",
    "# Already handled via deck encoding\n",
    "\n",
    "# Drop intermediate categorical columns\n",
    "df = df.drop(columns=['age_group', 'fare_group'], errors='ignore')\n",
    "\n",
    "print(f\"\\nTotal features after engineering: {df.shape[1] - 1} (excluding target)\")\n",
    "print(f\"\\nNew engineered features:\")\n",
    "engineered_cols = ['family_size', 'is_alone', 'fare_per_person', 'age_class_interaction']\n",
    "for col in engineered_cols:\n",
    "    if col in df.columns:\n",
    "        print(f\"  ‚Ä¢ {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection: Analyze feature importance using simple tree model\n",
    "\n",
    "# Prepare feature matrix\n",
    "feature_cols = [col for col in df.columns if col != 'survived']\n",
    "X_all = df[feature_cols].copy()\n",
    "y_all = df['survived'].copy()\n",
    "\n",
    "# Handle any remaining missing values\n",
    "X_all = X_all.fillna(0)\n",
    "\n",
    "print(f\"Feature matrix shape: {X_all.shape}\")\n",
    "print(f\"Target shape: {y_all.shape}\")\n",
    "print(f\"Target distribution: {y_all.value_counts().to_dict()}\")\n",
    "\n",
    "# Quick feature importance using Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_quick = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "rf_quick.fit(X_all, y_all)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': rf_quick.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Plot top 20 features\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_20 = feature_importance.head(20)\n",
    "plt.barh(range(len(top_20)), top_20['importance'], \n",
    "        color=plt.cm.viridis(np.linspace(0, 1, len(top_20))), edgecolor='black')\n",
    "plt.yticks(range(len(top_20)), top_20['feature'])\n",
    "plt.xlabel('Importance', fontweight='bold')\n",
    "plt.title('Top 20 Feature Importances (Random Forest)', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(feature_importance.head(10).to_string(index=False))\n",
    "\n",
    "# Select top features (optional - keep all for now)\n",
    "# We'll let the models handle feature selection via regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "# Since we don't have separate test set, we'll create our own split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_all, y_all, test_size=0.2, random_state=RANDOM_STATE, stratify=y_all\n",
    ")\n",
    "\n",
    "print(\"Train-Test Split:\")\n",
    "print(f\"  Training set: {X_train.shape[0]} samples ({len(y_train)/len(y_all)*100:.1f}%)\")\n",
    "print(f\"  Test set: {X_test.shape[0]} samples ({len(y_test)/len(y_all)*100:.1f}%)\")\n",
    "print(f\"\\nTrain class distribution: {y_train.value_counts().to_dict()}\")\n",
    "print(f\"Test class distribution: {y_test.value_counts().to_dict()}\")\n",
    "print(f\"\\nNumber of features: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 5: Baseline Models (15 min)\n",
    "\n",
    "Establish baseline performance with simple models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance tracker\n",
    "results_tracker = []\n",
    "\n",
    "def evaluate_model(model, model_name, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Evaluate model and store results.\n",
    "    \"\"\"\n",
    "    # Train\n",
    "    import time\n",
    "    start = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = time.time() - start\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Cross-validation score\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    cv_mean = cv_scores.mean()\n",
    "    cv_std = cv_scores.std()\n",
    "    \n",
    "    # Store results\n",
    "    results_tracker.append({\n",
    "        'Model': model_name,\n",
    "        'Test Accuracy': accuracy,\n",
    "        'CV Mean': cv_mean,\n",
    "        'CV Std': cv_std,\n",
    "        'Train Time (s)': train_time\n",
    "    })\n",
    "    \n",
    "    print(f\"{model_name}:\")\n",
    "    print(f\"  Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  CV Accuracy: {cv_mean:.4f} ¬± {cv_std:.4f}\")\n",
    "    print(f\"  Train Time: {train_time:.3f}s\")\n",
    "    print()\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"Baseline Models Evaluation\")\n",
    "print(\"=\" * 80)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Logistic Regression (simple linear baseline)\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=RANDOM_STATE)\n",
    "lr_model = evaluate_model(lr_model, 'Logistic Regression', X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Decision Tree (simple non-linear baseline)\n",
    "dt_model = DecisionTreeClassifier(max_depth=5, random_state=RANDOM_STATE)\n",
    "dt_model = evaluate_model(dt_model, 'Decision Tree', X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display baseline results\n",
    "baseline_df = pd.DataFrame(results_tracker)\n",
    "print(\"\\nBaseline Results Summary:\")\n",
    "print(\"=\" * 80)\n",
    "print(baseline_df.to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "x = np.arange(len(baseline_df))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, baseline_df['Test Accuracy'], width, label='Test Accuracy', alpha=0.8, edgecolor='black')\n",
    "plt.bar(x + width/2, baseline_df['CV Mean'], width, label='CV Mean', alpha=0.8, edgecolor='black')\n",
    "\n",
    "plt.xlabel('Model', fontweight='bold')\n",
    "plt.ylabel('Accuracy', fontweight='bold')\n",
    "plt.title('Baseline Model Performance', fontsize=14, fontweight='bold')\n",
    "plt.xticks(x, baseline_df['Model'])\n",
    "plt.legend()\n",
    "plt.ylim(0.6, 1.0)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Baseline established!\")\n",
    "best_baseline = baseline_df.loc[baseline_df['Test Accuracy'].idxmax()]\n",
    "print(f\"  Best baseline: {best_baseline['Model']} with {best_baseline['Test Accuracy']:.4f} accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 6: Ensemble Methods (30 min)\n",
    "\n",
    "Test all major ensemble algorithms with default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ensemble Methods Evaluation\")\n",
    "print(\"=\" * 80)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Random Forest\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100, \n",
    "    max_depth=10,\n",
    "    random_state=RANDOM_STATE, \n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_model = evaluate_model(rf_model, 'Random Forest', X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. XGBoost\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    "    verbosity=0\n",
    ")\n",
    "xgb_model = evaluate_model(xgb_model, 'XGBoost', X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. LightGBM\n",
    "lgb_model = lgb.LGBMClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    "    verbose=-1\n",
    ")\n",
    "lgb_model = evaluate_model(lgb_model, 'LightGBM', X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. CatBoost\n",
    "cat_model = CatBoostClassifier(\n",
    "    iterations=100,\n",
    "    depth=3,\n",
    "    learning_rate=0.1,\n",
    "    random_state=RANDOM_STATE,\n",
    "    verbose=0\n",
    ")\n",
    "cat_model = evaluate_model(cat_model, 'CatBoost', X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all results so far\n",
    "all_results_df = pd.DataFrame(results_tracker)\n",
    "all_results_df = all_results_df.sort_values('Test Accuracy', ascending=False)\n",
    "\n",
    "print(\"\\nAll Models Performance Summary:\")\n",
    "print(\"=\" * 80)\n",
    "print(all_results_df.to_string(index=False))\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Accuracy comparison\n",
    "ax = axes[0]\n",
    "sorted_df = all_results_df.sort_values('Test Accuracy', ascending=True)\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(sorted_df)))\n",
    "bars = ax.barh(sorted_df['Model'], sorted_df['Test Accuracy'], color=colors, edgecolor='black')\n",
    "ax.set_xlabel('Test Accuracy', fontweight='bold')\n",
    "ax.set_title('Model Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_xlim(0.7, 0.9)\n",
    "for bar, val in zip(bars, sorted_df['Test Accuracy']):\n",
    "    ax.text(val + 0.005, bar.get_y() + bar.get_height()/2, \n",
    "           f'{val:.4f}', va='center', fontweight='bold')\n",
    "\n",
    "# Training time comparison\n",
    "ax = axes[1]\n",
    "sorted_df_time = all_results_df.sort_values('Train Time (s)', ascending=True)\n",
    "colors = plt.cm.plasma(np.linspace(0, 1, len(sorted_df_time)))\n",
    "bars = ax.barh(sorted_df_time['Model'], sorted_df_time['Train Time (s)'], color=colors, edgecolor='black')\n",
    "ax.set_xlabel('Training Time (seconds)', fontweight='bold')\n",
    "ax.set_title('Model Training Time Comparison', fontsize=14, fontweight='bold')\n",
    "for bar, val in zip(bars, sorted_df_time['Train Time (s)']):\n",
    "    ax.text(val + 0.01, bar.get_y() + bar.get_height()/2, \n",
    "           f'{val:.3f}s', va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ensemble_comparison.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüèÜ Current Best Model:\")\n",
    "best_model_row = all_results_df.iloc[0]\n",
    "print(f\"  {best_model_row['Model']}\")\n",
    "print(f\"  Test Accuracy: {best_model_row['Test Accuracy']:.4f}\")\n",
    "print(f\"  CV Accuracy: {best_model_row['CV Mean']:.4f} ¬± {best_model_row['CV Std']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 7: Hyperparameter Optimization (20 min)\n",
    "\n",
    "Use Optuna to optimize the top performing models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hyperparameter Optimization with Optuna\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# We'll optimize the top 2-3 models\n",
    "# For demonstration, we'll optimize XGBoost and Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize XGBoost\n",
    "\n",
    "def objective_xgb(trial):\n",
    "    \"\"\"\n",
    "    Optuna objective function for XGBoost hyperparameter tuning.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'n_jobs': -1,\n",
    "        'verbosity': 0\n",
    "    }\n",
    "    \n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    \n",
    "    return cv_scores.mean()\n",
    "\n",
    "print(\"Optimizing XGBoost...\")\n",
    "study_xgb = optuna.create_study(direction='maximize', study_name='xgboost_optimization')\n",
    "study_xgb.optimize(objective_xgb, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\n‚úì Best XGBoost CV Accuracy: {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\nBest Parameters:\")\n",
    "for param, value in study_xgb.best_params.items():\n",
    "    print(f\"  {param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize Random Forest\n",
    "\n",
    "def objective_rf(trial):\n",
    "    \"\"\"\n",
    "    Optuna objective function for Random Forest hyperparameter tuning.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 20),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "    \n",
    "    model = RandomForestClassifier(**params)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    \n",
    "    return cv_scores.mean()\n",
    "\n",
    "print(\"Optimizing Random Forest...\")\n",
    "study_rf = optuna.create_study(direction='maximize', study_name='random_forest_optimization')\n",
    "study_rf.optimize(objective_rf, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\n‚úì Best Random Forest CV Accuracy: {study_rf.best_value:.4f}\")\n",
    "print(f\"\\nBest Parameters:\")\n",
    "for param, value in study_rf.best_params.items():\n",
    "    print(f\"  {param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize LightGBM\n",
    "\n",
    "def objective_lgb(trial):\n",
    "    \"\"\"\n",
    "    Optuna objective function for LightGBM hyperparameter tuning.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'n_jobs': -1,\n",
    "        'verbose': -1\n",
    "    }\n",
    "    \n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    \n",
    "    return cv_scores.mean()\n",
    "\n",
    "print(\"Optimizing LightGBM...\")\n",
    "study_lgb = optuna.create_study(direction='maximize', study_name='lightgbm_optimization')\n",
    "study_lgb.optimize(objective_lgb, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\n‚úì Best LightGBM CV Accuracy: {study_lgb.best_value:.4f}\")\n",
    "print(f\"\\nBest Parameters:\")\n",
    "for param, value in study_lgb.best_params.items():\n",
    "    print(f\"  {param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train optimized models and evaluate\n",
    "\n",
    "print(\"Training optimized models...\\n\")\n",
    "\n",
    "# Optimized XGBoost\n",
    "xgb_optimized = xgb.XGBClassifier(**study_xgb.best_params, random_state=RANDOM_STATE, n_jobs=-1, verbosity=0)\n",
    "xgb_optimized = evaluate_model(xgb_optimized, 'XGBoost (Optimized)', X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Optimized Random Forest\n",
    "rf_optimized = RandomForestClassifier(**study_rf.best_params, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "rf_optimized = evaluate_model(rf_optimized, 'Random Forest (Optimized)', X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Optimized LightGBM\n",
    "lgb_optimized = lgb.LGBMClassifier(**study_lgb.best_params, random_state=RANDOM_STATE, n_jobs=-1, verbose=-1)\n",
    "lgb_optimized = evaluate_model(lgb_optimized, 'LightGBM (Optimized)', X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare default vs optimized\n",
    "all_results_df = pd.DataFrame(results_tracker)\n",
    "all_results_df = all_results_df.sort_values('Test Accuracy', ascending=False)\n",
    "\n",
    "print(\"\\nDefault vs Optimized Comparison:\")\n",
    "print(\"=\" * 80)\n",
    "print(all_results_df.to_string(index=False))\n",
    "\n",
    "# Visualize improvement\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Group by base model\n",
    "model_groups = {\n",
    "    'XGBoost': ['XGBoost', 'XGBoost (Optimized)'],\n",
    "    'Random Forest': ['Random Forest', 'Random Forest (Optimized)'],\n",
    "    'LightGBM': ['LightGBM', 'LightGBM (Optimized)']\n",
    "}\n",
    "\n",
    "x_pos = 0\n",
    "for group_name, model_names in model_groups.items():\n",
    "    for model_name in model_names:\n",
    "        if model_name in all_results_df['Model'].values:\n",
    "            row = all_results_df[all_results_df['Model'] == model_name].iloc[0]\n",
    "            color = 'lightblue' if 'Optimized' not in model_name else 'darkgreen'\n",
    "            bar = ax.bar(x_pos, row['Test Accuracy'], color=color, edgecolor='black', alpha=0.8)\n",
    "            ax.text(x_pos, row['Test Accuracy'] + 0.005, f\"{row['Test Accuracy']:.4f}\", \n",
    "                   ha='center', fontweight='bold', fontsize=9)\n",
    "            x_pos += 1\n",
    "    x_pos += 0.5  # Gap between groups\n",
    "\n",
    "ax.set_ylabel('Test Accuracy', fontweight='bold')\n",
    "ax.set_title('Default vs Optimized Model Performance', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim(0.75, 0.88)\n",
    "ax.set_xticks(range(int(x_pos)))\n",
    "ax.set_xticklabels([m for models in model_groups.values() for m in models if m in all_results_df['Model'].values], \n",
    "                   rotation=45, ha='right')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor='lightblue', edgecolor='black', label='Default'),\n",
    "                  Patch(facecolor='darkgreen', edgecolor='black', label='Optimized')]\n",
    "ax.legend(handles=legend_elements)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('optimization_comparison.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Optimization improved performance for all models!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 8: Model Stacking (15 min)\n",
    "\n",
    "Combine the best models using stacking ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating Stacking Ensemble...\\n\")\n",
    "\n",
    "# Select diverse base models (optimized versions)\n",
    "base_estimators = [\n",
    "    ('rf', rf_optimized),\n",
    "    ('xgb', xgb_optimized),\n",
    "    ('lgb', lgb_optimized)\n",
    "]\n",
    "\n",
    "# Meta-learner: Logistic Regression (simple and interpretable)\n",
    "meta_learner = LogisticRegression(max_iter=1000, random_state=RANDOM_STATE)\n",
    "\n",
    "# Create stacking classifier\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=base_estimators,\n",
    "    final_estimator=meta_learner,\n",
    "    cv=5,  # Use cross-validation to generate meta-features\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Base models:\")\n",
    "for name, model in base_estimators:\n",
    "    print(f\"  ‚Ä¢ {name}: {model.__class__.__name__}\")\n",
    "print(f\"\\nMeta-learner: {meta_learner.__class__.__name__}\")\n",
    "print(f\"CV folds: 5\\n\")\n",
    "\n",
    "# Train and evaluate\n",
    "stacking_model = evaluate_model(stacking_model, 'Stacking Ensemble', X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also try Voting Ensemble for comparison\n",
    "\n",
    "print(\"Creating Voting Ensemble...\\n\")\n",
    "\n",
    "voting_model = VotingClassifier(\n",
    "    estimators=base_estimators,\n",
    "    voting='soft',  # Use probability predictions\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "voting_model = evaluate_model(voting_model, 'Voting Ensemble', X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comparison\n",
    "final_results_df = pd.DataFrame(results_tracker)\n",
    "final_results_df = final_results_df.sort_values('Test Accuracy', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FINAL MODEL RANKINGS\")\n",
    "print(\"=\" * 80)\n",
    "print(final_results_df.to_string(index=False))\n",
    "\n",
    "# Top 5 models\n",
    "print(\"\\nüèÜ TOP 5 MODELS:\")\n",
    "for i, row in final_results_df.head(5).iterrows():\n",
    "    rank = final_results_df.index.get_loc(i) + 1\n",
    "    print(f\"  {rank}. {row['Model']:25s} - Accuracy: {row['Test Accuracy']:.4f} (CV: {row['CV Mean']:.4f} ¬± {row['CV Std']:.4f})\")\n",
    "\n",
    "# Best model\n",
    "best_final = final_results_df.iloc[0]\n",
    "print(f\"\\nüéØ BEST MODEL: {best_final['Model']}\")\n",
    "print(f\"   Test Accuracy: {best_final['Test Accuracy']:.4f}\")\n",
    "print(f\"   CV Accuracy: {best_final['CV Mean']:.4f} ¬± {best_final['CV Std']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 9: Model Interpretation (15 min)\n",
    "\n",
    "Understand what the models learned using feature importance and SHAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance from best tree-based model\n",
    "# Let's use XGBoost optimized\n",
    "\n",
    "feature_names = X_train.columns.tolist()\n",
    "importances = xgb_optimized.feature_importances_\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importance (XGBoost Optimized):\")\n",
    "print(\"=\" * 80)\n",
    "print(importance_df.head(15).to_string(index=False))\n",
    "\n",
    "# Visualize top 15 features\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_15 = importance_df.head(15)\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(top_15)))\n",
    "plt.barh(range(len(top_15)), top_15['Importance'], color=colors, edgecolor='black')\n",
    "plt.yticks(range(len(top_15)), top_15['Feature'])\n",
    "plt.xlabel('Importance', fontweight='bold')\n",
    "plt.title('Top 15 Feature Importances (XGBoost)', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig('final_feature_importance.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP values for model interpretation\n",
    "\n",
    "print(\"Calculating SHAP values...\")\n",
    "\n",
    "# Create SHAP explainer\n",
    "explainer = shap.TreeExplainer(xgb_optimized)\n",
    "\n",
    "# Calculate SHAP values for test set (use subset for speed)\n",
    "shap_sample_size = min(100, len(X_test))\n",
    "X_shap = X_test.iloc[:shap_sample_size]\n",
    "shap_values = explainer.shap_values(X_shap)\n",
    "\n",
    "print(f\"‚úì SHAP values calculated for {shap_sample_size} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Summary Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "shap.summary_plot(shap_values, X_shap, show=False, max_display=15)\n",
    "plt.title('SHAP Feature Importance Summary', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('shap_summary.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° SHAP Interpretation:\")\n",
    "print(\"  ‚Ä¢ Red dots = High feature value\")\n",
    "print(\"  ‚Ä¢ Blue dots = Low feature value\")\n",
    "print(\"  ‚Ä¢ X-axis: Impact on prediction (positive = increase survival probability)\")\n",
    "print(\"  ‚Ä¢ Y-axis: Features ordered by importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual prediction explanation (first passenger in test set)\n",
    "\n",
    "sample_idx = 0\n",
    "sample_passenger = X_test.iloc[sample_idx:sample_idx+1]\n",
    "sample_shap = shap_values[sample_idx]\n",
    "\n",
    "# Actual prediction\n",
    "pred_proba = xgb_optimized.predict_proba(sample_passenger)[0]\n",
    "pred_class = xgb_optimized.predict(sample_passenger)[0]\n",
    "actual_class = y_test.iloc[sample_idx]\n",
    "\n",
    "print(f\"Individual Prediction Explanation (Passenger #{sample_idx}):\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Prediction: {'Survived' if pred_class == 1 else 'Did not survive'}\")\n",
    "print(f\"Probability: {pred_proba[1]:.2%} (survive), {pred_proba[0]:.2%} (not survive)\")\n",
    "print(f\"Actual: {'Survived' if actual_class == 1 else 'Did not survive'}\")\n",
    "print(f\"\\nTop features influencing this prediction:\")\n",
    "\n",
    "# Get top SHAP values for this instance\n",
    "feature_shap = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Value': sample_passenger.values[0],\n",
    "    'SHAP': sample_shap\n",
    "}).sort_values('SHAP', key=abs, ascending=False)\n",
    "\n",
    "print(feature_shap.head(10).to_string(index=False))\n",
    "\n",
    "# Waterfall plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "shap.waterfall_plot(shap.Explanation(values=sample_shap, \n",
    "                                     base_values=explainer.expected_value,\n",
    "                                     data=sample_passenger.values[0],\n",
    "                                     feature_names=feature_names),\n",
    "                   max_display=15, show=False)\n",
    "plt.title(f'SHAP Waterfall Plot - Passenger #{sample_idx}', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('shap_waterfall.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 10: Final Submission (10 min)\n",
    "\n",
    "Generate predictions and create submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model for final predictions\n",
    "best_model = xgb_optimized  # Or whichever performed best\n",
    "best_model_name = 'XGBoost (Optimized)'\n",
    "\n",
    "print(f\"Selected model for submission: {best_model_name}\")\n",
    "print(f\"Test Accuracy: {best_final['Test Accuracy']:.4f}\")\n",
    "\n",
    "# Generate predictions on test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(f\"\\nGenerated {len(y_pred)} predictions\")\n",
    "print(f\"Predicted survival rate: {y_pred.mean():.2%}\")\n",
    "print(f\"Actual survival rate: {y_test.mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission dataframe\n",
    "# In real Kaggle, this would be PassengerId from test.csv\n",
    "# For our demo, we'll use test set indices\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'PassengerId': range(1, len(y_pred) + 1),\n",
    "    'Survived': y_pred\n",
    "})\n",
    "\n",
    "print(\"Submission file preview:\")\n",
    "print(submission.head(10))\n",
    "\n",
    "# Save to CSV\n",
    "submission_path = 'titanic_submission.csv'\n",
    "submission.to_csv(submission_path, index=False)\n",
    "print(f\"\\n‚úì Submission saved to: {submission_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate submission file\n",
    "\n",
    "print(\"Submission File Validation:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check format\n",
    "loaded_submission = pd.read_csv(submission_path)\n",
    "print(f\"‚úì File can be loaded\")\n",
    "print(f\"‚úì Shape: {loaded_submission.shape}\")\n",
    "print(f\"‚úì Columns: {loaded_submission.columns.tolist()}\")\n",
    "\n",
    "# Check values\n",
    "assert loaded_submission['Survived'].isin([0, 1]).all(), \"Invalid survival values!\"\n",
    "print(f\"‚úì All survival values are 0 or 1\")\n",
    "\n",
    "assert not loaded_submission.isnull().any().any(), \"Missing values found!\"\n",
    "print(f\"‚úì No missing values\")\n",
    "\n",
    "print(f\"\\n‚úì Submission file is valid and ready!\")\n",
    "\n",
    "# Distribution\n",
    "print(f\"\\nPrediction distribution:\")\n",
    "print(loaded_submission['Survived'].value_counts())\n",
    "print(f\"Predicted survival rate: {loaded_submission['Survived'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix and classification report\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', square=True, \n",
    "           xticklabels=['Did not survive', 'Survived'],\n",
    "           yticklabels=['Did not survive', 'Survived'])\n",
    "plt.xlabel('Predicted', fontweight='bold')\n",
    "plt.ylabel('Actual', fontweight='bold')\n",
    "plt.title('Confusion Matrix - Final Model', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(\"=\" * 80)\n",
    "print(classification_report(y_test, y_pred, target_names=['Did not survive', 'Survived']))\n",
    "\n",
    "# ROC curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random classifier')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontweight='bold')\n",
    "plt.ylabel('True Positive Rate', fontweight='bold')\n",
    "plt.title('ROC Curve - Final Model', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('roc_curve.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüéØ ROC AUC Score: {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Production Deployment Considerations (10 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the complete prediction pipeline\n",
    "\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "# Create models directory\n",
    "models_dir = Path('models')\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save best model\n",
    "model_path = models_dir / 'titanic_xgboost_optimized.joblib'\n",
    "joblib.dump(best_model, model_path)\n",
    "print(f\"‚úì Model saved to: {model_path}\")\n",
    "\n",
    "# Save feature names (important for production inference!)\n",
    "feature_names_path = models_dir / 'feature_names.joblib'\n",
    "joblib.dump(feature_names, feature_names_path)\n",
    "print(f\"‚úì Feature names saved to: {feature_names_path}\")\n",
    "\n",
    "# Test loading\n",
    "loaded_model = joblib.load(model_path)\n",
    "loaded_features = joblib.load(feature_names_path)\n",
    "\n",
    "# Verify predictions match\n",
    "test_pred = loaded_model.predict(X_test)\n",
    "assert np.array_equal(test_pred, y_pred), \"Loaded model predictions don't match!\"\n",
    "print(f\"‚úì Model loaded successfully and predictions verified\")\n",
    "\n",
    "print(f\"\\nModel file size: {model_path.stat().st_size / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inference function for production\n",
    "\n",
    "def predict_survival(passenger_data, model_path='models/titanic_xgboost_optimized.joblib'):\n",
    "    \"\"\"\n",
    "    Production inference function for Titanic survival prediction.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    passenger_data : dict or pd.DataFrame\n",
    "        Passenger features (must match training features)\n",
    "    model_path : str\n",
    "        Path to saved model file\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Prediction results with probability and class\n",
    "    \"\"\"\n",
    "    # Load model\n",
    "    model = joblib.load(model_path)\n",
    "    \n",
    "    # Convert to DataFrame if dict\n",
    "    if isinstance(passenger_data, dict):\n",
    "        passenger_data = pd.DataFrame([passenger_data])\n",
    "    \n",
    "    # Predict\n",
    "    prediction = model.predict(passenger_data)[0]\n",
    "    probability = model.predict_proba(passenger_data)[0]\n",
    "    \n",
    "    return {\n",
    "        'survived': bool(prediction),\n",
    "        'survival_probability': float(probability[1]),\n",
    "        'confidence': float(max(probability))\n",
    "    }\n",
    "\n",
    "# Test inference function\n",
    "sample = X_test.iloc[0].to_dict()\n",
    "result = predict_survival(sample)\n",
    "\n",
    "print(\"Production Inference Test:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Input: {len(sample)} features\")\n",
    "print(f\"\\nOutput:\")\n",
    "for key, value in result.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\n‚úì Inference function working correctly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production deployment checklist\n",
    "\n",
    "print(\"Production Deployment Checklist\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "checklist = \"\"\"\n",
    "‚úì Model trained and validated\n",
    "‚úì Hyperparameters optimized\n",
    "‚úì Cross-validation performed (5-fold)\n",
    "‚úì Test set accuracy: {:.2%}\n",
    "‚úì ROC AUC: {:.3f}\n",
    "‚úì Model saved to disk\n",
    "‚úì Feature names saved\n",
    "‚úì Inference function created and tested\n",
    "‚úì Submission file generated\n",
    "\n",
    "NEXT STEPS FOR PRODUCTION:\n",
    "‚ñ° Set up monitoring for prediction distribution\n",
    "‚ñ° Implement feature drift detection\n",
    "‚ñ° Create API endpoint (Flask/FastAPI)\n",
    "‚ñ° Add input validation\n",
    "‚ñ° Set up logging\n",
    "‚ñ° Create model versioning system\n",
    "‚ñ° Plan retraining schedule\n",
    "‚ñ° Document model assumptions and limitations\n",
    "‚ñ° Prepare model card (description, performance, ethical considerations)\n",
    "‚ñ° Set up A/B testing framework\n",
    "\"\"\".format(best_final['Test Accuracy'], roc_auc)\n",
    "\n",
    "print(checklist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Learnings\n",
    "\n",
    "### Project Achievements\n",
    "\n",
    "We completed a full end-to-end machine learning competition workflow:\n",
    "\n",
    "1. **Data Exploration**: Analyzed 891 Titanic passengers with comprehensive EDA\n",
    "2. **Feature Engineering**: Created 10+ derived features (family size, age groups, fare bins, etc.)\n",
    "3. **Model Comparison**: Tested 10+ models from simple baselines to advanced ensembles\n",
    "4. **Hyperparameter Optimization**: Used Optuna to find optimal parameters\n",
    "5. **Ensemble Stacking**: Combined best models for maximum performance\n",
    "6. **Model Interpretation**: Used SHAP to understand predictions\n",
    "7. **Production Readiness**: Created inference pipeline and deployment checklist\n",
    "\n",
    "### Performance Summary\n",
    "\n",
    "**Best Model**: XGBoost (Optimized)\n",
    "- **Test Accuracy**: ~83-85% (varies by random seed)\n",
    "- **ROC AUC**: ~0.85-0.88\n",
    "- **Key Features**: Sex, fare, age, passenger class\n",
    "\n",
    "### Key Insights About Titanic Survival\n",
    "\n",
    "1. **Gender was the strongest predictor**: Women had ~74% survival vs ~19% for men\n",
    "2. **Socioeconomic status mattered**: 1st class had 63% survival vs 24% for 3rd class\n",
    "3. **Age played a role**: Children had higher survival rates (\"women and children first\")\n",
    "4. **Family size effect**: Solo travelers and very large families had lower survival\n",
    "5. **Fare as a proxy**: Higher fares correlated with survival (linked to class)\n",
    "\n",
    "### Technical Lessons Learned\n",
    "\n",
    "1. **Feature Engineering >> Model Choice**: Good features matter more than complex models\n",
    "2. **Baseline First**: Always establish simple baseline before complex ensembles\n",
    "3. **Cross-Validation is Critical**: Single train-test split can be misleading\n",
    "4. **Hyperparameter Tuning Helps**: 2-5% accuracy improvement from optimization\n",
    "5. **Stacking Has Limits**: Sometimes doesn't improve over best single model\n",
    "6. **Interpretability Matters**: SHAP helps validate that model learned sensible patterns\n",
    "\n",
    "### Ensemble Methods Comparison\n",
    "\n",
    "**For this dataset:**\n",
    "- **XGBoost**: Best performance after tuning, fast inference\n",
    "- **Random Forest**: Close second, more robust to hyperparameters\n",
    "- **LightGBM**: Fastest training, competitive accuracy\n",
    "- **Stacking**: Marginal improvement, much more complex\n",
    "\n",
    "### Kaggle Competition Tips\n",
    "\n",
    "1. **Spend time on EDA**: Understand the data deeply\n",
    "2. **Feature engineering is key**: Create meaningful derived features\n",
    "3. **Try multiple models**: Don't commit to one too early\n",
    "4. **Optimize systematically**: Use Optuna or similar tools\n",
    "5. **Ensemble carefully**: Stack diverse models, not similar ones\n",
    "6. **Validate properly**: Use cross-validation + holdout test\n",
    "7. **Iterate quickly**: Fast experiments > perfect first try\n",
    "\n",
    "### Production Deployment Insights\n",
    "\n",
    "1. **Save everything**: Model, feature names, preprocessing steps\n",
    "2. **Version control**: Track data, code, and model versions\n",
    "3. **Monitor in production**: Prediction distribution, feature drift\n",
    "4. **Plan for retraining**: Schedule and criteria for updates\n",
    "5. **Document assumptions**: What the model expects, limitations\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "To further improve this solution:\n",
    "\n",
    "1. **More Feature Engineering**:\n",
    "   - Title extraction from names (Mr, Mrs, Master, etc.)\n",
    "   - Deck location analysis\n",
    "   - Family survival interaction features\n",
    "\n",
    "2. **Advanced Ensembling**:\n",
    "   - Multi-level stacking\n",
    "   - Blending with different CV strategies\n",
    "   - Neural network meta-learner\n",
    "\n",
    "3. **Data Augmentation**:\n",
    "   - External data (historical records)\n",
    "   - Synthetic minority oversampling\n",
    "\n",
    "4. **Model Calibration**:\n",
    "   - Probability calibration (Platt scaling, isotonic regression)\n",
    "   - Threshold optimization for business metrics\n",
    "\n",
    "### Congratulations!\n",
    "\n",
    "You've completed the Ensemble Methods module with a comprehensive Kaggle-style project. You now have:\n",
    "\n",
    "‚úì **Practical Experience**: Full ML pipeline from data to deployment\n",
    "‚úì **Technical Skills**: Ensemble methods, optimization, interpretation\n",
    "‚úì **Best Practices**: Production-ready code and workflows\n",
    "‚úì **Portfolio Piece**: Complete project to showcase\n",
    "\n",
    "**Next Steps**: Apply these techniques to other datasets, participate in real Kaggle competitions, and continue building your ML expertise!\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- **Kaggle Learn**: https://www.kaggle.com/learn\n",
    "- **XGBoost Documentation**: https://xgboost.readthedocs.io/\n",
    "- **LightGBM Documentation**: https://lightgbm.readthedocs.io/\n",
    "- **SHAP Documentation**: https://shap.readthedocs.io/\n",
    "- **Optuna Documentation**: https://optuna.readthedocs.io/\n",
    "- **Feature Engineering Book**: \"Feature Engineering for Machine Learning\" by Alice Zheng\n",
    "- **Kaggle Titanic**: https://www.kaggle.com/c/titanic (original competition)\n",
    "\n",
    "Happy modeling! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
