{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 09: Voting Classifiers and Regressors\n",
    "\n",
    "**Difficulty**: ⭐⭐\n",
    "**Estimated Time**: 40 minutes\n",
    "**Prerequisites**: \n",
    "- Module 00: Introduction to Ensemble Methods\n",
    "- Module 01: Bagging and Bootstrap Aggregation\n",
    "- Module 02: Random Forest\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Understand the difference between hard and soft voting\n",
    "2. Implement VotingClassifier and VotingRegressor with scikit-learn\n",
    "3. Combine different types of algorithms effectively\n",
    "4. Tune voting weights for optimal performance\n",
    "5. Know when voting ensembles help and when they don't\n",
    "6. Compare voting with stacking and other ensemble methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Voting Ensembles\n",
    "\n",
    "### What is Voting?\n",
    "\n",
    "**Voting** is one of the simplest ensemble methods where multiple models \"vote\" on the final prediction. The final output is determined by combining the votes of all models.\n",
    "\n",
    "### Types of Voting:\n",
    "\n",
    "#### 1. **Hard Voting (Majority Voting)**\n",
    "\n",
    "Each model casts a vote for a class, and the class with the most votes wins.\n",
    "\n",
    "```\n",
    "Example for 3-class classification:\n",
    "\n",
    "Model 1:  Class A\n",
    "Model 2:  Class B\n",
    "Model 3:  Class A    } Hard Vote → Class A wins (2 votes)\n",
    "Model 4:  Class A\n",
    "Model 5:  Class B\n",
    "\n",
    "Final Prediction: Class A\n",
    "```\n",
    "\n",
    "#### 2. **Soft Voting (Weighted Voting)**\n",
    "\n",
    "Models output probability estimates, which are averaged. The class with highest average probability wins.\n",
    "\n",
    "```\n",
    "Example for binary classification:\n",
    "\n",
    "           Class 0   Class 1\n",
    "Model 1:    0.4       0.6\n",
    "Model 2:    0.3       0.7    } Average: 0.35, 0.65\n",
    "Model 3:    0.4       0.6\n",
    "\n",
    "Final Prediction: Class 1 (probability = 0.65)\n",
    "```\n",
    "\n",
    "**Soft voting usually performs better** because it uses more information (probabilities vs just labels).\n",
    "\n",
    "### Voting for Regression:\n",
    "\n",
    "For regression, voting simply **averages the predictions** of all models:\n",
    "\n",
    "```\n",
    "Model 1 predicts: 45.2\n",
    "Model 2 predicts: 47.8\n",
    "Model 3 predicts: 46.1\n",
    "\n",
    "Final prediction: (45.2 + 47.8 + 46.1) / 3 = 46.37\n",
    "```\n",
    "\n",
    "### Why Voting Works:\n",
    "\n",
    "- **Reduces variance**: Individual model errors cancel out\n",
    "- **Robust**: Not dependent on any single model\n",
    "- **Simple**: Easy to understand and implement\n",
    "- **Diversity**: Different models make different errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.datasets import make_classification, load_breast_cancer, load_diabetes\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import (\n",
    "    VotingClassifier, VotingRegressor,\n",
    "    RandomForestClassifier, RandomForestRegressor,\n",
    "    GradientBoostingClassifier, GradientBoostingRegressor\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    mean_squared_error, r2_score\n",
    ")\n",
    "\n",
    "# Boosting libraries\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "\n",
    "# Configuration\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hard Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "cancer_data = load_breast_cancer()\n",
    "X = cancer_data.data\n",
    "y = cancer_data.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Dataset: {X.shape}\")\n",
    "print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create diverse classifiers\n",
    "clf1 = LogisticRegression(random_state=42, max_iter=1000)\n",
    "clf2 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf3 = GaussianNB()\n",
    "clf4 = SVC(kernel='rbf', random_state=42)\n",
    "\n",
    "# Create hard voting classifier\n",
    "hard_voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3), ('svc', clf4)],\n",
    "    voting='hard'\n",
    ")\n",
    "\n",
    "# Train\n",
    "hard_voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_hard = hard_voting_clf.predict(X_test)\n",
    "hard_voting_acc = accuracy_score(y_test, y_pred_hard)\n",
    "\n",
    "print(\"Hard Voting Classifier:\")\n",
    "print(f\"Test Accuracy: {hard_voting_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with individual classifiers\n",
    "print(\"\\nIndividual Classifier Performance:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "individual_scores = {}\n",
    "for name, clf in [('LogisticRegression', clf1), ('RandomForest', clf2), \n",
    "                  ('GaussianNB', clf3), ('SVC', clf4)]:\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    individual_scores[name] = acc\n",
    "    print(f\"{name:20s}: {acc:.4f}\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'Hard Voting':20s}: {hard_voting_acc:.4f}\")\n",
    "print(f\"\\nAverage of individual scores: {np.mean(list(individual_scores.values())):.4f}\")\n",
    "print(f\"Best individual score: {max(individual_scores.values()):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Soft Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For soft voting, all classifiers must support predict_proba\n",
    "# SVC needs probability=True\n",
    "clf1_soft = LogisticRegression(random_state=42, max_iter=1000)\n",
    "clf2_soft = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf3_soft = GaussianNB()\n",
    "clf4_soft = SVC(kernel='rbf', probability=True, random_state=42)  # Enable probability\n",
    "\n",
    "# Create soft voting classifier\n",
    "soft_voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', clf1_soft), ('rf', clf2_soft), ('gnb', clf3_soft), ('svc', clf4_soft)],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "# Train\n",
    "soft_voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_soft = soft_voting_clf.predict(X_test)\n",
    "soft_voting_acc = accuracy_score(y_test, y_pred_soft)\n",
    "\n",
    "print(\"Soft Voting Classifier:\")\n",
    "print(f\"Test Accuracy: {soft_voting_acc:.4f}\")\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\"Hard Voting: {hard_voting_acc:.4f}\")\n",
    "print(f\"Soft Voting: {soft_voting_acc:.4f}\")\n",
    "print(f\"Improvement: {soft_voting_acc - hard_voting_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "methods = ['LogReg', 'RF', 'GNB', 'SVC', 'Hard Vote', 'Soft Vote']\n",
    "scores = list(individual_scores.values()) + [hard_voting_acc, soft_voting_acc]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "colors = ['skyblue'] * 4 + ['orange', 'green']\n",
    "bars = plt.bar(methods, scores, color=colors, alpha=0.7)\n",
    "\n",
    "# Highlight voting methods\n",
    "bars[4].set_edgecolor('darkorange')\n",
    "bars[4].set_linewidth(2)\n",
    "bars[5].set_edgecolor('darkgreen')\n",
    "bars[5].set_linewidth(2)\n",
    "\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Voting Ensemble vs Individual Classifiers')\n",
    "plt.ylim([min(scores) - 0.01, 1.0])\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for i, (method, score) in enumerate(zip(methods, scores)):\n",
    "    plt.text(i, score + 0.002, f'{score:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Weighted Voting\n",
    "\n",
    "We can assign different weights to classifiers based on their performance or reliability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate weights based on cross-validation scores\n",
    "print(\"Calculating optimal weights using cross-validation...\\n\")\n",
    "\n",
    "weights = []\n",
    "for name, clf in [('lr', clf1_soft), ('rf', clf2_soft), ('gnb', clf3_soft), ('svc', clf4_soft)]:\n",
    "    cv_scores = cross_val_score(clf, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    mean_score = cv_scores.mean()\n",
    "    weights.append(mean_score)\n",
    "    print(f\"{name:5s}: CV Accuracy = {mean_score:.4f}\")\n",
    "\n",
    "# Normalize weights\n",
    "weights = np.array(weights)\n",
    "weights_normalized = weights / weights.sum()\n",
    "\n",
    "print(f\"\\nNormalized Weights: {weights_normalized}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create weighted voting classifier\n",
    "weighted_voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', clf1_soft), ('rf', clf2_soft), ('gnb', clf3_soft), ('svc', clf4_soft)],\n",
    "    voting='soft',\n",
    "    weights=weights.tolist()  # Use CV scores as weights\n",
    ")\n",
    "\n",
    "# Train\n",
    "weighted_voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_weighted = weighted_voting_clf.predict(X_test)\n",
    "weighted_voting_acc = accuracy_score(y_test, y_pred_weighted)\n",
    "\n",
    "print(\"\\nWeighted Voting Results:\")\n",
    "print(f\"Equal weights (soft voting):    {soft_voting_acc:.4f}\")\n",
    "print(f\"Performance-based weights:      {weighted_voting_acc:.4f}\")\n",
    "print(f\"Improvement: {weighted_voting_acc - soft_voting_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Voting with Boosting Algorithms\n",
    "\n",
    "Let's create a powerful voting ensemble combining different boosting algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ensemble of boosting algorithms\n",
    "boosting_ensemble = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "        ('gb', GradientBoostingClassifier(n_estimators=100, random_state=42)),\n",
    "        ('xgb', XGBClassifier(n_estimators=100, random_state=42, eval_metric='logloss')),\n",
    "        ('lgbm', LGBMClassifier(n_estimators=100, random_state=42, verbose=-1))\n",
    "    ],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "# Train\n",
    "boosting_ensemble.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_boosting = boosting_ensemble.predict(X_test)\n",
    "boosting_ensemble_acc = accuracy_score(y_test, y_pred_boosting)\n",
    "\n",
    "print(\"Boosting Ensemble (Voting):\")\n",
    "print(f\"Test Accuracy: {boosting_ensemble_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare individual boosting models\n",
    "print(\"\\nIndividual Boosting Model Performance:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "boosting_models = [\n",
    "    ('RandomForest', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "    ('GradientBoosting', GradientBoostingClassifier(n_estimators=100, random_state=42)),\n",
    "    ('XGBoost', XGBClassifier(n_estimators=100, random_state=42, eval_metric='logloss')),\n",
    "    ('LightGBM', LGBMClassifier(n_estimators=100, random_state=42, verbose=-1))\n",
    "]\n",
    "\n",
    "boosting_scores = {}\n",
    "for name, model in boosting_models:\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    boosting_scores[name] = acc\n",
    "    print(f\"{name:20s}: {acc:.4f}\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'Voting Ensemble':20s}: {boosting_ensemble_acc:.4f}\")\n",
    "print(f\"\\nBest individual: {max(boosting_scores.values()):.4f}\")\n",
    "print(f\"Improvement over best: {boosting_ensemble_acc - max(boosting_scores.values()):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Voting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load regression dataset\n",
    "diabetes = load_diabetes()\n",
    "X_reg = diabetes.data\n",
    "y_reg = diabetes.target\n",
    "\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Regression dataset: {X_reg.shape}\")\n",
    "print(f\"Train: {X_train_reg.shape}, Test: {X_test_reg.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create diverse regressors\n",
    "reg1 = Ridge(alpha=1.0)\n",
    "reg2 = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "reg3 = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "reg4 = SVR(kernel='rbf')\n",
    "\n",
    "# Create voting regressor\n",
    "voting_reg = VotingRegressor(\n",
    "    estimators=[('ridge', reg1), ('rf', reg2), ('gb', reg3), ('svr', reg4)]\n",
    ")\n",
    "\n",
    "# Train\n",
    "voting_reg.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "# Predict\n",
    "y_pred_voting = voting_reg.predict(X_test_reg)\n",
    "voting_r2 = r2_score(y_test_reg, y_pred_voting)\n",
    "voting_mse = mean_squared_error(y_test_reg, y_pred_voting)\n",
    "\n",
    "print(\"Voting Regressor:\")\n",
    "print(f\"R² Score: {voting_r2:.4f}\")\n",
    "print(f\"MSE: {voting_mse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with individual regressors\n",
    "print(\"\\nIndividual Regressor Performance:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "reg_scores = {}\n",
    "for name, model in [('Ridge', reg1), ('RandomForest', reg2), \n",
    "                    ('GradientBoosting', reg3), ('SVR', reg4)]:\n",
    "    model.fit(X_train_reg, y_train_reg)\n",
    "    y_pred = model.predict(X_test_reg)\n",
    "    r2 = r2_score(y_test_reg, y_pred)\n",
    "    mse = mean_squared_error(y_test_reg, y_pred)\n",
    "    reg_scores[name] = r2\n",
    "    print(f\"{name:20s}: R² = {r2:.4f}, MSE = {mse:.2f}\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Voting Regressor':20s}: R² = {voting_r2:.4f}, MSE = {voting_mse:.2f}\")\n",
    "print(f\"\\nBest individual R²: {max(reg_scores.values()):.4f}\")\n",
    "print(f\"Improvement: {voting_r2 - max(reg_scores.values()):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Individual models\n",
    "models = [('Ridge', reg1), ('RF', reg2), ('GB', reg3), ('SVR', reg4)]\n",
    "for idx, (name, model) in enumerate(models):\n",
    "    y_pred = model.predict(X_test_reg)\n",
    "    r2 = r2_score(y_test_reg, y_pred)\n",
    "    \n",
    "    axes[idx].scatter(y_test_reg, y_pred, alpha=0.5)\n",
    "    axes[idx].plot([y_test_reg.min(), y_test_reg.max()],\n",
    "                   [y_test_reg.min(), y_test_reg.max()],\n",
    "                   'r--', linewidth=2)\n",
    "    axes[idx].set_xlabel('Actual')\n",
    "    axes[idx].set_ylabel('Predicted')\n",
    "    axes[idx].set_title(f'{name} (R² = {r2:.4f})')\n",
    "\n",
    "# Voting ensemble\n",
    "axes[4].scatter(y_test_reg, y_pred_voting, alpha=0.5, color='green')\n",
    "axes[4].plot([y_test_reg.min(), y_test_reg.max()],\n",
    "             [y_test_reg.min(), y_test_reg.max()],\n",
    "             'r--', linewidth=2)\n",
    "axes[4].set_xlabel('Actual')\n",
    "axes[4].set_ylabel('Predicted')\n",
    "axes[4].set_title(f'Voting Ensemble (R² = {voting_r2:.4f})', fontweight='bold')\n",
    "\n",
    "# Hide last subplot\n",
    "axes[5].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. When Voting Helps and When It Doesn't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1: Voting with similar models (should not help much)\n",
    "print(\"Experiment 1: Voting with Similar Models\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Multiple random forests with different random seeds\n",
    "similar_models = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('rf1', RandomForestClassifier(n_estimators=100, random_state=1)),\n",
    "        ('rf2', RandomForestClassifier(n_estimators=100, random_state=2)),\n",
    "        ('rf3', RandomForestClassifier(n_estimators=100, random_state=3)),\n",
    "        ('rf4', RandomForestClassifier(n_estimators=100, random_state=4))\n",
    "    ],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "similar_models.fit(X_train, y_train)\n",
    "similar_acc = similar_models.score(X_test, y_test)\n",
    "\n",
    "# Single random forest for comparison\n",
    "single_rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "single_rf.fit(X_train, y_train)\n",
    "single_acc = single_rf.score(X_test, y_test)\n",
    "\n",
    "print(f\"Single Random Forest:           {single_acc:.4f}\")\n",
    "print(f\"Voting (4 Random Forests):      {similar_acc:.4f}\")\n",
    "print(f\"Improvement: {similar_acc - single_acc:.4f}\")\n",
    "print(\"\\nConclusion: Minimal improvement - models are too similar!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2: Voting with diverse models (should help)\n",
    "print(\"Experiment 2: Voting with Diverse Models\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Diverse algorithms\n",
    "diverse_models = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('lr', LogisticRegression(random_state=42, max_iter=1000)),\n",
    "        ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "        ('xgb', XGBClassifier(n_estimators=100, random_state=42, eval_metric='logloss')),\n",
    "        ('knn', KNeighborsClassifier(n_neighbors=5))\n",
    "    ],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "diverse_models.fit(X_train, y_train)\n",
    "diverse_acc = diverse_models.score(X_test, y_test)\n",
    "\n",
    "# Best individual from diverse set\n",
    "best_individual = max([\n",
    "    ('LogReg', LogisticRegression(random_state=42, max_iter=1000)),\n",
    "    ('RF', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "    ('XGB', XGBClassifier(n_estimators=100, random_state=42, eval_metric='logloss')),\n",
    "    ('KNN', KNeighborsClassifier(n_neighbors=5))\n",
    "], key=lambda x: cross_val_score(x[1], X_train, y_train, cv=3).mean())\n",
    "\n",
    "best_individual[1].fit(X_train, y_train)\n",
    "best_acc = best_individual[1].score(X_test, y_test)\n",
    "\n",
    "print(f\"Best individual model:          {best_acc:.4f}\")\n",
    "print(f\"Voting (diverse models):        {diverse_acc:.4f}\")\n",
    "print(f\"Improvement: {diverse_acc - best_acc:.4f}\")\n",
    "print(\"\\nConclusion: Significant improvement - diversity helps!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Custom Voting Ensemble\n",
    "\n",
    "Create a voting classifier with at least 5 diverse models. Compare hard voting vs soft voting. Which performs better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Optimal Weight Finding\n",
    "\n",
    "Create a voting classifier with 3-4 models. Experiment with different weight combinations to find the optimal weights that maximize test accuracy. Compare with equal weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Voting vs Stacking\n",
    "\n",
    "Using the same base models, compare:\n",
    "1. Soft voting\n",
    "2. Stacking with logistic regression meta-learner\n",
    "\n",
    "Which performs better? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Weighted Voting Regressor\n",
    "\n",
    "Create a voting regressor with weights based on cross-validation R² scores. Compare performance with equal-weight voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Diversity Analysis\n",
    "\n",
    "Create two voting ensembles:\n",
    "1. 5 similar models (e.g., decision trees with different max_depth)\n",
    "2. 5 diverse models (different algorithm families)\n",
    "\n",
    "Measure the correlation between predictions. Which ensemble has lower correlation? Which performs better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "In this notebook, you learned about Voting Ensembles:\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "1. **Hard Voting**:\n",
    "   - Each model votes for a class\n",
    "   - Majority wins\n",
    "   - Simple but less informative\n",
    "\n",
    "2. **Soft Voting**:\n",
    "   - Models output probability estimates\n",
    "   - Probabilities are averaged\n",
    "   - Usually performs better than hard voting\n",
    "   - Requires `predict_proba` support\n",
    "\n",
    "3. **Weighted Voting**:\n",
    "   - Different models get different weights\n",
    "   - Weights based on performance or confidence\n",
    "   - Can significantly improve results\n",
    "\n",
    "4. **Voting for Regression**:\n",
    "   - Simply averages predictions\n",
    "   - Can use weights for better results\n",
    "\n",
    "### When Voting Works Best:\n",
    "\n",
    "✅ **Models are diverse** (different algorithms, different assumptions)\n",
    "✅ **Models have similar performance** (no one model dominates)\n",
    "✅ **Errors are uncorrelated** (models make different mistakes)\n",
    "✅ **Simplicity is valued** (voting is easy to understand and implement)\n",
    "✅ **Computational budget allows** training multiple models\n",
    "\n",
    "### When Voting Doesn't Help:\n",
    "\n",
    "❌ **Models are too similar** (e.g., multiple random forests)\n",
    "❌ **One model is much better** than others\n",
    "❌ **Models are highly correlated** (make same mistakes)\n",
    "❌ **Too few diverse models** (at least 3-5 needed)\n",
    "❌ **Models are poorly tuned** (garbage in, garbage out)\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "1. **Use Diverse Models**:\n",
    "   - Mix algorithm families (linear, tree, distance-based, etc.)\n",
    "   - Avoid multiple instances of the same algorithm\n",
    "   - Example: LogReg + RF + XGBoost + SVM + KNN\n",
    "\n",
    "2. **Prefer Soft Voting**:\n",
    "   - Uses more information (probabilities)\n",
    "   - Usually outperforms hard voting\n",
    "   - Ensure all models support `predict_proba`\n",
    "\n",
    "3. **Tune Weights**:\n",
    "   - Use cross-validation scores as weights\n",
    "   - Or optimize weights on validation set\n",
    "   - Equal weights work well when models perform similarly\n",
    "\n",
    "4. **Check if It Helps**:\n",
    "   - Compare with best individual model\n",
    "   - If improvement < 0.5%, may not be worth complexity\n",
    "   - Use cross-validation for robust comparison\n",
    "\n",
    "### Comparison with Other Ensemble Methods:\n",
    "\n",
    "| Method | Complexity | Performance | Interpretability | Training Time |\n",
    "|--------|-----------|-------------|------------------|---------------|\n",
    "| **Voting** | Low | Good | High | Medium |\n",
    "| **Bagging** | Low | Good | Medium | Fast |\n",
    "| **Boosting** | Medium | Excellent | Low | Medium |\n",
    "| **Stacking** | High | Excellent | Low | Slow |\n",
    "\n",
    "### Voting vs Stacking:\n",
    "\n",
    "**Voting**:\n",
    "- Simple weighted average\n",
    "- Faster to train\n",
    "- More interpretable\n",
    "- Good when models perform similarly\n",
    "\n",
    "**Stacking**:\n",
    "- Meta-learner learns optimal combination\n",
    "- Can discover non-linear combinations\n",
    "- Usually higher performance\n",
    "- More complex, requires cross-validation\n",
    "\n",
    "### Practical Applications:\n",
    "\n",
    "1. **Quick Ensemble**: When you need fast improvement over single model\n",
    "2. **Model Robustness**: Reduce dependence on single model\n",
    "3. **Kaggle Competitions**: Simple baseline before complex ensembling\n",
    "4. **Production Systems**: Easy to understand and maintain\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In the next module, we'll do a **Comprehensive Comparison** of all ensemble methods:\n",
    "- Performance benchmarks on multiple datasets\n",
    "- Speed/accuracy trade-offs\n",
    "- Decision framework for choosing the right method\n",
    "- Best practices for production deployment\n",
    "\n",
    "This will help you choose the right ensemble method for your specific problem!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "- [Scikit-learn Voting Classifier](https://scikit-learn.org/stable/modules/ensemble.html#voting-classifier)\n",
    "- [Ensemble Learning Tutorial](https://machinelearningmastery.com/voting-ensembles-with-python/)\n",
    "- [Combining Classifiers](https://sebastianraschka.com/Articles/2014_ensemble_classifier.html)\n",
    "- [Weighted Voting Strategies](https://www.sciencedirect.com/science/article/pii/S0031320315001831)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
