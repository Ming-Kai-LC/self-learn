{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 09: Voting Classifiers and Regressors\n",
    "\n",
    "**Difficulty**: ‚≠ê‚≠ê‚≠ê Advanced\n",
    "**Estimated Time**: 70 minutes\n",
    "**Prerequisites**: \n",
    "- Module 02: Random Forests\n",
    "- Module 05: XGBoost\n",
    "- Module 08: Stacking and Blending\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Understand voting as the simplest ensemble combination method\n",
    "2. Implement hard voting and soft voting for classification\n",
    "3. Apply averaging for regression ensembles\n",
    "4. Find optimal weights for weighted voting using grid search\n",
    "5. Compare voting with stacking approaches\n",
    "6. Combine models from different families effectively\n",
    "7. Determine when voting is preferable to more complex ensembles\n",
    "8. Apply voting in real-world scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import time\n",
    "import warnings\n",
    "from itertools import product\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.datasets import load_breast_cancer, load_diabetes, make_classification\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_auc_score, roc_curve, log_loss,\n",
    "    mean_squared_error, r2_score, mean_absolute_error\n",
    ")\n",
    "\n",
    "# Voting ensembles\n",
    "from sklearn.ensemble import VotingClassifier, VotingRegressor\n",
    "\n",
    "# Base models\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, RandomForestRegressor,\n",
    "    GradientBoostingClassifier, GradientBoostingRegressor,\n",
    "    AdaBoostClassifier, AdaBoostRegressor\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression, Ridge, Lasso\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Gradient boosting libraries\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    XGB_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    LGB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    LGB_AVAILABLE = False\n",
    "\n",
    "# Configuration\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 4)\n",
    "\n",
    "print(\"\\nSetup complete! All libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is Voting?\n",
    "\n",
    "### Voting = Democratic Decision Making\n",
    "\n",
    "Voting is the simplest ensemble combination method:\n",
    "- Train multiple independent models\n",
    "- Each model \"votes\" on the prediction\n",
    "- Final prediction based on majority or average\n",
    "\n",
    "### Types of Voting\n",
    "\n",
    "#### 1.1 Hard Voting (Classification)\n",
    "\n",
    "Majority vote from predicted class labels:\n",
    "\n",
    "```\n",
    "Sample X:\n",
    "  Model A predicts: Class 1\n",
    "  Model B predicts: Class 0\n",
    "  Model C predicts: Class 1\n",
    "  \n",
    "Final prediction: Class 1 (majority)\n",
    "```\n",
    "\n",
    "**Formula**:\n",
    "$$\\hat{y} = \\text{mode}(h_1(x), h_2(x), ..., h_n(x))$$\n",
    "\n",
    "#### 1.2 Soft Voting (Classification)\n",
    "\n",
    "Average of predicted probabilities:\n",
    "\n",
    "```\n",
    "Sample X:\n",
    "  Model A predicts: [0.4, 0.6] ‚Üí Class 1 with 60%\n",
    "  Model B predicts: [0.7, 0.3] ‚Üí Class 0 with 70%\n",
    "  Model C predicts: [0.3, 0.7] ‚Üí Class 1 with 70%\n",
    "  \n",
    "Average: [0.467, 0.533]\n",
    "Final prediction: Class 1 (higher probability)\n",
    "```\n",
    "\n",
    "**Formula**:\n",
    "$$\\hat{y} = \\arg\\max_c \\frac{1}{n} \\sum_{i=1}^n p_i(c|x)$$\n",
    "\n",
    "**Soft voting is generally better** because:\n",
    "- Uses more information (probabilities vs hard decisions)\n",
    "- Accounts for model confidence\n",
    "- Smoother decision boundaries\n",
    "\n",
    "#### 1.3 Averaging (Regression)\n",
    "\n",
    "Simple average of predictions:\n",
    "\n",
    "```\n",
    "Sample X:\n",
    "  Model A predicts: 105.3\n",
    "  Model B predicts: 98.7\n",
    "  Model C predicts: 102.1\n",
    "  \n",
    "Final prediction: (105.3 + 98.7 + 102.1) / 3 = 102.0\n",
    "```\n",
    "\n",
    "### Weighted Voting\n",
    "\n",
    "Assign different weights to models:\n",
    "\n",
    "$$\\hat{y} = \\arg\\max_c \\sum_{i=1}^n w_i \\cdot p_i(c|x)$$\n",
    "\n",
    "Where $w_i$ is the weight for model $i$, and $\\sum w_i = 1$.\n",
    "\n",
    "Better models get higher weights!\n",
    "\n",
    "### Voting vs Stacking\n",
    "\n",
    "| Aspect | Voting | Stacking |\n",
    "|--------|--------|----------|\n",
    "| **Complexity** | Simple | Complex |\n",
    "| **Training** | Parallel | Sequential |\n",
    "| **Combination** | Fixed rule | Learned |\n",
    "| **Overfitting risk** | Low | Higher |\n",
    "| **Flexibility** | Limited | High |\n",
    "| **Performance** | Good | Better |\n",
    "| **Interpretability** | High | Lower |\n",
    "\n",
    "### When to Use Voting\n",
    "\n",
    "**Best for**:\n",
    "- Simple, interpretable ensembles\n",
    "- Production systems (easier to deploy)\n",
    "- Limited training data\n",
    "- Models are already strong\n",
    "- Want robustness over complexity\n",
    "\n",
    "**Consider stacking if**:\n",
    "- Need maximum accuracy\n",
    "- Sufficient training data\n",
    "- Complexity acceptable\n",
    "- Models have complex interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load classification dataset\n",
    "cancer_data = load_breast_cancer()\n",
    "X, y = cancer_data.data, cancer_data.target\n",
    "feature_names = cancer_data.feature_names\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"Dataset: {len(X_train)} train, {len(X_test)} test, {X.shape[1]} features\")\n",
    "print(f\"Classes: {np.unique(y)}, Distribution: {np.bincount(y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hard Voting vs Soft Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define diverse base models\n",
    "base_classifiers = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE)),\n",
    "    ('gb', GradientBoostingClassifier(n_estimators=100, random_state=RANDOM_STATE)),\n",
    "    ('svm', SVC(probability=True, random_state=RANDOM_STATE)),  # probability=True for soft voting\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=5))\n",
    "]\n",
    "\n",
    "print(\"Base classifiers:\")\n",
    "for name, clf in base_classifiers:\n",
    "    print(f\"  - {name}: {type(clf).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard Voting\n",
    "print(\"\\nTraining Hard Voting Classifier...\")\n",
    "hard_voting = VotingClassifier(\n",
    "    estimators=base_classifiers,\n",
    "    voting='hard'\n",
    ")\n",
    "hard_voting.fit(X_train, y_train)\n",
    "hard_pred = hard_voting.predict(X_test)\n",
    "hard_acc = accuracy_score(y_test, hard_pred)\n",
    "\n",
    "print(f\"Hard Voting Accuracy: {hard_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soft Voting\n",
    "print(\"\\nTraining Soft Voting Classifier...\")\n",
    "soft_voting = VotingClassifier(\n",
    "    estimators=base_classifiers,\n",
    "    voting='soft'\n",
    ")\n",
    "soft_voting.fit(X_train, y_train)\n",
    "soft_pred = soft_voting.predict(X_test)\n",
    "soft_proba = soft_voting.predict_proba(X_test)\n",
    "soft_acc = accuracy_score(y_test, soft_pred)\n",
    "soft_auc = roc_auc_score(y_test, soft_proba[:, 1])\n",
    "\n",
    "print(f\"Soft Voting Accuracy: {soft_acc:.4f}\")\n",
    "print(f\"Soft Voting AUC-ROC: {soft_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with individual models and voting methods\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Performance Comparison\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "results = []\n",
    "\n",
    "# Individual models\n",
    "for name, clf in base_classifiers:\n",
    "    clf.fit(X_train, y_train)\n",
    "    pred = clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test, pred)\n",
    "    \n",
    "    if hasattr(clf, 'predict_proba'):\n",
    "        proba = clf.predict_proba(X_test)[:, 1]\n",
    "        auc = roc_auc_score(y_test, proba)\n",
    "    else:\n",
    "        auc = np.nan\n",
    "    \n",
    "    results.append({'Model': name.upper(), 'Type': 'Base', 'Accuracy': acc, 'AUC': auc})\n",
    "\n",
    "# Voting ensembles\n",
    "results.append({'Model': 'HARD VOTING', 'Type': 'Ensemble', 'Accuracy': hard_acc, 'AUC': np.nan})\n",
    "results.append({'Model': 'SOFT VOTING', 'Type': 'Ensemble', 'Accuracy': soft_acc, 'AUC': soft_auc})\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "print(df_results.to_string(index=False))\n",
    "\n",
    "# Best base model\n",
    "best_base_acc = df_results[df_results['Type'] == 'Base']['Accuracy'].max()\n",
    "improvement_hard = (hard_acc - best_base_acc) * 100\n",
    "improvement_soft = (soft_acc - best_base_acc) * 100\n",
    "\n",
    "print(f\"\\n‚úÖ Hard Voting improvement: +{improvement_hard:.2f}% over best base\")\n",
    "print(f\"‚úÖ Soft Voting improvement: +{improvement_soft:.2f}% over best base\")\n",
    "print(f\"\\nüí° Soft Voting typically performs better than Hard Voting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy\n",
    "models_base = df_results[df_results['Type'] == 'Base']['Model'].tolist()\n",
    "acc_base = df_results[df_results['Type'] == 'Base']['Accuracy'].tolist()\n",
    "models_ensemble = df_results[df_results['Type'] == 'Ensemble']['Model'].tolist()\n",
    "acc_ensemble = df_results[df_results['Type'] == 'Ensemble']['Accuracy'].tolist()\n",
    "\n",
    "x_base = np.arange(len(models_base))\n",
    "x_ensemble = np.arange(len(models_base), len(models_base) + len(models_ensemble))\n",
    "\n",
    "axes[0].bar(x_base, acc_base, color='steelblue', edgecolor='black', label='Base Models')\n",
    "axes[0].bar(x_ensemble, acc_ensemble, color=['#e74c3c', '#2ecc71'], \n",
    "            edgecolor='black', label='Voting')\n",
    "axes[0].set_xticks(range(len(df_results)))\n",
    "axes[0].set_xticklabels(df_results['Model'], rotation=45, ha='right')\n",
    "axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[0].set_title('Model Accuracy Comparison', fontsize=13, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "axes[0].set_ylim([0.9, 1.0])\n",
    "\n",
    "# AUC (only for models with predict_proba)\n",
    "df_with_auc = df_results.dropna(subset=['AUC'])\n",
    "axes[1].bar(range(len(df_with_auc)), df_with_auc['AUC'], \n",
    "            color=['steelblue'] * (len(df_with_auc) - 1) + ['#2ecc71'],\n",
    "            edgecolor='black')\n",
    "axes[1].set_xticks(range(len(df_with_auc)))\n",
    "axes[1].set_xticklabels(df_with_auc['Model'], rotation=45, ha='right')\n",
    "axes[1].set_ylabel('AUC-ROC', fontsize=12)\n",
    "axes[1].set_title('Model AUC Comparison', fontsize=13, fontweight='bold')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "axes[1].set_ylim([0.9, 1.0])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Weighted Voting\n",
    "\n",
    "Assign different weights to models based on their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate individual model performance to determine weights\n",
    "print(\"Evaluating individual models with cross-validation...\\n\")\n",
    "\n",
    "cv_scores = {}\n",
    "for name, clf in base_classifiers:\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    cv_scores[name] = scores.mean()\n",
    "    print(f\"{name:10s} - CV Accuracy: {scores.mean():.4f} (+/- {scores.std():.4f})\")\n",
    "\n",
    "# Convert scores to weights (normalize to sum to 1)\n",
    "total_score = sum(cv_scores.values())\n",
    "weights = [cv_scores[name] / total_score for name, _ in base_classifiers]\n",
    "\n",
    "print(f\"\\nWeights (normalized):\")\n",
    "for (name, _), weight in zip(base_classifiers, weights):\n",
    "    print(f\"  {name:10s}: {weight:.4f}\")\n",
    "print(f\"  Sum: {sum(weights):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create weighted voting classifier\n",
    "weighted_voting = VotingClassifier(\n",
    "    estimators=base_classifiers,\n",
    "    voting='soft',\n",
    "    weights=weights\n",
    ")\n",
    "\n",
    "print(\"Training Weighted Voting Classifier...\")\n",
    "weighted_voting.fit(X_train, y_train)\n",
    "weighted_pred = weighted_voting.predict(X_test)\n",
    "weighted_proba = weighted_voting.predict_proba(X_test)\n",
    "weighted_acc = accuracy_score(y_test, weighted_pred)\n",
    "weighted_auc = roc_auc_score(y_test, weighted_proba[:, 1])\n",
    "\n",
    "print(f\"\\nWeighted Voting Results:\")\n",
    "print(f\"Accuracy: {weighted_acc:.4f}\")\n",
    "print(f\"AUC-ROC: {weighted_auc:.4f}\")\n",
    "\n",
    "# Compare\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\"  Equal weights (soft voting): {soft_acc:.4f}\")\n",
    "print(f\"  Performance-based weights:   {weighted_acc:.4f}\")\n",
    "print(f\"  Improvement: {(weighted_acc - soft_acc) * 100:+.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Optimal Weight Finding with Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal weights using GridSearchCV\n",
    "print(\"Searching for optimal weights...\\n\")\n",
    "\n",
    "# Define weight grid (coarse search)\n",
    "# Each weight can be 1, 2, or 3 (will be normalized)\n",
    "weight_options = [1, 2, 3]\n",
    "param_grid = {\n",
    "    'weights': [\n",
    "        [w1, w2, w3, w4] \n",
    "        for w1 in weight_options \n",
    "        for w2 in weight_options \n",
    "        for w3 in weight_options\n",
    "        for w4 in weight_options\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(f\"Testing {len(param_grid['weights'])} weight combinations...\")\n",
    "\n",
    "# Grid search\n",
    "grid_search = GridSearchCV(\n",
    "    VotingClassifier(estimators=base_classifiers, voting='soft'),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best weights\n",
    "best_weights = grid_search.best_params_['weights']\n",
    "best_cv_score = grid_search.best_score_\n",
    "\n",
    "print(f\"\\n‚úÖ Best weights found:\")\n",
    "for (name, _), weight in zip(base_classifiers, best_weights):\n",
    "    print(f\"  {name:10s}: {weight}\")\n",
    "print(f\"\\nBest CV score: {best_cv_score:.4f}\")\n",
    "\n",
    "# Test on test set\n",
    "best_voting = grid_search.best_estimator_\n",
    "best_pred = best_voting.predict(X_test)\n",
    "best_acc = accuracy_score(y_test, best_pred)\n",
    "best_auc = roc_auc_score(y_test, best_voting.predict_proba(X_test)[:, 1])\n",
    "\n",
    "print(f\"\\nTest set performance:\")\n",
    "print(f\"Accuracy: {best_acc:.4f}\")\n",
    "print(f\"AUC-ROC: {best_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Voting Regressor\n",
    "\n",
    "Apply voting to regression tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load regression dataset\n",
    "diabetes_data = load_diabetes()\n",
    "X_reg, y_reg = diabetes_data.data, diabetes_data.target\n",
    "\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"Regression dataset: {len(X_train_reg)} train, {len(X_test_reg)} test\")\n",
    "print(f\"Features: {X_reg.shape[1]}\")\n",
    "print(f\"Target range: [{y_reg.min():.1f}, {y_reg.max():.1f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base regressors\n",
    "base_regressors = [\n",
    "    ('rf', RandomForestRegressor(n_estimators=100, random_state=RANDOM_STATE)),\n",
    "    ('gb', GradientBoostingRegressor(n_estimators=100, random_state=RANDOM_STATE)),\n",
    "    ('ridge', Ridge(random_state=RANDOM_STATE)),\n",
    "    ('svr', SVR())\n",
    "]\n",
    "\n",
    "print(\"Base regressors:\")\n",
    "for name, reg in base_regressors:\n",
    "    print(f\"  - {name}: {type(reg).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train individual regressors\n",
    "print(\"\\nTraining individual regressors...\\n\")\n",
    "\n",
    "reg_results = []\n",
    "for name, reg in base_regressors:\n",
    "    reg.fit(X_train_reg, y_train_reg)\n",
    "    pred = reg.predict(X_test_reg)\n",
    "    \n",
    "    mse = mean_squared_error(y_test_reg, pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test_reg, pred)\n",
    "    r2 = r2_score(y_test_reg, pred)\n",
    "    \n",
    "    reg_results.append({\n",
    "        'Model': name.upper(),\n",
    "        'Type': 'Base',\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R¬≤': r2\n",
    "    })\n",
    "    \n",
    "    print(f\"{name:10s} - RMSE: {rmse:.2f}, MAE: {mae:.2f}, R¬≤: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create voting regressor (averaging)\n",
    "print(\"\\nTraining Voting Regressor...\")\n",
    "\n",
    "voting_reg = VotingRegressor(estimators=base_regressors)\n",
    "voting_reg.fit(X_train_reg, y_train_reg)\n",
    "voting_pred = voting_reg.predict(X_test_reg)\n",
    "\n",
    "voting_mse = mean_squared_error(y_test_reg, voting_pred)\n",
    "voting_rmse = np.sqrt(voting_mse)\n",
    "voting_mae = mean_absolute_error(y_test_reg, voting_pred)\n",
    "voting_r2 = r2_score(y_test_reg, voting_pred)\n",
    "\n",
    "reg_results.append({\n",
    "    'Model': 'VOTING',\n",
    "    'Type': 'Ensemble',\n",
    "    'RMSE': voting_rmse,\n",
    "    'MAE': voting_mae,\n",
    "    'R¬≤': voting_r2\n",
    "})\n",
    "\n",
    "print(f\"\\nVoting Regressor Results:\")\n",
    "print(f\"RMSE: {voting_rmse:.2f}\")\n",
    "print(f\"MAE: {voting_mae:.2f}\")\n",
    "print(f\"R¬≤: {voting_r2:.4f}\")\n",
    "\n",
    "# Compare\n",
    "df_reg_results = pd.DataFrame(reg_results)\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Regression Results Comparison\")\n",
    "print(\"=\" * 70)\n",
    "print(df_reg_results.to_string(index=False))\n",
    "\n",
    "best_base_r2 = df_reg_results[df_reg_results['Type'] == 'Base']['R¬≤'].max()\n",
    "improvement = voting_r2 - best_base_r2\n",
    "print(f\"\\n‚úÖ Voting improvement: +{improvement:.4f} R¬≤ over best base model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Performance comparison\n",
    "axes[0].bar(range(len(df_reg_results)), df_reg_results['R¬≤'],\n",
    "            color=['steelblue'] * (len(df_reg_results) - 1) + ['#2ecc71'],\n",
    "            edgecolor='black')\n",
    "axes[0].set_xticks(range(len(df_reg_results)))\n",
    "axes[0].set_xticklabels(df_reg_results['Model'], rotation=45, ha='right')\n",
    "axes[0].set_ylabel('R¬≤ Score', fontsize=12)\n",
    "axes[0].set_title('Regressor R¬≤ Comparison', fontsize=13, fontweight='bold')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Prediction scatter\n",
    "axes[1].scatter(y_test_reg, voting_pred, alpha=0.6, edgecolors='black')\n",
    "axes[1].plot([y_test_reg.min(), y_test_reg.max()], \n",
    "             [y_test_reg.min(), y_test_reg.max()], \n",
    "             'r--', linewidth=2, label='Perfect Prediction')\n",
    "axes[1].set_xlabel('True Values', fontsize=12)\n",
    "axes[1].set_ylabel('Predicted Values', fontsize=12)\n",
    "axes[1].set_title('Voting Regressor Predictions', fontsize=13, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Voting vs Stacking: Head-to-Head Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "print(\"Comparing Voting vs Stacking...\\n\")\n",
    "\n",
    "# Voting (soft)\n",
    "voting_model = VotingClassifier(\n",
    "    estimators=base_classifiers,\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "# Stacking\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=base_classifiers,\n",
    "    final_estimator=LogisticRegression(random_state=RANDOM_STATE),\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "# Train and evaluate\n",
    "comparison_results = []\n",
    "\n",
    "for name, model in [('Voting', voting_model), ('Stacking', stacking_model)]:\n",
    "    print(f\"Training {name}...\")\n",
    "    start = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = time.time() - start\n",
    "    \n",
    "    pred = model.predict(X_test)\n",
    "    proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    acc = accuracy_score(y_test, pred)\n",
    "    auc = roc_auc_score(y_test, proba)\n",
    "    \n",
    "    comparison_results.append({\n",
    "        'Method': name,\n",
    "        'Training Time': train_time,\n",
    "        'Accuracy': acc,\n",
    "        'AUC': auc\n",
    "    })\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_results)\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Voting vs Stacking Comparison\")\n",
    "print(\"=\" * 70)\n",
    "print(df_comparison.to_string(index=False))\n",
    "\n",
    "print(\"\\nüí° Analysis:\")\n",
    "print(\"   - Voting: Simpler, faster, easier to interpret\")\n",
    "print(\"   - Stacking: Potentially better accuracy, more complex\")\n",
    "print(\"   - Choice depends on: data size, complexity tolerance, accuracy needs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Heterogeneous Ensemble Design\n",
    "\n",
    "Create the most diverse voting ensemble possible:\n",
    "\n",
    "1. Select models from different families:\n",
    "   - Linear models\n",
    "   - Tree-based models\n",
    "   - Instance-based models\n",
    "   - Probabilistic models\n",
    "   - Neural networks (if available)\n",
    "2. For each model:\n",
    "   - Tune hyperparameters individually\n",
    "   - Measure diversity (correlation of predictions)\n",
    "3. Create voting ensembles with different subsets:\n",
    "   - Most accurate models\n",
    "   - Most diverse models\n",
    "   - Balanced (accuracy + diversity)\n",
    "4. Compare all strategies\n",
    "5. Determine optimal diversity/accuracy trade-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Dynamic Weighting Strategies\n",
    "\n",
    "Implement and compare different weighting schemes:\n",
    "\n",
    "1. **Performance-based**: Weight by cross-validation accuracy\n",
    "2. **Confidence-based**: Weight by average prediction confidence\n",
    "3. **Inverse error**: Weight inversely to error rate\n",
    "4. **Learned weights**: Use optimization (grid search, Bayesian opt)\n",
    "5. **Adaptive weights**: Different weights for different regions of feature space\n",
    "\n",
    "For each strategy:\n",
    "- Implement weighting calculation\n",
    "- Apply to voting ensemble\n",
    "- Evaluate on test set\n",
    "- Compare with equal weights\n",
    "\n",
    "Determine which weighting strategy works best and why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Voting Ensemble Complexity Analysis\n",
    "\n",
    "Analyze how ensemble size affects performance:\n",
    "\n",
    "1. Create pool of 10 diverse models\n",
    "2. For ensemble sizes from 3 to 10:\n",
    "   - Try multiple random combinations\n",
    "   - Measure accuracy, AUC, training time\n",
    "   - Calculate prediction diversity\n",
    "3. Plot:\n",
    "   - Performance vs ensemble size\n",
    "   - Training time vs ensemble size\n",
    "   - Diminishing returns curve\n",
    "4. Find optimal number of models\n",
    "5. Test hypothesis: \"More models ‚Üí always better?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Production Deployment Simulation\n",
    "\n",
    "Compare voting vs stacking for production deployment:\n",
    "\n",
    "1. Train both voting and stacking ensembles\n",
    "2. Measure production metrics:\n",
    "   - **Latency**: Single prediction time\n",
    "   - **Throughput**: Predictions per second\n",
    "   - **Memory**: Model size and runtime memory\n",
    "   - **Maintenance**: Complexity score (subjective)\n",
    "3. Simulate production scenarios:\n",
    "   - High-throughput (batch predictions)\n",
    "   - Low-latency (real-time predictions)\n",
    "   - Resource-constrained (limited memory/CPU)\n",
    "4. Create deployment recommendation matrix\n",
    "5. Determine when to use each approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Voting = Simple Ensemble Combination**:\n",
    "   - Train models independently\n",
    "   - Combine predictions by voting or averaging\n",
    "   - No learning in combination step\n",
    "   - Simple, interpretable, effective\n",
    "\n",
    "2. **Types of Voting**:\n",
    "   - **Hard Voting**: Majority vote on predicted classes\n",
    "   - **Soft Voting**: Average predicted probabilities (usually better)\n",
    "   - **Weighted Voting**: Assign different weights to models\n",
    "   - **Averaging**: For regression tasks\n",
    "\n",
    "3. **Soft Voting Advantages**:\n",
    "   - Uses full probability information\n",
    "   - Accounts for model confidence\n",
    "   - Typically outperforms hard voting\n",
    "   - Smoother decision boundaries\n",
    "\n",
    "4. **Weighting Strategies**:\n",
    "   - Equal weights: Simple baseline\n",
    "   - Performance-based: Weight by accuracy\n",
    "   - Optimized: Grid search for best weights\n",
    "   - Typically provides 0.5-2% improvement\n",
    "\n",
    "5. **Voting vs Stacking**:\n",
    "   - Voting: Simpler, faster, more interpretable\n",
    "   - Stacking: More powerful, learned combination\n",
    "   - Voting: Lower overfitting risk\n",
    "   - Stacking: Higher performance ceiling\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Model Selection**:\n",
    "   - Use 3-7 diverse models\n",
    "   - Different algorithm families\n",
    "   - Check prediction correlation (lower = better)\n",
    "   - Balance accuracy and diversity\n",
    "\n",
    "2. **Voting Type**:\n",
    "   - Prefer soft voting over hard voting\n",
    "   - Ensure all models support `predict_proba()`\n",
    "   - Use hard voting only if necessary\n",
    "\n",
    "3. **Weighting**:\n",
    "   - Start with equal weights\n",
    "   - Try performance-based weights\n",
    "   - Use grid search if critical\n",
    "   - Validate on separate data\n",
    "\n",
    "4. **When to Use Voting**:\n",
    "   - ‚úÖ Need simple, interpretable ensemble\n",
    "   - ‚úÖ Production deployment (easier)\n",
    "   - ‚úÖ Limited training data\n",
    "   - ‚úÖ Models already well-tuned\n",
    "   - ‚úÖ Want robustness\n",
    "\n",
    "5. **When to Use Stacking Instead**:\n",
    "   - Need maximum accuracy\n",
    "   - Sufficient training data\n",
    "   - Can handle complexity\n",
    "   - Models have complex interactions\n",
    "\n",
    "### Common Mistakes\n",
    "\n",
    "‚ùå **Using highly similar models**\n",
    "  ‚Üí Ensure diversity!\n",
    "\n",
    "‚ùå **Too many models**\n",
    "  ‚Üí 3-7 is optimal, diminishing returns after\n",
    "\n",
    "‚ùå **Not using soft voting**\n",
    "  ‚Üí Soft almost always better than hard\n",
    "\n",
    "‚ùå **Over-optimizing weights**\n",
    "  ‚Üí Can lead to overfitting, validate carefully\n",
    "\n",
    "‚ùå **Ignoring base model quality**\n",
    "  ‚Üí Garbage in, garbage out - tune base models first\n",
    "\n",
    "### Performance Expectations\n",
    "\n",
    "Typical improvements from voting:\n",
    "- **vs best base model**: +0.5% to +2% accuracy\n",
    "- **Hard vs soft voting**: +0.2% to +1%\n",
    "- **Equal vs optimal weights**: +0.3% to +1.5%\n",
    "- **Voting vs stacking**: Stacking typically +0.2% to +1% better\n",
    "\n",
    "### Advantages of Voting\n",
    "\n",
    "1. **Simplicity**: Easy to understand and implement\n",
    "2. **Parallelization**: Train all models independently\n",
    "3. **Interpretability**: Clear how decision is made\n",
    "4. **Robustness**: Reduces overfitting\n",
    "5. **Production-friendly**: Easy to deploy\n",
    "6. **No hyperparameters**: (except weights)\n",
    "7. **Low overfitting risk**: No learning in combination\n",
    "\n",
    "### Disadvantages of Voting\n",
    "\n",
    "1. **Fixed combination**: Can't learn optimal weighting\n",
    "2. **Less powerful**: Stacking often better\n",
    "3. **Assumes independence**: Doesn't model correlations\n",
    "4. **Limited flexibility**: Can't capture interactions\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In **Module 10: Model Comparison and Selection**, we'll:\n",
    "- Benchmark ALL ensemble methods systematically\n",
    "- Compare single trees, bagging, boosting, stacking, voting\n",
    "- Analyze trade-offs: speed vs accuracy\n",
    "- Create decision framework for method selection\n",
    "- Production deployment considerations\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- **sklearn**: [Voting Classifier Documentation](https://scikit-learn.org/stable/modules/ensemble.html#voting-classifier)\n",
    "- **sklearn**: [Voting Regressor Documentation](https://scikit-learn.org/stable/modules/ensemble.html#voting-regressor)\n",
    "- **Paper**: \"A Comparison of Voting and Meta-Learning for Combining Classifiers\"\n",
    "- **Tutorial**: [Ensemble Learning Methods](https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205)\n",
    "- **Book**: \"Pattern Recognition and Machine Learning\" by Bishop (Chapter on Combining Models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
