{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 00: Introduction to Ensemble Learning\n",
    "\n",
    "**Difficulty**: ⭐⭐⭐ Advanced\n",
    "**Estimated Time**: 60 minutes\n",
    "**Prerequisites**: \n",
    "- Machine Learning Fundamentals (decision trees, model evaluation)\n",
    "- Feature Engineering basics\n",
    "- Understanding of bias-variance tradeoff\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Explain the concept of ensemble learning and the \"wisdom of crowds\"\n",
    "2. Understand the bias-variance tradeoff in ensemble methods\n",
    "3. Distinguish between different ensemble strategies (bagging, boosting, stacking)\n",
    "4. Implement a simple ensemble from scratch\n",
    "5. Identify when ensemble methods provide the most value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.datasets import make_classification, make_regression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "\n",
    "# Configuration\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 4)\n",
    "\n",
    "print(\"Setup complete! All libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is Ensemble Learning?\n",
    "\n",
    "### The Wisdom of Crowds\n",
    "\n",
    "**Core Idea**: Combining multiple models often produces better predictions than any single model.\n",
    "\n",
    "**Real-world analogy**: \n",
    "- Asking 100 people to estimate the number of jelly beans in a jar\n",
    "- The average of all guesses is often more accurate than individual expert estimates\n",
    "- Why? Individual errors tend to cancel out\n",
    "\n",
    "**Key Requirements for Effective Ensembles**:\n",
    "1. **Diversity**: Models should make different types of errors\n",
    "2. **Independence**: Models should be trained differently\n",
    "3. **Reasonable accuracy**: Models should perform better than random guessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration: Wisdom of crowds with simulated estimators\n",
    "true_value = 500  # True number of jelly beans\n",
    "\n",
    "# Simulate 100 people's guesses (with some error)\n",
    "# Each person has their own bias and noise\n",
    "np.random.seed(RANDOM_STATE)\n",
    "individual_estimates = np.random.normal(loc=true_value, scale=100, size=100)\n",
    "\n",
    "# Calculate average estimate\n",
    "crowd_estimate = np.mean(individual_estimates)\n",
    "\n",
    "# Find the best individual estimate\n",
    "best_individual = individual_estimates[np.argmin(np.abs(individual_estimates - true_value))]\n",
    "\n",
    "print(f\"True value: {true_value}\")\n",
    "print(f\"Crowd estimate (average): {crowd_estimate:.2f}\")\n",
    "print(f\"Best individual estimate: {best_individual:.2f}\")\n",
    "print(f\"\\nCrowd error: {abs(crowd_estimate - true_value):.2f}\")\n",
    "print(f\"Best individual error: {abs(best_individual - true_value):.2f}\")\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(individual_estimates, bins=30, alpha=0.7, edgecolor='black')\n",
    "plt.axvline(true_value, color='green', linestyle='--', linewidth=2, label='True value')\n",
    "plt.axvline(crowd_estimate, color='red', linestyle='--', linewidth=2, label='Crowd estimate')\n",
    "plt.xlabel('Estimate')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Individual Estimates')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "errors = np.abs(individual_estimates - true_value)\n",
    "plt.scatter(range(len(errors)), errors, alpha=0.5)\n",
    "plt.axhline(abs(crowd_estimate - true_value), color='red', linestyle='--', \n",
    "            linewidth=2, label='Crowd error')\n",
    "plt.xlabel('Individual')\n",
    "plt.ylabel('Absolute Error')\n",
    "plt.title('Individual Errors vs Crowd Error')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bias-Variance Tradeoff in Ensembles\n",
    "\n",
    "### Understanding Error Decomposition\n",
    "\n",
    "**Total Error = Bias² + Variance + Irreducible Error**\n",
    "\n",
    "- **Bias**: Error from overly simplistic assumptions (underfitting)\n",
    "- **Variance**: Error from sensitivity to training data fluctuations (overfitting)\n",
    "- **Irreducible Error**: Noise in the data itself\n",
    "\n",
    "### How Ensembles Help\n",
    "\n",
    "1. **Bagging** (Bootstrap Aggregating): Reduces **variance**\n",
    "   - Trains models on different subsets of data\n",
    "   - Averages predictions to smooth out fluctuations\n",
    "   - Example: Random Forest\n",
    "\n",
    "2. **Boosting**: Reduces **bias**\n",
    "   - Sequentially trains models to correct previous errors\n",
    "   - Builds complex models from simple ones\n",
    "   - Example: XGBoost, AdaBoost\n",
    "\n",
    "3. **Stacking**: Can reduce both\n",
    "   - Uses meta-model to learn optimal combination\n",
    "   - Leverages strengths of diverse models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration: Bias-Variance with ensemble averaging\n",
    "# Create a synthetic regression problem\n",
    "X, y = make_regression(n_samples=200, n_features=1, noise=15, random_state=RANDOM_STATE)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Train multiple high-variance models (deep decision trees)\n",
    "n_models = 20\n",
    "models = []\n",
    "predictions = []\n",
    "\n",
    "for i in range(n_models):\n",
    "    # Use bootstrap sampling to create diverse models\n",
    "    # Sample with replacement from training data\n",
    "    indices = np.random.choice(len(X_train), size=len(X_train), replace=True)\n",
    "    X_bootstrap = X_train[indices]\n",
    "    y_bootstrap = y_train[indices]\n",
    "    \n",
    "    # Train a high-variance model (deep tree)\n",
    "    model = DecisionTreeRegressor(max_depth=10, random_state=i)\n",
    "    model.fit(X_bootstrap, y_bootstrap)\n",
    "    \n",
    "    models.append(model)\n",
    "    predictions.append(model.predict(X_test))\n",
    "\n",
    "# Ensemble prediction: average of all models\n",
    "ensemble_pred = np.mean(predictions, axis=0)\n",
    "\n",
    "# Calculate errors\n",
    "individual_errors = [mean_squared_error(y_test, pred) for pred in predictions]\n",
    "ensemble_error = mean_squared_error(y_test, ensemble_pred)\n",
    "\n",
    "print(\"Individual Model Performance:\")\n",
    "print(f\"  Average MSE: {np.mean(individual_errors):.2f}\")\n",
    "print(f\"  Best MSE: {np.min(individual_errors):.2f}\")\n",
    "print(f\"  Worst MSE: {np.max(individual_errors):.2f}\")\n",
    "print(f\"\\nEnsemble Performance:\")\n",
    "print(f\"  MSE: {ensemble_error:.2f}\")\n",
    "print(f\"\\nImprovement: {((np.mean(individual_errors) - ensemble_error) / np.mean(individual_errors) * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Sort for better visualization\n",
    "sort_idx = np.argsort(X_test[:, 0])\n",
    "X_test_sorted = X_test[sort_idx]\n",
    "y_test_sorted = y_test[sort_idx]\n",
    "\n",
    "# Plot 1: Individual model predictions\n",
    "plt.subplot(1, 2, 1)\n",
    "for i, pred in enumerate(predictions[:5]):  # Show first 5 models\n",
    "    plt.plot(X_test_sorted, pred[sort_idx], alpha=0.3, linewidth=1)\n",
    "plt.scatter(X_test_sorted, y_test_sorted, color='black', s=20, \n",
    "            alpha=0.5, label='True values')\n",
    "plt.xlabel('Feature value')\n",
    "plt.ylabel('Target value')\n",
    "plt.title('Individual Model Predictions (High Variance)')\n",
    "plt.legend()\n",
    "\n",
    "# Plot 2: Ensemble prediction\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_test_sorted, y_test_sorted, color='black', s=20, \n",
    "            alpha=0.5, label='True values')\n",
    "plt.plot(X_test_sorted, ensemble_pred[sort_idx], color='red', \n",
    "         linewidth=2, label='Ensemble prediction')\n",
    "plt.xlabel('Feature value')\n",
    "plt.ylabel('Target value')\n",
    "plt.title('Ensemble Prediction (Reduced Variance)')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Types of Ensemble Methods\n",
    "\n",
    "### 3.1 Parallel Ensembles (Bagging)\n",
    "\n",
    "**Strategy**: Train models independently in parallel\n",
    "- Each model sees different subset of data\n",
    "- Predictions are averaged (regression) or voted (classification)\n",
    "- **Goal**: Reduce variance\n",
    "\n",
    "**When to use**:\n",
    "- Base model has high variance (e.g., deep decision trees)\n",
    "- Large datasets where you can create diverse subsets\n",
    "- You want stable, robust predictions\n",
    "\n",
    "**Examples**: Random Forest, Bagged Decision Trees\n",
    "\n",
    "### 3.2 Sequential Ensembles (Boosting)\n",
    "\n",
    "**Strategy**: Train models sequentially, each correcting previous errors\n",
    "- Each new model focuses on hard-to-predict examples\n",
    "- Models are weighted by performance\n",
    "- **Goal**: Reduce bias\n",
    "\n",
    "**When to use**:\n",
    "- Base model has high bias (e.g., shallow decision trees)\n",
    "- You need high accuracy and can afford longer training\n",
    "- Dataset is clean (boosting can overfit to noise)\n",
    "\n",
    "**Examples**: AdaBoost, Gradient Boosting, XGBoost, LightGBM, CatBoost\n",
    "\n",
    "### 3.3 Heterogeneous Ensembles (Stacking/Blending)\n",
    "\n",
    "**Strategy**: Combine different types of models\n",
    "- Train diverse base models (e.g., trees, linear models, neural nets)\n",
    "- Meta-model learns how to best combine predictions\n",
    "- **Goal**: Leverage complementary strengths\n",
    "\n",
    "**When to use**:\n",
    "- You have computational resources for multiple model types\n",
    "- Different models capture different patterns\n",
    "- Competition settings (Kaggle)\n",
    "\n",
    "**Examples**: Stacking classifier, voting ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of ensemble strategies\n",
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Create a classification dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=1000, \n",
    "    n_features=20, \n",
    "    n_informative=15,\n",
    "    n_redundant=5,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# 1. Base model (single decision tree)\n",
    "base_model = DecisionTreeClassifier(max_depth=5, random_state=RANDOM_STATE)\n",
    "base_model.fit(X_train, y_train)\n",
    "base_score = accuracy_score(y_test, base_model.predict(X_test))\n",
    "\n",
    "# 2. Bagging ensemble (parallel)\n",
    "bagging_model = BaggingClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=5, random_state=RANDOM_STATE),\n",
    "    n_estimators=20,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "bagging_model.fit(X_train, y_train)\n",
    "bagging_score = accuracy_score(y_test, bagging_model.predict(X_test))\n",
    "\n",
    "# 3. Boosting ensemble (sequential)\n",
    "boosting_model = AdaBoostClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=5, random_state=RANDOM_STATE),\n",
    "    n_estimators=20,\n",
    "    random_state=RANDOM_STATE,\n",
    "    algorithm='SAMME'\n",
    ")\n",
    "boosting_model.fit(X_train, y_train)\n",
    "boosting_score = accuracy_score(y_test, boosting_model.predict(X_test))\n",
    "\n",
    "# 4. Voting ensemble (heterogeneous)\n",
    "voting_model = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('dt', DecisionTreeClassifier(max_depth=5, random_state=RANDOM_STATE)),\n",
    "        ('lr', LogisticRegression(max_iter=1000, random_state=RANDOM_STATE)),\n",
    "        ('svc', SVC(kernel='rbf', probability=True, random_state=RANDOM_STATE))\n",
    "    ],\n",
    "    voting='soft'\n",
    ")\n",
    "voting_model.fit(X_train, y_train)\n",
    "voting_score = accuracy_score(y_test, voting_model.predict(X_test))\n",
    "\n",
    "# Compare results\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['Base (Single Tree)', 'Bagging', 'Boosting', 'Voting'],\n",
    "    'Accuracy': [base_score, bagging_score, boosting_score, voting_score],\n",
    "    'Strategy': ['None', 'Parallel', 'Sequential', 'Heterogeneous'],\n",
    "    'Primary Benefit': ['Baseline', 'Reduce variance', 'Reduce bias', 'Combine strengths']\n",
    "})\n",
    "\n",
    "print(results.to_string(index=False))\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['gray', 'blue', 'green', 'orange']\n",
    "bars = plt.bar(results['Model'], results['Accuracy'], color=colors, alpha=0.7, edgecolor='black')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Comparison of Ensemble Strategies')\n",
    "plt.ylim(0.7, 1.0)\n",
    "plt.xticks(rotation=15, ha='right')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, score in zip(bars, results['Accuracy']):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "             f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building a Simple Ensemble from Scratch\n",
    "\n",
    "Let's implement a basic ensemble to understand the mechanics. We'll create a simple averaging ensemble for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleEnsembleRegressor:\n",
    "    \"\"\"\n",
    "    A simple ensemble that averages predictions from multiple models.\n",
    "    \n",
    "    This demonstrates the core concept of ensemble learning:\n",
    "    combining multiple models to improve predictions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, models):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        models : list\n",
    "            List of sklearn-compatible models to ensemble\n",
    "        \"\"\"\n",
    "        self.models = models\n",
    "        self.n_models = len(models)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train all models in the ensemble.\n",
    "        \n",
    "        Each model sees the same training data in this simple version.\n",
    "        More advanced ensembles (like bagging) would give each model\n",
    "        different subsets of data.\n",
    "        \"\"\"\n",
    "        for i, model in enumerate(self.models):\n",
    "            model.fit(X, y)\n",
    "            print(f\"Trained model {i+1}/{self.n_models}\")\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions by averaging all model predictions.\n",
    "        \n",
    "        This simple averaging works well when models make uncorrelated errors.\n",
    "        \"\"\"\n",
    "        # Get predictions from all models\n",
    "        predictions = np.array([model.predict(X) for model in self.models])\n",
    "        \n",
    "        # Average across models (axis=0 means average across models)\n",
    "        ensemble_prediction = np.mean(predictions, axis=0)\n",
    "        \n",
    "        return ensemble_prediction\n",
    "    \n",
    "    def get_individual_predictions(self, X):\n",
    "        \"\"\"\n",
    "        Get predictions from each model separately.\n",
    "        Useful for analyzing model diversity.\n",
    "        \"\"\"\n",
    "        return np.array([model.predict(X) for model in self.models])\n",
    "\n",
    "print(\"SimpleEnsembleRegressor class defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test our custom ensemble\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Create regression dataset\n",
    "X, y = make_regression(n_samples=300, n_features=10, noise=20, random_state=RANDOM_STATE)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Create diverse base models\n",
    "# Using different model types increases diversity\n",
    "base_models = [\n",
    "    DecisionTreeRegressor(max_depth=5, random_state=RANDOM_STATE),\n",
    "    DecisionTreeRegressor(max_depth=10, random_state=RANDOM_STATE + 1),\n",
    "    Ridge(alpha=1.0),\n",
    "    Lasso(alpha=1.0),\n",
    "    SVR(kernel='rbf', C=1.0)\n",
    "]\n",
    "\n",
    "# Create and train ensemble\n",
    "ensemble = SimpleEnsembleRegressor(base_models)\n",
    "ensemble.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "ensemble_pred = ensemble.predict(X_test)\n",
    "individual_preds = ensemble.get_individual_predictions(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"\\nPerformance Comparison:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, model in enumerate(base_models):\n",
    "    mse = mean_squared_error(y_test, individual_preds[i])\n",
    "    print(f\"Model {i+1} ({model.__class__.__name__:20s}): MSE = {mse:.2f}\")\n",
    "\n",
    "ensemble_mse = mean_squared_error(y_test, ensemble_pred)\n",
    "print(\"=\" * 50)\n",
    "print(f\"Ensemble (Average)                    : MSE = {ensemble_mse:.2f}\")\n",
    "\n",
    "# Calculate improvement\n",
    "avg_individual_mse = np.mean([mean_squared_error(y_test, pred) for pred in individual_preds])\n",
    "improvement = (avg_individual_mse - ensemble_mse) / avg_individual_mse * 100\n",
    "print(f\"\\nEnsemble improvement: {improvement:.1f}% better than average individual model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. When Do Ensembles Work Best?\n",
    "\n",
    "### Ideal Conditions\n",
    "\n",
    "1. **Diverse Models**: Models make different types of errors\n",
    "2. **Reasonable Accuracy**: Each model performs better than random\n",
    "3. **Uncorrelated Errors**: Models' mistakes are independent\n",
    "\n",
    "### Measuring Model Diversity\n",
    "\n",
    "We can measure correlation between model predictions to assess diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze model diversity\n",
    "# Higher correlation = less diversity = less benefit from ensembling\n",
    "\n",
    "# Calculate correlation matrix of predictions\n",
    "pred_df = pd.DataFrame(\n",
    "    individual_preds.T,\n",
    "    columns=[f'Model {i+1}' for i in range(len(base_models))]\n",
    ")\n",
    "\n",
    "correlation_matrix = pred_df.corr()\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.3f', cmap='coolwarm', \n",
    "            center=0, square=True, linewidths=1, cbar_kws={'label': 'Correlation'})\n",
    "plt.title('Model Prediction Correlation Matrix\\n(Lower correlation = More diversity)', \n",
    "          fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate average correlation (excluding diagonal)\n",
    "mask = np.ones_like(correlation_matrix, dtype=bool)\n",
    "np.fill_diagonal(mask, False)\n",
    "avg_correlation = correlation_matrix.where(mask).mean().mean()\n",
    "\n",
    "print(f\"\\nAverage pairwise correlation: {avg_correlation:.3f}\")\n",
    "print(\"\\nInterpretation:\")\n",
    "if avg_correlation < 0.7:\n",
    "    print(\"✓ Good diversity! Models make different predictions.\")\n",
    "elif avg_correlation < 0.85:\n",
    "    print(\"○ Moderate diversity. Ensemble will help, but could be better.\")\n",
    "else:\n",
    "    print(\"✗ Low diversity. Models are too similar. Consider different architectures.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Real-World Applications\n",
    "\n",
    "### Where Ensembles Excel\n",
    "\n",
    "1. **Kaggle Competitions**: Most winning solutions use ensembles\n",
    "   - Combine 10-50+ models for maximum accuracy\n",
    "   - Stack different model types (trees, neural nets, linear models)\n",
    "\n",
    "2. **Production ML Systems**: Especially for critical applications\n",
    "   - Credit risk assessment\n",
    "   - Fraud detection\n",
    "   - Medical diagnosis\n",
    "   - Recommendation systems\n",
    "\n",
    "3. **Tabular Data**: Gradient boosting dominates\n",
    "   - XGBoost, LightGBM, CatBoost are state-of-the-art\n",
    "   - Often outperform neural networks on structured data\n",
    "\n",
    "### Trade-offs to Consider\n",
    "\n",
    "**Advantages**:\n",
    "- Higher accuracy and robustness\n",
    "- Reduced overfitting (especially bagging)\n",
    "- Better generalization\n",
    "\n",
    "**Disadvantages**:\n",
    "- Longer training time (N models vs 1 model)\n",
    "- Higher memory usage\n",
    "- Less interpretable (harder to explain predictions)\n",
    "- More complex deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration: Training time comparison\n",
    "import time\n",
    "\n",
    "# Create larger dataset\n",
    "X_large, y_large = make_classification(\n",
    "    n_samples=10000, n_features=50, n_informative=30, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Time single model\n",
    "single_model = DecisionTreeClassifier(max_depth=10, random_state=RANDOM_STATE)\n",
    "start = time.time()\n",
    "single_model.fit(X_large, y_large)\n",
    "single_time = time.time() - start\n",
    "\n",
    "# Time ensemble (50 models)\n",
    "ensemble_model = BaggingClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=10, random_state=RANDOM_STATE),\n",
    "    n_estimators=50,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1  # Use all CPU cores for parallel training\n",
    ")\n",
    "start = time.time()\n",
    "ensemble_model.fit(X_large, y_large)\n",
    "ensemble_time = time.time() - start\n",
    "\n",
    "print(\"Training Time Comparison:\")\n",
    "print(f\"Single model: {single_time:.3f} seconds\")\n",
    "print(f\"Ensemble (50 models): {ensemble_time:.3f} seconds\")\n",
    "print(f\"\\nTime multiplier: {ensemble_time/single_time:.1f}x\")\n",
    "print(\"\\nNote: Parallel processing (n_jobs=-1) helps, but ensembles\")\n",
    "print(\"still take longer. The accuracy gain often justifies this cost.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Test your understanding of ensemble learning concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Ensemble Size vs Performance\n",
    "\n",
    "Investigate how ensemble size affects performance. Create bagging ensembles with different numbers of models (1, 5, 10, 20, 50, 100) and plot how accuracy changes.\n",
    "\n",
    "**Questions to answer**:\n",
    "- At what point do diminishing returns set in?\n",
    "- Is there a \"sweet spot\" for ensemble size?\n",
    "- How does variance in accuracy change with ensemble size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: Use BaggingClassifier with different n_estimators values\n",
    "# Hint: Use cross_val_score to get robust accuracy estimates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Model Diversity Experiment\n",
    "\n",
    "Create two ensembles:\n",
    "1. **Low diversity**: 5 decision trees with similar parameters\n",
    "2. **High diversity**: 5 different model types (tree, linear, SVM, etc.)\n",
    "\n",
    "Compare their performance and analyze prediction correlations.\n",
    "\n",
    "**Hypothesis to test**: Does higher diversity lead to better ensemble performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Create classification dataset\n",
    "# Build two ensembles with different diversity levels\n",
    "# Compare performance and correlations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Bias-Variance Analysis\n",
    "\n",
    "Create a dataset with a known true function (e.g., polynomial). Train:\n",
    "1. A single high-bias model (shallow tree, max_depth=2)\n",
    "2. A single high-variance model (deep tree, max_depth=20)\n",
    "3. A bagging ensemble of high-variance models\n",
    "4. A boosting ensemble of high-bias models\n",
    "\n",
    "Visualize predictions and calculate errors to see how ensembles address bias and variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Create synthetic regression data with known function\n",
    "# Train different model types\n",
    "# Visualize and compare predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Custom Weighted Ensemble\n",
    "\n",
    "Extend the `SimpleEnsembleRegressor` class to support weighted averaging. Instead of equal weights, allow users to specify custom weights for each model.\n",
    "\n",
    "**Bonus**: Implement a method that automatically learns optimal weights based on validation performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Modify SimpleEnsembleRegressor to accept weights parameter\n",
    "# Implement weighted averaging in predict() method\n",
    "# Test with different weight configurations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Ensemble Learning**: Combining multiple models improves predictions through error averaging\n",
    "\n",
    "2. **Bias-Variance Tradeoff**:\n",
    "   - Bagging reduces variance (parallel ensembles)\n",
    "   - Boosting reduces bias (sequential ensembles)\n",
    "   - Both can outperform single models\n",
    "\n",
    "3. **Ensemble Strategies**:\n",
    "   - **Bagging**: Independent models, average predictions\n",
    "   - **Boosting**: Sequential models, focus on errors\n",
    "   - **Stacking**: Different model types, meta-learning\n",
    "\n",
    "4. **Success Requirements**:\n",
    "   - Model diversity (different errors)\n",
    "   - Reasonable individual accuracy\n",
    "   - Uncorrelated predictions\n",
    "\n",
    "5. **Trade-offs**:\n",
    "   - Higher accuracy vs longer training\n",
    "   - Better predictions vs interpretability\n",
    "   - Robustness vs complexity\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In the following notebooks, we'll dive deep into specific ensemble methods:\n",
    "\n",
    "- **Module 01**: Bagging and Bootstrap Aggregation\n",
    "- **Module 02**: Random Forests (the most popular bagging ensemble)\n",
    "- **Module 03**: AdaBoost (the first successful boosting algorithm)\n",
    "- **Module 04-07**: Modern gradient boosting (XGBoost, LightGBM, CatBoost)\n",
    "- **Module 08-10**: Advanced ensembles (stacking, voting, comparison)\n",
    "- **Module 11**: Kaggle-style competition project\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- **Paper**: \"Ensemble Methods in Machine Learning\" by Dietterich (2000)\n",
    "- **Book**: \"Ensemble Machine Learning\" edited by Zhang & Ma\n",
    "- **Documentation**: Scikit-learn ensemble methods guide\n",
    "- **Practice**: Kaggle competitions for real-world ensemble applications"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
