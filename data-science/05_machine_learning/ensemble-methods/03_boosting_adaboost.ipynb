{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 03: Boosting Fundamentals - AdaBoost\n",
    "\n",
    "**Difficulty**: ‚≠ê‚≠ê‚≠ê Advanced\n",
    "\n",
    "**Estimated Time**: 80 minutes\n",
    "\n",
    "**Prerequisites**: \n",
    "- Module 00: Introduction to Ensemble Learning\n",
    "- Module 01: Bagging and Bootstrap Aggregation\n",
    "- Understanding of weighted samples and exponential loss\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Understand how boosting differs from bagging (sequential vs parallel)\n",
    "2. Explain the AdaBoost algorithm and its mathematical foundation\n",
    "3. Implement AdaBoost from scratch to understand the mechanics\n",
    "4. Apply AdaBoost to classification problems using scikit-learn\n",
    "5. Tune AdaBoost hyperparameters for optimal performance\n",
    "6. Understand when to use boosting vs bagging\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn models and utilities\n",
    "from sklearn.datasets import make_classification, make_moons, load_breast_cancer\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_val_score,\n",
    "    learning_curve,\n",
    "    validation_curve\n",
    ")\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import (\n",
    "    AdaBoostClassifier,\n",
    "    RandomForestClassifier,\n",
    "    BaggingClassifier\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 4)\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Boosting vs Bagging: Key Differences\n",
    "\n",
    "### Philosophical Difference\n",
    "\n",
    "**Bagging (Module 01-02)**:\n",
    "- **Democracy approach**: All models vote equally\n",
    "- Train models independently in parallel\n",
    "- Each model sees different random sample\n",
    "- Reduces variance, doesn't affect bias\n",
    "- Use complex base models (deep trees)\n",
    "\n",
    "**Boosting**:\n",
    "- **Iterative learning**: Each model corrects previous errors\n",
    "- Train models sequentially (each depends on previous)\n",
    "- Each model focuses on hard examples\n",
    "- Reduces both bias and variance\n",
    "- Use simple base models (shallow trees)\n",
    "\n",
    "### Visual Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization comparing bagging and boosting\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bagging illustration\n",
    "ax = axes[0]\n",
    "ax.text(0.5, 0.95, 'Original Data', ha='center', fontsize=11, fontweight='bold')\n",
    "for i in range(3):\n",
    "    # Bootstrap samples\n",
    "    ax.add_patch(plt.Rectangle((0.15 + i*0.25, 0.75), 0.15, 0.12, \n",
    "                                fill=True, color='lightblue', alpha=0.7, edgecolor='black'))\n",
    "    ax.text(0.225 + i*0.25, 0.81, f'Bootstrap\\nSample {i+1}', \n",
    "            ha='center', fontsize=8, va='center')\n",
    "    \n",
    "    # Models (parallel)\n",
    "    ax.arrow(0.225 + i*0.25, 0.75, 0, -0.13, head_width=0.025, \n",
    "             head_length=0.02, fc='gray', alpha=0.5)\n",
    "    ax.add_patch(plt.Circle((0.225 + i*0.25, 0.55), 0.06, \n",
    "                             fill=True, color='orange', alpha=0.7, edgecolor='black'))\n",
    "    ax.text(0.225 + i*0.25, 0.55, f'M{i+1}', ha='center', va='center', fontsize=9)\n",
    "    \n",
    "    # Arrows to final\n",
    "    ax.arrow(0.225 + i*0.25, 0.49, (0.5 - (0.225 + i*0.25))*0.8, -0.2, \n",
    "             head_width=0.02, head_length=0.02, fc='black', alpha=0.3)\n",
    "\n",
    "# Final prediction\n",
    "ax.add_patch(plt.Rectangle((0.35, 0.15), 0.3, 0.12, \n",
    "                            fill=True, color='gold', alpha=0.8, edgecolor='black', linewidth=2))\n",
    "ax.text(0.5, 0.21, 'Average/Vote\\n(Equal Weight)', ha='center', va='center', \n",
    "        fontsize=9, fontweight='bold')\n",
    "\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.axis('off')\n",
    "ax.set_title('BAGGING\\n(Parallel, Independent)', fontweight='bold', fontsize=12)\n",
    "\n",
    "# Boosting illustration\n",
    "ax = axes[1]\n",
    "ax.text(0.5, 0.95, 'Original Data', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "colors = ['lightblue', 'lightcoral', 'lightgreen']\n",
    "for i in range(3):\n",
    "    y_pos = 0.75 - i*0.21\n",
    "    \n",
    "    # Weighted data\n",
    "    ax.add_patch(plt.Rectangle((0.05, y_pos), 0.35, 0.12, \n",
    "                                fill=True, color=colors[i], alpha=0.7, edgecolor='black'))\n",
    "    if i == 0:\n",
    "        ax.text(0.225, y_pos + 0.06, 'All samples\\nequal weight', \n",
    "                ha='center', va='center', fontsize=7)\n",
    "    else:\n",
    "        ax.text(0.225, y_pos + 0.06, f'Reweight\\n(focus on errors)', \n",
    "                ha='center', va='center', fontsize=7)\n",
    "    \n",
    "    # Model\n",
    "    ax.arrow(0.4, y_pos + 0.06, 0.1, 0, head_width=0.02, \n",
    "             head_length=0.02, fc='gray', alpha=0.5)\n",
    "    ax.add_patch(plt.Circle((0.6, y_pos + 0.06), 0.06, \n",
    "                             fill=True, color='orange', alpha=0.7, edgecolor='black'))\n",
    "    ax.text(0.6, y_pos + 0.06, f'M{i+1}', ha='center', va='center', fontsize=9)\n",
    "    \n",
    "    # Weight\n",
    "    ax.text(0.75, y_pos + 0.06, f'Œ±{i+1}', fontsize=10, fontweight='bold', \n",
    "            style='italic', color='red')\n",
    "    \n",
    "    # Feedback arrow (except last)\n",
    "    if i < 2:\n",
    "        ax.annotate('', xy=(0.1, y_pos - 0.06), xytext=(0.6, y_pos),\n",
    "                   arrowprops=dict(arrowstyle='->', lw=1.5, color='red', alpha=0.6))\n",
    "        ax.text(0.35, y_pos - 0.09, 'Update\\nweights', fontsize=7, \n",
    "                ha='center', color='red', style='italic')\n",
    "\n",
    "# Final prediction\n",
    "ax.add_patch(plt.Rectangle((0.25, 0.02), 0.5, 0.1, \n",
    "                            fill=True, color='gold', alpha=0.8, edgecolor='black', linewidth=2))\n",
    "ax.text(0.5, 0.07, 'Weighted Sum\\n(Œ±‚ÇÅM‚ÇÅ + Œ±‚ÇÇM‚ÇÇ + Œ±‚ÇÉM‚ÇÉ)', \n",
    "        ha='center', va='center', fontsize=8, fontweight='bold')\n",
    "\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.axis('off')\n",
    "ax.set_title('BOOSTING\\n(Sequential, Adaptive)', fontweight='bold', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Key Differences Summary:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nBAGGING:\")\n",
    "print(\"  ‚Ä¢ Models trained in parallel (independent)\")\n",
    "print(\"  ‚Ä¢ Each model has equal weight\")\n",
    "print(\"  ‚Ä¢ Focus: Reduce variance\")\n",
    "print(\"  ‚Ä¢ Best with: Complex models (deep trees)\")\n",
    "print(\"\\nBOOSTING:\")\n",
    "print(\"  ‚Ä¢ Models trained sequentially (dependent)\")\n",
    "print(\"  ‚Ä¢ Models have different weights based on performance\")\n",
    "print(\"  ‚Ä¢ Focus: Reduce both bias and variance\")\n",
    "print(\"  ‚Ä¢ Best with: Simple models (shallow trees, stumps)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. AdaBoost Algorithm\n",
    "\n",
    "### The Name: Adaptive Boosting\n",
    "\n",
    "**AdaBoost** = **Ada**ptive **Boost**ing\n",
    "- **Adaptive**: Weights adapt to focus on misclassified examples\n",
    "- **Boosting**: Sequential combination of weak learners\n",
    "\n",
    "### The Algorithm (Binary Classification)\n",
    "\n",
    "**Input**: Training data $(x_1, y_1), ..., (x_n, y_n)$ where $y_i \\in \\{-1, +1\\}$\n",
    "\n",
    "**Initialize**: Sample weights $w_i^{(1)} = \\frac{1}{n}$ for all $i$\n",
    "\n",
    "**For** $t = 1$ **to** $T$:\n",
    "\n",
    "1. **Train weak learner**: $h_t(x)$ on weighted dataset\n",
    "\n",
    "2. **Calculate error**: $\\epsilon_t = \\sum_{i: h_t(x_i) \\neq y_i} w_i^{(t)}$\n",
    "\n",
    "3. **Calculate model weight**: $\\alpha_t = \\frac{1}{2} \\ln\\left(\\frac{1 - \\epsilon_t}{\\epsilon_t}\\right)$\n",
    "\n",
    "4. **Update sample weights**: \n",
    "   $$w_i^{(t+1)} = w_i^{(t)} \\cdot e^{-\\alpha_t y_i h_t(x_i)}$$\n",
    "   \n",
    "5. **Normalize**: $w_i^{(t+1)} = \\frac{w_i^{(t+1)}}{\\sum_j w_j^{(t+1)}}$\n",
    "\n",
    "**Final Model**: $H(x) = \\text{sign}\\left(\\sum_{t=1}^T \\alpha_t h_t(x)\\right)$\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "1. **Error-based weighting**: Better models get higher $\\alpha$ (more influence)\n",
    "2. **Misclassification focus**: Misclassified samples get higher weights\n",
    "3. **Exponential loss**: Severely penalizes misclassifications\n",
    "4. **Weak learners**: Even slightly-better-than-random models help!\n",
    "\n",
    "### Model Weight Interpretation\n",
    "\n",
    "$$\\alpha_t = \\frac{1}{2} \\ln\\left(\\frac{1 - \\epsilon_t}{\\epsilon_t}\\right)$$\n",
    "\n",
    "- If $\\epsilon_t = 0.5$ (random): $\\alpha_t = 0$ (no contribution)\n",
    "- If $\\epsilon_t < 0.5$ (better than random): $\\alpha_t > 0$ (positive contribution)\n",
    "- If $\\epsilon_t \\to 0$ (perfect): $\\alpha_t \\to \\infty$ (maximum contribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize relationship between error and model weight\n",
    "errors = np.linspace(0.01, 0.99, 100)\n",
    "alphas = 0.5 * np.log((1 - errors) / errors)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(errors, alphas, linewidth=2.5, color='darkblue')\n",
    "plt.axhline(y=0, color='red', linestyle='--', alpha=0.5, label='Œ± = 0 (no contribution)')\n",
    "plt.axvline(x=0.5, color='orange', linestyle='--', alpha=0.5, label='Œµ = 0.5 (random)')\n",
    "plt.xlabel('Model Error (Œµ)', fontsize=12)\n",
    "plt.ylabel('Model Weight (Œ±)', fontsize=12)\n",
    "plt.title('AdaBoost: Model Weight as Function of Error', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(fontsize=10)\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(-3, 3)\n",
    "\n",
    "# Add annotations\n",
    "plt.annotate('Perfect model\\n(Œµ ‚Üí 0, Œ± ‚Üí ‚àû)', xy=(0.1, 2), fontsize=9,\n",
    "            bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7))\n",
    "plt.annotate('Random model\\n(Œµ = 0.5, Œ± = 0)', xy=(0.5, 0.3), fontsize=9,\n",
    "            bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.7))\n",
    "plt.annotate('Worse than random\\n(Œµ > 0.5, Œ± < 0)', xy=(0.7, -1.5), fontsize=9,\n",
    "            bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.7))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Interpretation:\")\n",
    "print(\"  ‚Ä¢ Low error (< 0.5) ‚Üí Positive weight ‚Üí Model contributes positively\")\n",
    "print(\"  ‚Ä¢ High error (> 0.5) ‚Üí Negative weight ‚Üí Model contributes negatively\")\n",
    "print(\"  ‚Ä¢ Random (= 0.5) ‚Üí Zero weight ‚Üí Model ignored\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Exercise 1: Understanding Sample Reweighting\n",
    "\n",
    "Implement the sample weight update mechanism:\n",
    "\n",
    "1. Create a simple dataset with 10 samples\n",
    "2. Initialize equal weights (1/10 each)\n",
    "3. Simulate a weak learner that misclassifies 3 samples\n",
    "4. Calculate $\\alpha$ based on error\n",
    "5. Update and normalize sample weights\n",
    "6. Visualize how weights change (which samples get emphasized?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. AdaBoost from Scratch\n",
    "\n",
    "Let's implement a simplified version of AdaBoost to understand the mechanics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAdaBoost:\n",
    "    \"\"\"\n",
    "    Simplified AdaBoost implementation for binary classification.\n",
    "    \n",
    "    This implementation helps understand the core algorithm.\n",
    "    For production use, always use sklearn's AdaBoostClassifier.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_estimators=50):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.estimators_ = []\n",
    "        self.alphas_ = []\n",
    "        self.weight_history_ = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train AdaBoost ensemble.\n",
    "        \n",
    "        Args:\n",
    "            X: Feature matrix (n_samples, n_features)\n",
    "            y: Target vector (n_samples,), values in {-1, +1}\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        # Initialize weights uniformly\n",
    "        weights = np.ones(n_samples) / n_samples\n",
    "        \n",
    "        for t in range(self.n_estimators):\n",
    "            # Save weights for visualization\n",
    "            self.weight_history_.append(weights.copy())\n",
    "            \n",
    "            # Train weak learner on weighted data\n",
    "            # Decision stump (depth 1 tree) is classic weak learner\n",
    "            stump = DecisionTreeClassifier(max_depth=1, random_state=t)\n",
    "            stump.fit(X, y, sample_weight=weights)\n",
    "            \n",
    "            # Get predictions\n",
    "            predictions = stump.predict(X)\n",
    "            \n",
    "            # Calculate weighted error\n",
    "            incorrect = (predictions != y)\n",
    "            error = np.sum(weights * incorrect) / np.sum(weights)\n",
    "            \n",
    "            # Avoid division by zero and log(0)\n",
    "            error = np.clip(error, 1e-10, 1 - 1e-10)\n",
    "            \n",
    "            # Calculate model weight (alpha)\n",
    "            alpha = 0.5 * np.log((1 - error) / error)\n",
    "            \n",
    "            # Update sample weights\n",
    "            weights *= np.exp(-alpha * y * predictions)\n",
    "            \n",
    "            # Normalize weights\n",
    "            weights /= np.sum(weights)\n",
    "            \n",
    "            # Store estimator and its weight\n",
    "            self.estimators_.append(stump)\n",
    "            self.alphas_.append(alpha)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions using weighted majority voting.\n",
    "        \n",
    "        Args:\n",
    "            X: Feature matrix (n_samples, n_features)\n",
    "        \n",
    "        Returns:\n",
    "            predictions: Predicted classes (n_samples,)\n",
    "        \"\"\"\n",
    "        # Get weighted sum of predictions\n",
    "        weighted_sum = np.zeros(X.shape[0])\n",
    "        \n",
    "        for alpha, estimator in zip(self.alphas_, self.estimators_):\n",
    "            weighted_sum += alpha * estimator.predict(X)\n",
    "        \n",
    "        # Return sign (-1 or +1)\n",
    "        return np.sign(weighted_sum)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Calculate accuracy score.\n",
    "        \"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions == y)\n",
    "\n",
    "# Test our implementation\n",
    "X, y = make_classification(\n",
    "    n_samples=200,\n",
    "    n_features=2,\n",
    "    n_informative=2,\n",
    "    n_redundant=0,\n",
    "    n_clusters_per_class=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Convert to {-1, +1}\n",
    "y = 2 * y - 1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Train our AdaBoost\n",
    "ada_custom = SimpleAdaBoost(n_estimators=50)\n",
    "ada_custom.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "train_acc = ada_custom.score(X_train, y_train)\n",
    "test_acc = ada_custom.score(X_test, y_test)\n",
    "\n",
    "print(\"\\nüìä Custom AdaBoost Performance:\")\n",
    "print(f\"Training Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Test Accuracy:     {test_acc:.4f}\")\n",
    "print(f\"\\nNumber of estimators: {len(ada_custom.estimators_)}\")\n",
    "print(f\"\\nModel weights (Œ±) range: [{min(ada_custom.alphas_):.4f}, {max(ada_custom.alphas_):.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize AdaBoost Learning Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how sample weights evolve\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Show weight evolution at different stages\n",
    "stages = [0, 5, 15, 49]  # Iteration numbers to visualize\n",
    "\n",
    "for idx, stage in enumerate(stages):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Get weights at this stage\n",
    "    weights = ada_custom.weight_history_[stage]\n",
    "    \n",
    "    # Scale weights for visualization (larger circles = higher weight)\n",
    "    sizes = weights * 50000\n",
    "    \n",
    "    # Plot samples colored by true class, sized by weight\n",
    "    scatter = ax.scatter(\n",
    "        X_train[:, 0], \n",
    "        X_train[:, 1],\n",
    "        c=y_train,\n",
    "        s=sizes,\n",
    "        alpha=0.6,\n",
    "        cmap='coolwarm',\n",
    "        edgecolors='black',\n",
    "        linewidth=0.5\n",
    "    )\n",
    "    \n",
    "    ax.set_xlabel('Feature 1', fontsize=10)\n",
    "    ax.set_ylabel('Feature 2', fontsize=10)\n",
    "    ax.set_title(f'Iteration {stage + 1}\\n(Larger circles = Higher weight)', \n",
    "                 fontsize=11, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Observation:\")\n",
    "print(\"  As iterations progress, misclassified samples (near decision boundary)\")\n",
    "print(\"  get larger weights (bigger circles), forcing subsequent models to focus on them.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Exercise 2: Tracking Model Evolution\n",
    "\n",
    "Analyze how the ensemble improves over iterations:\n",
    "\n",
    "1. Calculate training and test accuracy after each iteration\n",
    "2. Plot accuracy curves for both sets\n",
    "3. Calculate and plot the ensemble's weighted voting margin\n",
    "4. At what iteration does performance plateau?\n",
    "5. Does AdaBoost overfit with too many iterations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. AdaBoost with Scikit-learn\n",
    "\n",
    "### Using AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load real dataset\n",
    "data = load_breast_cancer()\n",
    "X_cancer = data.data\n",
    "y_cancer = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_cancer, y_cancer, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Single decision stump (baseline)\n",
    "stump = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
    "stump.fit(X_train, y_train)\n",
    "stump_acc = stump.score(X_test, y_test)\n",
    "\n",
    "# AdaBoost with default settings\n",
    "ada_default = AdaBoostClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=50,\n",
    "    learning_rate=1.0,\n",
    "    random_state=42\n",
    ")\n",
    "ada_default.fit(X_train, y_train)\n",
    "ada_acc = ada_default.score(X_test, y_test)\n",
    "\n",
    "# Random Forest (for comparison)\n",
    "rf = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_acc = rf.score(X_test, y_test)\n",
    "\n",
    "# Compare\n",
    "print(\"\\nüìä Performance Comparison:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nSingle Decision Stump: {stump_acc:.4f}\")\n",
    "print(f\"AdaBoost (50 stumps):  {ada_acc:.4f}\")\n",
    "print(f\"Random Forest:         {rf_acc:.4f}\")\n",
    "print(f\"\\n‚úÖ AdaBoost improvement over stump: {(ada_acc - stump_acc):.4f}\")\n",
    "\n",
    "# Visualize\n",
    "models = ['Single\\nStump', 'AdaBoost\\n(50 stumps)', 'Random\\nForest']\n",
    "accuracies = [stump_acc, ada_acc, rf_acc]\n",
    "colors = ['lightcoral', 'lightgreen', 'skyblue']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(models, accuracies, color=colors, edgecolor='black', linewidth=2)\n",
    "plt.ylabel('Test Accuracy', fontsize=12)\n",
    "plt.title('Model Comparison: Weak Learner vs Ensembles', fontsize=14, fontweight='bold')\n",
    "plt.ylim(0.85, 1.0)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, height + 0.005,\n",
    "            f'{acc:.4f}', ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hyperparameter Tuning\n",
    "\n",
    "### Key AdaBoost Hyperparameters\n",
    "\n",
    "1. **n_estimators**: Number of weak learners\n",
    "   - More is usually better (with diminishing returns)\n",
    "   - Can overfit with too many on noisy data\n",
    "   \n",
    "2. **learning_rate**: Shrinkage parameter\n",
    "   - Controls contribution of each weak learner\n",
    "   - Lower values ‚Üí more robust but need more estimators\n",
    "   - Trade-off: n_estimators ‚Üë + learning_rate ‚Üì\n",
    "   \n",
    "3. **base_estimator**: The weak learner\n",
    "   - Decision stumps (max_depth=1) are classic choice\n",
    "   - Can use deeper trees for more complex patterns\n",
    "   - Should be simple to avoid overfitting\n",
    "   \n",
    "4. **algorithm**: 'SAMME' or 'SAMME.R'\n",
    "   - SAMME.R (default): Uses class probabilities (faster convergence)\n",
    "   - SAMME: Uses class labels (original AdaBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Study effect of n_estimators and learning_rate\n",
    "n_estimators_range = [10, 25, 50, 100, 200, 500]\n",
    "learning_rates = [0.01, 0.1, 0.5, 1.0, 2.0]\n",
    "\n",
    "results = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    scores = []\n",
    "    for n_est in n_estimators_range:\n",
    "        ada = AdaBoostClassifier(\n",
    "            estimator=DecisionTreeClassifier(max_depth=1),\n",
    "            n_estimators=n_est,\n",
    "            learning_rate=lr,\n",
    "            random_state=42\n",
    "        )\n",
    "        ada.fit(X_train, y_train)\n",
    "        scores.append(ada.score(X_test, y_test))\n",
    "    results.append(scores)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "for lr, scores in zip(learning_rates, results):\n",
    "    plt.plot(n_estimators_range, scores, marker='o', linewidth=2, \n",
    "             markersize=8, label=f'learning_rate={lr}')\n",
    "\n",
    "plt.xlabel('Number of Estimators', fontsize=12)\n",
    "plt.ylabel('Test Accuracy', fontsize=12)\n",
    "plt.title('AdaBoost: Effect of n_estimators and learning_rate', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10, loc='lower right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xscale('log')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Observations:\")\n",
    "print(\"  ‚Ä¢ Higher learning_rate converges faster but may overfit\")\n",
    "print(\"  ‚Ä¢ Lower learning_rate is more stable but needs more estimators\")\n",
    "print(\"  ‚Ä¢ Optimal: Balance between n_estimators and learning_rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Exercise 3: Base Estimator Complexity\n",
    "\n",
    "Investigate effect of base estimator complexity:\n",
    "\n",
    "1. Train AdaBoost with different max_depth values [1, 2, 3, 5, 10]\n",
    "2. For each depth, vary n_estimators [10, 50, 100, 200]\n",
    "3. Create heatmap showing test accuracy\n",
    "4. Which combination works best?\n",
    "5. Why might very deep trees perform worse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. When to Use AdaBoost\n",
    "\n",
    "### ‚úÖ Use AdaBoost When:\n",
    "\n",
    "1. **Simple base models underfit**\n",
    "   - Single decision stump too weak\n",
    "   - Need to reduce bias\n",
    "\n",
    "2. **Interpretability somewhat important**\n",
    "   - Can analyze individual stumps\n",
    "   - More interpretable than deep trees\n",
    "\n",
    "3. **Clean, well-labeled data**\n",
    "   - AdaBoost sensitive to noisy labels\n",
    "   - Outliers get high weights\n",
    "\n",
    "4. **Binary classification**\n",
    "   - Original algorithm designed for this\n",
    "   - SAMME extension handles multiclass\n",
    "\n",
    "### ‚ùå Avoid AdaBoost When:\n",
    "\n",
    "1. **Data is very noisy**\n",
    "   - Focuses on hard examples (including noise/outliers)\n",
    "   - Can overfit to noise\n",
    "\n",
    "2. **Need real-time predictions**\n",
    "   - Sequential nature makes it slower than Random Forest\n",
    "   - Can't parallelize as easily\n",
    "\n",
    "3. **High-dimensional sparse data**\n",
    "   - Gradient boosting (XGBoost, LightGBM) often better\n",
    "   - AdaBoost can struggle with sparsity\n",
    "\n",
    "### AdaBoost vs Random Forest\n",
    "\n",
    "| Aspect | AdaBoost | Random Forest |\n",
    "|--------|----------|---------------|\n",
    "| **Training** | Sequential | Parallel |\n",
    "| **Base learner** | Shallow trees (stumps) | Deep trees |\n",
    "| **Focus** | Reduce bias | Reduce variance |\n",
    "| **Noise sensitivity** | High | Low |\n",
    "| **Overfitting risk** | Medium | Low |\n",
    "| **Speed** | Slower | Faster (parallel) |\n",
    "| **Best for** | Clean data, need accuracy | Noisy data, need robustness |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Exercise 4: Robustness to Noise\n",
    "\n",
    "Test how AdaBoost and Random Forest handle noisy labels:\n",
    "\n",
    "1. Create a clean dataset\n",
    "2. Flip random percentage of labels (0%, 5%, 10%, 20%, 30%)\n",
    "3. Train both AdaBoost and Random Forest on each\n",
    "4. Evaluate on clean test set\n",
    "5. Plot performance vs noise level\n",
    "6. Which is more robust to label noise?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Next Steps\n",
    "\n",
    "### üéì Key Takeaways\n",
    "\n",
    "1. **Boosting Philosophy**:\n",
    "   - Sequential learning: each model corrects previous errors\n",
    "   - Adaptive weighting: hard examples get more attention\n",
    "   - Weak learners: even simple models help when combined\n",
    "\n",
    "2. **AdaBoost Algorithm**:\n",
    "   - Adaptively reweight samples based on errors\n",
    "   - Weight models based on performance\n",
    "   - Combine via weighted majority voting\n",
    "\n",
    "3. **Key Hyperparameters**:\n",
    "   - `n_estimators`: Number of weak learners (50-500 typical)\n",
    "   - `learning_rate`: Shrinkage factor (0.1-1.0)\n",
    "   - Base estimator: Usually decision stumps (max_depth=1)\n",
    "\n",
    "4. **When to Use**:\n",
    "   - ‚úÖ Clean data, need to reduce bias\n",
    "   - ‚ùå Noisy data, need robustness\n",
    "\n",
    "5. **Advantages**:\n",
    "   - Simple and effective\n",
    "   - Often achieves high accuracy\n",
    "   - Works with various base learners\n",
    "   - Some interpretability (can analyze stumps)\n",
    "\n",
    "6. **Limitations**:\n",
    "   - Sensitive to noisy data and outliers\n",
    "   - Can overfit with too many estimators\n",
    "   - Sequential (not parallelizable)\n",
    "   - Requires proper tuning of learning_rate\n",
    "\n",
    "### üìö What's Next?\n",
    "\n",
    "- **Module 04**: Gradient Boosting Machines (more general framework)\n",
    "- **Module 05**: XGBoost (optimized, regularized boosting)\n",
    "- **Module 06**: LightGBM (fast, efficient boosting)\n",
    "- **Module 07**: CatBoost (categorical feature handling)\n",
    "\n",
    "### üéØ Practice Recommendations\n",
    "\n",
    "1. Apply AdaBoost to your own classification problem\n",
    "2. Compare with Random Forest on same data\n",
    "3. Experiment with different base learners (not just trees)\n",
    "4. Analyze which samples get highest weights (are they outliers?)\n",
    "5. Try on a Kaggle dataset\n",
    "\n",
    "### üìñ Additional Resources\n",
    "\n",
    "- **Original Paper**: Freund & Schapire (1997). \"A Decision-Theoretic Generalization of On-Line Learning\"\n",
    "- **Tutorial**: \"A Short Introduction to Boosting\" by Freund & Schapire\n",
    "- **Sklearn Guide**: https://scikit-learn.org/stable/modules/ensemble.html#adaboost\n",
    "- **Book**: \"The Elements of Statistical Learning\" Chapter 10\n",
    "\n",
    "---\n",
    "\n",
    "**üöÄ Ready for more powerful boosting? Let's explore Gradient Boosting in Module 04!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
