{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 02: Random Forests\n",
    "\n",
    "**Difficulty**: ‚≠ê‚≠ê‚≠ê Advanced\n",
    "\n",
    "**Estimated Time**: 90 minutes\n",
    "\n",
    "**Prerequisites**: \n",
    "- Module 00: Introduction to Ensemble Learning\n",
    "- Module 01: Bagging and Bootstrap Aggregation\n",
    "- Understanding of decision trees and Gini impurity\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Understand how Random Forests improve upon basic bagging with feature randomness\n",
    "2. Tune Random Forest hyperparameters systematically\n",
    "3. Extract and interpret feature importances\n",
    "4. Handle imbalanced datasets using class weights\n",
    "5. Compare Random Forests with single decision trees and basic bagging\n",
    "6. Apply Random Forests to real-world classification and regression problems\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn models and utilities\n",
    "from sklearn.datasets import (\n",
    "    make_classification, \n",
    "    load_breast_cancer,\n",
    "    fetch_california_housing\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_val_score,\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV\n",
    ")\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    RandomForestRegressor,\n",
    "    BaggingClassifier\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    mean_squared_error,\n",
    "    r2_score\n",
    ")\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Configuration\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 4)\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Random Forests: Bagging + Feature Randomness\n",
    "\n",
    "### What Makes Random Forests \"Random\"?\n",
    "\n",
    "Random Forests extend basic bagging with an additional source of randomness:\n",
    "\n",
    "**Standard Bagging**:\n",
    "1. ‚úÖ Bootstrap sampling (random subset of samples)\n",
    "2. ‚ùå Use ALL features at each split\n",
    "\n",
    "**Random Forest**:\n",
    "1. ‚úÖ Bootstrap sampling (random subset of samples)\n",
    "2. ‚úÖ **Feature randomness** (random subset of features at each split)\n",
    "\n",
    "### Why Feature Randomness?\n",
    "\n",
    "**Problem with Standard Bagging**:\n",
    "- If there's one very strong predictor, most trees will use it for the first split\n",
    "- Trees become correlated (similar structure)\n",
    "- Correlated predictions don't average out errors as effectively\n",
    "\n",
    "**Solution: Random Feature Selection**:\n",
    "- At each node, consider only random subset of $m$ features (typically $m = \\sqrt{p}$ for classification)\n",
    "- Forces trees to be more diverse\n",
    "- Strong predictors don't dominate every tree\n",
    "- Lower correlation between trees ‚Üí better variance reduction\n",
    "\n",
    "### Mathematical Insight\n",
    "\n",
    "Recall from Module 01:\n",
    "\n",
    "$$\\text{Ensemble Variance} = \\rho\\sigma^2 + \\frac{1-\\rho}{B}\\sigma^2$$\n",
    "\n",
    "Where:\n",
    "- $\\rho$ = average correlation between trees\n",
    "- $\\sigma^2$ = variance of individual trees\n",
    "- $B$ = number of trees\n",
    "\n",
    "**Key**: By reducing $\\rho$ (correlation), Random Forests achieve lower ensemble variance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Tree Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset with one strong predictor\n",
    "X, y = make_classification(\n",
    "    n_samples=500,\n",
    "    n_features=20,\n",
    "    n_informative=5,\n",
    "    n_redundant=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Standard bagging (all features at each split)\n",
    "bagging = BaggingClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=10),\n",
    "    n_estimators=20,\n",
    "    max_features=1.0,  # All features\n",
    "    random_state=42\n",
    ")\n",
    "bagging.fit(X_train, y_train)\n",
    "\n",
    "# Random Forest (subset of features at each split)\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=20,\n",
    "    max_depth=10,\n",
    "    max_features='sqrt',  # sqrt(20) ‚âà 4 features per split\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Get predictions from all trees in both ensembles\n",
    "bagging_preds = np.array([\n",
    "    tree.predict(X_test) for tree in bagging.estimators_\n",
    "])\n",
    "\n",
    "rf_preds = np.array([\n",
    "    tree.predict(X_test) for tree in rf.estimators_\n",
    "])\n",
    "\n",
    "# Calculate pairwise correlations between trees\n",
    "def calculate_tree_correlations(predictions):\n",
    "    \"\"\"\n",
    "    Calculate average pairwise correlation between tree predictions.\n",
    "    \n",
    "    Args:\n",
    "        predictions: Array of shape (n_trees, n_samples)\n",
    "    \n",
    "    Returns:\n",
    "        Average correlation coefficient\n",
    "    \"\"\"\n",
    "    n_trees = predictions.shape[0]\n",
    "    correlations = []\n",
    "    \n",
    "    for i in range(n_trees):\n",
    "        for j in range(i + 1, n_trees):\n",
    "            corr = np.corrcoef(predictions[i], predictions[j])[0, 1]\n",
    "            correlations.append(corr)\n",
    "    \n",
    "    return np.mean(correlations)\n",
    "\n",
    "bagging_corr = calculate_tree_correlations(bagging_preds)\n",
    "rf_corr = calculate_tree_correlations(rf_preds)\n",
    "\n",
    "print(\"üå≤ Tree Correlation Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nStandard Bagging (all features):\")\n",
    "print(f\"  Average tree correlation: {bagging_corr:.4f}\")\n",
    "print(f\"  Test accuracy: {bagging.score(X_test, y_test):.4f}\")\n",
    "print(f\"\\nRandom Forest (subset of features):\")\n",
    "print(f\"  Average tree correlation: {rf_corr:.4f}\")\n",
    "print(f\"  Test accuracy: {rf.score(X_test, y_test):.4f}\")\n",
    "print(f\"\\n‚úÖ Random Forest reduces correlation by {(1 - rf_corr/bagging_corr)*100:.1f}%!\")\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "methods = ['Standard\\nBagging', 'Random\\nForest']\n",
    "correlations = [bagging_corr, rf_corr]\n",
    "colors = ['lightcoral', 'lightgreen']\n",
    "\n",
    "bars = ax.bar(methods, correlations, color=colors, edgecolor='black', linewidth=2)\n",
    "ax.set_ylabel('Average Tree Correlation', fontsize=12)\n",
    "ax.set_title('Tree Correlation: Bagging vs Random Forest', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim(0, max(correlations) * 1.2)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar, corr in zip(bars, correlations):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, height + 0.01,\n",
    "            f'{corr:.4f}', ha='center', va='bottom', fontweight='bold', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Exercise 1: Feature Subset Size\n",
    "\n",
    "Experiment with different values of `max_features`:\n",
    "\n",
    "1. Train Random Forests with max_features = [1, 2, 5, 10, 20, 'sqrt', 'log2']\n",
    "2. For each, calculate:\n",
    "   - Average tree correlation\n",
    "   - Test set accuracy\n",
    "3. Plot correlation and accuracy vs max_features\n",
    "4. What's the optimal value? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random Forest Hyperparameters\n",
    "\n",
    "### Key Hyperparameters\n",
    "\n",
    "Random Forests have many hyperparameters. Here are the most important ones:\n",
    "\n",
    "#### 1. Number of Trees (`n_estimators`)\n",
    "- **Default**: 100\n",
    "- **Effect**: More trees ‚Üí better performance (with diminishing returns)\n",
    "- **Recommendation**: Start with 100-500, increase if underfitting\n",
    "- **Note**: Unlike boosting, RF doesn't overfit with more trees\n",
    "\n",
    "#### 2. Max Features (`max_features`)\n",
    "- **Default**: 'sqrt' for classification, '1.0' for regression\n",
    "- **Effect**: Controls tree diversity and correlation\n",
    "- **Options**:\n",
    "  - 'sqrt': $\\sqrt{n\\_features}$ (good default for classification)\n",
    "  - 'log2': $\\log_2(n\\_features)$\n",
    "  - int: Specific number of features\n",
    "  - float: Fraction of features\n",
    "- **Recommendation**: Start with 'sqrt', tune if needed\n",
    "\n",
    "#### 3. Tree Depth (`max_depth`)\n",
    "- **Default**: None (fully grown trees)\n",
    "- **Effect**: Controls individual tree complexity\n",
    "- **Recommendation**: Usually leave as None, but limit if memory is concern\n",
    "\n",
    "#### 4. Min Samples Split (`min_samples_split`)\n",
    "- **Default**: 2\n",
    "- **Effect**: Minimum samples required to split a node\n",
    "- **Recommendation**: Increase (5-20) to prevent overfitting\n",
    "\n",
    "#### 5. Min Samples Leaf (`min_samples_leaf`)\n",
    "- **Default**: 1\n",
    "- **Effect**: Minimum samples required in leaf nodes\n",
    "- **Recommendation**: Increase (2-10) for smoother predictions\n",
    "\n",
    "#### 6. Bootstrap (`bootstrap`)\n",
    "- **Default**: True\n",
    "- **Effect**: Whether to use bootstrap sampling\n",
    "- **Recommendation**: Always keep True (it's what makes it a Random Forest!)\n",
    "\n",
    "#### 7. OOB Score (`oob_score`)\n",
    "- **Default**: False\n",
    "- **Effect**: Calculate out-of-bag score during training\n",
    "- **Recommendation**: Set to True for free validation score\n",
    "\n",
    "#### 8. Class Weight (`class_weight`)\n",
    "- **Default**: None\n",
    "- **Effect**: Handle imbalanced datasets\n",
    "- **Options**: 'balanced', 'balanced_subsample', or custom dict\n",
    "- **Recommendation**: Use 'balanced' for imbalanced data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load breast cancer dataset (binary classification)\n",
    "data = load_breast_cancer()\n",
    "X_cancer = data.data\n",
    "y_cancer = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_cancer, y_cancer, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Dataset: Breast Cancer Wisconsin\")\n",
    "print(f\"Samples: {len(X_cancer)}\")\n",
    "print(f\"Features: {len(data.feature_names)}\")\n",
    "print(f\"Classes: {data.target_names}\")\n",
    "print(f\"Class distribution: {np.bincount(y_cancer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_features': ['sqrt', 'log2', 0.5],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Create base model\n",
    "rf_base = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "\n",
    "# Grid search with cross-validation\n",
    "print(\"\\nüîç Starting Grid Search...\")\n",
    "print(f\"Total combinations to test: {np.prod([len(v) for v in param_grid.values()])}\")\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf_base,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n‚úÖ Grid Search Complete!\")\n",
    "print(\"\\nüèÜ Best Parameters:\")\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "print(f\"\\nüìä Best CV Score: {grid_search.best_score_:.4f}\")\n",
    "print(f\"Test Score: {grid_search.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomized Search for Large Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomizedSearchCV is faster for large hyperparameter spaces\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Define distributions for randomized search\n",
    "param_distributions = {\n",
    "    'n_estimators': randint(50, 500),\n",
    "    'max_features': uniform(0.3, 0.7),  # Sample between 0.3 and 1.0\n",
    "    'max_depth': [None, 5, 10, 15, 20, 30],\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'min_samples_leaf': randint(1, 10),\n",
    "    'bootstrap': [True]\n",
    "}\n",
    "\n",
    "# Randomized search\n",
    "print(\"\\nüé≤ Starting Randomized Search...\")\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf_base,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=50,  # Try 50 random combinations\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n‚úÖ Randomized Search Complete!\")\n",
    "print(\"\\nüèÜ Best Parameters:\")\n",
    "for param, value in random_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "print(f\"\\nüìä Best CV Score: {random_search.best_score_:.4f}\")\n",
    "print(f\"Test Score: {random_search.score(X_test, y_test):.4f}\")\n",
    "\n",
    "# Compare with grid search\n",
    "print(f\"\\n‚ö° Grid Search tested {len(grid_search.cv_results_['params'])} combinations\")\n",
    "print(f\"‚ö° Random Search tested {len(random_search.cv_results_['params'])} combinations\")\n",
    "print(f\"\\nBest scores: Grid={grid_search.best_score_:.4f}, Random={random_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Exercise 2: Custom Hyperparameter Tuning\n",
    "\n",
    "Design your own hyperparameter search:\n",
    "\n",
    "1. Create a different synthetic dataset with different properties (high dimensions, imbalanced, etc.)\n",
    "2. Define a custom hyperparameter grid based on dataset characteristics\n",
    "3. Use GridSearchCV or RandomizedSearchCV to find optimal parameters\n",
    "4. Visualize how different hyperparameters affect performance\n",
    "5. Compare tuned model with default Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Importance\n",
    "\n",
    "### Why Feature Importance?\n",
    "\n",
    "Feature importance helps us:\n",
    "1. **Understand model**: Which features drive predictions?\n",
    "2. **Feature selection**: Remove unimportant features\n",
    "3. **Domain insights**: Discover relationships in data\n",
    "4. **Debug models**: Identify data leakage or spurious correlations\n",
    "\n",
    "### Two Types of Feature Importance\n",
    "\n",
    "#### 1. Impurity-Based (Default in sklearn)\n",
    "- Measures average decrease in impurity (Gini/entropy) when feature is used\n",
    "- **Pros**: Fast to compute (calculated during training)\n",
    "- **Cons**: Biased toward high-cardinality features\n",
    "\n",
    "#### 2. Permutation-Based\n",
    "- Measures decrease in model score when feature values are randomly shuffled\n",
    "- **Pros**: Unbiased, works with any model\n",
    "- **Cons**: Slower (requires multiple predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest for feature importance analysis\n",
    "rf_importance = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_importance.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "feature_names = data.feature_names\n",
    "importances = rf_importance.feature_importances_\n",
    "std = np.std([\n",
    "    tree.feature_importances_ for tree in rf_importance.estimators_\n",
    "], axis=0)\n",
    "\n",
    "# Create DataFrame for better visualization\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': importances,\n",
    "    'std': std\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nüìä Top 10 Most Important Features (Impurity-Based):\")\n",
    "print(importance_df.head(10).to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_features = importance_df.head(15)\n",
    "plt.barh(range(len(top_features)), top_features['importance'], \n",
    "         xerr=top_features['std'], alpha=0.7, edgecolor='black')\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Feature Importance', fontsize=12)\n",
    "plt.title('Top 15 Features by Importance (with std across trees)', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate permutation importance on test set\n",
    "perm_importance = permutation_importance(\n",
    "    rf_importance, \n",
    "    X_test, \n",
    "    y_test,\n",
    "    n_repeats=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Create DataFrame\n",
    "perm_importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': perm_importance.importances_mean,\n",
    "    'std': perm_importance.importances_std\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nüìä Top 10 Most Important Features (Permutation-Based):\")\n",
    "print(perm_importance_df.head(10).to_string(index=False))\n",
    "\n",
    "# Compare both methods\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Impurity-based\n",
    "top_impurity = importance_df.head(10)\n",
    "ax1.barh(range(len(top_impurity)), top_impurity['importance'], \n",
    "         alpha=0.7, color='skyblue', edgecolor='black')\n",
    "ax1.set_yticks(range(len(top_impurity)))\n",
    "ax1.set_yticklabels(top_impurity['feature'])\n",
    "ax1.set_xlabel('Importance', fontsize=12)\n",
    "ax1.set_title('Impurity-Based Importance', fontsize=13, fontweight='bold')\n",
    "ax1.invert_yaxis()\n",
    "ax1.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Permutation-based\n",
    "top_perm = perm_importance_df.head(10)\n",
    "ax2.barh(range(len(top_perm)), top_perm['importance'], \n",
    "         xerr=top_perm['std'], alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "ax2.set_yticks(range(len(top_perm)))\n",
    "ax2.set_yticklabels(top_perm['feature'])\n",
    "ax2.set_xlabel('Importance (Accuracy Drop)', fontsize=12)\n",
    "ax2.set_title('Permutation-Based Importance', fontsize=13, fontweight='bold')\n",
    "ax2.invert_yaxis()\n",
    "ax2.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Note: Both methods may rank features differently!\")\n",
    "print(\"   Impurity-based: Shows what model uses internally\")\n",
    "print(\"   Permutation: Shows what actually affects predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Exercise 3: Feature Selection\n",
    "\n",
    "Use feature importance for feature selection:\n",
    "\n",
    "1. Train a Random Forest on all features\n",
    "2. Select top k features (try k = 5, 10, 15, 20)\n",
    "3. Train new Random Forest on only selected features\n",
    "4. Compare performance and training time\n",
    "5. Plot accuracy vs number of features\n",
    "6. Find optimal number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Handling Imbalanced Datasets\n",
    "\n",
    "### The Imbalanced Data Problem\n",
    "\n",
    "Many real-world datasets are imbalanced (e.g., fraud detection: 99% legitimate, 1% fraud).\n",
    "\n",
    "**Problem**: Standard RF may ignore minority class to maximize overall accuracy.\n",
    "\n",
    "**Solutions**:\n",
    "1. **Class weights**: Penalize misclassification of minority class more\n",
    "2. **Balanced sampling**: Ensure each bootstrap sample is balanced\n",
    "3. **Adjust threshold**: Change decision threshold from 0.5\n",
    "4. **Use appropriate metrics**: F1, AUC-ROC instead of accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create imbalanced dataset (10:1 ratio)\n",
    "X_imb, y_imb = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=20,\n",
    "    n_informative=15,\n",
    "    n_classes=2,\n",
    "    weights=[0.9, 0.1],  # 90% class 0, 10% class 1\n",
    "    random_state=42,\n",
    "    flip_y=0.05\n",
    ")\n",
    "\n",
    "X_train_imb, X_test_imb, y_train_imb, y_test_imb = train_test_split(\n",
    "    X_imb, y_imb, test_size=0.3, random_state=42, stratify=y_imb\n",
    ")\n",
    "\n",
    "print(f\"Training set distribution:\")\n",
    "print(f\"  Class 0: {np.sum(y_train_imb == 0)} samples ({np.sum(y_train_imb == 0)/len(y_train_imb)*100:.1f}%)\")\n",
    "print(f\"  Class 1: {np.sum(y_train_imb == 1)} samples ({np.sum(y_train_imb == 1)/len(y_train_imb)*100:.1f}%)\")\n",
    "\n",
    "# Standard RF (no balancing)\n",
    "rf_standard = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    ")\n",
    "rf_standard.fit(X_train_imb, y_train_imb)\n",
    "y_pred_standard = rf_standard.predict(X_test_imb)\n",
    "\n",
    "# RF with balanced class weights\n",
    "rf_balanced = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    class_weight='balanced',  # Automatically adjust weights\n",
    "    random_state=42\n",
    ")\n",
    "rf_balanced.fit(X_train_imb, y_train_imb)\n",
    "y_pred_balanced = rf_balanced.predict(X_test_imb)\n",
    "\n",
    "# RF with balanced subsample\n",
    "rf_balanced_sub = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    class_weight='balanced_subsample',  # Balance each bootstrap sample\n",
    "    random_state=42\n",
    ")\n",
    "rf_balanced_sub.fit(X_train_imb, y_train_imb)\n",
    "y_pred_balanced_sub = rf_balanced_sub.predict(X_test_imb)\n",
    "\n",
    "# Compare\n",
    "print(\"\\nüìä Performance Comparison:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for name, y_pred in [(\"Standard\", y_pred_standard), \n",
    "                      (\"Balanced\", y_pred_balanced),\n",
    "                      (\"Balanced Subsample\", y_pred_balanced_sub)]:\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(classification_report(y_test_imb, y_pred, target_names=['Class 0', 'Class 1']))\n",
    "\n",
    "# ROC curves\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "models = [\n",
    "    (\"Standard\", rf_standard),\n",
    "    (\"Balanced\", rf_balanced),\n",
    "    (\"Balanced Subsample\", rf_balanced_sub)\n",
    "]\n",
    "\n",
    "for name, model in models:\n",
    "    RocCurveDisplay.from_estimator(model, X_test_imb, y_test_imb, ax=ax, name=name)\n",
    "\n",
    "ax.plot([0, 1], [0, 1], 'k--', label='Chance')\n",
    "ax.set_xlabel('False Positive Rate', fontsize=12)\n",
    "ax.set_ylabel('True Positive Rate', fontsize=12)\n",
    "ax.set_title('ROC Curves: Handling Imbalanced Data', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Exercise 4: Extreme Imbalance\n",
    "\n",
    "Create and handle an extremely imbalanced dataset:\n",
    "\n",
    "1. Generate dataset with 100:1 imbalance ratio\n",
    "2. Try different approaches:\n",
    "   - Standard RF\n",
    "   - Balanced class weights\n",
    "   - Custom class weights {0: 1, 1: 50}\n",
    "   - Adjusting decision threshold\n",
    "3. Compare using precision-recall curves\n",
    "4. Which approach works best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary and Next Steps\n",
    "\n",
    "### üéì Key Takeaways\n",
    "\n",
    "1. **Random Forest = Bagging + Feature Randomness**:\n",
    "   - Bootstrap sampling creates diversity in training data\n",
    "   - Random feature selection reduces tree correlation\n",
    "   - Lower correlation ‚Üí better ensemble performance\n",
    "\n",
    "2. **Key Hyperparameters**:\n",
    "   - `n_estimators`: More is better (100-500 typical)\n",
    "   - `max_features`: 'sqrt' for classification, tune if needed\n",
    "   - `min_samples_split/leaf`: Control tree complexity\n",
    "   - `class_weight`: Handle imbalanced data\n",
    "\n",
    "3. **Feature Importance**:\n",
    "   - Impurity-based: Fast, built-in, may be biased\n",
    "   - Permutation-based: Unbiased, slower, more reliable\n",
    "   - Use for feature selection and model interpretation\n",
    "\n",
    "4. **Imbalanced Data**:\n",
    "   - Use `class_weight='balanced'` or 'balanced_subsample'\n",
    "   - Evaluate with F1, AUC-ROC, precision-recall\n",
    "   - Consider threshold adjustment\n",
    "\n",
    "5. **Advantages of Random Forests**:\n",
    "   - ‚úÖ Robust to overfitting\n",
    "   - ‚úÖ Handles high-dimensional data\n",
    "   - ‚úÖ Provides feature importances\n",
    "   - ‚úÖ Minimal hyperparameter tuning needed\n",
    "   - ‚úÖ Parallelizable (fast training)\n",
    "\n",
    "6. **Limitations**:\n",
    "   - ‚ùå Less interpretable than single trees\n",
    "   - ‚ùå Larger model size\n",
    "   - ‚ùå Slower prediction than single trees\n",
    "   - ‚ùå Can struggle with very imbalanced data\n",
    "\n",
    "### üìö What's Next?\n",
    "\n",
    "- **Module 03**: AdaBoost (sequential ensemble that reduces bias)\n",
    "- **Module 04**: Gradient Boosting (powerful sequential method)\n",
    "- **Module 05**: XGBoost (optimized gradient boosting)\n",
    "\n",
    "### üéØ Practice Recommendations\n",
    "\n",
    "1. Apply Random Forest to your own classification problem\n",
    "2. Perform hyperparameter tuning with GridSearchCV\n",
    "3. Analyze feature importances to gain insights\n",
    "4. Compare with single decision tree and logistic regression\n",
    "5. Try on a real Kaggle dataset\n",
    "\n",
    "### üìñ Additional Resources\n",
    "\n",
    "- **Original Paper**: Breiman, L. (2001). \"Random Forests\"\n",
    "- **Sklearn Documentation**: https://scikit-learn.org/stable/modules/ensemble.html#forest\n",
    "- **Interpretability**: \"Interpretable Machine Learning\" by Molnar\n",
    "- **Feature Importance**: Strobl et al. (2007) on permutation importance\n",
    "\n",
    "---\n",
    "\n",
    "**üöÄ Ready to learn boosting? Let's move to Module 03: AdaBoost!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
