{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 10: Model Comparison and Selection\n",
    "\n",
    "**Difficulty**: ⭐⭐⭐\n",
    "**Estimated Time**: 60 minutes\n",
    "**Prerequisites**: \n",
    "- All previous modules (00-09)\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Compare all ensemble methods on multiple datasets and metrics\n",
    "2. Understand speed vs accuracy trade-offs for each method\n",
    "3. Choose the right ensemble method for different scenarios\n",
    "4. Create a decision framework for model selection\n",
    "5. Benchmark ensemble methods systematically\n",
    "6. Understand when to use which ensemble technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from time import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.datasets import (\n",
    "    load_breast_cancer, load_wine, load_digits,\n",
    "    make_classification, make_regression\n",
    ")\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, RandomForestRegressor,\n",
    "    GradientBoostingClassifier, GradientBoostingRegressor,\n",
    "    AdaBoostClassifier, BaggingClassifier,\n",
    "    VotingClassifier, StackingClassifier\n",
    ")\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, mean_squared_error, r2_score\n",
    "\n",
    "# Boosting libraries\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "try:\n",
    "    from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "    CATBOOST_AVAILABLE = True\n",
    "except ImportError:\n",
    "    CATBOOST_AVAILABLE = False\n",
    "\n",
    "# Configuration\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Comprehensive Benchmark Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_models(models_dict, X_train, X_test, y_train, y_test, task='classification'):\n",
    "    \"\"\"\n",
    "    Benchmark multiple models and return performance metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    models_dict : dict\n",
    "        Dictionary of {model_name: model_instance}\n",
    "    X_train, X_test, y_train, y_test : arrays\n",
    "        Training and test data\n",
    "    task : str\n",
    "        'classification' or 'regression'\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame with benchmark results\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for name, model in models_dict.items():\n",
    "        print(f\"Training {name}...\", end=' ')\n",
    "        \n",
    "        # Training time\n",
    "        start = time()\n",
    "        model.fit(X_train, y_train)\n",
    "        train_time = time() - start\n",
    "        \n",
    "        # Prediction time\n",
    "        start = time()\n",
    "        y_pred = model.predict(X_test)\n",
    "        pred_time = time() - start\n",
    "        \n",
    "        # Performance metrics\n",
    "        if task == 'classification':\n",
    "            train_score = accuracy_score(y_train, model.predict(X_train))\n",
    "            test_score = accuracy_score(y_test, y_pred)\n",
    "            metric_name = 'Accuracy'\n",
    "        else:\n",
    "            train_score = r2_score(y_train, model.predict(X_train))\n",
    "            test_score = r2_score(y_test, y_pred)\n",
    "            metric_name = 'R²'\n",
    "        \n",
    "        results.append({\n",
    "            'Model': name,\n",
    "            f'Train {metric_name}': train_score,\n",
    "            f'Test {metric_name}': test_score,\n",
    "            'Overfit Gap': train_score - test_score,\n",
    "            'Train Time (s)': train_time,\n",
    "            'Pred Time (s)': pred_time\n",
    "        })\n",
    "        \n",
    "        print(f\"Done ({train_time:.2f}s)\")\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Benchmark 1: Small Dataset (Breast Cancer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load small dataset\n",
    "cancer = load_breast_cancer()\n",
    "X, y = cancer.data, cancer.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Small Dataset: {X.shape}\")\n",
    "print(f\"Train: {X_train.shape}, Test: {X_test.shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models for comparison\n",
    "models_small = {\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Bagging': BaggingClassifier(n_estimators=100, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'AdaBoost': AdaBoostClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'XGBoost': XGBClassifier(n_estimators=100, random_state=42, eval_metric='logloss'),\n",
    "    'LightGBM': LGBMClassifier(n_estimators=100, random_state=42, verbose=-1),\n",
    "}\n",
    "\n",
    "if CATBOOST_AVAILABLE:\n",
    "    models_small['CatBoost'] = CatBoostClassifier(iterations=100, random_state=42, verbose=False)\n",
    "\n",
    "# Run benchmark\n",
    "results_small = benchmark_models(models_small, X_train, X_test, y_train, y_test)\n",
    "results_small = results_small.sort_values('Test Accuracy', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BENCHMARK RESULTS - SMALL DATASET\")\n",
    "print(\"=\"*80)\n",
    "print(results_small.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Accuracy comparison\n",
    "ax1 = axes[0]\n",
    "results_plot = results_small.sort_values('Test Accuracy')\n",
    "y_pos = np.arange(len(results_plot))\n",
    "ax1.barh(y_pos, results_plot['Test Accuracy'], alpha=0.7)\n",
    "ax1.set_yticks(y_pos)\n",
    "ax1.set_yticklabels(results_plot['Model'])\n",
    "ax1.set_xlabel('Test Accuracy')\n",
    "ax1.set_title('Model Accuracy Comparison')\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Training time\n",
    "ax2 = axes[1]\n",
    "results_plot = results_small.sort_values('Train Time (s)')\n",
    "y_pos = np.arange(len(results_plot))\n",
    "ax2.barh(y_pos, results_plot['Train Time (s)'], alpha=0.7, color='orange')\n",
    "ax2.set_yticks(y_pos)\n",
    "ax2.set_yticklabels(results_plot['Model'])\n",
    "ax2.set_xlabel('Training Time (seconds)')\n",
    "ax2.set_title('Training Speed Comparison')\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Overfitting gap\n",
    "ax3 = axes[2]\n",
    "results_plot = results_small.sort_values('Overfit Gap')\n",
    "y_pos = np.arange(len(results_plot))\n",
    "colors = ['green' if x < 0.05 else 'orange' if x < 0.1 else 'red' \n",
    "          for x in results_plot['Overfit Gap']]\n",
    "ax3.barh(y_pos, results_plot['Overfit Gap'], alpha=0.7, color=colors)\n",
    "ax3.set_yticks(y_pos)\n",
    "ax3.set_yticklabels(results_plot['Model'])\n",
    "ax3.set_xlabel('Train-Test Gap')\n",
    "ax3.set_title('Overfitting Analysis')\n",
    "ax3.axvline(x=0.05, color='green', linestyle='--', alpha=0.5, label='Good (<0.05)')\n",
    "ax3.axvline(x=0.1, color='orange', linestyle='--', alpha=0.5, label='Moderate (<0.1)')\n",
    "ax3.legend()\n",
    "ax3.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Benchmark 2: Large Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create large synthetic dataset\n",
    "X_large, y_large = make_classification(\n",
    "    n_samples=50000,\n",
    "    n_features=50,\n",
    "    n_informative=30,\n",
    "    n_redundant=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train_large, X_test_large, y_train_large, y_test_large = train_test_split(\n",
    "    X_large, y_large, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Large Dataset: {X_large.shape}\")\n",
    "print(f\"Train: {X_train_large.shape}, Test: {X_test_large.shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models optimized for large datasets\n",
    "models_large = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'XGBoost': XGBClassifier(n_estimators=100, random_state=42, eval_metric='logloss', n_jobs=-1),\n",
    "    'LightGBM': LGBMClassifier(n_estimators=100, random_state=42, verbose=-1, n_jobs=-1),\n",
    "}\n",
    "\n",
    "if CATBOOST_AVAILABLE:\n",
    "    models_large['CatBoost'] = CatBoostClassifier(iterations=100, random_state=42, verbose=False)\n",
    "\n",
    "# Run benchmark\n",
    "results_large = benchmark_models(models_large, X_train_large, X_test_large, \n",
    "                                 y_train_large, y_test_large)\n",
    "results_large = results_large.sort_values('Test Accuracy', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BENCHMARK RESULTS - LARGE DATASET\")\n",
    "print(\"=\"*80)\n",
    "print(results_large.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speed vs Accuracy scatter plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.scatter(results_large['Train Time (s)'], results_large['Test Accuracy'], \n",
    "          s=200, alpha=0.6, c=range(len(results_large)), cmap='viridis')\n",
    "\n",
    "for idx, row in results_large.iterrows():\n",
    "    ax.annotate(row['Model'], \n",
    "               (row['Train Time (s)'], row['Test Accuracy']),\n",
    "               xytext=(5, 5), textcoords='offset points', fontsize=10)\n",
    "\n",
    "ax.set_xlabel('Training Time (seconds)', fontsize=12)\n",
    "ax.set_ylabel('Test Accuracy', fontsize=12)\n",
    "ax.set_title('Speed vs Accuracy Trade-off (Large Dataset)', fontsize=14, fontweight='bold')\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Insights:\")\n",
    "print(\"- Upper-left corner: Fast AND accurate (ideal)\")\n",
    "print(\"- Lower-right corner: Slow AND less accurate (avoid)\")\n",
    "print(\"- Trade-off: Choose based on priorities (speed vs accuracy)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Ensemble Method Selection Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create decision matrix\n",
    "decision_matrix = pd.DataFrame({\n",
    "    'Method': ['Random Forest', 'Gradient Boosting', 'XGBoost', 'LightGBM', 'CatBoost', \n",
    "               'AdaBoost', 'Bagging', 'Stacking', 'Voting'],\n",
    "    'Accuracy': ['★★★★', '★★★★★', '★★★★★', '★★★★★', '★★★★★', \n",
    "                '★★★', '★★★', '★★★★★', '★★★★'],\n",
    "    'Speed': ['★★★★', '★★★', '★★★★', '★★★★★', '★★★', \n",
    "             '★★★', '★★★★★', '★★', '★★★'],\n",
    "    'Small Data': ['★★★★', '★★★★', '★★★★', '★★★', '★★★★★', \n",
    "                  '★★★', '★★★★', '★★★★', '★★★★'],\n",
    "    'Large Data': ['★★★★', '★★★', '★★★★', '★★★★★', '★★★★', \n",
    "                  '★★', '★★★', '★★★', '★★★'],\n",
    "    'Categorical Features': ['★★', '★★', '★★', '★★★★', '★★★★★', \n",
    "                            '★★', '★★', '★★★', '★★'],\n",
    "    'Interpretability': ['★★★', '★★', '★★', '★★', '★★', \n",
    "                        '★★★', '★★★', '★', '★★'],\n",
    "    'Overfitting Resistance': ['★★★★', '★★★', '★★★★', '★★★', '★★★★★', \n",
    "                              '★★', '★★★★', '★★★', '★★★★']\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"ENSEMBLE METHOD SELECTION GUIDE\")\n",
    "print(\"=\"*100)\n",
    "print(decision_matrix.to_string(index=False))\n",
    "print(\"\\n★ = Poor, ★★★ = Good, ★★★★★ = Excellent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Decision Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_ensemble(dataset_size, has_categorical, accuracy_priority, \n",
    "                      interpretability_needed, training_time_limit):\n",
    "    \"\"\"\n",
    "    Recommend ensemble method based on requirements.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataset_size : str\n",
    "        'small' (<10K), 'medium' (10K-1M), 'large' (>1M)\n",
    "    has_categorical : bool\n",
    "        Whether dataset has many categorical features\n",
    "    accuracy_priority : str\n",
    "        'highest', 'high', 'moderate'\n",
    "    interpretability_needed : bool\n",
    "        Whether model interpretability is important\n",
    "    training_time_limit : str\n",
    "        'strict', 'moderate', 'flexible'\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    List of recommended methods with rationale\n",
    "    \"\"\"\n",
    "    recommendations = []\n",
    "    \n",
    "    # Categorical features dominate\n",
    "    if has_categorical:\n",
    "        recommendations.append({\n",
    "            'method': 'CatBoost',\n",
    "            'reason': 'Best categorical feature handling',\n",
    "            'priority': 1\n",
    "        })\n",
    "        recommendations.append({\n",
    "            'method': 'LightGBM',\n",
    "            'reason': 'Good categorical support + fast',\n",
    "            'priority': 2\n",
    "        })\n",
    "    \n",
    "    # Large dataset\n",
    "    if dataset_size == 'large':\n",
    "        recommendations.append({\n",
    "            'method': 'LightGBM',\n",
    "            'reason': 'Fastest on large data',\n",
    "            'priority': 1\n",
    "        })\n",
    "        recommendations.append({\n",
    "            'method': 'XGBoost',\n",
    "            'reason': 'Good speed/accuracy balance',\n",
    "            'priority': 2\n",
    "        })\n",
    "    \n",
    "    # Small dataset\n",
    "    elif dataset_size == 'small':\n",
    "        recommendations.append({\n",
    "            'method': 'CatBoost',\n",
    "            'reason': 'Best overfitting resistance',\n",
    "            'priority': 1\n",
    "        })\n",
    "        recommendations.append({\n",
    "            'method': 'Random Forest',\n",
    "            'reason': 'Robust and easy to tune',\n",
    "            'priority': 2\n",
    "        })\n",
    "    \n",
    "    # Highest accuracy needed\n",
    "    if accuracy_priority == 'highest':\n",
    "        recommendations.append({\n",
    "            'method': 'Stacking',\n",
    "            'reason': 'Combines strengths of multiple models',\n",
    "            'priority': 1\n",
    "        })\n",
    "    \n",
    "    # Interpretability needed\n",
    "    if interpretability_needed:\n",
    "        recommendations.append({\n",
    "            'method': 'Random Forest',\n",
    "            'reason': 'Feature importance + relatively interpretable',\n",
    "            'priority': 1\n",
    "        })\n",
    "        recommendations.append({\n",
    "            'method': 'Voting Ensemble',\n",
    "            'reason': 'Simple averaging, easy to explain',\n",
    "            'priority': 2\n",
    "        })\n",
    "    \n",
    "    # Strict time limit\n",
    "    if training_time_limit == 'strict':\n",
    "        recommendations.append({\n",
    "            'method': 'Random Forest',\n",
    "            'reason': 'Parallelizable, fast training',\n",
    "            'priority': 1\n",
    "        })\n",
    "        recommendations.append({\n",
    "            'method': 'LightGBM',\n",
    "            'reason': 'Fastest boosting algorithm',\n",
    "            'priority': 2\n",
    "        })\n",
    "    \n",
    "    # Remove duplicates and sort by priority\n",
    "    unique_recs = {}\n",
    "    for rec in recommendations:\n",
    "        method = rec['method']\n",
    "        if method not in unique_recs or rec['priority'] < unique_recs[method]['priority']:\n",
    "            unique_recs[method] = rec\n",
    "    \n",
    "    final_recs = sorted(unique_recs.values(), key=lambda x: x['priority'])\n",
    "    \n",
    "    return final_recs[:3]  # Return top 3\n",
    "\n",
    "# Example usage\n",
    "print(\"\\nExample Recommendation Scenarios:\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Scenario 1\n",
    "print(\"\\nScenario 1: Large dataset, many categorical features, need high accuracy\")\n",
    "recs = recommend_ensemble('large', True, 'high', False, 'moderate')\n",
    "for i, rec in enumerate(recs, 1):\n",
    "    print(f\"{i}. {rec['method']:15s} - {rec['reason']}\")\n",
    "\n",
    "# Scenario 2\n",
    "print(\"\\nScenario 2: Small dataset, need interpretability, strict time limit\")\n",
    "recs = recommend_ensemble('small', False, 'moderate', True, 'strict')\n",
    "for i, rec in enumerate(recs, 1):\n",
    "    print(f\"{i}. {rec['method']:15s} - {rec['reason']}\")\n",
    "\n",
    "# Scenario 3\n",
    "print(\"\\nScenario 3: Medium dataset, need highest accuracy, flexible time\")\n",
    "recs = recommend_ensemble('medium', False, 'highest', False, 'flexible')\n",
    "for i, rec in enumerate(recs, 1):\n",
    "    print(f\"{i}. {rec['method']:15s} - {rec['reason']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary: Complete Decision Guide\n",
    "\n",
    "### Quick Selection Guide:\n",
    "\n",
    "#### By Dataset Size:\n",
    "\n",
    "**Small Data (<10K rows)**:\n",
    "1. **CatBoost** - Best overfitting resistance\n",
    "2. **Random Forest** - Robust, easy to tune\n",
    "3. **XGBoost** - Regularization helps prevent overfitting\n",
    "\n",
    "**Medium Data (10K-1M rows)**:\n",
    "1. **XGBoost** - Best all-around performance\n",
    "2. **LightGBM** - Fast with good accuracy\n",
    "3. **CatBoost** - Excellent defaults\n",
    "\n",
    "**Large Data (>1M rows)**:\n",
    "1. **LightGBM** - Fastest training\n",
    "2. **XGBoost** - Good speed/accuracy balance\n",
    "3. **Random Forest** - Parallelizable\n",
    "\n",
    "#### By Feature Type:\n",
    "\n",
    "**Many Categorical Features**:\n",
    "1. **CatBoost** - Native categorical handling\n",
    "2. **LightGBM** - Good categorical support\n",
    "3. **Target Encoding + XGBoost** - Manual encoding needed\n",
    "\n",
    "**Numerical Features Only**:\n",
    "1. **XGBoost** - State-of-the-art\n",
    "2. **LightGBM** - Faster alternative\n",
    "3. **Gradient Boosting** - scikit-learn baseline\n",
    "\n",
    "#### By Objective:\n",
    "\n",
    "**Maximum Accuracy (competitions)**:\n",
    "1. **Stacking** (XGBoost + LightGBM + CatBoost)\n",
    "2. **Voting Ensemble** - Simpler alternative\n",
    "3. **Multi-level Stacking** - Ultimate performance\n",
    "\n",
    "**Speed Critical**:\n",
    "1. **LightGBM** - Fastest boosting\n",
    "2. **Random Forest** - Parallelizable\n",
    "3. **Voting** - Simple averaging\n",
    "\n",
    "**Interpretability Needed**:\n",
    "1. **Random Forest** - Feature importance\n",
    "2. **Gradient Boosting** - SHAP values\n",
    "3. **Single Decision Tree** - Most interpretable\n",
    "\n",
    "**Production Deployment**:\n",
    "1. **LightGBM** - Fast prediction\n",
    "2. **XGBoost** - Stable, well-tested\n",
    "3. **Random Forest** - Robust, simple\n",
    "\n",
    "### Method Comparison Summary:\n",
    "\n",
    "| Method | When to Use | When to Avoid |\n",
    "|--------|------------|---------------|\n",
    "| **Random Forest** | Small-medium data, baseline, interpretability | Large data, highest accuracy needed |\n",
    "| **Gradient Boosting** | Scikit-learn ecosystem, baseline | Speed critical, large data |\n",
    "| **XGBoost** | General purpose, competitions | Extreme speed needed |\n",
    "| **LightGBM** | Large data, speed critical | Small data (<10K rows) |\n",
    "| **CatBoost** | Categorical features, small data | Extreme speed needed |\n",
    "| **AdaBoost** | Simple boosting, teaching | Production use (outdated) |\n",
    "| **Bagging** | Reduce variance, simple | Need maximum accuracy |\n",
    "| **Stacking** | Maximum accuracy, competitions | Speed needed, interpretability |\n",
    "| **Voting** | Quick ensemble, robustness | Complex patterns, extreme accuracy |\n",
    "\n",
    "### Production Considerations:\n",
    "\n",
    "1. **Prediction Latency**:\n",
    "   - Fastest: LightGBM, Random Forest\n",
    "   - Moderate: XGBoost, CatBoost\n",
    "   - Slower: Stacking (multiple models)\n",
    "\n",
    "2. **Model Size**:\n",
    "   - Smallest: Boosting models (compressed)\n",
    "   - Larger: Random Forest (stores all trees)\n",
    "   - Largest: Stacking (multiple models)\n",
    "\n",
    "3. **Maintenance**:\n",
    "   - Easiest: Random Forest, Voting\n",
    "   - Moderate: XGBoost, LightGBM, CatBoost\n",
    "   - Complex: Stacking (multiple models to update)\n",
    "\n",
    "### Final Recommendations:\n",
    "\n",
    "**Default Choice**: Start with **XGBoost**\n",
    "- Best all-around performance\n",
    "- Well-documented\n",
    "- Works for most problems\n",
    "\n",
    "**Speed Critical**: Use **LightGBM**\n",
    "- Fastest training and prediction\n",
    "- Good accuracy\n",
    "- Excellent for large data\n",
    "\n",
    "**Categorical Heavy**: Use **CatBoost**\n",
    "- Best categorical handling\n",
    "- Great defaults\n",
    "- Resistant to overfitting\n",
    "\n",
    "**Maximum Accuracy**: Use **Stacking**\n",
    "- Combine XGBoost + LightGBM + CatBoost\n",
    "- Worth the complexity for critical applications\n",
    "- Test if improvement justifies cost\n",
    "\n",
    "**Simplicity/Interpretability**: Use **Random Forest**\n",
    "- Easy to understand\n",
    "- Good feature importance\n",
    "- Robust defaults\n",
    "\n",
    "### Remember:\n",
    "\n",
    "1. **Always benchmark** on your specific problem\n",
    "2. **Cross-validate** for reliable estimates\n",
    "3. **Check if ensemble helps** vs simple models\n",
    "4. **Consider deployment constraints** early\n",
    "5. **Monitor performance** in production\n",
    "6. **Update models** as data distribution changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Custom Benchmark\n",
    "\n",
    "Create your own benchmark comparing at least 5 ensemble methods on a dataset of your choice. Include:\n",
    "- Accuracy metrics\n",
    "- Training and prediction time\n",
    "- Overfitting analysis\n",
    "- Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Hyperparameter Impact\n",
    "\n",
    "For XGBoost, LightGBM, and CatBoost, compare:\n",
    "1. Default parameters\n",
    "2. Conservative parameters (prevent overfitting)\n",
    "3. Aggressive parameters (maximize accuracy)\n",
    "\n",
    "Which tuning strategy works best for different dataset sizes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Ensemble Combination\n",
    "\n",
    "Create a comprehensive ensemble that combines:\n",
    "- XGBoost, LightGBM, CatBoost as base models\n",
    "- Stacking OR voting (choose based on data size)\n",
    "- Compare with best individual model\n",
    "\n",
    "Is the ensemble worth the added complexity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "- [Scikit-learn Ensemble Guide](https://scikit-learn.org/stable/modules/ensemble.html)\n",
    "- [XGBoost Documentation](https://xgboost.readthedocs.io/)\n",
    "- [LightGBM Documentation](https://lightgbm.readthedocs.io/)\n",
    "- [CatBoost Documentation](https://catboost.ai/docs/)\n",
    "- [Kaggle Ensemble Guide](https://mlwave.com/kaggle-ensembling-guide/)\n",
    "- [Model Selection Best Practices](https://machinelearningmastery.com/a-gentle-introduction-to-model-selection-for-machine-learning/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
