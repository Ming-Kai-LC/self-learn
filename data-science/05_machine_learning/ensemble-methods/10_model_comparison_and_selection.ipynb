{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 10: Model Comparison and Selection\n",
    "\n",
    "**Difficulty**: ‚≠ê‚≠ê‚≠ê Advanced\n",
    "**Estimated Time**: 100 minutes\n",
    "**Prerequisites**: \n",
    "- All modules 00-09 (complete ensemble methods series)\n",
    "- Understanding of cross-validation and model evaluation\n",
    "- Familiarity with all ensemble algorithms\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Design comprehensive benchmarking experiments for ensemble methods\n",
    "2. Compare 10+ ensemble algorithms across multiple datasets and metrics\n",
    "3. Evaluate trade-offs: accuracy, speed, memory, interpretability\n",
    "4. Analyze hyperparameter sensitivity for different ensemble methods\n",
    "5. Make informed decisions about which ensemble method to use in production\n",
    "6. Create a decision framework for selecting ensemble methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import time\n",
    "import pickle\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.datasets import (\n",
    "    load_breast_cancer, load_wine, load_diabetes, \n",
    "    fetch_california_housing, make_classification, make_regression\n",
    ")\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, roc_auc_score, mean_squared_error, \n",
    "    r2_score, mean_absolute_error\n",
    ")\n",
    "\n",
    "# Ensemble methods\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import (\n",
    "    BaggingClassifier, BaggingRegressor,\n",
    "    RandomForestClassifier, RandomForestRegressor,\n",
    "    AdaBoostClassifier, AdaBoostRegressor,\n",
    "    GradientBoostingClassifier, GradientBoostingRegressor,\n",
    "    StackingClassifier, StackingRegressor,\n",
    "    VotingClassifier, VotingRegressor\n",
    ")\n",
    "\n",
    "# Advanced ensemble libraries\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "\n",
    "# For interpretability\n",
    "import shap\n",
    "\n",
    "# Configuration\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 4)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"‚úì Setup complete! All libraries imported successfully.\")\n",
    "print(f\"‚úì XGBoost version: {xgb.__version__}\")\n",
    "print(f\"‚úì LightGBM version: {lgb.__version__}\")\n",
    "print(f\"‚úì SHAP version: {shap.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Framework for Choosing Ensemble Methods\n",
    "\n",
    "### Decision Factors\n",
    "\n",
    "Choosing the right ensemble method depends on:\n",
    "\n",
    "1. **Dataset Characteristics**:\n",
    "   - Size (rows and columns)\n",
    "   - Feature types (numerical, categorical, mixed)\n",
    "   - Data quality (missing values, outliers)\n",
    "   - Class balance (for classification)\n",
    "\n",
    "2. **Performance Requirements**:\n",
    "   - Accuracy/R¬≤ target\n",
    "   - Training time budget\n",
    "   - Inference latency constraints\n",
    "   - Memory limitations\n",
    "\n",
    "3. **Business Constraints**:\n",
    "   - Interpretability needs\n",
    "   - Production deployment complexity\n",
    "   - Model maintenance overhead\n",
    "   - Cost of errors (false positives vs false negatives)\n",
    "\n",
    "4. **Technical Environment**:\n",
    "   - Available hardware (CPU, GPU, memory)\n",
    "   - Software stack compatibility\n",
    "   - Team expertise\n",
    "   - Existing infrastructure\n",
    "\n",
    "### Methodology\n",
    "\n",
    "We'll systematically evaluate ensemble methods using:\n",
    "- **Multiple datasets** (diverse characteristics)\n",
    "- **Standardized protocol** (same splits, same metrics)\n",
    "- **Comprehensive metrics** (accuracy, speed, memory, interpretability)\n",
    "- **Statistical rigor** (cross-validation, error bars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Benchmark Setup\n",
    "\n",
    "### Datasets\n",
    "\n",
    "We'll use 4 diverse datasets:\n",
    "\n",
    "**Classification:**\n",
    "1. **Breast Cancer** (569 samples, 30 features) - Binary, medical domain\n",
    "2. **Wine** (178 samples, 13 features) - Multiclass, small dataset\n",
    "\n",
    "**Regression:**\n",
    "3. **Diabetes** (442 samples, 10 features) - Medical progression\n",
    "4. **California Housing** (20,640 samples, 8 features) - Large dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare classification datasets\n",
    "def load_classification_datasets():\n",
    "    \"\"\"\n",
    "    Load and prepare classification datasets for benchmarking.\n",
    "    Returns dictionary of datasets with train/test splits.\n",
    "    \"\"\"\n",
    "    datasets = {}\n",
    "    \n",
    "    # 1. Breast Cancer (binary classification)\n",
    "    cancer = load_breast_cancer()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        cancer.data, cancer.target, test_size=0.2, random_state=RANDOM_STATE, stratify=cancer.target\n",
    "    )\n",
    "    datasets['breast_cancer'] = {\n",
    "        'X_train': X_train, 'X_test': X_test,\n",
    "        'y_train': y_train, 'y_test': y_test,\n",
    "        'task': 'binary',\n",
    "        'description': 'Breast Cancer (569 samples, 30 features)'\n",
    "    }\n",
    "    \n",
    "    # 2. Wine (multiclass classification)\n",
    "    wine = load_wine()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        wine.data, wine.target, test_size=0.2, random_state=RANDOM_STATE, stratify=wine.target\n",
    "    )\n",
    "    datasets['wine'] = {\n",
    "        'X_train': X_train, 'X_test': X_test,\n",
    "        'y_train': y_train, 'y_test': y_test,\n",
    "        'task': 'multiclass',\n",
    "        'description': 'Wine (178 samples, 13 features)'\n",
    "    }\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "# Load and prepare regression datasets\n",
    "def load_regression_datasets():\n",
    "    \"\"\"\n",
    "    Load and prepare regression datasets for benchmarking.\n",
    "    Returns dictionary of datasets with train/test splits.\n",
    "    \"\"\"\n",
    "    datasets = {}\n",
    "    \n",
    "    # 1. Diabetes (regression)\n",
    "    diabetes = load_diabetes()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        diabetes.data, diabetes.target, test_size=0.2, random_state=RANDOM_STATE\n",
    "    )\n",
    "    datasets['diabetes'] = {\n",
    "        'X_train': X_train, 'X_test': X_test,\n",
    "        'y_train': y_train, 'y_test': y_test,\n",
    "        'description': 'Diabetes (442 samples, 10 features)'\n",
    "    }\n",
    "    \n",
    "    # 2. California Housing (large regression)\n",
    "    housing = fetch_california_housing()\n",
    "    # Use subset for faster benchmarking\n",
    "    X_subset = housing.data[:5000]\n",
    "    y_subset = housing.target[:5000]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_subset, y_subset, test_size=0.2, random_state=RANDOM_STATE\n",
    "    )\n",
    "    datasets['housing'] = {\n",
    "        'X_train': X_train, 'X_test': X_test,\n",
    "        'y_train': y_train, 'y_test': y_test,\n",
    "        'description': 'California Housing (5000 samples, 8 features)'\n",
    "    }\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "# Load all datasets\n",
    "classification_data = load_classification_datasets()\n",
    "regression_data = load_regression_datasets()\n",
    "\n",
    "print(\"Classification Datasets:\")\n",
    "for name, data in classification_data.items():\n",
    "    print(f\"  ‚Ä¢ {data['description']}\")\n",
    "    print(f\"    Train: {len(data['y_train'])} samples, Test: {len(data['y_test'])} samples\")\n",
    "\n",
    "print(\"\\nRegression Datasets:\")\n",
    "for name, data in regression_data.items():\n",
    "    print(f\"  ‚Ä¢ {data['description']}\")\n",
    "    print(f\"    Train: {len(data['y_train'])} samples, Test: {len(data['y_test'])} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark Utilities\n",
    "\n",
    "Create standardized utilities for timing, memory profiling, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from typing import Dict, Any\n",
    "\n",
    "class ModelBenchmark:\n",
    "    \"\"\"\n",
    "    Comprehensive benchmarking utility for comparing ensemble methods.\n",
    "    \n",
    "    Tracks:\n",
    "    - Training time\n",
    "    - Prediction time\n",
    "    - Model size (memory)\n",
    "    - Performance metrics\n",
    "    - Cross-validation scores\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, model_name: str):\n",
    "        self.model = model\n",
    "        self.model_name = model_name\n",
    "        self.results = {}\n",
    "        \n",
    "    def benchmark_classification(self, X_train, X_test, y_train, y_test, \n",
    "                                 cv_folds: int = 5) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Run comprehensive benchmark for classification task.\n",
    "        \"\"\"\n",
    "        results = {'model_name': self.model_name}\n",
    "        \n",
    "        # 1. Training time\n",
    "        start_time = time.time()\n",
    "        self.model.fit(X_train, y_train)\n",
    "        train_time = time.time() - start_time\n",
    "        results['train_time'] = train_time\n",
    "        \n",
    "        # 2. Prediction time\n",
    "        start_time = time.time()\n",
    "        y_pred = self.model.predict(X_test)\n",
    "        pred_time = time.time() - start_time\n",
    "        results['pred_time'] = pred_time\n",
    "        results['pred_time_per_sample'] = pred_time / len(X_test)\n",
    "        \n",
    "        # 3. Performance metrics\n",
    "        results['accuracy'] = accuracy_score(y_test, y_pred)\n",
    "        results['f1'] = f1_score(y_test, y_pred, average='weighted')\n",
    "        \n",
    "        # ROC AUC (if binary and has predict_proba)\n",
    "        if len(np.unique(y_test)) == 2 and hasattr(self.model, 'predict_proba'):\n",
    "            y_proba = self.model.predict_proba(X_test)[:, 1]\n",
    "            results['roc_auc'] = roc_auc_score(y_test, y_proba)\n",
    "        else:\n",
    "            results['roc_auc'] = np.nan\n",
    "        \n",
    "        # 4. Cross-validation score\n",
    "        cv_scores = cross_val_score(self.model, X_train, y_train, cv=cv_folds, n_jobs=-1)\n",
    "        results['cv_mean'] = cv_scores.mean()\n",
    "        results['cv_std'] = cv_scores.std()\n",
    "        \n",
    "        # 5. Model size (approximate)\n",
    "        results['model_size_mb'] = self._estimate_model_size()\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def benchmark_regression(self, X_train, X_test, y_train, y_test, \n",
    "                            cv_folds: int = 5) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Run comprehensive benchmark for regression task.\n",
    "        \"\"\"\n",
    "        results = {'model_name': self.model_name}\n",
    "        \n",
    "        # 1. Training time\n",
    "        start_time = time.time()\n",
    "        self.model.fit(X_train, y_train)\n",
    "        train_time = time.time() - start_time\n",
    "        results['train_time'] = train_time\n",
    "        \n",
    "        # 2. Prediction time\n",
    "        start_time = time.time()\n",
    "        y_pred = self.model.predict(X_test)\n",
    "        pred_time = time.time() - start_time\n",
    "        results['pred_time'] = pred_time\n",
    "        results['pred_time_per_sample'] = pred_time / len(X_test)\n",
    "        \n",
    "        # 3. Performance metrics\n",
    "        results['r2'] = r2_score(y_test, y_pred)\n",
    "        results['mse'] = mean_squared_error(y_test, y_pred)\n",
    "        results['rmse'] = np.sqrt(results['mse'])\n",
    "        results['mae'] = mean_absolute_error(y_test, y_pred)\n",
    "        \n",
    "        # 4. Cross-validation score\n",
    "        cv_scores = cross_val_score(self.model, X_train, y_train, cv=cv_folds, \n",
    "                                   scoring='r2', n_jobs=-1)\n",
    "        results['cv_mean'] = cv_scores.mean()\n",
    "        results['cv_std'] = cv_scores.std()\n",
    "        \n",
    "        # 5. Model size (approximate)\n",
    "        results['model_size_mb'] = self._estimate_model_size()\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _estimate_model_size(self) -> float:\n",
    "        \"\"\"\n",
    "        Estimate model size in MB by pickling.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            pickled = pickle.dumps(self.model)\n",
    "            size_mb = sys.getsizeof(pickled) / (1024 * 1024)\n",
    "            return size_mb\n",
    "        except:\n",
    "            return np.nan\n",
    "\n",
    "print(\"‚úì ModelBenchmark class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Configuration\n",
    "\n",
    "Define all ensemble methods to compare. We'll use reasonable default parameters for fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification_models():\n",
    "    \"\"\"\n",
    "    Get dictionary of classification models for benchmarking.\n",
    "    All models use reasonable default parameters.\n",
    "    \"\"\"\n",
    "    models = {\n",
    "        'Decision Tree': DecisionTreeClassifier(\n",
    "            max_depth=10, random_state=RANDOM_STATE\n",
    "        ),\n",
    "        \n",
    "        'Bagging': BaggingClassifier(\n",
    "            estimator=DecisionTreeClassifier(max_depth=10, random_state=RANDOM_STATE),\n",
    "            n_estimators=50, random_state=RANDOM_STATE, n_jobs=-1\n",
    "        ),\n",
    "        \n",
    "        'Random Forest': RandomForestClassifier(\n",
    "            n_estimators=100, max_depth=10, random_state=RANDOM_STATE, n_jobs=-1\n",
    "        ),\n",
    "        \n",
    "        'AdaBoost': AdaBoostClassifier(\n",
    "            estimator=DecisionTreeClassifier(max_depth=3, random_state=RANDOM_STATE),\n",
    "            n_estimators=50, learning_rate=1.0, random_state=RANDOM_STATE,\n",
    "            algorithm='SAMME'\n",
    "        ),\n",
    "        \n",
    "        'Gradient Boosting': GradientBoostingClassifier(\n",
    "            n_estimators=100, learning_rate=0.1, max_depth=3, \n",
    "            random_state=RANDOM_STATE\n",
    "        ),\n",
    "        \n",
    "        'XGBoost': xgb.XGBClassifier(\n",
    "            n_estimators=100, learning_rate=0.1, max_depth=3,\n",
    "            random_state=RANDOM_STATE, n_jobs=-1, verbosity=0\n",
    "        ),\n",
    "        \n",
    "        'LightGBM': lgb.LGBMClassifier(\n",
    "            n_estimators=100, learning_rate=0.1, max_depth=3,\n",
    "            random_state=RANDOM_STATE, n_jobs=-1, verbose=-1\n",
    "        ),\n",
    "        \n",
    "        'CatBoost': CatBoostClassifier(\n",
    "            iterations=100, learning_rate=0.1, depth=3,\n",
    "            random_state=RANDOM_STATE, verbose=0\n",
    "        ),\n",
    "    }\n",
    "    \n",
    "    return models\n",
    "\n",
    "def get_regression_models():\n",
    "    \"\"\"\n",
    "    Get dictionary of regression models for benchmarking.\n",
    "    All models use reasonable default parameters.\n",
    "    \"\"\"\n",
    "    models = {\n",
    "        'Decision Tree': DecisionTreeRegressor(\n",
    "            max_depth=10, random_state=RANDOM_STATE\n",
    "        ),\n",
    "        \n",
    "        'Bagging': BaggingRegressor(\n",
    "            estimator=DecisionTreeRegressor(max_depth=10, random_state=RANDOM_STATE),\n",
    "            n_estimators=50, random_state=RANDOM_STATE, n_jobs=-1\n",
    "        ),\n",
    "        \n",
    "        'Random Forest': RandomForestRegressor(\n",
    "            n_estimators=100, max_depth=10, random_state=RANDOM_STATE, n_jobs=-1\n",
    "        ),\n",
    "        \n",
    "        'AdaBoost': AdaBoostRegressor(\n",
    "            estimator=DecisionTreeRegressor(max_depth=3, random_state=RANDOM_STATE),\n",
    "            n_estimators=50, learning_rate=1.0, random_state=RANDOM_STATE\n",
    "        ),\n",
    "        \n",
    "        'Gradient Boosting': GradientBoostingRegressor(\n",
    "            n_estimators=100, learning_rate=0.1, max_depth=3, \n",
    "            random_state=RANDOM_STATE\n",
    "        ),\n",
    "        \n",
    "        'XGBoost': xgb.XGBRegressor(\n",
    "            n_estimators=100, learning_rate=0.1, max_depth=3,\n",
    "            random_state=RANDOM_STATE, n_jobs=-1, verbosity=0\n",
    "        ),\n",
    "        \n",
    "        'LightGBM': lgb.LGBMRegressor(\n",
    "            n_estimators=100, learning_rate=0.1, max_depth=3,\n",
    "            random_state=RANDOM_STATE, n_jobs=-1, verbose=-1\n",
    "        ),\n",
    "        \n",
    "        'CatBoost': CatBoostRegressor(\n",
    "            iterations=100, learning_rate=0.1, depth=3,\n",
    "            random_state=RANDOM_STATE, verbose=0\n",
    "        ),\n",
    "    }\n",
    "    \n",
    "    return models\n",
    "\n",
    "print(\"‚úì Model configurations defined!\")\n",
    "print(f\"  Classification models: {len(get_classification_models())}\")\n",
    "print(f\"  Regression models: {len(get_regression_models())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Performance Comparison\n",
    "\n",
    "Run comprehensive benchmarks across all models and datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark classification models\n",
    "print(\"Benchmarking Classification Models...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "classification_results = []\n",
    "\n",
    "for dataset_name, dataset in classification_data.items():\n",
    "    print(f\"\\nDataset: {dataset['description']}\")\n",
    "    \n",
    "    models = get_classification_models()\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"  Testing {model_name}...\", end=' ')\n",
    "        \n",
    "        benchmark = ModelBenchmark(model, model_name)\n",
    "        results = benchmark.benchmark_classification(\n",
    "            dataset['X_train'], dataset['X_test'],\n",
    "            dataset['y_train'], dataset['y_test']\n",
    "        )\n",
    "        results['dataset'] = dataset_name\n",
    "        classification_results.append(results)\n",
    "        \n",
    "        print(f\"Accuracy: {results['accuracy']:.4f}, Time: {results['train_time']:.3f}s\")\n",
    "\n",
    "# Convert to DataFrame for easy analysis\n",
    "clf_results_df = pd.DataFrame(classification_results)\n",
    "print(\"\\n‚úì Classification benchmarking complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark regression models\n",
    "print(\"Benchmarking Regression Models...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "regression_results = []\n",
    "\n",
    "for dataset_name, dataset in regression_data.items():\n",
    "    print(f\"\\nDataset: {dataset['description']}\")\n",
    "    \n",
    "    models = get_regression_models()\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"  Testing {model_name}...\", end=' ')\n",
    "        \n",
    "        benchmark = ModelBenchmark(model, model_name)\n",
    "        results = benchmark.benchmark_regression(\n",
    "            dataset['X_train'], dataset['X_test'],\n",
    "            dataset['y_train'], dataset['y_test']\n",
    "        )\n",
    "        results['dataset'] = dataset_name\n",
    "        regression_results.append(results)\n",
    "        \n",
    "        print(f\"R¬≤: {results['r2']:.4f}, Time: {results['train_time']:.3f}s\")\n",
    "\n",
    "# Convert to DataFrame for easy analysis\n",
    "reg_results_df = pd.DataFrame(regression_results)\n",
    "print(\"\\n‚úì Regression benchmarking complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display comprehensive classification results\n",
    "print(\"Classification Results Summary\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for dataset_name in clf_results_df['dataset'].unique():\n",
    "    print(f\"\\n{dataset_name.upper()} Dataset:\")\n",
    "    subset = clf_results_df[clf_results_df['dataset'] == dataset_name].copy()\n",
    "    subset = subset.sort_values('accuracy', ascending=False)\n",
    "    \n",
    "    display_cols = ['model_name', 'accuracy', 'f1', 'train_time', 'pred_time', 'model_size_mb']\n",
    "    print(subset[display_cols].to_string(index=False))\n",
    "    \n",
    "    # Highlight best performers\n",
    "    best_model = subset.iloc[0]\n",
    "    print(f\"\\n  üèÜ Best Model: {best_model['model_name']} (Accuracy: {best_model['accuracy']:.4f})\")\n",
    "    print(f\"  ‚ö° Fastest Training: {subset.loc[subset['train_time'].idxmin(), 'model_name']} ({subset['train_time'].min():.3f}s)\")\n",
    "    print(f\"  üíæ Smallest Size: {subset.loc[subset['model_size_mb'].idxmin(), 'model_name']} ({subset['model_size_mb'].min():.2f} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize classification results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "datasets = clf_results_df['dataset'].unique()\n",
    "\n",
    "for idx, dataset_name in enumerate(datasets):\n",
    "    subset = clf_results_df[clf_results_df['dataset'] == dataset_name].copy()\n",
    "    subset = subset.sort_values('accuracy', ascending=True)\n",
    "    \n",
    "    row = idx // 2\n",
    "    col = idx % 2\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    # Accuracy comparison\n",
    "    bars = ax.barh(subset['model_name'], subset['accuracy'], \n",
    "                   color=plt.cm.viridis(subset['accuracy']), edgecolor='black')\n",
    "    ax.set_xlabel('Accuracy', fontsize=11, fontweight='bold')\n",
    "    ax.set_title(f'{dataset_name.replace(\"_\", \" \").title()}', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlim(0.7, 1.0)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (bar, val) in enumerate(zip(bars, subset['accuracy'])):\n",
    "        ax.text(val + 0.01, bar.get_y() + bar.get_height()/2, \n",
    "               f'{val:.3f}', va='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('classification_accuracy_comparison.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Visualization saved as 'classification_accuracy_comparison.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display comprehensive regression results\n",
    "print(\"Regression Results Summary\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for dataset_name in reg_results_df['dataset'].unique():\n",
    "    print(f\"\\n{dataset_name.upper()} Dataset:\")\n",
    "    subset = reg_results_df[reg_results_df['dataset'] == dataset_name].copy()\n",
    "    subset = subset.sort_values('r2', ascending=False)\n",
    "    \n",
    "    display_cols = ['model_name', 'r2', 'rmse', 'mae', 'train_time', 'pred_time', 'model_size_mb']\n",
    "    print(subset[display_cols].to_string(index=False))\n",
    "    \n",
    "    # Highlight best performers\n",
    "    best_model = subset.iloc[0]\n",
    "    print(f\"\\n  üèÜ Best Model: {best_model['model_name']} (R¬≤: {best_model['r2']:.4f})\")\n",
    "    print(f\"  ‚ö° Fastest Training: {subset.loc[subset['train_time'].idxmin(), 'model_name']} ({subset['train_time'].min():.3f}s)\")\n",
    "    print(f\"  üíæ Smallest Size: {subset.loc[subset['model_size_mb'].idxmin(), 'model_name']} ({subset['model_size_mb'].min():.2f} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize regression results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "datasets = reg_results_df['dataset'].unique()\n",
    "\n",
    "for idx, dataset_name in enumerate(datasets):\n",
    "    subset = reg_results_df[reg_results_df['dataset'] == dataset_name].copy()\n",
    "    subset = subset.sort_values('r2', ascending=True)\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # R¬≤ comparison\n",
    "    bars = ax.barh(subset['model_name'], subset['r2'], \n",
    "                   color=plt.cm.viridis(subset['r2']), edgecolor='black')\n",
    "    ax.set_xlabel('R¬≤ Score', fontsize=11, fontweight='bold')\n",
    "    ax.set_title(f'{dataset_name.replace(\"_\", \" \").title()}', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (bar, val) in enumerate(zip(bars, subset['r2'])):\n",
    "        ax.text(val + 0.01, bar.get_y() + bar.get_height()/2, \n",
    "               f'{val:.3f}', va='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('regression_r2_comparison.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Visualization saved as 'regression_r2_comparison.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speed vs Accuracy Trade-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize speed-accuracy tradeoff for classification\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "for idx, dataset_name in enumerate(clf_results_df['dataset'].unique()):\n",
    "    subset = clf_results_df[clf_results_df['dataset'] == dataset_name]\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Scatter plot: training time vs accuracy\n",
    "    scatter = ax.scatter(subset['train_time'], subset['accuracy'], \n",
    "                        s=200, alpha=0.6, c=range(len(subset)), cmap='viridis',\n",
    "                        edgecolors='black', linewidth=1.5)\n",
    "    \n",
    "    # Annotate points\n",
    "    for _, row in subset.iterrows():\n",
    "        ax.annotate(row['model_name'], \n",
    "                   (row['train_time'], row['accuracy']),\n",
    "                   xytext=(5, 5), textcoords='offset points',\n",
    "                   fontsize=8, fontweight='bold')\n",
    "    \n",
    "    ax.set_xlabel('Training Time (seconds)', fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel('Accuracy', fontsize=11, fontweight='bold')\n",
    "    ax.set_title(f'{dataset_name.replace(\"_\", \" \").title()}\\nSpeed vs Accuracy Trade-off', \n",
    "                fontsize=12, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Highlight Pareto frontier (best speed-accuracy combinations)\n",
    "    # Models that are not dominated by any other model\n",
    "    pareto_optimal = []\n",
    "    for i, row1 in subset.iterrows():\n",
    "        dominated = False\n",
    "        for j, row2 in subset.iterrows():\n",
    "            if i != j:\n",
    "                # row2 dominates row1 if it's both faster and more accurate\n",
    "                if row2['train_time'] < row1['train_time'] and row2['accuracy'] > row1['accuracy']:\n",
    "                    dominated = True\n",
    "                    break\n",
    "        if not dominated:\n",
    "            pareto_optimal.append(row1)\n",
    "    \n",
    "    if pareto_optimal:\n",
    "        pareto_df = pd.DataFrame(pareto_optimal).sort_values('train_time')\n",
    "        ax.plot(pareto_df['train_time'], pareto_df['accuracy'], \n",
    "               'r--', linewidth=2, alpha=0.5, label='Pareto Frontier')\n",
    "        ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('speed_accuracy_tradeoff.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Speed-accuracy tradeoff visualization saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Interpretability Comparison\n",
    "\n",
    "Compare ensemble methods on interpretability dimensions:\n",
    "- Feature importance availability\n",
    "- SHAP support\n",
    "- Model complexity\n",
    "- Debugging ease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interpretability comparison matrix\n",
    "interpretability_matrix = {\n",
    "    'Model': [\n",
    "        'Decision Tree', 'Bagging', 'Random Forest', 'AdaBoost',\n",
    "        'Gradient Boosting', 'XGBoost', 'LightGBM', 'CatBoost'\n",
    "    ],\n",
    "    'Feature Importance': [\n",
    "        'Native', 'Native', 'Native', 'Native',\n",
    "        'Native', 'Native', 'Native', 'Native'\n",
    "    ],\n",
    "    'SHAP Support': [\n",
    "        'TreeExplainer', 'Limited', 'TreeExplainer', 'Limited',\n",
    "        'TreeExplainer', 'TreeExplainer', 'TreeExplainer', 'TreeExplainer'\n",
    "    ],\n",
    "    'Complexity': [\n",
    "        'Low', 'Medium', 'Medium', 'Medium',\n",
    "        'High', 'High', 'High', 'High'\n",
    "    ],\n",
    "    'Debugging Ease': [\n",
    "        'Easy', 'Hard', 'Hard', 'Medium',\n",
    "        'Hard', 'Medium', 'Medium', 'Medium'\n",
    "    ],\n",
    "    'Visualization': [\n",
    "        'Tree plots', 'Limited', 'Limited', 'Limited',\n",
    "        'Limited', 'Tree plots', 'Tree plots', 'Tree plots'\n",
    "    ],\n",
    "    'Interpretability Score': [\n",
    "        9, 4, 5, 6,\n",
    "        5, 7, 7, 7\n",
    "    ]\n",
    "}\n",
    "\n",
    "interp_df = pd.DataFrame(interpretability_matrix)\n",
    "print(\"Interpretability Comparison Matrix\")\n",
    "print(\"=\" * 100)\n",
    "print(interp_df.to_string(index=False))\n",
    "\n",
    "# Visualize interpretability scores\n",
    "plt.figure(figsize=(12, 6))\n",
    "colors = plt.cm.RdYlGn(interp_df['Interpretability Score'] / 10)\n",
    "bars = plt.barh(interp_df['Model'], interp_df['Interpretability Score'], \n",
    "               color=colors, edgecolor='black', linewidth=1.5)\n",
    "plt.xlabel('Interpretability Score (1-10)', fontsize=11, fontweight='bold')\n",
    "plt.title('Model Interpretability Comparison\\n(Higher = More Interpretable)', \n",
    "         fontsize=12, fontweight='bold')\n",
    "plt.xlim(0, 10)\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars, interp_df['Interpretability Score']):\n",
    "    plt.text(val + 0.2, bar.get_y() + bar.get_height()/2, \n",
    "            f'{val}/10', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('interpretability_comparison.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP Value Demonstration\n",
    "\n",
    "Demonstrate SHAP interpretation for tree-based ensembles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models for SHAP demonstration\n",
    "X_train = classification_data['breast_cancer']['X_train']\n",
    "X_test = classification_data['breast_cancer']['X_test']\n",
    "y_train = classification_data['breast_cancer']['y_train']\n",
    "\n",
    "# Train XGBoost and Random Forest for comparison\n",
    "xgb_model = xgb.XGBClassifier(n_estimators=50, max_depth=3, random_state=RANDOM_STATE, verbosity=0)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=50, max_depth=10, random_state=RANDOM_STATE)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Create SHAP explainers\n",
    "xgb_explainer = shap.TreeExplainer(xgb_model)\n",
    "rf_explainer = shap.TreeExplainer(rf_model)\n",
    "\n",
    "# Calculate SHAP values for test set (use subset for speed)\n",
    "sample_size = min(100, len(X_test))\n",
    "X_sample = X_test[:sample_size]\n",
    "\n",
    "xgb_shap_values = xgb_explainer.shap_values(X_sample)\n",
    "rf_shap_values = rf_explainer.shap_values(X_sample)\n",
    "\n",
    "print(\"‚úì SHAP values calculated successfully!\")\n",
    "print(f\"  Sample size: {sample_size}\")\n",
    "print(f\"  XGBoost SHAP values shape: {xgb_shap_values.shape if isinstance(xgb_shap_values, np.ndarray) else 'list of arrays'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize SHAP summary plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# XGBoost SHAP\n",
    "plt.sca(axes[0])\n",
    "shap.summary_plot(xgb_shap_values, X_sample, show=False, max_display=10)\n",
    "axes[0].set_title('XGBoost - SHAP Feature Importance', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Random Forest SHAP  \n",
    "plt.sca(axes[1])\n",
    "# For multi-output (binary classification), use first class\n",
    "shap_vals = rf_shap_values[1] if isinstance(rf_shap_values, list) else rf_shap_values\n",
    "shap.summary_plot(shap_vals, X_sample, show=False, max_display=10)\n",
    "axes[1].set_title('Random Forest - SHAP Feature Importance', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('shap_comparison.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Key Insights:\")\n",
    "print(\"  ‚Ä¢ SHAP values show both feature importance AND direction of impact\")\n",
    "print(\"  ‚Ä¢ Red = high feature value, Blue = low feature value\")\n",
    "print(\"  ‚Ä¢ Position on x-axis shows positive or negative impact on prediction\")\n",
    "print(\"  ‚Ä¢ XGBoost and Random Forest may identify different important features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hyperparameter Sensitivity Analysis\n",
    "\n",
    "Analyze how sensitive each ensemble method is to key hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test n_estimators sensitivity (number of trees/iterations)\n",
    "n_estimators_range = [10, 25, 50, 100, 200]\n",
    "\n",
    "X_train = classification_data['breast_cancer']['X_train']\n",
    "X_test = classification_data['breast_cancer']['X_test']\n",
    "y_train = classification_data['breast_cancer']['y_train']\n",
    "y_test = classification_data['breast_cancer']['y_test']\n",
    "\n",
    "sensitivity_results = []\n",
    "\n",
    "print(\"Testing n_estimators sensitivity...\")\n",
    "\n",
    "for n_est in n_estimators_range:\n",
    "    # Random Forest\n",
    "    rf = RandomForestClassifier(n_estimators=n_est, max_depth=10, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "    rf.fit(X_train, y_train)\n",
    "    rf_score = accuracy_score(y_test, rf.predict(X_test))\n",
    "    sensitivity_results.append({'Model': 'Random Forest', 'n_estimators': n_est, 'Accuracy': rf_score})\n",
    "    \n",
    "    # XGBoost\n",
    "    xgb_model = xgb.XGBClassifier(n_estimators=n_est, max_depth=3, random_state=RANDOM_STATE, verbosity=0)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    xgb_score = accuracy_score(y_test, xgb_model.predict(X_test))\n",
    "    sensitivity_results.append({'Model': 'XGBoost', 'n_estimators': n_est, 'Accuracy': xgb_score})\n",
    "    \n",
    "    # LightGBM\n",
    "    lgb_model = lgb.LGBMClassifier(n_estimators=n_est, max_depth=3, random_state=RANDOM_STATE, verbose=-1)\n",
    "    lgb_model.fit(X_train, y_train)\n",
    "    lgb_score = accuracy_score(y_test, lgb_model.predict(X_test))\n",
    "    sensitivity_results.append({'Model': 'LightGBM', 'n_estimators': n_est, 'Accuracy': lgb_score})\n",
    "    \n",
    "    print(f\"  n_estimators={n_est}: RF={rf_score:.4f}, XGB={xgb_score:.4f}, LGB={lgb_score:.4f}\")\n",
    "\n",
    "sensitivity_df = pd.DataFrame(sensitivity_results)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 6))\n",
    "for model_name in sensitivity_df['Model'].unique():\n",
    "    subset = sensitivity_df[sensitivity_df['Model'] == model_name]\n",
    "    plt.plot(subset['n_estimators'], subset['Accuracy'], marker='o', \n",
    "            linewidth=2, markersize=8, label=model_name)\n",
    "\n",
    "plt.xlabel('Number of Estimators', fontsize=11, fontweight='bold')\n",
    "plt.ylabel('Accuracy', fontsize=11, fontweight='bold')\n",
    "plt.title('Hyperparameter Sensitivity: n_estimators\\n(Breast Cancer Dataset)', \n",
    "         fontsize=12, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('n_estimators_sensitivity.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Insights:\")\n",
    "print(\"  ‚Ä¢ Performance typically plateaus after 50-100 estimators\")\n",
    "print(\"  ‚Ä¢ Boosting methods (XGBoost, LightGBM) may converge faster\")\n",
    "print(\"  ‚Ä¢ More estimators = longer training but diminishing returns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test learning rate sensitivity (for boosting methods)\n",
    "learning_rates = [0.01, 0.05, 0.1, 0.3, 0.5, 1.0]\n",
    "\n",
    "lr_results = []\n",
    "\n",
    "print(\"Testing learning rate sensitivity...\")\n",
    "\n",
    "for lr in learning_rates:\n",
    "    # XGBoost\n",
    "    xgb_model = xgb.XGBClassifier(n_estimators=100, learning_rate=lr, max_depth=3, \n",
    "                                 random_state=RANDOM_STATE, verbosity=0)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    xgb_score = accuracy_score(y_test, xgb_model.predict(X_test))\n",
    "    lr_results.append({'Model': 'XGBoost', 'learning_rate': lr, 'Accuracy': xgb_score})\n",
    "    \n",
    "    # LightGBM\n",
    "    lgb_model = lgb.LGBMClassifier(n_estimators=100, learning_rate=lr, max_depth=3, \n",
    "                                  random_state=RANDOM_STATE, verbose=-1)\n",
    "    lgb_model.fit(X_train, y_train)\n",
    "    lgb_score = accuracy_score(y_test, lgb_model.predict(X_test))\n",
    "    lr_results.append({'Model': 'LightGBM', 'learning_rate': lr, 'Accuracy': lgb_score})\n",
    "    \n",
    "    # Gradient Boosting\n",
    "    gb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=lr, max_depth=3, \n",
    "                                         random_state=RANDOM_STATE)\n",
    "    gb_model.fit(X_train, y_train)\n",
    "    gb_score = accuracy_score(y_test, gb_model.predict(X_test))\n",
    "    lr_results.append({'Model': 'Gradient Boosting', 'learning_rate': lr, 'Accuracy': gb_score})\n",
    "    \n",
    "    print(f\"  learning_rate={lr:.2f}: XGB={xgb_score:.4f}, LGB={lgb_score:.4f}, GB={gb_score:.4f}\")\n",
    "\n",
    "lr_df = pd.DataFrame(lr_results)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 6))\n",
    "for model_name in lr_df['Model'].unique():\n",
    "    subset = lr_df[lr_df['Model'] == model_name]\n",
    "    plt.plot(subset['learning_rate'], subset['Accuracy'], marker='o', \n",
    "            linewidth=2, markersize=8, label=model_name)\n",
    "\n",
    "plt.xlabel('Learning Rate', fontsize=11, fontweight='bold')\n",
    "plt.ylabel('Accuracy', fontsize=11, fontweight='bold')\n",
    "plt.title('Hyperparameter Sensitivity: Learning Rate\\n(Boosting Methods Only)', \n",
    "         fontsize=12, fontweight='bold')\n",
    "plt.xscale('log')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('learning_rate_sensitivity.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Insights:\")\n",
    "print(\"  ‚Ä¢ Learning rate 0.1 is often a good default\")\n",
    "print(\"  ‚Ä¢ Too high (>0.5): risk of overfitting and instability\")\n",
    "print(\"  ‚Ä¢ Too low (<0.01): very slow convergence, need more estimators\")\n",
    "print(\"  ‚Ä¢ Optimal rate depends on n_estimators (inverse relationship)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Decision Framework\n",
    "\n",
    "Based on our analysis, here's a practical decision framework for selecting ensemble methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create decision framework table\n",
    "decision_framework = {\n",
    "    'Scenario': [\n",
    "        'Small dataset (<1000 samples)',\n",
    "        'Large dataset (>100k samples)',\n",
    "        'Need high interpretability',\n",
    "        'Categorical features',\n",
    "        'Speed critical (fast inference)',\n",
    "        'Maximum accuracy (any cost)',\n",
    "        'Limited memory',\n",
    "        'Quick prototyping',\n",
    "        'Production deployment',\n",
    "        'Imbalanced classes'\n",
    "    ],\n",
    "    'Recommended Method': [\n",
    "        'Random Forest',\n",
    "        'LightGBM',\n",
    "        'Decision Tree ‚Üí Random Forest',\n",
    "        'CatBoost',\n",
    "        'LightGBM',\n",
    "        'Stacking (XGB + LGB + RF)',\n",
    "        'LightGBM or Decision Tree',\n",
    "        'Random Forest or XGBoost',\n",
    "        'XGBoost or LightGBM',\n",
    "        'XGBoost or LightGBM (with scale_pos_weight)'\n",
    "    ],\n",
    "    'Alternative': [\n",
    "        'XGBoost',\n",
    "        'XGBoost',\n",
    "        'Gradient Boosting + SHAP',\n",
    "        'LightGBM (categorical_feature)',\n",
    "        'XGBoost',\n",
    "        'Voting Ensemble',\n",
    "        'Bagging',\n",
    "        'Gradient Boosting',\n",
    "        'CatBoost',\n",
    "        'Random Forest (balanced class weights)'\n",
    "    ],\n",
    "    'Rationale': [\n",
    "        'RF robust to overfitting on small data',\n",
    "        'LGB optimized for large-scale data',\n",
    "        'Tree models + feature importance',\n",
    "        'Native categorical handling',\n",
    "        'Fast inference, small model size',\n",
    "        'Combine strengths of multiple models',\n",
    "        'Memory-efficient implementations',\n",
    "        'Good default performance, easy to use',\n",
    "        'Proven reliability, good tooling',\n",
    "        'Built-in class weighting support'\n",
    "    ]\n",
    "}\n",
    "\n",
    "framework_df = pd.DataFrame(decision_framework)\n",
    "print(\"Decision Framework for Ensemble Method Selection\")\n",
    "print(\"=\" * 120)\n",
    "print(framework_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\\nüí° General Guidelines:\")\n",
    "print(\"\\n1. START SIMPLE:\")\n",
    "print(\"   ‚Ä¢ Always baseline with single decision tree or logistic regression\")\n",
    "print(\"   ‚Ä¢ Then try Random Forest (easy, robust, good default)\")\n",
    "print(\"   ‚Ä¢ If RF works well, try gradient boosting for extra performance\")\n",
    "\n",
    "print(\"\\n2. CHOOSE BOOSTING METHOD:\")\n",
    "print(\"   ‚Ä¢ XGBoost: Most mature, best documentation, widest support\")\n",
    "print(\"   ‚Ä¢ LightGBM: Fastest, most memory-efficient, great for large data\")\n",
    "print(\"   ‚Ä¢ CatBoost: Best for categorical features, good default parameters\")\n",
    "\n",
    "print(\"\\n3. WHEN TO STACK/VOTE:\")\n",
    "print(\"   ‚Ä¢ Competition: Stack everything for maximum performance\")\n",
    "print(\"   ‚Ä¢ Production: Usually stick to single best model (simpler)\")\n",
    "print(\"   ‚Ä¢ Voting can help if models disagree in useful ways\")\n",
    "\n",
    "print(\"\\n4. HYPERPARAMETER TUNING:\")\n",
    "print(\"   ‚Ä¢ Start with defaults\")\n",
    "print(\"   ‚Ä¢ Tune n_estimators first (use early stopping for boosting)\")\n",
    "print(\"   ‚Ä¢ Then max_depth or num_leaves\")\n",
    "print(\"   ‚Ä¢ Finally learning_rate (lower = better but slower)\")\n",
    "\n",
    "print(\"\\n5. VALIDATION STRATEGY:\")\n",
    "print(\"   ‚Ä¢ Always use cross-validation for reliable estimates\")\n",
    "print(\"   ‚Ä¢ Watch for overfitting: train vs validation gap\")\n",
    "print(\"   ‚Ä¢ Use stratified CV for imbalanced classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Production Considerations\n",
    "\n",
    "Practical considerations for deploying ensemble models in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model serialization comparison\n",
    "print(\"Model Serialization Comparison\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Train models\n",
    "models_to_serialize = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE),\n",
    "    'XGBoost': xgb.XGBClassifier(n_estimators=100, random_state=RANDOM_STATE, verbosity=0),\n",
    "    'LightGBM': lgb.LGBMClassifier(n_estimators=100, random_state=RANDOM_STATE, verbose=-1)\n",
    "}\n",
    "\n",
    "serialization_results = []\n",
    "\n",
    "for name, model in models_to_serialize.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Test pickle\n",
    "    pickle_path = f'/tmp/model_{name.replace(\" \", \"_\")}.pkl'\n",
    "    start = time.time()\n",
    "    with open(pickle_path, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    pickle_time = time.time() - start\n",
    "    pickle_size = Path(pickle_path).stat().st_size / (1024 * 1024)  # MB\n",
    "    \n",
    "    # Test joblib\n",
    "    joblib_path = f'/tmp/model_{name.replace(\" \", \"_\")}.joblib'\n",
    "    start = time.time()\n",
    "    joblib.dump(model, joblib_path)\n",
    "    joblib_time = time.time() - start\n",
    "    joblib_size = Path(joblib_path).stat().st_size / (1024 * 1024)  # MB\n",
    "    \n",
    "    # Load and inference time\n",
    "    start = time.time()\n",
    "    loaded_model = joblib.load(joblib_path)\n",
    "    _ = loaded_model.predict(X_test)\n",
    "    inference_time = time.time() - start\n",
    "    \n",
    "    serialization_results.append({\n",
    "        'Model': name,\n",
    "        'Pickle Size (MB)': pickle_size,\n",
    "        'Joblib Size (MB)': joblib_size,\n",
    "        'Save Time (s)': joblib_time,\n",
    "        'Load + Inference (s)': inference_time\n",
    "    })\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Pickle: {pickle_size:.2f} MB\")\n",
    "    print(f\"  Joblib: {joblib_size:.2f} MB (recommended)\")\n",
    "    print(f\"  Load + Inference: {inference_time:.4f}s\")\n",
    "    print()\n",
    "\n",
    "serial_df = pd.DataFrame(serialization_results)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Model size comparison\n",
    "ax = axes[0]\n",
    "x = np.arange(len(serial_df))\n",
    "width = 0.35\n",
    "ax.bar(x - width/2, serial_df['Pickle Size (MB)'], width, label='Pickle', alpha=0.8)\n",
    "ax.bar(x + width/2, serial_df['Joblib Size (MB)'], width, label='Joblib', alpha=0.8)\n",
    "ax.set_ylabel('Size (MB)', fontweight='bold')\n",
    "ax.set_title('Serialized Model Size', fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(serial_df['Model'])\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Inference latency\n",
    "ax = axes[1]\n",
    "bars = ax.bar(serial_df['Model'], serial_df['Load + Inference (s)'], \n",
    "             color=plt.cm.viridis(np.linspace(0, 1, len(serial_df))), \n",
    "             edgecolor='black', alpha=0.8)\n",
    "ax.set_ylabel('Time (seconds)', fontweight='bold')\n",
    "ax.set_title('Load + Inference Latency', fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars, serial_df['Load + Inference (s)']):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, val + 0.001, \n",
    "           f'{val:.4f}s', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('production_metrics.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Production Recommendations:\")\n",
    "print(\"  ‚Ä¢ Use joblib for serialization (faster, more efficient than pickle)\")\n",
    "print(\"  ‚Ä¢ LightGBM often has smallest deployment size\")\n",
    "print(\"  ‚Ä¢ Consider model compression for edge deployment\")\n",
    "print(\"  ‚Ä¢ Monitor inference latency in production (p50, p95, p99)\")\n",
    "print(\"  ‚Ä¢ Set up model versioning and A/B testing infrastructure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Production Deployment Checklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Production Deployment Checklist for Ensemble Models\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "checklist = \"\"\"\n",
    "PRE-DEPLOYMENT:\n",
    "‚ñ° Model validation on held-out test set\n",
    "‚ñ° Cross-validation scores documented\n",
    "‚ñ° Feature importance analysis completed\n",
    "‚ñ° Model interpretability requirements met\n",
    "‚ñ° Hyperparameters logged and reproducible\n",
    "‚ñ° Training data versioned\n",
    "‚ñ° Model artifacts saved (joblib recommended)\n",
    "‚ñ° Preprocessing pipeline included with model\n",
    "‚ñ° Input validation logic implemented\n",
    "‚ñ° Output calibration checked (for probabilities)\n",
    "\n",
    "DEPLOYMENT:\n",
    "‚ñ° Inference latency measured (p50, p95, p99)\n",
    "‚ñ° Memory footprint acceptable\n",
    "‚ñ° Batch vs real-time inference decided\n",
    "‚ñ° Model versioning system in place\n",
    "‚ñ° A/B testing framework ready\n",
    "‚ñ° Rollback procedure defined\n",
    "‚ñ° Error handling for edge cases\n",
    "‚ñ° API endpoint designed and documented\n",
    "‚ñ° Load testing completed\n",
    "‚ñ° Logging and monitoring configured\n",
    "\n",
    "POST-DEPLOYMENT:\n",
    "‚ñ° Prediction distribution monitoring\n",
    "‚ñ° Feature drift detection\n",
    "‚ñ° Model performance tracking\n",
    "‚ñ° Retraining schedule defined\n",
    "‚ñ° Alert thresholds set\n",
    "‚ñ° Business metrics tracked\n",
    "‚ñ° Feedback loop for model improvement\n",
    "‚ñ° Documentation updated\n",
    "‚ñ° Team trained on model maintenance\n",
    "‚ñ° Incident response plan ready\n",
    "\"\"\"\n",
    "\n",
    "print(checklist)\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  Common Production Pitfalls:\")\n",
    "print(\"  1. Training-serving skew (different preprocessing)\")\n",
    "print(\"  2. Data drift (features change over time)\")\n",
    "print(\"  3. Missing values handled differently\")\n",
    "print(\"  4. Categorical encoding inconsistencies\")\n",
    "print(\"  5. Model staleness (not retrained regularly)\")\n",
    "print(\"  6. Insufficient monitoring and alerting\")\n",
    "print(\"  7. No rollback plan when model fails\")\n",
    "print(\"  8. Overfitting to cross-validation folds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Best Practices Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Practices for Ensemble Methods\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "best_practices = \"\"\"\n",
    "1. START SIMPLE, INCREASE COMPLEXITY\n",
    "   ‚Ä¢ Baseline: Single decision tree or logistic regression\n",
    "   ‚Ä¢ Next: Random Forest (robust, good defaults)\n",
    "   ‚Ä¢ Then: Gradient boosting if you need more performance\n",
    "   ‚Ä¢ Finally: Stacking/voting for competitions\n",
    "\n",
    "2. ALWAYS BASELINE FIRST\n",
    "   ‚Ä¢ Establish simple baseline performance\n",
    "   ‚Ä¢ Measure improvement from ensemble methods\n",
    "   ‚Ä¢ Justify complexity with clear gains\n",
    "   ‚Ä¢ Document baseline and ensemble comparison\n",
    "\n",
    "3. CROSS-VALIDATE PROPERLY\n",
    "   ‚Ä¢ Use stratified K-fold for classification\n",
    "   ‚Ä¢ Use time-based splits for time series\n",
    "   ‚Ä¢ Report mean and std of CV scores\n",
    "   ‚Ä¢ Watch for train-val gap (overfitting indicator)\n",
    "\n",
    "4. MONITOR FOR OVERFITTING\n",
    "   ‚Ä¢ Track train vs validation performance\n",
    "   ‚Ä¢ Use learning curves to diagnose\n",
    "   ‚Ä¢ Apply early stopping for boosting methods\n",
    "   ‚Ä¢ Regularize (max_depth, min_samples_split, etc.)\n",
    "\n",
    "5. CONSIDER BUSINESS CONSTRAINTS\n",
    "   ‚Ä¢ Interpretability requirements\n",
    "   ‚Ä¢ Inference latency budget\n",
    "   ‚Ä¢ Model size limitations\n",
    "   ‚Ä¢ Training time acceptable\n",
    "   ‚Ä¢ Cost of different error types\n",
    "\n",
    "6. HYPERPARAMETER TUNING STRATEGY\n",
    "   ‚Ä¢ Start with default parameters\n",
    "   ‚Ä¢ Use random search or Bayesian optimization\n",
    "   ‚Ä¢ Tune most important parameters first:\n",
    "     - n_estimators (with early stopping)\n",
    "     - max_depth or num_leaves\n",
    "     - learning_rate (inversely related to n_estimators)\n",
    "   ‚Ä¢ Validate on separate test set\n",
    "\n",
    "7. FEATURE ENGINEERING MATTERS\n",
    "   ‚Ä¢ Good features > complex models\n",
    "   ‚Ä¢ Tree ensembles handle non-linearity well\n",
    "   ‚Ä¢ But they can't create interactions automatically\n",
    "   ‚Ä¢ Feature engineering often more impactful than model choice\n",
    "\n",
    "8. PRODUCTION READINESS\n",
    "   ‚Ä¢ Save preprocessing pipeline with model\n",
    "   ‚Ä¢ Version everything (data, code, models)\n",
    "   ‚Ä¢ Monitor drift and performance\n",
    "   ‚Ä¢ Plan for retraining schedule\n",
    "   ‚Ä¢ Have rollback strategy\n",
    "\n",
    "9. INTERPRETABILITY\n",
    "   ‚Ä¢ Use feature importance for global understanding\n",
    "   ‚Ä¢ Use SHAP for instance-level explanations\n",
    "   ‚Ä¢ Validate that important features make business sense\n",
    "   ‚Ä¢ Consider simpler model if interpretation is critical\n",
    "\n",
    "10. CONTINUOUS IMPROVEMENT\n",
    "    ‚Ä¢ Collect feedback on model predictions\n",
    "    ‚Ä¢ Retrain with new data regularly\n",
    "    ‚Ä¢ A/B test model updates\n",
    "    ‚Ä¢ Keep iterating on features and hyperparameters\n",
    "\"\"\"\n",
    "\n",
    "print(best_practices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Apply what you've learned about comparing and selecting ensemble methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Custom Benchmark on New Dataset\n",
    "\n",
    "Choose a dataset from scikit-learn (or load your own) and run a complete benchmark:\n",
    "1. Load and split the data\n",
    "2. Test at least 5 ensemble methods\n",
    "3. Compare accuracy, speed, and memory\n",
    "4. Create visualizations\n",
    "5. Make a recommendation with justification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Suggested datasets: load_digits(), fetch_covtype(), make_classification()\n",
    "# Use the ModelBenchmark class defined earlier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Method Selection for Specific Scenarios\n",
    "\n",
    "For each scenario below, choose the best ensemble method and explain why:\n",
    "\n",
    "**Scenario A**: Medical diagnosis system\n",
    "- 5,000 patient records\n",
    "- High interpretability required (doctors need to understand)\n",
    "- False negatives very costly\n",
    "\n",
    "**Scenario B**: Ad click prediction\n",
    "- 10 million examples\n",
    "- Inference must be <10ms\n",
    "- Many categorical features (user_id, location, etc.)\n",
    "\n",
    "**Scenario C**: Kaggle competition\n",
    "- Tabular data, 100k rows\n",
    "- Any method allowed\n",
    "- Only metric: AUC score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your analysis here\n",
    "# For each scenario:\n",
    "# 1. List key constraints\n",
    "# 2. Choose method and explain\n",
    "# 3. Suggest hyperparameters\n",
    "# 4. Note potential issues\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Production Optimization Challenge\n",
    "\n",
    "You've deployed an XGBoost model that's too slow for production:\n",
    "- Current: 500ms inference latency\n",
    "- Target: <50ms\n",
    "- Must maintain >95% of current accuracy\n",
    "\n",
    "Explore optimization strategies:\n",
    "1. Reduce number of estimators\n",
    "2. Reduce max depth\n",
    "3. Switch to LightGBM\n",
    "4. Feature selection (fewer features)\n",
    "5. Model compression\n",
    "\n",
    "Implement and compare at least 3 strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# 1. Create baseline slow model\n",
    "# 2. Test optimization strategies\n",
    "# 3. Track latency and accuracy trade-off\n",
    "# 4. Visualize results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Hyperparameter Sensitivity Experiment\n",
    "\n",
    "Design an experiment to test how sensitive different ensemble methods are to suboptimal hyperparameters:\n",
    "\n",
    "1. Choose 3 ensemble methods\n",
    "2. For each, deliberately use bad hyperparameters (too shallow, too few trees, etc.)\n",
    "3. Measure performance degradation vs optimal\n",
    "4. Determine which method is most \"robust\" to poor tuning\n",
    "\n",
    "**Hypothesis**: Some methods (like Random Forest) are less sensitive to hyperparameters than others (like XGBoost)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Create systematic hyperparameter sweep\n",
    "# Test: {very bad, bad, default, good, optimal}\n",
    "# Quantify sensitivity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **No Universal Best Method**: Choice depends on dataset, requirements, and constraints\n",
    "\n",
    "2. **Modern Gradient Boosting Dominates Tabular Data**:\n",
    "   - XGBoost: Most mature, best documentation\n",
    "   - LightGBM: Fastest, most scalable\n",
    "   - CatBoost: Best for categorical features\n",
    "\n",
    "3. **Random Forest: Excellent Default Choice**:\n",
    "   - Robust, less sensitive to hyperparameters\n",
    "   - Good baseline before trying boosting\n",
    "   - Works well on small to medium datasets\n",
    "\n",
    "4. **Trade-offs Matter**:\n",
    "   - Accuracy vs Speed\n",
    "   - Performance vs Interpretability\n",
    "   - Complexity vs Maintainability\n",
    "\n",
    "5. **Production Considerations**:\n",
    "   - Model size and inference latency\n",
    "   - Serialization and versioning\n",
    "   - Monitoring and maintenance\n",
    "   - Business constraints\n",
    "\n",
    "6. **Hyperparameter Sensitivity**:\n",
    "   - Some methods more robust than others\n",
    "   - n_estimators and learning_rate most impactful\n",
    "   - Use automated tuning (Optuna, Hyperopt)\n",
    "\n",
    "7. **Decision Framework**:\n",
    "   - Start simple ‚Üí Random Forest ‚Üí Boosting ‚Üí Stacking\n",
    "   - Match method to dataset characteristics\n",
    "   - Consider business and technical constraints\n",
    "   - Validate thoroughly before deployment\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "**Module 11**: Final Kaggle Competition Project\n",
    "- Apply everything learned in a complete workflow\n",
    "- Titanic dataset (classic benchmark)\n",
    "- Full pipeline: EDA ‚Üí Feature Engineering ‚Üí Model Selection ‚Üí Optimization\n",
    "- Production-ready code and best practices\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- **XGBoost Documentation**: https://xgboost.readthedocs.io/\n",
    "- **LightGBM Documentation**: https://lightgbm.readthedocs.io/\n",
    "- **CatBoost Documentation**: https://catboost.ai/docs/\n",
    "- **SHAP Documentation**: https://shap.readthedocs.io/\n",
    "- **Paper**: \"Gradient Boosting Machines: A Tutorial\" (Natekin & Knoll, 2013)\n",
    "- **Kaggle**: Practice on real competitions to master ensemble methods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
