{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 07: CatBoost\n",
    "\n",
    "**Difficulty**: ‚≠ê‚≠ê‚≠ê Advanced\n",
    "**Estimated Time**: 90 minutes\n",
    "**Prerequisites**: \n",
    "- Module 05: XGBoost\n",
    "- Module 06: LightGBM\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Understand CatBoost's unique algorithms: ordered boosting and symmetric trees\n",
    "2. Leverage CatBoost's superior categorical encoding without preprocessing\n",
    "3. Explain how ordered boosting prevents target leakage\n",
    "4. Optimize CatBoost hyperparameters for best performance\n",
    "5. Use built-in overfitting detection and visualization tools\n",
    "6. Compare XGBoost, LightGBM, and CatBoost systematically\n",
    "7. Apply CatBoost's model analysis tools for interpretation\n",
    "8. Make informed decisions about which gradient boosting library to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.datasets import load_breast_cancer, make_classification\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    roc_auc_score, roc_curve, log_loss\n",
    ")\n",
    "\n",
    "# Gradient boosting libraries for comparison\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    XGB_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    LGB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    LGB_AVAILABLE = False\n",
    "\n",
    "# CatBoost\n",
    "try:\n",
    "    import catboost as cb\n",
    "    from catboost import CatBoostClassifier, CatBoostRegressor, Pool\n",
    "    print(f\"CatBoost version: {cb.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"Installing CatBoost...\")\n",
    "    !pip install catboost -q\n",
    "    import catboost as cb\n",
    "    from catboost import CatBoostClassifier, CatBoostRegressor, Pool\n",
    "    print(f\"CatBoost version: {cb.__version__}\")\n",
    "\n",
    "# SHAP for interpretability\n",
    "try:\n",
    "    import shap\n",
    "    SHAP_AVAILABLE = True\n",
    "    print(f\"SHAP version: {shap.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"SHAP not available. Install with: pip install shap\")\n",
    "    SHAP_AVAILABLE = False\n",
    "\n",
    "# Configuration\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 4)\n",
    "\n",
    "print(\"\\nSetup complete! All libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What Makes CatBoost Special?\n",
    "\n",
    "### CatBoost = Categorical Boosting\n",
    "\n",
    "Developed by Yandex (2017), CatBoost is the newest of the three major gradient boosting libraries.\n",
    "\n",
    "### Key Innovations\n",
    "\n",
    "#### 1.1 Ordered Boosting\n",
    "\n",
    "**Problem with standard gradient boosting**:\n",
    "- Predictions on training samples used to compute gradients\n",
    "- Same samples used to build trees\n",
    "- **Target leakage**: Model sees its own predictions during training\n",
    "- Can cause overfitting\n",
    "\n",
    "**CatBoost's solution: Ordered Boosting**:\n",
    "```\n",
    "Standard boosting:\n",
    "1. Predict on all samples\n",
    "2. Compute gradients\n",
    "3. Build tree using same samples  ‚Üê Leakage!\n",
    "\n",
    "Ordered boosting:\n",
    "1. For each sample i:\n",
    "   - Use only samples before i (j < i) to compute prediction\n",
    "   - No target leakage!\n",
    "2. Compute gradients\n",
    "3. Build tree\n",
    "```\n",
    "\n",
    "**Benefits**:\n",
    "- Prevents target leakage\n",
    "- Better generalization\n",
    "- More robust predictions\n",
    "\n",
    "#### 1.2 Symmetric Trees (Oblivious Trees)\n",
    "\n",
    "**Traditional trees**:\n",
    "- Each node can split on different features\n",
    "- Asymmetric structure\n",
    "\n",
    "**CatBoost symmetric trees**:\n",
    "- Same splitting criterion at each level\n",
    "- All nodes at same level split on same feature and threshold\n",
    "- Balanced binary tree structure\n",
    "\n",
    "```\n",
    "Symmetric tree (depth=3):\n",
    "              [Feature A > 5]\n",
    "             /              \\\n",
    "      [Feature B > 3]   [Feature B > 3]\n",
    "       /        \\         /        \\\n",
    "   [C>1]     [C>1]    [C>1]     [C>1]\n",
    "   /  \\      /  \\     /  \\      /  \\\n",
    "  L0  L1    L2  L3   L4  L5    L6  L7\n",
    "```\n",
    "\n",
    "**Benefits**:\n",
    "- **Fast prediction**: O(depth) instead of O(depth √ó features)\n",
    "- **Less overfitting**: Simpler structure\n",
    "- **Better regularization**: Constrained tree structure\n",
    "- **CPU cache friendly**: Sequential memory access\n",
    "\n",
    "#### 1.3 Superior Categorical Encoding\n",
    "\n",
    "Unlike one-hot encoding or label encoding, CatBoost uses:\n",
    "\n",
    "**Ordered Target Statistics**:\n",
    "- For each category, compute average target value\n",
    "- Use **ordered** scheme to prevent target leakage\n",
    "- For sample i, use only samples j < i with same category\n",
    "- Add prior to prevent overfitting on rare categories\n",
    "\n",
    "**Formula**:\n",
    "$$\\text{TargetStat}(cat) = \\frac{\\sum_{j<i, cat_j=cat} y_j + \\alpha \\cdot P}{\\sum_{j<i, cat_j=cat} 1 + \\alpha}$$\n",
    "\n",
    "Where:\n",
    "- $P$ = prior (overall average)\n",
    "- $\\alpha$ = prior weight (smoothing parameter)\n",
    "\n",
    "**Benefits**:\n",
    "- No preprocessing needed\n",
    "- No one-hot encoding explosion\n",
    "- Handles high-cardinality categories\n",
    "- No target leakage (ordered scheme)\n",
    "- Often better accuracy than manual encoding\n",
    "\n",
    "#### 1.4 Built-in Overfitting Detection\n",
    "\n",
    "CatBoost automatically:\n",
    "- Monitors train vs validation metrics\n",
    "- Detects when overfitting starts\n",
    "- Can visualize overfitting detector\n",
    "- Stops early if overfitting detected\n",
    "\n",
    "### CatBoost vs XGBoost vs LightGBM\n",
    "\n",
    "| Feature | XGBoost | LightGBM | CatBoost |\n",
    "|---------|---------|----------|----------|\n",
    "| Tree growth | Level-wise | Leaf-wise | Level-wise (symmetric) |\n",
    "| Split finding | Exact/Approx | Histogram | Symmetric splits |\n",
    "| Categorical | Manual encoding | Native (basic) | Native (advanced) |\n",
    "| Target leakage | Possible | Possible | Prevented (ordered) |\n",
    "| Speed | Fast | Fastest | Moderate |\n",
    "| Overfitting | Moderate risk | Higher risk | Lower risk |\n",
    "| Default params | Good | Need tuning | Excellent |\n",
    "| Prediction speed | Fast | Fast | Fastest |\n",
    "| Training speed | Fast | Fastest | Slower |\n",
    "| Interpretability | Good | Good | Excellent |\n",
    "| GPU support | Yes | Yes | Yes |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset with categorical features\n",
    "print(\"Creating dataset with categorical features...\")\n",
    "\n",
    "n_samples = 10000\n",
    "\n",
    "# Numeric features\n",
    "X_numeric = np.random.randn(n_samples, 10)\n",
    "\n",
    "# Categorical features with varying cardinality\n",
    "cat_low = np.random.choice(['A', 'B', 'C'], size=n_samples)\n",
    "cat_medium = np.random.choice([f'Cat{i}' for i in range(10)], size=n_samples)\n",
    "cat_high = np.random.choice([f'ID{i}' for i in range(100)], size=n_samples)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(X_numeric, columns=[f'num_{i}' for i in range(10)])\n",
    "df['cat_low_card'] = cat_low\n",
    "df['cat_med_card'] = cat_medium\n",
    "df['cat_high_card'] = cat_high\n",
    "\n",
    "# Create target influenced by both numeric and categorical features\n",
    "target_numeric = X_numeric[:, :3].sum(axis=1)\n",
    "target_cat_low = (cat_low == 'A').astype(float) * 3\n",
    "target_cat_med = np.array([hash(c) % 5 for c in cat_medium]) / 5\n",
    "y_continuous = target_numeric + target_cat_low + target_cat_med + np.random.randn(n_samples)\n",
    "y = (y_continuous > y_continuous.median()).astype(int)\n",
    "\n",
    "print(f\"\\nDataset created:\")\n",
    "print(f\"Samples: {len(df):,}\")\n",
    "print(f\"Numeric features: 10\")\n",
    "print(f\"Categorical features: 3\")\n",
    "print(f\"  - Low cardinality: {df['cat_low_card'].nunique()} categories\")\n",
    "print(f\"  - Medium cardinality: {df['cat_med_card'].nunique()} categories\")\n",
    "print(f\"  - High cardinality: {df['cat_high_card'].nunique()} categories\")\n",
    "print(f\"\\nTarget distribution: {np.bincount(y)}\")\n",
    "print(f\"\\nSample data:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df, y, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Identify categorical columns\n",
    "cat_features = ['cat_low_card', 'cat_med_card', 'cat_high_card']\n",
    "cat_feature_indices = [df.columns.get_loc(col) for col in cat_features]\n",
    "\n",
    "print(f\"Training samples: {len(X_train):,}\")\n",
    "print(f\"Test samples: {len(X_test):,}\")\n",
    "print(f\"Categorical feature indices: {cat_feature_indices}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CatBoost Basics\n",
    "\n",
    "### Using CatBoost with Categorical Features\n",
    "\n",
    "CatBoost makes it incredibly easy - just specify which columns are categorical!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CatBoost with categorical features\n",
    "print(\"Training CatBoost with native categorical support...\\n\")\n",
    "\n",
    "cat_model = CatBoostClassifier(\n",
    "    iterations=100,\n",
    "    learning_rate=0.1,\n",
    "    depth=6,\n",
    "    random_state=RANDOM_STATE,\n",
    "    verbose=20  # Print every 20 iterations\n",
    ")\n",
    "\n",
    "# Fit - just specify cat_features!\n",
    "cat_model.fit(\n",
    "    X_train, y_train,\n",
    "    cat_features=cat_features,\n",
    "    eval_set=(X_test, y_test),\n",
    "    plot=False\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "train_acc = cat_model.score(X_train, y_train)\n",
    "test_acc = cat_model.score(X_test, y_test)\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"Train accuracy: {train_acc:.4f}\")\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "print(f\"Overfitting: {train_acc - test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comparison with Target Encoding\n",
    "\n",
    "Let's compare CatBoost's ordered target statistics with manual target encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual target encoding (naive - causes leakage!)\n",
    "def target_encode(X_train, y_train, X_test, cat_cols):\n",
    "    \"\"\"Simple target encoding (has leakage - for comparison only)\"\"\"\n",
    "    X_train_encoded = X_train.copy()\n",
    "    X_test_encoded = X_test.copy()\n",
    "    \n",
    "    for col in cat_cols:\n",
    "        # Compute mean target for each category\n",
    "        target_means = X_train.groupby(col)[y_train.name].mean()\n",
    "        \n",
    "        # Replace categories with means\n",
    "        X_train_encoded[col] = X_train[col].map(target_means).fillna(y_train.mean())\n",
    "        X_test_encoded[col] = X_test[col].map(target_means).fillna(y_train.mean())\n",
    "    \n",
    "    return X_train_encoded, X_test_encoded\n",
    "\n",
    "# Apply target encoding\n",
    "X_train_copy = X_train.copy()\n",
    "X_test_copy = X_test.copy()\n",
    "y_train_series = pd.Series(y_train, index=X_train.index, name='target')\n",
    "\n",
    "X_train_te, X_test_te = target_encode(\n",
    "    X_train_copy, y_train_series, X_test_copy, cat_features\n",
    ")\n",
    "\n",
    "print(\"Target encoding applied.\")\n",
    "print(f\"\\nExample - cat_low_card mapping:\")\n",
    "print(X_train.groupby('cat_low_card')[y_train_series.name].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CatBoost on target-encoded data (no cat_features specified)\n",
    "print(\"Training CatBoost on target-encoded features...\")\n",
    "\n",
    "cat_model_te = CatBoostClassifier(\n",
    "    iterations=100,\n",
    "    learning_rate=0.1,\n",
    "    depth=6,\n",
    "    random_state=RANDOM_STATE,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "cat_model_te.fit(\n",
    "    X_train_te, y_train,\n",
    "    eval_set=(X_test_te, y_test)\n",
    ")\n",
    "\n",
    "train_acc_te = cat_model_te.score(X_train_te, y_train)\n",
    "test_acc_te = cat_model_te.score(X_test_te, y_test)\n",
    "\n",
    "# Compare\n",
    "comparison = pd.DataFrame([\n",
    "    {\n",
    "        'Method': 'CatBoost Native',\n",
    "        'Train Acc': train_acc,\n",
    "        'Test Acc': test_acc,\n",
    "        'Overfitting': train_acc - test_acc\n",
    "    },\n",
    "    {\n",
    "        'Method': 'Target Encoding',\n",
    "        'Train Acc': train_acc_te,\n",
    "        'Test Acc': test_acc_te,\n",
    "        'Overfitting': train_acc_te - test_acc_te\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CatBoost Native vs Manual Target Encoding\")\n",
    "print(\"=\" * 70)\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "print(\"\\n‚úÖ CatBoost's ordered target statistics:\")\n",
    "print(\"   - Prevents target leakage\")\n",
    "print(\"   - Better generalization\")\n",
    "print(\"   - No manual preprocessing needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. CatBoost Hyperparameters\n",
    "\n",
    "### Core Parameters\n",
    "\n",
    "**`iterations`**: Number of trees (like n_estimators)\n",
    "- Default: 1000\n",
    "- Use early stopping\n",
    "\n",
    "**`learning_rate`**: Shrinkage rate\n",
    "- Default: auto (usually ~0.03)\n",
    "- Lower = need more iterations\n",
    "- Typical: 0.01-0.3\n",
    "\n",
    "**`depth`**: Tree depth\n",
    "- Default: 6\n",
    "- CatBoost trees are symmetric, so depth is very important\n",
    "- Each increase doubles number of leaves: 2^depth leaves\n",
    "- Typical: 4-10\n",
    "\n",
    "**`l2_leaf_reg`**: L2 regularization\n",
    "- Default: 3.0\n",
    "- Higher = more regularization\n",
    "- Typical: 1-10\n",
    "\n",
    "### Advanced Parameters\n",
    "\n",
    "**`border_count`**: Number of splits for numerical features\n",
    "- Default: 254 (like LightGBM's max_bin)\n",
    "- Lower = faster, less accurate\n",
    "- Typical: 32, 64, 128, 254\n",
    "\n",
    "**`bagging_temperature`**: Bayesian bootstrap parameter\n",
    "- Default: 1.0\n",
    "- 0 = no randomness, > 0 = more randomness\n",
    "- Alternative to subsample\n",
    "\n",
    "**`random_strength`**: Amount of randomness for splits\n",
    "- Default: 1.0\n",
    "- Higher = more randomness = less overfitting\n",
    "\n",
    "**`rsm`** (random subspace method): Feature fraction\n",
    "- Default: 1.0\n",
    "- Fraction of features to use per tree\n",
    "- Like colsample_bytree in XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of depth parameter\n",
    "depths = [3, 4, 5, 6, 7, 8, 10]\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "train_times = []\n",
    "\n",
    "for depth in depths:\n",
    "    model = CatBoostClassifier(\n",
    "        iterations=100,\n",
    "        depth=depth,\n",
    "        learning_rate=0.1,\n",
    "        random_state=RANDOM_STATE,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    start = time.time()\n",
    "    model.fit(X_train, y_train, cat_features=cat_features)\n",
    "    train_times.append(time.time() - start)\n",
    "    \n",
    "    train_accs.append(model.score(X_train, y_train))\n",
    "    test_accs.append(model.score(X_test, y_test))\n",
    "\n",
    "# Plot results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy\n",
    "axes[0].plot(depths, train_accs, marker='o', linewidth=2, markersize=8, label='Train')\n",
    "axes[0].plot(depths, test_accs, marker='s', linewidth=2, markersize=8, label='Test')\n",
    "axes[0].axvline(6, color='red', linestyle='--', alpha=0.5, label='Default (6)')\n",
    "axes[0].set_xlabel('Tree Depth', fontsize=12)\n",
    "axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[0].set_title('Effect of Tree Depth', fontsize=13, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Training time\n",
    "axes[1].bar(range(len(depths)), train_times, color='steelblue', edgecolor='black')\n",
    "axes[1].set_xticks(range(len(depths)))\n",
    "axes[1].set_xticklabels(depths)\n",
    "axes[1].set_xlabel('Tree Depth', fontsize=12)\n",
    "axes[1].set_ylabel('Training Time (seconds)', fontsize=12)\n",
    "axes[1].set_title('Training Time vs Depth', fontsize=13, fontweight='bold')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "best_depth = depths[np.argmax(test_accs)]\n",
    "print(f\"\\nOptimal depth: {best_depth}\")\n",
    "print(f\"Test accuracy: {max(test_accs):.4f}\")\n",
    "print(f\"\\nNote: Depth {10} has 2^{10} = {2**10} leaves!\")\n",
    "print(\"Higher depth = more complex model, longer training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of L2 regularization\n",
    "l2_values = [0.1, 1, 3, 5, 10, 20]\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "\n",
    "for l2 in l2_values:\n",
    "    model = CatBoostClassifier(\n",
    "        iterations=100,\n",
    "        depth=6,\n",
    "        l2_leaf_reg=l2,\n",
    "        learning_rate=0.1,\n",
    "        random_state=RANDOM_STATE,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train, cat_features=cat_features)\n",
    "    train_accs.append(model.score(X_train, y_train))\n",
    "    test_accs.append(model.score(X_test, y_test))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(l2_values, train_accs, marker='o', linewidth=2, markersize=8, label='Train')\n",
    "plt.plot(l2_values, test_accs, marker='s', linewidth=2, markersize=8, label='Test')\n",
    "plt.axvline(3, color='red', linestyle='--', alpha=0.5, label='Default (3)')\n",
    "plt.xlabel('L2 Regularization (l2_leaf_reg)', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('Effect of L2 Regularization', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservation:\")\n",
    "print(\"Higher L2 regularization reduces overfitting but may underfit.\")\n",
    "print(\"Default value of 3 is often a good starting point.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Built-in Cross-Validation\n",
    "\n",
    "CatBoost has excellent built-in cross-validation with the `cv` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data as Pool object\n",
    "train_pool = Pool(\n",
    "    data=X_train,\n",
    "    label=y_train,\n",
    "    cat_features=cat_features\n",
    ")\n",
    "\n",
    "# Set parameters\n",
    "params = {\n",
    "    'iterations': 200,\n",
    "    'depth': 6,\n",
    "    'learning_rate': 0.05,\n",
    "    'loss_function': 'Logloss',\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'verbose': False\n",
    "}\n",
    "\n",
    "# Run cross-validation\n",
    "print(\"Running 5-fold cross-validation...\\n\")\n",
    "cv_results = cb.cv(\n",
    "    pool=train_pool,\n",
    "    params=params,\n",
    "    fold_count=5,\n",
    "    stratified=True,\n",
    "    partition_random_seed=RANDOM_STATE,\n",
    "    plot=False,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"Cross-Validation Results:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Best iteration: {cv_results['test-Logloss-mean'].idxmin()}\")\n",
    "print(f\"Best CV Logloss: {cv_results['test-Logloss-mean'].min():.4f}\")\n",
    "print(f\"Std deviation: {cv_results.loc[cv_results['test-Logloss-mean'].idxmin(), 'test-Logloss-std']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize CV results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(cv_results['test-Logloss-mean'], label='CV Mean', linewidth=2)\n",
    "plt.fill_between(\n",
    "    range(len(cv_results)),\n",
    "    cv_results['test-Logloss-mean'] - cv_results['test-Logloss-std'],\n",
    "    cv_results['test-Logloss-mean'] + cv_results['test-Logloss-std'],\n",
    "    alpha=0.2,\n",
    "    label='CV Std'\n",
    ")\n",
    "best_iter = cv_results['test-Logloss-mean'].idxmin()\n",
    "plt.axvline(best_iter, color='red', linestyle='--', linewidth=2,\n",
    "            label=f'Best iteration ({best_iter})')\n",
    "plt.xlabel('Iteration', fontsize=12)\n",
    "plt.ylabel('Logloss', fontsize=12)\n",
    "plt.title('CatBoost Cross-Validation', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nBuilt-in CV is fast and easy!\")\n",
    "print(\"Use to find optimal number of iterations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Overfitting Detection\n",
    "\n",
    "CatBoost has built-in overfitting detection that monitors the difference between train and validation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for validation\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Train with overfitting detector\n",
    "model_overfit = CatBoostClassifier(\n",
    "    iterations=500,\n",
    "    depth=8,  # Deep trees to demonstrate overfitting\n",
    "    learning_rate=0.1,\n",
    "    random_state=RANDOM_STATE,\n",
    "    od_type='Iter',  # Overfitting detection type\n",
    "    od_wait=20,      # Wait 20 iterations before stopping\n",
    "    verbose=50\n",
    ")\n",
    "\n",
    "print(\"Training with overfitting detection...\\n\")\n",
    "model_overfit.fit(\n",
    "    X_train_split, y_train_split,\n",
    "    cat_features=cat_features,\n",
    "    eval_set=(X_val, y_val),\n",
    "    plot=False\n",
    ")\n",
    "\n",
    "print(f\"\\nBest iteration: {model_overfit.get_best_iteration()}\")\n",
    "print(f\"Stopped at iteration: {model_overfit.tree_count_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training progress\n",
    "train_logloss = model_overfit.evals_result_['learn']['Logloss']\n",
    "val_logloss = model_overfit.evals_result_['validation']['Logloss']\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train_logloss, label='Train', linewidth=2)\n",
    "plt.plot(val_logloss, label='Validation', linewidth=2)\n",
    "best_iter = model_overfit.get_best_iteration()\n",
    "plt.axvline(best_iter, color='red', linestyle='--', linewidth=2,\n",
    "            label=f'Best iteration ({best_iter})')\n",
    "plt.xlabel('Iteration', fontsize=12)\n",
    "plt.ylabel('Logloss', fontsize=12)\n",
    "plt.title('Training with Overfitting Detection', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Overfitting detector automatically found optimal stopping point!\")\n",
    "print(\"No need to manually monitor validation metrics.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Three-Way Comparison: XGBoost vs LightGBM vs CatBoost\n",
    "\n",
    "Let's systematically compare all three libraries on the same dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load breast cancer dataset for fair comparison\n",
    "cancer_data = load_breast_cancer()\n",
    "X_cancer, y_cancer = cancer_data.data, cancer_data.target\n",
    "\n",
    "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(\n",
    "    X_cancer, y_cancer, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"Dataset: {len(X_train_c)} train, {len(X_test_c)} test, {X_cancer.shape[1]} features\")\n",
    "\n",
    "# Common parameters (as similar as possible)\n",
    "n_trees = 100\n",
    "max_depth = 6\n",
    "lr = 0.1\n",
    "\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. CatBoost\n",
    "print(\"Training CatBoost...\")\n",
    "start = time.time()\n",
    "cat_comp = CatBoostClassifier(\n",
    "    iterations=n_trees,\n",
    "    depth=max_depth,\n",
    "    learning_rate=lr,\n",
    "    random_state=RANDOM_STATE,\n",
    "    verbose=False\n",
    ")\n",
    "cat_comp.fit(X_train_c, y_train_c)\n",
    "cat_time = time.time() - start\n",
    "\n",
    "start_pred = time.time()\n",
    "cat_pred = cat_comp.predict(X_test_c)\n",
    "cat_pred_time = time.time() - start_pred\n",
    "\n",
    "cat_acc = accuracy_score(y_test_c, cat_pred)\n",
    "cat_auc = roc_auc_score(y_test_c, cat_comp.predict_proba(X_test_c)[:, 1])\n",
    "\n",
    "results.append({\n",
    "    'Model': 'CatBoost',\n",
    "    'Train Time': cat_time,\n",
    "    'Pred Time': cat_pred_time,\n",
    "    'Accuracy': cat_acc,\n",
    "    'AUC': cat_auc\n",
    "})\n",
    "\n",
    "print(f\"  Time: {cat_time:.3f}s, Accuracy: {cat_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. XGBoost\n",
    "if XGB_AVAILABLE:\n",
    "    print(\"Training XGBoost...\")\n",
    "    start = time.time()\n",
    "    xgb_comp = xgb.XGBClassifier(\n",
    "        n_estimators=n_trees,\n",
    "        max_depth=max_depth,\n",
    "        learning_rate=lr,\n",
    "        random_state=RANDOM_STATE,\n",
    "        eval_metric='logloss',\n",
    "        verbosity=0\n",
    "    )\n",
    "    xgb_comp.fit(X_train_c, y_train_c)\n",
    "    xgb_time = time.time() - start\n",
    "    \n",
    "    start_pred = time.time()\n",
    "    xgb_pred = xgb_comp.predict(X_test_c)\n",
    "    xgb_pred_time = time.time() - start_pred\n",
    "    \n",
    "    xgb_acc = accuracy_score(y_test_c, xgb_pred)\n",
    "    xgb_auc = roc_auc_score(y_test_c, xgb_comp.predict_proba(X_test_c)[:, 1])\n",
    "    \n",
    "    results.append({\n",
    "        'Model': 'XGBoost',\n",
    "        'Train Time': xgb_time,\n",
    "        'Pred Time': xgb_pred_time,\n",
    "        'Accuracy': xgb_acc,\n",
    "        'AUC': xgb_auc\n",
    "    })\n",
    "    \n",
    "    print(f\"  Time: {xgb_time:.3f}s, Accuracy: {xgb_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. LightGBM\n",
    "if LGB_AVAILABLE:\n",
    "    print(\"Training LightGBM...\")\n",
    "    start = time.time()\n",
    "    lgb_comp = lgb.LGBMClassifier(\n",
    "        n_estimators=n_trees,\n",
    "        max_depth=max_depth,\n",
    "        learning_rate=lr,\n",
    "        num_leaves=2**max_depth - 1,  # Approximate equivalent\n",
    "        random_state=RANDOM_STATE,\n",
    "        verbose=-1\n",
    "    )\n",
    "    lgb_comp.fit(X_train_c, y_train_c)\n",
    "    lgb_time = time.time() - start\n",
    "    \n",
    "    start_pred = time.time()\n",
    "    lgb_pred = lgb_comp.predict(X_test_c)\n",
    "    lgb_pred_time = time.time() - start_pred\n",
    "    \n",
    "    lgb_acc = accuracy_score(y_test_c, lgb_pred)\n",
    "    lgb_auc = roc_auc_score(y_test_c, lgb_comp.predict_proba(X_test_c)[:, 1])\n",
    "    \n",
    "    results.append({\n",
    "        'Model': 'LightGBM',\n",
    "        'Train Time': lgb_time,\n",
    "        'Pred Time': lgb_pred_time,\n",
    "        'Accuracy': lgb_acc,\n",
    "        'AUC': lgb_auc\n",
    "    })\n",
    "    \n",
    "    print(f\"  Time: {lgb_time:.3f}s, Accuracy: {lgb_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display comparison\n",
    "df_comparison = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"XGBoost vs LightGBM vs CatBoost - Comprehensive Comparison\")\n",
    "print(\"=\" * 80)\n",
    "print(df_comparison.to_string(index=False))\n",
    "\n",
    "# Find best in each category\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Winners:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Fastest training: {df_comparison.loc[df_comparison['Train Time'].idxmin(), 'Model']}\")\n",
    "print(f\"Fastest prediction: {df_comparison.loc[df_comparison['Pred Time'].idxmin(), 'Model']}\")\n",
    "print(f\"Best accuracy: {df_comparison.loc[df_comparison['Accuracy'].idxmax(), 'Model']}\")\n",
    "print(f\"Best AUC: {df_comparison.loc[df_comparison['AUC'].idxmax(), 'Model']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "if len(results) > 1:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Training time\n",
    "    axes[0, 0].bar(df_comparison['Model'], df_comparison['Train Time'], \n",
    "                   color=['#f39c12', '#e74c3c', '#2ecc71'], edgecolor='black')\n",
    "    axes[0, 0].set_ylabel('Time (seconds)', fontsize=11)\n",
    "    axes[0, 0].set_title('Training Time', fontsize=12, fontweight='bold')\n",
    "    axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Prediction time\n",
    "    axes[0, 1].bar(df_comparison['Model'], df_comparison['Pred Time'] * 1000,  # Convert to ms\n",
    "                   color=['#f39c12', '#e74c3c', '#2ecc71'], edgecolor='black')\n",
    "    axes[0, 1].set_ylabel('Time (milliseconds)', fontsize=11)\n",
    "    axes[0, 1].set_title('Prediction Time', fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Accuracy\n",
    "    axes[1, 0].bar(df_comparison['Model'], df_comparison['Accuracy'], \n",
    "                   color=['#f39c12', '#e74c3c', '#2ecc71'], edgecolor='black')\n",
    "    axes[1, 0].set_ylabel('Accuracy', fontsize=11)\n",
    "    axes[1, 0].set_title('Test Accuracy', fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].set_ylim([0.9, 1.0])\n",
    "    axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # AUC\n",
    "    axes[1, 1].bar(df_comparison['Model'], df_comparison['AUC'], \n",
    "                   color=['#f39c12', '#e74c3c', '#2ecc71'], edgecolor='black')\n",
    "    axes[1, 1].set_ylabel('AUC-ROC', fontsize=11)\n",
    "    axes[1, 1].set_title('Test AUC-ROC', fontsize=12, fontweight='bold')\n",
    "    axes[1, 1].set_ylim([0.9, 1.0])\n",
    "    axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nüìä All three libraries are excellent!\")\n",
    "print(\"Choice depends on your specific use case and priorities.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. CatBoost Model Analysis Tools\n",
    "\n",
    "CatBoost provides excellent tools for model interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "feature_importance = cat_comp.get_feature_importance()\n",
    "feature_names_cancer = cancer_data.feature_names\n",
    "\n",
    "fi_df = pd.DataFrame({\n",
    "    'Feature': feature_names_cancer,\n",
    "    'Importance': feature_importance\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"Top 10 Features:\")\n",
    "print(\"=\" * 70)\n",
    "print(fi_df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "top_features = fi_df.head(15)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(range(len(top_features)), top_features['Importance'], \n",
    "         color='#f39c12', edgecolor='black')\n",
    "plt.yticks(range(len(top_features)), top_features['Feature'])\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xlabel('Importance', fontsize=12)\n",
    "plt.title('CatBoost Feature Importance (Top 15)', fontsize=14, fontweight='bold')\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP values with CatBoost\n",
    "if SHAP_AVAILABLE:\n",
    "    print(\"Computing SHAP values for CatBoost...\")\n",
    "    explainer = shap.TreeExplainer(cat_comp)\n",
    "    X_test_sample = X_test_c[:100]\n",
    "    shap_values = explainer.shap_values(X_test_sample)\n",
    "    \n",
    "    print(f\"SHAP values computed! Shape: {shap_values.shape}\")\n",
    "else:\n",
    "    print(\"SHAP not available. Install with: pip install shap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHAP_AVAILABLE:\n",
    "    # Global feature importance\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    shap.summary_plot(shap_values, X_test_sample, \n",
    "                     feature_names=feature_names_cancer,\n",
    "                     plot_type='bar', show=False)\n",
    "    plt.title('CatBoost - Global Feature Importance (SHAP)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHAP_AVAILABLE:\n",
    "    # Detailed SHAP summary\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    shap.summary_plot(shap_values, X_test_sample,\n",
    "                     feature_names=feature_names_cancer, show=False)\n",
    "    plt.title('CatBoost - SHAP Summary Plot', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nCatBoost integrates seamlessly with SHAP!\")\n",
    "    print(\"Use for production model interpretation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Hyperparameter Tuning Comparison\n",
    "\n",
    "Compare hyperparameter tuning for all three libraries:\n",
    "\n",
    "1. Create a classification dataset\n",
    "2. For each library (XGBoost, LightGBM, CatBoost):\n",
    "   - Define parameter grid with equivalent parameters\n",
    "   - Use RandomizedSearchCV or GridSearchCV\n",
    "   - Find optimal parameters\n",
    "   - Measure tuning time\n",
    "3. Compare:\n",
    "   - Best cross-validation scores\n",
    "   - Time to find best parameters\n",
    "   - Sensitivity to hyperparameters\n",
    "4. Determine which library is most sensitive to tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Categorical Feature Handling Showdown\n",
    "\n",
    "Create comprehensive test of categorical handling:\n",
    "\n",
    "1. Generate dataset with:\n",
    "   - 3 low-cardinality categoricals (3-5 values)\n",
    "   - 3 medium-cardinality (20-50 values)\n",
    "   - 2 high-cardinality (100+ values)\n",
    "   - 10 numeric features\n",
    "2. Test encoding strategies:\n",
    "   - CatBoost native\n",
    "   - LightGBM native\n",
    "   - XGBoost + one-hot encoding\n",
    "   - XGBoost + target encoding\n",
    "   - XGBoost + label encoding\n",
    "3. Measure for each:\n",
    "   - Accuracy\n",
    "   - Training time\n",
    "   - Memory usage\n",
    "   - Code complexity (lines of preprocessing)\n",
    "4. Determine clear winner for categorical handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Ordered Boosting Impact Analysis\n",
    "\n",
    "Test CatBoost's ordered boosting claim:\n",
    "\n",
    "1. Create small dataset prone to overfitting (2000 samples, many features)\n",
    "2. Train CatBoost with different boosting types:\n",
    "   - `boosting_type='Ordered'` (default)\n",
    "   - `boosting_type='Plain'` (like standard gradient boosting)\n",
    "3. For each:\n",
    "   - Train multiple times with different train/val splits\n",
    "   - Measure train vs validation gap\n",
    "   - Test generalization on holdout set\n",
    "4. Compare:\n",
    "   - Overfitting degree\n",
    "   - Stability across runs\n",
    "   - Final test performance\n",
    "5. Determine if ordered boosting provides measurable benefit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Symmetric Tree Analysis\n",
    "\n",
    "Investigate CatBoost's symmetric tree structure:\n",
    "\n",
    "1. Train CatBoost model and save tree structure\n",
    "2. Use `get_feature_statistics()` to analyze tree properties\n",
    "3. Compare with XGBoost/LightGBM trees:\n",
    "   - Average tree depth\n",
    "   - Number of splits\n",
    "   - Prediction time\n",
    "4. Test prediction speed scaling:\n",
    "   - Measure prediction time vs tree depth\n",
    "   - Compare CatBoost vs others\n",
    "5. Verify that symmetric trees are faster for prediction\n",
    "\n",
    "**Hint**: Use CatBoost's `calc_feature_statistics()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Production Deployment Comparison\n",
    "\n",
    "Simulate production deployment scenario:\n",
    "\n",
    "1. Train all three models (XGBoost, LightGBM, CatBoost) on realistic dataset\n",
    "2. Save models to disk (each library's format)\n",
    "3. Measure:\n",
    "   - Model file size\n",
    "   - Load time from disk\n",
    "   - Prediction latency (single sample)\n",
    "   - Prediction throughput (batch of 10,000)\n",
    "   - Memory footprint during prediction\n",
    "4. Test deployment features:\n",
    "   - Model versioning\n",
    "   - Feature importance extraction\n",
    "   - Prediction explanation (SHAP)\n",
    "5. Create deployment recommendation matrix:\n",
    "   - When to use each library in production\n",
    "   - Trade-offs (speed vs accuracy vs ease of use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **CatBoost Innovations**:\n",
    "   - **Ordered boosting**: Prevents target leakage, better generalization\n",
    "   - **Symmetric trees**: Faster prediction, less overfitting, cache-friendly\n",
    "   - **Superior categorical encoding**: Ordered target statistics, no preprocessing\n",
    "   - **Built-in overfitting detection**: Automatic monitoring and early stopping\n",
    "\n",
    "2. **Critical Hyperparameters**:\n",
    "   - `iterations`: Number of trees (use early stopping)\n",
    "   - `depth`: Tree depth, critical due to symmetric structure (4-10)\n",
    "   - `learning_rate`: Shrinkage rate (0.01-0.3)\n",
    "   - `l2_leaf_reg`: L2 regularization (1-10)\n",
    "   - `border_count`: Number of splits (32-254)\n",
    "   - `bagging_temperature`: Bayesian bootstrap randomness\n",
    "\n",
    "3. **Best Practices**:\n",
    "   - Use categorical features natively - don't encode!\n",
    "   - Start with default parameters (often excellent)\n",
    "   - Use built-in cross-validation for hyperparameter search\n",
    "   - Enable overfitting detection (`od_type='Iter'`)\n",
    "   - Lower learning rate + more iterations = better results\n",
    "   - Monitor train/validation gap\n",
    "\n",
    "4. **Advantages**:\n",
    "   - **Best categorical handling**: No preprocessing, excellent accuracy\n",
    "   - **Excellent defaults**: Works well out-of-the-box\n",
    "   - **Less overfitting**: Ordered boosting and symmetric trees\n",
    "   - **Fast prediction**: Symmetric tree structure\n",
    "   - **Great interpretability**: Built-in tools and SHAP support\n",
    "   - **Robust**: Less sensitive to hyperparameters\n",
    "\n",
    "5. **When to Use CatBoost**:\n",
    "   - ‚úÖ Many categorical features\n",
    "   - ‚úÖ Need robust default parameters\n",
    "   - ‚úÖ Want less overfitting risk\n",
    "   - ‚úÖ Need fast prediction in production\n",
    "   - ‚úÖ Limited time for hyperparameter tuning\n",
    "   - ‚úÖ Interpretability is important\n",
    "   - ‚ö†Ô∏è Training speed critical: Consider LightGBM\n",
    "   - ‚ö†Ô∏è Very large datasets: LightGBM may be faster\n",
    "\n",
    "### Strengths\n",
    "\n",
    "- Superior categorical feature handling\n",
    "- Excellent default parameters\n",
    "- Robust to overfitting\n",
    "- Fast prediction speed\n",
    "- Great interpretability tools\n",
    "- Built-in overfitting detection\n",
    "- Easy to use\n",
    "- Good documentation\n",
    "\n",
    "### Weaknesses\n",
    "\n",
    "- Slower training than LightGBM\n",
    "- Symmetric trees may limit expressiveness\n",
    "- Newer library (smaller community than XGBoost)\n",
    "- GPU support less mature than others\n",
    "- Less control over tree structure\n",
    "\n",
    "### XGBoost vs LightGBM vs CatBoost: Decision Guide\n",
    "\n",
    "**Choose XGBoost when**:\n",
    "- Need maximum stability and maturity\n",
    "- Large community and resources important\n",
    "- Numeric features only\n",
    "- Want fine-grained control\n",
    "- Proven track record matters\n",
    "\n",
    "**Choose LightGBM when**:\n",
    "- Training speed is critical\n",
    "- Very large datasets (100K+ samples)\n",
    "- High-dimensional sparse data\n",
    "- Limited memory\n",
    "- Some categorical features\n",
    "\n",
    "**Choose CatBoost when**:\n",
    "- Many categorical features\n",
    "- Limited tuning time (good defaults)\n",
    "- Prediction speed matters\n",
    "- Need robust out-of-box performance\n",
    "- Want less overfitting\n",
    "- Interpretability important\n",
    "\n",
    "**Truth**: All three are excellent! Try all and pick what works best.\n",
    "\n",
    "### Comparison Summary\n",
    "\n",
    "| Aspect | XGBoost | LightGBM | CatBoost |\n",
    "|--------|---------|----------|----------|\n",
    "| **Training Speed** | Fast | Fastest | Moderate |\n",
    "| **Prediction Speed** | Fast | Fast | Fastest |\n",
    "| **Accuracy** | Excellent | Excellent | Excellent |\n",
    "| **Categorical Handling** | Manual | Good | Best |\n",
    "| **Default Parameters** | Good | Need tuning | Excellent |\n",
    "| **Overfitting Risk** | Moderate | Higher | Lower |\n",
    "| **Large Datasets** | Good | Best | Good |\n",
    "| **Interpretability** | Good | Good | Excellent |\n",
    "| **Ease of Use** | Moderate | Moderate | Easiest |\n",
    "| **Maturity** | Most mature | Mature | Newer |\n",
    "| **Community** | Largest | Large | Growing |\n",
    "\n",
    "### Tuning Strategy\n",
    "\n",
    "**Phase 1: Start simple**\n",
    "1. Use default parameters first\n",
    "2. Enable overfitting detection\n",
    "3. Use built-in CV\n",
    "\n",
    "**Phase 2: Adjust complexity**\n",
    "4. Tune `depth` (start 6, try 4-10)\n",
    "5. Adjust `l2_leaf_reg` if overfitting\n",
    "\n",
    "**Phase 3: Optimize learning**\n",
    "6. Lower `learning_rate` (0.03-0.05)\n",
    "7. Increase `iterations` accordingly\n",
    "8. Use early stopping\n",
    "\n",
    "**Phase 4: Fine-tune**\n",
    "9. Adjust `border_count` for speed/accuracy\n",
    "10. Try `bagging_temperature` for regularization\n",
    "\n",
    "### Production Tips\n",
    "\n",
    "1. **Model persistence**: Use `save_model()` / `load_model()`\n",
    "2. **Categorical features**: Save category mappings\n",
    "3. **Versioning**: Include hyperparameters in model metadata\n",
    "4. **Monitoring**: Track feature importance over time\n",
    "5. **Prediction speed**: Leverage symmetric trees\n",
    "6. **A/B testing**: Compare model versions\n",
    "7. **Retraining**: Automated pipeline with overfitting detection\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In **Module 08: Stacking and Blending**, we'll explore:\n",
    "- Meta-learning and ensemble of ensembles\n",
    "- Combining XGBoost, LightGBM, and CatBoost\n",
    "- Multi-level stacking\n",
    "- Blending vs stacking differences\n",
    "- Choosing optimal meta-model\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- **Paper**: \"CatBoost: unbiased boosting with categorical features\" (Prokhorenkova et al., 2018)\n",
    "- **Documentation**: [CatBoost Official Docs](https://catboost.ai/docs/)\n",
    "- **Tutorial**: [CatBoost Tutorial](https://github.com/catboost/tutorials)\n",
    "- **Comparison**: [Benchmarking Gradient Boosting Libraries](https://catboost.ai/docs/concepts/benchmarks.html)\n",
    "- **GitHub**: [Yandex CatBoost](https://github.com/catboost/catboost)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
