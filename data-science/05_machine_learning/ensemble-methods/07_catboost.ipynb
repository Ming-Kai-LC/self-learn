{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 07: CatBoost - Categorical Boosting\n",
    "\n",
    "**Difficulty**: ⭐⭐\n",
    "**Estimated Time**: 50 minutes\n",
    "**Prerequisites**: \n",
    "- Module 04: Gradient Boosting Machines\n",
    "- Module 05: XGBoost\n",
    "- Module 06: LightGBM\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Understand CatBoost's unique features: ordered boosting and symmetric trees\n",
    "2. Install and use CatBoost for classification and regression\n",
    "3. Handle categorical features natively without any preprocessing\n",
    "4. Compare CatBoost with XGBoost and LightGBM in terms of performance and accuracy\n",
    "5. Leverage GPU acceleration for faster training\n",
    "6. Tune CatBoost-specific hyperparameters effectively\n",
    "7. Understand when to choose CatBoost over other gradient boosting libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to CatBoost\n",
    "\n",
    "### What is CatBoost?\n",
    "\n",
    "**CatBoost** (Categorical Boosting) is a gradient boosting library developed by Yandex in 2017. It's designed to:\n",
    "- Handle **categorical features optimally** without preprocessing\n",
    "- Reduce **overfitting** through ordered boosting\n",
    "- Provide **great performance out-of-the-box** with minimal tuning\n",
    "- Support **GPU acceleration** efficiently\n",
    "\n",
    "### Key Innovations:\n",
    "\n",
    "#### 1. **Ordered Boosting**\n",
    "\n",
    "**Problem**: Standard gradient boosting suffers from prediction shift:\n",
    "- Training uses the same data to calculate gradients and fit trees\n",
    "- This leads to overfitting and biased gradient estimates\n",
    "\n",
    "**CatBoost Solution**: Ordered Boosting\n",
    "- Uses different random permutations of the training data\n",
    "- For each sample, uses only \"previous\" samples (in permutation order) to compute gradients\n",
    "- Eliminates prediction shift and reduces overfitting\n",
    "- Makes the model more robust\n",
    "\n",
    "#### 2. **Ordered Target Statistics for Categorical Features**\n",
    "\n",
    "**Problem with naive encoding**:\n",
    "- Target encoding (mean target per category) causes target leakage\n",
    "- Label encoding loses information about feature importance\n",
    "- One-hot encoding creates too many features for high-cardinality categories\n",
    "\n",
    "**CatBoost Solution**: Ordered Target Statistics\n",
    "- For each sample, calculates target statistics using only \"prior\" samples\n",
    "- Adds random noise for regularization\n",
    "- No target leakage, better generalization\n",
    "\n",
    "Example:\n",
    "```\n",
    "Data (in random order):\n",
    "Category  Target\n",
    "   A        1\n",
    "   B        0\n",
    "   A        1     <- Encoding for this A uses only previous A (one sample, target=1)\n",
    "   B        1     <- Encoding for this B uses only previous B (one sample, target=0)\n",
    "   A        0     <- Encoding uses previous A's (targets: 1, 1)\n",
    "```\n",
    "\n",
    "#### 3. **Symmetric (Oblivious) Decision Trees**\n",
    "\n",
    "**Structure**:\n",
    "- All nodes at the same level use the **same splitting criterion**\n",
    "- Results in a **balanced tree** structure\n",
    "- Each leaf can be indexed by a binary code\n",
    "\n",
    "**Advantages**:\n",
    "- **Faster prediction**: Can use binary search or lookup table\n",
    "- **Less overfitting**: More regularized than asymmetric trees\n",
    "- **Better for CPU cache**: Predictable access patterns\n",
    "\n",
    "```\n",
    "Regular Tree:              Symmetric Tree:\n",
    "      X1<5                      X1<5\n",
    "     /    \\                    /    \\\n",
    "  X2<3   X3<7              X2<3      X2<3\n",
    "  /  \\    /  \\             /  \\      /  \\\n",
    " L1  L2  L3  L4          L1  L2    L3  L4\n",
    "\n",
    "Same split (X2<3) at both nodes in level 2\n",
    "```\n",
    "\n",
    "### CatBoost vs XGBoost vs LightGBM:\n",
    "\n",
    "| Feature | XGBoost | LightGBM | CatBoost |\n",
    "|---------|---------|----------|----------|\n",
    "| Speed | Fast | **Faster** | Moderate |\n",
    "| Categorical Support | Manual | Good | **Best** |\n",
    "| Overfitting Resistance | Good | Moderate | **Best** |\n",
    "| Default Performance | Good | Good | **Best** |\n",
    "| Hyperparameter Tuning | Needs tuning | Needs tuning | **Works well with defaults** |\n",
    "| Tree Structure | Level-wise | Leaf-wise | **Symmetric** |\n",
    "| Prediction Speed | Fast | Fast | **Fastest** |\n",
    "| GPU Support | Yes | Yes | **Yes (better)** |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install CatBoost if not already installed\n",
    "# Uncomment the line below if needed\n",
    "# !pip install catboost\n",
    "\n",
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from time import time\n",
    "\n",
    "# CatBoost\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor, Pool\n",
    "\n",
    "# Other boosting libraries for comparison\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.datasets import make_classification, make_regression, load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    mean_squared_error, r2_score, log_loss\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check version\n",
    "import catboost\n",
    "print(f\"CatBoost version: {catboost.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CatBoost for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load breast cancer dataset\n",
    "cancer_data = load_breast_cancer()\n",
    "X = cancer_data.data\n",
    "y = cancer_data.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"Features: {len(cancer_data.feature_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a basic CatBoost classifier\n",
    "cat_clf = CatBoostClassifier(\n",
    "    iterations=100,\n",
    "    learning_rate=0.1,\n",
    "    depth=3,\n",
    "    random_state=42,\n",
    "    verbose=False  # Set to True to see training progress\n",
    ")\n",
    "\n",
    "# Time the training\n",
    "start_time = time()\n",
    "cat_clf.fit(X_train, y_train)\n",
    "cat_time = time() - start_time\n",
    "\n",
    "# Predictions\n",
    "y_pred_train = cat_clf.predict(X_train)\n",
    "y_pred_test = cat_clf.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "train_acc = accuracy_score(y_train, y_pred_train)\n",
    "test_acc = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"CatBoost Classifier Performance:\")\n",
    "print(f\"Training time: {cat_time:.4f} seconds\")\n",
    "print(f\"Train Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_test, target_names=cancer_data.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Purples',\n",
    "            xticklabels=cancer_data.target_names,\n",
    "            yticklabels=cancer_data.target_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('CatBoost Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparison: CatBoost vs XGBoost vs LightGBM\n",
    "\n",
    "Let's compare all three libraries on the same dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a larger dataset for better comparison\n",
    "X_large, y_large = make_classification(\n",
    "    n_samples=30000,\n",
    "    n_features=30,\n",
    "    n_informative=20,\n",
    "    n_redundant=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train_large, X_test_large, y_train_large, y_test_large = train_test_split(\n",
    "    X_large, y_large, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Dataset size - Train: {X_train_large.shape}, Test: {X_test_large.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common parameters for fair comparison\n",
    "n_estimators = 100\n",
    "learning_rate = 0.1\n",
    "max_depth = 5\n",
    "\n",
    "results = {}\n",
    "\n",
    "# 1. CatBoost\n",
    "print(\"Training CatBoost...\")\n",
    "start = time()\n",
    "cat_model = CatBoostClassifier(\n",
    "    iterations=n_estimators,\n",
    "    learning_rate=learning_rate,\n",
    "    depth=max_depth,\n",
    "    random_state=42,\n",
    "    verbose=False\n",
    ")\n",
    "cat_model.fit(X_train_large, y_train_large)\n",
    "cat_train_time = time() - start\n",
    "cat_acc = cat_model.score(X_test_large, y_test_large)\n",
    "results['CatBoost'] = {'time': cat_train_time, 'accuracy': cat_acc}\n",
    "print(f\"  Time: {cat_train_time:.2f}s, Accuracy: {cat_acc:.4f}\")\n",
    "\n",
    "# 2. XGBoost\n",
    "print(\"\\nTraining XGBoost...\")\n",
    "start = time()\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=n_estimators,\n",
    "    learning_rate=learning_rate,\n",
    "    max_depth=max_depth,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss',\n",
    "    verbosity=0\n",
    ")\n",
    "xgb_model.fit(X_train_large, y_train_large)\n",
    "xgb_train_time = time() - start\n",
    "xgb_acc = xgb_model.score(X_test_large, y_test_large)\n",
    "results['XGBoost'] = {'time': xgb_train_time, 'accuracy': xgb_acc}\n",
    "print(f\"  Time: {xgb_train_time:.2f}s, Accuracy: {xgb_acc:.4f}\")\n",
    "\n",
    "# 3. LightGBM\n",
    "print(\"\\nTraining LightGBM...\")\n",
    "start = time()\n",
    "lgb_model = LGBMClassifier(\n",
    "    n_estimators=n_estimators,\n",
    "    learning_rate=learning_rate,\n",
    "    max_depth=max_depth,\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "lgb_model.fit(X_train_large, y_train_large)\n",
    "lgb_train_time = time() - start\n",
    "lgb_acc = lgb_model.score(X_test_large, y_test_large)\n",
    "results['LightGBM'] = {'time': lgb_train_time, 'accuracy': lgb_acc}\n",
    "print(f\"  Time: {lgb_train_time:.2f}s, Accuracy: {lgb_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "models = list(results.keys())\n",
    "times = [results[m]['time'] for m in models]\n",
    "accuracies = [results[m]['accuracy'] for m in models]\n",
    "colors = ['purple', 'orange', 'green']\n",
    "\n",
    "# Training time comparison\n",
    "axes[0].bar(models, times, color=colors, alpha=0.7)\n",
    "axes[0].set_ylabel('Training Time (seconds)')\n",
    "axes[0].set_title('Training Speed Comparison')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "for i, (model, t) in enumerate(zip(models, times)):\n",
    "    axes[0].text(i, t + 0.1, f'{t:.2f}s', ha='center', va='bottom')\n",
    "\n",
    "# Accuracy comparison\n",
    "axes[1].bar(models, accuracies, color=colors, alpha=0.7)\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Accuracy Comparison')\n",
    "axes[1].set_ylim([min(accuracies) - 0.01, 1.0])\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "for i, (model, acc) in enumerate(zip(models, accuracies)):\n",
    "    axes[1].text(i, acc + 0.001, f'{acc:.4f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary\n",
    "print(\"\\nSummary:\")\n",
    "fastest = min(models, key=lambda m: results[m]['time'])\n",
    "most_accurate = max(models, key=lambda m: results[m]['accuracy'])\n",
    "print(f\"Fastest: {fastest} ({results[fastest]['time']:.2f}s)\")\n",
    "print(f\"Most Accurate: {most_accurate} ({results[most_accurate]['accuracy']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Categorical Feature Handling\n",
    "\n",
    "CatBoost's strongest feature is its superior handling of categorical variables. Let's see it in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset with categorical features\n",
    "np.random.seed(42)\n",
    "n_samples = 2000\n",
    "\n",
    "# Create DataFrame with mixed features\n",
    "df_cat = pd.DataFrame({\n",
    "    'num_feat1': np.random.randn(n_samples),\n",
    "    'num_feat2': np.random.randn(n_samples),\n",
    "    'num_feat3': np.random.randn(n_samples),\n",
    "    'category1': np.random.choice(['A', 'B', 'C', 'D', 'E'], n_samples),\n",
    "    'category2': np.random.choice(['Low', 'Medium', 'High', 'Very High'], n_samples),\n",
    "    'category3': np.random.choice(range(20), n_samples),  # High cardinality\n",
    "    'category4': np.random.choice(['Type1', 'Type2', 'Type3'], n_samples)\n",
    "})\n",
    "\n",
    "# Create target influenced by categorical features\n",
    "y_cat = (\n",
    "    df_cat['num_feat1'] +\n",
    "    (df_cat['category1'] == 'A').astype(int) * 2 +\n",
    "    (df_cat['category2'] == 'High').astype(int) * 1.5 +\n",
    "    (df_cat['category3'] > 10).astype(int) * 1 +\n",
    "    np.random.randn(n_samples) * 0.5\n",
    ") > 1\n",
    "y_cat = y_cat.astype(int)\n",
    "\n",
    "print(\"Dataset with categorical features:\")\n",
    "print(df_cat.head())\n",
    "print(f\"\\nShape: {df_cat.shape}\")\n",
    "print(f\"\\nData types:\")\n",
    "print(df_cat.dtypes)\n",
    "print(f\"\\nTarget distribution: {pd.Series(y_cat).value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_cat = df_cat\n",
    "X_train_cat, X_test_cat, y_train_cat, y_test_cat = train_test_split(\n",
    "    X_cat, y_cat, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Identify categorical features (columns 3-6)\n",
    "cat_features = ['category1', 'category2', 'category3', 'category4']\n",
    "cat_indices = [3, 4, 5, 6]  # Column indices\n",
    "\n",
    "print(f\"Categorical features: {cat_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CatBoost with categorical features\n",
    "# No preprocessing needed!\n",
    "cat_with_cat = CatBoostClassifier(\n",
    "    iterations=100,\n",
    "    learning_rate=0.1,\n",
    "    depth=5,\n",
    "    random_state=42,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Specify categorical features by index or name\n",
    "cat_with_cat.fit(\n",
    "    X_train_cat, y_train_cat,\n",
    "    cat_features=cat_indices  # Can also use cat_features (column names)\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "train_acc_cat = cat_with_cat.score(X_train_cat, y_train_cat)\n",
    "test_acc_cat = cat_with_cat.score(X_test_cat, y_test_cat)\n",
    "\n",
    "print(\"CatBoost with Native Categorical Support:\")\n",
    "print(f\"Train Accuracy: {train_acc_cat:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc_cat:.4f}\")\n",
    "print(f\"\\nNo preprocessing needed - CatBoost handles categorical features automatically!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with one-hot encoding approach\n",
    "X_encoded = pd.get_dummies(X_cat, columns=cat_features[:3], drop_first=True)\n",
    "# Keep category4 as is for comparison\n",
    "X_encoded['category4'] = X_cat['category4']\n",
    "\n",
    "X_train_enc, X_test_enc, y_train_enc, y_test_enc = train_test_split(\n",
    "    X_encoded, y_cat, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train with one-hot encoded features\n",
    "start_time = time()\n",
    "cat_encoded = CatBoostClassifier(\n",
    "    iterations=100,\n",
    "    learning_rate=0.1,\n",
    "    depth=5,\n",
    "    random_state=42,\n",
    "    verbose=False\n",
    ")\n",
    "cat_encoded.fit(X_train_enc, y_train_enc)\n",
    "encoded_time = time() - start_time\n",
    "encoded_acc = cat_encoded.score(X_test_enc, y_test_enc)\n",
    "\n",
    "# Train with native categorical\n",
    "start_time = time()\n",
    "cat_native = CatBoostClassifier(\n",
    "    iterations=100,\n",
    "    learning_rate=0.1,\n",
    "    depth=5,\n",
    "    random_state=42,\n",
    "    verbose=False\n",
    ")\n",
    "cat_native.fit(X_train_cat, y_train_cat, cat_features=cat_indices)\n",
    "native_time = time() - start_time\n",
    "native_acc = cat_native.score(X_test_cat, y_test_cat)\n",
    "\n",
    "# Compare\n",
    "print(\"Comparison: One-Hot Encoding vs Native Categorical:\")\n",
    "print(f\"\\nOne-Hot Encoded:\")\n",
    "print(f\"  Features: {X_encoded.shape[1]}\")\n",
    "print(f\"  Training time: {encoded_time:.4f}s\")\n",
    "print(f\"  Test Accuracy: {encoded_acc:.4f}\")\n",
    "print(f\"\\nNative Categorical:\")\n",
    "print(f\"  Features: {X_cat.shape[1]}\")\n",
    "print(f\"  Training time: {native_time:.4f}s\")\n",
    "print(f\"  Test Accuracy: {native_acc:.4f}\")\n",
    "print(f\"\\nAccuracy improvement: {(native_acc - encoded_acc):.4f}\")\n",
    "print(f\"Speed improvement: {encoded_time/native_time:.2f}x faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Insight**: CatBoost's native categorical handling not only simplifies the workflow (no encoding needed) but often leads to better accuracy and faster training, especially with high-cardinality categorical features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. CatBoost Hyperparameters\n",
    "\n",
    "CatBoost uses different parameter names than XGBoost/LightGBM:\n",
    "\n",
    "### Core Parameters:\n",
    "\n",
    "1. **iterations** (n_estimators):\n",
    "   - Number of boosting rounds\n",
    "   - Default: 1000\n",
    "\n",
    "2. **learning_rate**:\n",
    "   - Step size shrinkage\n",
    "   - Default: 0.03 (more conservative than others)\n",
    "   - Typical range: 0.01-0.3\n",
    "\n",
    "3. **depth** (max_depth):\n",
    "   - Maximum tree depth\n",
    "   - Default: 6\n",
    "   - Typical range: 4-10\n",
    "\n",
    "4. **l2_leaf_reg** (reg_lambda):\n",
    "   - L2 regularization\n",
    "   - Default: 3.0\n",
    "\n",
    "### CatBoost-Specific Parameters:\n",
    "\n",
    "5. **border_count**:\n",
    "   - Number of splits for numerical features\n",
    "   - Default: 254 (CPU), 128 (GPU)\n",
    "   - Higher → more accurate but slower\n",
    "\n",
    "6. **bagging_temperature**:\n",
    "   - Controls intensity of Bayesian bootstrap\n",
    "   - Default: 1.0\n",
    "   - Higher → more aggressive bootstrap\n",
    "\n",
    "7. **random_strength**:\n",
    "   - Amount of randomness for scoring splits\n",
    "   - Default: 1.0\n",
    "   - Higher → more randomness\n",
    "\n",
    "8. **grow_policy**:\n",
    "   - Tree growing strategy\n",
    "   - 'SymmetricTree' (default): Symmetric trees\n",
    "   - 'Depthwise': Like XGBoost\n",
    "   - 'Lossguide': Like LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of tree depth\n",
    "depths = [3, 4, 6, 8, 10]\n",
    "depth_results = []\n",
    "\n",
    "for depth in depths:\n",
    "    model = CatBoostClassifier(\n",
    "        iterations=100,\n",
    "        learning_rate=0.1,\n",
    "        depth=depth,\n",
    "        random_state=42,\n",
    "        verbose=False\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    train_acc = model.score(X_train, y_train)\n",
    "    test_acc = model.score(X_test, y_test)\n",
    "    \n",
    "    depth_results.append({\n",
    "        'depth': depth,\n",
    "        'train_acc': train_acc,\n",
    "        'test_acc': test_acc,\n",
    "        'gap': train_acc - test_acc\n",
    "    })\n",
    "    \n",
    "    print(f\"Depth={depth}: Train={train_acc:.4f}, Test={test_acc:.4f}\")\n",
    "\n",
    "depth_df = pd.DataFrame(depth_results)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axes[0].plot(depth_df['depth'], depth_df['train_acc'], 'o-', \n",
    "             label='Train', linewidth=2, markersize=8, color='purple')\n",
    "axes[0].plot(depth_df['depth'], depth_df['test_acc'], 's-', \n",
    "             label='Test', linewidth=2, markersize=8, color='orange')\n",
    "axes[0].set_xlabel('Tree Depth')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('Accuracy vs Tree Depth')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "axes[1].plot(depth_df['depth'], depth_df['gap'], 'ro-', linewidth=2, markersize=8)\n",
    "axes[1].set_xlabel('Tree Depth')\n",
    "axes[1].set_ylabel('Train-Test Gap')\n",
    "axes[1].set_title('Overfitting Gap vs Tree Depth')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of learning rate\n",
    "learning_rates = [0.01, 0.03, 0.1, 0.3]\n",
    "lr_results = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    model = CatBoostClassifier(\n",
    "        iterations=100,\n",
    "        learning_rate=lr,\n",
    "        depth=6,\n",
    "        random_state=42,\n",
    "        verbose=False\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    train_acc = model.score(X_train, y_train)\n",
    "    test_acc = model.score(X_test, y_test)\n",
    "    \n",
    "    lr_results.append({\n",
    "        'learning_rate': lr,\n",
    "        'train_acc': train_acc,\n",
    "        'test_acc': test_acc\n",
    "    })\n",
    "    \n",
    "    print(f\"LR={lr}: Train={train_acc:.4f}, Test={test_acc:.4f}\")\n",
    "\n",
    "lr_df = pd.DataFrame(lr_results)\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x = np.arange(len(learning_rates))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, lr_df['train_acc'], width, label='Train', \n",
    "       color='purple', alpha=0.7)\n",
    "ax.bar(x + width/2, lr_df['test_acc'], width, label='Test', \n",
    "       color='orange', alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Learning Rate')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Effect of Learning Rate on Performance')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(learning_rates)\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Early Stopping and Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create validation set\n",
    "X_train_sub, X_val, y_train_sub, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "# Train with early stopping\n",
    "cat_early = CatBoostClassifier(\n",
    "    iterations=500,\n",
    "    learning_rate=0.05,\n",
    "    depth=6,\n",
    "    early_stopping_rounds=20,\n",
    "    random_state=42,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Fit with validation set\n",
    "cat_early.fit(\n",
    "    X_train_sub, y_train_sub,\n",
    "    eval_set=(X_val, y_val),\n",
    "    use_best_model=True  # Use the best model according to validation\n",
    ")\n",
    "\n",
    "print(f\"Best iteration: {cat_early.get_best_iteration()}\")\n",
    "print(f\"Best score: {cat_early.get_best_score()['validation']['Logloss']:.4f}\")\n",
    "print(f\"Test Accuracy: {cat_early.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "# CatBoost stores training history automatically\n",
    "train_metrics = cat_early.get_evals_result()['learn']['Logloss']\n",
    "val_metrics = cat_early.get_evals_result()['validation']['Logloss']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_metrics, label='Training', linewidth=2, color='purple')\n",
    "plt.plot(val_metrics, label='Validation', linewidth=2, color='orange')\n",
    "plt.axvline(x=cat_early.get_best_iteration(), color='red', linestyle='--', \n",
    "           label=f'Best iteration ({cat_early.get_best_iteration()})')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Log Loss')\n",
    "plt.title('CatBoost Training Progress')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "feature_importance = cat_clf.get_feature_importance()\n",
    "feature_names = cancer_data.feature_names\n",
    "\n",
    "# Create DataFrame\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importance\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"Top 10 Most Important Features:\")\n",
    "print(importance_df.head(10))\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_features = importance_df.head(15)\n",
    "plt.barh(range(len(top_features)), top_features['Importance'], \n",
    "         color='purple', alpha=0.7)\n",
    "plt.yticks(range(len(top_features)), top_features['Feature'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Top 15 Feature Importances - CatBoost')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. CatBoost for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create regression dataset\n",
    "X_reg, y_reg = make_regression(\n",
    "    n_samples=1000,\n",
    "    n_features=10,\n",
    "    n_informative=8,\n",
    "    noise=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train CatBoost Regressor\n",
    "cat_reg = CatBoostRegressor(\n",
    "    iterations=100,\n",
    "    learning_rate=0.1,\n",
    "    depth=6,\n",
    "    random_state=42,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "cat_reg.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train_reg = cat_reg.predict(X_train_reg)\n",
    "y_pred_test_reg = cat_reg.predict(X_test_reg)\n",
    "\n",
    "# Evaluate\n",
    "train_r2 = r2_score(y_train_reg, y_pred_train_reg)\n",
    "test_r2 = r2_score(y_test_reg, y_pred_test_reg)\n",
    "train_mse = mean_squared_error(y_train_reg, y_pred_train_reg)\n",
    "test_mse = mean_squared_error(y_test_reg, y_pred_test_reg)\n",
    "\n",
    "print(\"CatBoost Regressor Performance:\")\n",
    "print(f\"Train R²: {train_r2:.4f}\")\n",
    "print(f\"Test R²: {test_r2:.4f}\")\n",
    "print(f\"Train MSE: {train_mse:.2f}\")\n",
    "print(f\"Test MSE: {test_mse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axes[0].scatter(y_train_reg, y_pred_train_reg, alpha=0.5, color='purple')\n",
    "axes[0].plot([y_train_reg.min(), y_train_reg.max()],\n",
    "             [y_train_reg.min(), y_train_reg.max()],\n",
    "             'r--', linewidth=2, label='Perfect prediction')\n",
    "axes[0].set_xlabel('Actual values')\n",
    "axes[0].set_ylabel('Predicted values')\n",
    "axes[0].set_title(f'Training Set (R² = {train_r2:.4f})')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].scatter(y_test_reg, y_pred_test_reg, alpha=0.5, color='purple')\n",
    "axes[1].plot([y_test_reg.min(), y_test_reg.max()],\n",
    "             [y_test_reg.min(), y_test_reg.max()],\n",
    "             'r--', linewidth=2, label='Perfect prediction')\n",
    "axes[1].set_xlabel('Actual values')\n",
    "axes[1].set_ylabel('Predicted values')\n",
    "axes[1].set_title(f'Test Set (R² = {test_r2:.4f})')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. CatBoost Pool (Advanced)\n",
    "\n",
    "CatBoost has a special `Pool` object for efficient data handling, especially with categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pool objects\n",
    "train_pool = Pool(\n",
    "    data=X_train_cat,\n",
    "    label=y_train_cat,\n",
    "    cat_features=cat_indices\n",
    ")\n",
    "\n",
    "test_pool = Pool(\n",
    "    data=X_test_cat,\n",
    "    label=y_test_cat,\n",
    "    cat_features=cat_indices\n",
    ")\n",
    "\n",
    "# Train using Pool\n",
    "cat_pool_model = CatBoostClassifier(\n",
    "    iterations=100,\n",
    "    learning_rate=0.1,\n",
    "    depth=5,\n",
    "    random_state=42,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "cat_pool_model.fit(train_pool)\n",
    "\n",
    "# Evaluate\n",
    "pool_acc = cat_pool_model.score(test_pool)\n",
    "print(f\"Test Accuracy using Pool: {pool_acc:.4f}\")\n",
    "print(\"\\nPool objects provide:\")\n",
    "print(\"- More efficient memory usage\")\n",
    "print(\"- Faster data loading\")\n",
    "print(\"- Better handling of categorical features\")\n",
    "print(\"- Support for weights and baselines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Basic CatBoost Training\n",
    "\n",
    "Create a synthetic classification dataset with 1500 samples and 20 features. Train a CatBoost classifier with:\n",
    "- 150 iterations\n",
    "- Learning rate of 0.05\n",
    "- Depth of 7\n",
    "\n",
    "Report training and test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Three-Way Comparison\n",
    "\n",
    "Create a dataset with 20,000 samples and 40 features. Train CatBoost, XGBoost, and LightGBM with identical parameters. Compare:\n",
    "1. Training time\n",
    "2. Test accuracy\n",
    "3. Create visualization showing both metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Categorical Feature Experiment\n",
    "\n",
    "Create a dataset with:\n",
    "- 3 numerical features\n",
    "- 4 categorical features (varying cardinalities: 5, 10, 20, 50)\n",
    "- 3000 samples\n",
    "\n",
    "Compare CatBoost's native categorical handling vs one-hot encoding. Which performs better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Hyperparameter Tuning\n",
    "\n",
    "Using the breast cancer dataset, test different combinations of:\n",
    "- depth: [4, 6, 8]\n",
    "- learning_rate: [0.01, 0.05, 0.1]\n",
    "- l2_leaf_reg: [1, 3, 5]\n",
    "\n",
    "Find the best combination and report the improvement over defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Early Stopping Analysis\n",
    "\n",
    "Train a CatBoost model on breast cancer data with:\n",
    "- iterations=1000\n",
    "- Early stopping with 50 rounds patience\n",
    "- Learning rate of 0.02\n",
    "\n",
    "Plot the training/validation curves and identify:\n",
    "1. When did training stop?\n",
    "2. Was there overfitting before stopping?\n",
    "3. What's the final test accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary\n",
    "\n",
    "In this notebook, you learned about CatBoost, Yandex's state-of-the-art gradient boosting library:\n",
    "\n",
    "### Key Innovations:\n",
    "\n",
    "1. **Ordered Boosting**:\n",
    "   - Eliminates prediction shift\n",
    "   - Uses random permutations to compute unbiased gradients\n",
    "   - Result: Better generalization, less overfitting\n",
    "\n",
    "2. **Ordered Target Statistics**:\n",
    "   - Optimal encoding for categorical features\n",
    "   - No target leakage\n",
    "   - Superior to one-hot and label encoding\n",
    "\n",
    "3. **Symmetric (Oblivious) Trees**:\n",
    "   - Same split at each level\n",
    "   - Faster prediction\n",
    "   - Better CPU cache utilization\n",
    "   - More regularized\n",
    "\n",
    "### Important Parameters:\n",
    "\n",
    "- **iterations**: Number of trees\n",
    "- **learning_rate**: Step size (default 0.03, more conservative)\n",
    "- **depth**: Tree depth (default 6)\n",
    "- **l2_leaf_reg**: L2 regularization (default 3.0)\n",
    "- **border_count**: Numerical feature splits\n",
    "- **bagging_temperature**: Bootstrap intensity\n",
    "- **random_strength**: Split randomness\n",
    "\n",
    "### Advantages:\n",
    "\n",
    "✅ **Best categorical feature handling** - no preprocessing needed\n",
    "✅ **Excellent default parameters** - works well out-of-the-box\n",
    "✅ **Resistant to overfitting** - ordered boosting\n",
    "✅ **High accuracy** - often best performance with minimal tuning\n",
    "✅ **Fastest prediction** - symmetric trees\n",
    "✅ **GPU support** - efficient implementation\n",
    "✅ **Built-in cross-validation**\n",
    "✅ **Handles missing values** automatically\n",
    "\n",
    "### Disadvantages:\n",
    "\n",
    "❌ **Slower training** than LightGBM on large datasets\n",
    "❌ **More memory usage** than other libraries\n",
    "❌ **Fewer hyperparameters to tune** (can be good or bad)\n",
    "❌ **Less documentation** than XGBoost\n",
    "\n",
    "### When to Choose CatBoost:\n",
    "\n",
    "**Use CatBoost when:**\n",
    "- Working with **categorical features** (especially high-cardinality)\n",
    "- Want **best accuracy with minimal tuning**\n",
    "- Need **robust default parameters**\n",
    "- **Overfitting** is a concern\n",
    "- Want **fastest prediction time**\n",
    "- Dataset is **medium-sized** (10K - 1M rows)\n",
    "\n",
    "**Consider alternatives when:**\n",
    "- Dataset is **very large** (> 10M rows) → Use LightGBM\n",
    "- **Training speed** is critical → Use LightGBM\n",
    "- Need **extensive community support** → Use XGBoost\n",
    "- Working with **image/text data** → Use deep learning\n",
    "\n",
    "### Comparison Summary:\n",
    "\n",
    "| Metric | XGBoost | LightGBM | CatBoost |\n",
    "|--------|---------|----------|----------|\n",
    "| **Training Speed** | ★★★ | ★★★★★ | ★★★ |\n",
    "| **Prediction Speed** | ★★★★ | ★★★★ | ★★★★★ |\n",
    "| **Accuracy** | ★★★★ | ★★★★ | ★★★★★ |\n",
    "| **Categorical Support** | ★ | ★★★ | ★★★★★ |\n",
    "| **Default Performance** | ★★★ | ★★★ | ★★★★★ |\n",
    "| **Overfitting Resistance** | ★★★ | ★★ | ★★★★★ |\n",
    "| **Documentation** | ★★★★★ | ★★★★ | ★★★ |\n",
    "| **Community** | ★★★★★ | ★★★★ | ★★★ |\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "1. **Start with defaults** - CatBoost works well without tuning:\n",
    "   ```python\n",
    "   CatBoostClassifier(\n",
    "       iterations=1000,\n",
    "       learning_rate=0.03,  # Conservative default\n",
    "       early_stopping_rounds=50\n",
    "   )\n",
    "   ```\n",
    "\n",
    "2. **Always specify categorical features** for best results\n",
    "3. **Use early stopping** to find optimal tree count\n",
    "4. **Use Pool objects** for large datasets\n",
    "5. **Monitor train/val curves** though overfitting is less common\n",
    "6. **Consider GPU** for faster training on large data\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In the next module, we'll explore **Stacking and Blending**, advanced ensemble techniques that combine multiple models:\n",
    "- How to stack different types of models\n",
    "- Training meta-learners\n",
    "- Multi-level stacking\n",
    "- When stacking helps vs when it doesn't\n",
    "\n",
    "Stacking often combines the strengths of XGBoost, LightGBM, and CatBoost for even better performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "- [CatBoost Documentation](https://catboost.ai/docs/)\n",
    "- [CatBoost Parameters](https://catboost.ai/docs/concepts/python-reference_parameters-list.html)\n",
    "- [Original CatBoost Paper (Prokhorenkova et al., 2018)](https://arxiv.org/abs/1706.09516)\n",
    "- [CatBoost Tutorials](https://github.com/catboost/tutorials)\n",
    "- [Handling Categorical Features](https://catboost.ai/docs/concepts/algorithm-main-stages_cat-to-numberic.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
