{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 04: Gradient Boosting Machines\n",
    "\n",
    "**Difficulty**: ⭐⭐\n",
    "**Estimated Time**: 45 minutes\n",
    "**Prerequisites**: \n",
    "- Module 00: Introduction to Ensemble Methods\n",
    "- Module 01: Bagging and Bootstrap Aggregation\n",
    "- Module 02: Random Forest\n",
    "- Module 03: AdaBoost\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Understand the gradient boosting algorithm and how it differs from AdaBoost\n",
    "2. Explain the role of loss functions and gradient descent in boosting\n",
    "3. Implement gradient boosting for classification and regression tasks\n",
    "4. Tune key hyperparameters: learning rate, max depth, and n_estimators\n",
    "5. Use early stopping to prevent overfitting\n",
    "6. Analyze feature importance in gradient boosting models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Gradient Boosting\n",
    "\n",
    "### What is Gradient Boosting?\n",
    "\n",
    "Gradient Boosting is a powerful ensemble technique that builds models sequentially, where each new model corrects the errors made by previous models. Unlike AdaBoost which adjusts sample weights, **Gradient Boosting fits new models to the residual errors** (the difference between predictions and actual values).\n",
    "\n",
    "### Key Differences from AdaBoost:\n",
    "\n",
    "1. **AdaBoost**: Adjusts sample weights, focuses on misclassified samples\n",
    "2. **Gradient Boosting**: Fits new models to residuals (errors), uses gradient descent optimization\n",
    "\n",
    "### The Algorithm:\n",
    "\n",
    "1. Start with an initial prediction (usually the mean for regression, log-odds for classification)\n",
    "2. Calculate residuals (errors) between predictions and actual values\n",
    "3. Train a new weak learner (decision tree) to predict these residuals\n",
    "4. Add the new model's predictions (scaled by learning rate) to the ensemble\n",
    "5. Repeat steps 2-4 for a specified number of iterations\n",
    "\n",
    "### Why \"Gradient\"?\n",
    "\n",
    "The method uses gradient descent to minimize a loss function. Each new tree is fitted to the negative gradient of the loss function, moving the ensemble predictions in the direction that reduces the error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.datasets import make_classification, make_regression, load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, learning_curve\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    mean_squared_error, r2_score, mean_absolute_error\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Understanding Gradient Boosting with a Simple Example\n",
    "\n",
    "Let's build a simple gradient boosting model from scratch to understand how it works. We'll create a regression example where we can visualize the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple dataset for regression\n",
    "np.random.seed(42)\n",
    "X_simple = np.linspace(0, 10, 100).reshape(-1, 1)\n",
    "y_simple = 2 * X_simple.ravel() + np.sin(X_simple.ravel() * 2) + np.random.randn(100) * 0.5\n",
    "\n",
    "print(f\"Dataset shape: {X_simple.shape}\")\n",
    "print(f\"Target shape: {y_simple.shape}\")\n",
    "\n",
    "# Visualize the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_simple, y_simple, alpha=0.6, label='Actual data')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Simple Regression Dataset')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Gradient Boosting Implementation\n",
    "\n",
    "Let's implement a simplified version of gradient boosting to see how it works step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initialize with the mean (simplest prediction)\n",
    "initial_prediction = np.mean(y_simple)\n",
    "print(f\"Initial prediction (mean): {initial_prediction:.2f}\")\n",
    "\n",
    "# Create array to store ensemble predictions\n",
    "ensemble_predictions = np.full(len(y_simple), initial_prediction)\n",
    "\n",
    "# Learning rate - controls how much each tree contributes\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Number of boosting iterations\n",
    "n_iterations = 5\n",
    "\n",
    "# Store trees for visualization\n",
    "trees = []\n",
    "\n",
    "# Visualize the boosting process\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    # Step 2: Calculate residuals (negative gradient for MSE loss)\n",
    "    residuals = y_simple - ensemble_predictions\n",
    "    \n",
    "    # Step 3: Fit a tree to predict the residuals\n",
    "    tree = DecisionTreeRegressor(max_depth=3, random_state=42)\n",
    "    tree.fit(X_simple, residuals)\n",
    "    trees.append(tree)\n",
    "    \n",
    "    # Step 4: Update ensemble predictions\n",
    "    tree_predictions = tree.predict(X_simple)\n",
    "    ensemble_predictions += learning_rate * tree_predictions\n",
    "    \n",
    "    # Calculate error\n",
    "    mse = mean_squared_error(y_simple, ensemble_predictions)\n",
    "    \n",
    "    # Visualize\n",
    "    axes[i].scatter(X_simple, y_simple, alpha=0.5, label='Actual')\n",
    "    axes[i].plot(X_simple, ensemble_predictions, 'r-', linewidth=2, label='Prediction')\n",
    "    axes[i].set_title(f'Iteration {i+1} - MSE: {mse:.2f}')\n",
    "    axes[i].set_xlabel('X')\n",
    "    axes[i].set_ylabel('y')\n",
    "    axes[i].legend()\n",
    "    \n",
    "    print(f\"Iteration {i+1}: MSE = {mse:.4f}\")\n",
    "\n",
    "# Hide the last subplot if not used\n",
    "axes[-1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal MSE: {mean_squared_error(y_simple, ensemble_predictions):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Observation**: Notice how each iteration improves the fit by learning from the residuals. The ensemble gets progressively better at capturing the underlying pattern in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Gradient Boosting for Regression\n",
    "\n",
    "Now let's use scikit-learn's `GradientBoostingRegressor` on a more realistic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a regression dataset\n",
    "X_reg, y_reg = make_regression(\n",
    "    n_samples=1000,\n",
    "    n_features=10,\n",
    "    n_informative=8,\n",
    "    noise=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Split the data\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train_reg.shape}\")\n",
    "print(f\"Test set size: {X_test_reg.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Gradient Boosting Regressor\n",
    "gb_regressor = GradientBoostingRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "gb_regressor.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_train_reg = gb_regressor.predict(X_train_reg)\n",
    "y_pred_test_reg = gb_regressor.predict(X_test_reg)\n",
    "\n",
    "# Evaluate performance\n",
    "train_mse = mean_squared_error(y_train_reg, y_pred_train_reg)\n",
    "test_mse = mean_squared_error(y_test_reg, y_pred_test_reg)\n",
    "train_r2 = r2_score(y_train_reg, y_pred_train_reg)\n",
    "test_r2 = r2_score(y_test_reg, y_pred_test_reg)\n",
    "\n",
    "print(\"Gradient Boosting Regressor Performance:\")\n",
    "print(f\"Train MSE: {train_mse:.2f}\")\n",
    "print(f\"Test MSE: {test_mse:.2f}\")\n",
    "print(f\"Train R²: {train_r2:.4f}\")\n",
    "print(f\"Test R²: {test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions vs actual values\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Training set\n",
    "axes[0].scatter(y_train_reg, y_pred_train_reg, alpha=0.5)\n",
    "axes[0].plot([y_train_reg.min(), y_train_reg.max()], \n",
    "             [y_train_reg.min(), y_train_reg.max()], \n",
    "             'r--', linewidth=2, label='Perfect prediction')\n",
    "axes[0].set_xlabel('Actual values')\n",
    "axes[0].set_ylabel('Predicted values')\n",
    "axes[0].set_title(f'Training Set (R² = {train_r2:.4f})')\n",
    "axes[0].legend()\n",
    "\n",
    "# Test set\n",
    "axes[1].scatter(y_test_reg, y_pred_test_reg, alpha=0.5)\n",
    "axes[1].plot([y_test_reg.min(), y_test_reg.max()], \n",
    "             [y_test_reg.min(), y_test_reg.max()], \n",
    "             'r--', linewidth=2, label='Perfect prediction')\n",
    "axes[1].set_xlabel('Actual values')\n",
    "axes[1].set_ylabel('Predicted values')\n",
    "axes[1].set_title(f'Test Set (R² = {test_r2:.4f})')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Gradient Boosting for Classification\n",
    "\n",
    "Gradient boosting works slightly differently for classification. It uses log-loss (cross-entropy) as the loss function and predicts log-odds that are converted to probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load breast cancer dataset\n",
    "cancer_data = load_breast_cancer()\n",
    "X_clf = cancer_data.data\n",
    "y_clf = cancer_data.target\n",
    "\n",
    "# Split the data\n",
    "X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(\n",
    "    X_clf, y_clf, test_size=0.2, random_state=42, stratify=y_clf\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train_clf.shape}\")\n",
    "print(f\"Test set size: {X_test_clf.shape}\")\n",
    "print(f\"\\nClass distribution in training set:\")\n",
    "print(pd.Series(y_train_clf).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Gradient Boosting Classifier\n",
    "gb_classifier = GradientBoostingClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "gb_classifier.fit(X_train_clf, y_train_clf)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_train_clf = gb_classifier.predict(X_train_clf)\n",
    "y_pred_test_clf = gb_classifier.predict(X_test_clf)\n",
    "\n",
    "# Evaluate performance\n",
    "train_accuracy = accuracy_score(y_train_clf, y_pred_train_clf)\n",
    "test_accuracy = accuracy_score(y_test_clf, y_pred_test_clf)\n",
    "\n",
    "print(\"Gradient Boosting Classifier Performance:\")\n",
    "print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"\\nClassification Report (Test Set):\")\n",
    "print(classification_report(y_test_clf, y_pred_test_clf, \n",
    "                          target_names=cancer_data.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test_clf, y_pred_test_clf)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=cancer_data.target_names,\n",
    "            yticklabels=cancer_data.target_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix - Gradient Boosting Classifier')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Key Hyperparameters\n",
    "\n",
    "Gradient Boosting has several important hyperparameters that control model complexity and performance:\n",
    "\n",
    "### 1. **n_estimators**: Number of boosting stages (trees)\n",
    "- More trees → Better training performance but risk of overfitting\n",
    "- Too few trees → Underfitting\n",
    "\n",
    "### 2. **learning_rate** (also called shrinkage)\n",
    "- Controls the contribution of each tree\n",
    "- Lower values require more trees but often generalize better\n",
    "- Typical range: 0.01 to 0.3\n",
    "- Trade-off: `n_estimators` × `learning_rate`\n",
    "\n",
    "### 3. **max_depth**: Maximum depth of individual trees\n",
    "- Controls tree complexity\n",
    "- Typical range: 3 to 8\n",
    "- Deeper trees → More complex patterns but higher overfitting risk\n",
    "\n",
    "### 4. **subsample**: Fraction of samples used for fitting trees\n",
    "- Similar to Random Forest's bootstrap\n",
    "- Typical range: 0.5 to 1.0\n",
    "- Values < 1.0 add randomness and reduce overfitting\n",
    "\n",
    "### 5. **min_samples_split** and **min_samples_leaf**\n",
    "- Control when to stop splitting nodes\n",
    "- Higher values → Simpler trees, less overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different learning rates\n",
    "learning_rates = [0.01, 0.05, 0.1, 0.3]\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    gb = GradientBoostingClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=lr,\n",
    "        max_depth=3,\n",
    "        random_state=42\n",
    "    )\n",
    "    gb.fit(X_train_clf, y_train_clf)\n",
    "    \n",
    "    train_scores.append(gb.score(X_train_clf, y_train_clf))\n",
    "    test_scores.append(gb.score(X_test_clf, y_test_clf))\n",
    "    \n",
    "    print(f\"Learning Rate: {lr:.2f}\")\n",
    "    print(f\"  Train Accuracy: {train_scores[-1]:.4f}\")\n",
    "    print(f\"  Test Accuracy: {test_scores[-1]:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Visualize\n",
    "x_pos = np.arange(len(learning_rates))\n",
    "width = 0.35\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(x_pos - width/2, train_scores, width, label='Train', alpha=0.8)\n",
    "plt.bar(x_pos + width/2, test_scores, width, label='Test', alpha=0.8)\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Effect of Learning Rate on Model Performance')\n",
    "plt.xticks(x_pos, learning_rates)\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of Max Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different max depths\n",
    "max_depths = [1, 2, 3, 5, 7]\n",
    "train_scores_depth = []\n",
    "test_scores_depth = []\n",
    "\n",
    "for depth in max_depths:\n",
    "    gb = GradientBoostingClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=depth,\n",
    "        random_state=42\n",
    "    )\n",
    "    gb.fit(X_train_clf, y_train_clf)\n",
    "    \n",
    "    train_scores_depth.append(gb.score(X_train_clf, y_train_clf))\n",
    "    test_scores_depth.append(gb.score(X_test_clf, y_test_clf))\n",
    "    \n",
    "    print(f\"Max Depth: {depth}\")\n",
    "    print(f\"  Train Accuracy: {train_scores_depth[-1]:.4f}\")\n",
    "    print(f\"  Test Accuracy: {test_scores_depth[-1]:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(max_depths, train_scores_depth, 'o-', label='Train', linewidth=2, markersize=8)\n",
    "plt.plot(max_depths, test_scores_depth, 's-', label='Test', linewidth=2, markersize=8)\n",
    "plt.xlabel('Max Depth')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Effect of Max Depth on Model Performance')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Observation**: Notice how deeper trees lead to better training performance but may hurt test performance (overfitting). A max_depth of 3-5 often provides a good balance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Early Stopping\n",
    "\n",
    "Early stopping monitors validation performance and stops training when performance stops improving. This prevents overfitting and saves computational time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training data into train and validation\n",
    "X_train_sub, X_val, y_train_sub, y_val = train_test_split(\n",
    "    X_train_clf, y_train_clf, test_size=0.2, random_state=42, stratify=y_train_clf\n",
    ")\n",
    "\n",
    "# Train with early stopping\n",
    "gb_early_stop = GradientBoostingClassifier(\n",
    "    n_estimators=1000,  # Set high, early stopping will determine actual number\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    validation_fraction=0.2,  # Use 20% of training data for validation\n",
    "    n_iter_no_change=10,  # Stop if no improvement for 10 iterations\n",
    "    tol=1e-4,  # Minimum improvement required\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "gb_early_stop.fit(X_train_clf, y_train_clf)\n",
    "\n",
    "print(f\"Total estimators with early stopping: {gb_early_stop.n_estimators_}\")\n",
    "print(f\"Specified max estimators: 1000\")\n",
    "print(f\"\\nTest Accuracy: {gb_early_stop.score(X_test_clf, y_test_clf):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training progress\n",
    "# Train model with staged predictions to see performance at each stage\n",
    "gb_staged = GradientBoostingClassifier(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "gb_staged.fit(X_train_clf, y_train_clf)\n",
    "\n",
    "# Calculate accuracy at each stage\n",
    "train_scores_staged = []\n",
    "test_scores_staged = []\n",
    "\n",
    "for y_pred_train in gb_staged.staged_predict(X_train_clf):\n",
    "    train_scores_staged.append(accuracy_score(y_train_clf, y_pred_train))\n",
    "\n",
    "for y_pred_test in gb_staged.staged_predict(X_test_clf):\n",
    "    test_scores_staged.append(accuracy_score(y_test_clf, y_pred_test))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(train_scores_staged) + 1), train_scores_staged, \n",
    "         label='Train', linewidth=2)\n",
    "plt.plot(range(1, len(test_scores_staged) + 1), test_scores_staged, \n",
    "         label='Test', linewidth=2)\n",
    "plt.xlabel('Number of Boosting Iterations')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Learning Curve: Accuracy vs Number of Trees')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Find optimal number of estimators\n",
    "optimal_n_estimators = np.argmax(test_scores_staged) + 1\n",
    "print(f\"Optimal number of estimators: {optimal_n_estimators}\")\n",
    "print(f\"Best test accuracy: {test_scores_staged[optimal_n_estimators-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Insight**: The plot shows that test accuracy often peaks before training accuracy, indicating when the model starts to overfit. Early stopping helps prevent this by monitoring validation performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Importance\n",
    "\n",
    "Gradient Boosting provides feature importance scores based on how much each feature reduces the loss function across all trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "feature_importance = gb_classifier.feature_importances_\n",
    "feature_names = cancer_data.feature_names\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importance\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"Top 10 Most Important Features:\")\n",
    "print(importance_df.head(10))\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(range(10), importance_df['Importance'].head(10))\n",
    "plt.yticks(range(10), importance_df['Feature'].head(10))\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Top 10 Feature Importances in Gradient Boosting Model')\n",
    "plt.gca().invert_yaxis()  # Highest importance at the top\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Loss Functions\n",
    "\n",
    "Gradient Boosting supports different loss functions depending on the task:\n",
    "\n",
    "### For Regression:\n",
    "- **'squared_error'** (default): Mean squared error, standard choice\n",
    "- **'absolute_error'**: Mean absolute error, robust to outliers\n",
    "- **'huber'**: Combination of squared and absolute error\n",
    "- **'quantile'**: For predicting quantiles (e.g., median)\n",
    "\n",
    "### For Classification:\n",
    "- **'log_loss'** (default): Logistic regression loss, provides probability estimates\n",
    "- **'exponential'**: AdaBoost loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different loss functions for regression\n",
    "# Create data with outliers\n",
    "X_outlier, y_outlier = make_regression(\n",
    "    n_samples=200, n_features=1, noise=10, random_state=42\n",
    ")\n",
    "\n",
    "# Add some outliers\n",
    "y_outlier[::20] += np.random.randn(10) * 100\n",
    "\n",
    "# Split data\n",
    "X_train_out, X_test_out, y_train_out, y_test_out = train_test_split(\n",
    "    X_outlier, y_outlier, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train models with different loss functions\n",
    "loss_functions = ['squared_error', 'absolute_error', 'huber']\n",
    "models = {}\n",
    "\n",
    "for loss in loss_functions:\n",
    "    model = GradientBoostingRegressor(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=3,\n",
    "        loss=loss,\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train_out, y_train_out)\n",
    "    models[loss] = model\n",
    "    \n",
    "    # Evaluate\n",
    "    train_score = r2_score(y_train_out, model.predict(X_train_out))\n",
    "    test_score = r2_score(y_test_out, model.predict(X_test_out))\n",
    "    test_mae = mean_absolute_error(y_test_out, model.predict(X_test_out))\n",
    "    \n",
    "    print(f\"Loss Function: {loss}\")\n",
    "    print(f\"  Train R²: {train_score:.4f}\")\n",
    "    print(f\"  Test R²: {test_score:.4f}\")\n",
    "    print(f\"  Test MAE: {test_mae:.2f}\")\n",
    "    print()\n",
    "\n",
    "# Visualize predictions\n",
    "X_range = np.linspace(X_outlier.min(), X_outlier.max(), 300).reshape(-1, 1)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "for idx, (loss, model) in enumerate(models.items(), 1):\n",
    "    plt.subplot(1, 3, idx)\n",
    "    plt.scatter(X_train_out, y_train_out, alpha=0.5, label='Training data')\n",
    "    plt.plot(X_range, model.predict(X_range), 'r-', linewidth=2, label='Prediction')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('y')\n",
    "    plt.title(f'Loss: {loss}')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Exercises\n",
    "\n",
    "Now it's your turn to practice! Complete the following exercises to reinforce your understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Basic Gradient Boosting\n",
    "\n",
    "Create a synthetic classification dataset with 500 samples and 15 features. Train a Gradient Boosting Classifier with:\n",
    "- 50 estimators\n",
    "- Learning rate of 0.15\n",
    "- Max depth of 4\n",
    "\n",
    "Calculate and print the training and test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Step 1: Create dataset using make_classification\n",
    "# Step 2: Split into train and test sets\n",
    "# Step 3: Train GradientBoostingClassifier with specified parameters\n",
    "# Step 4: Calculate and print accuracies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Hyperparameter Tuning\n",
    "\n",
    "Using the breast cancer dataset:\n",
    "1. Test different combinations of `n_estimators` (50, 100, 200) and `learning_rate` (0.01, 0.1, 0.3)\n",
    "2. Store the test accuracy for each combination\n",
    "3. Create a heatmap showing the accuracy for each combination\n",
    "4. Which combination gives the best test accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Step 1: Create lists for n_estimators and learning_rate values\n",
    "# Step 2: Use nested loops to try all combinations\n",
    "# Step 3: Store results in a 2D array or DataFrame\n",
    "# Step 4: Create heatmap using seaborn\n",
    "# Step 5: Print the best combination\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Comparing Subsample Ratios\n",
    "\n",
    "The `subsample` parameter controls the fraction of samples used for fitting each tree. This adds randomness similar to Random Forest:\n",
    "\n",
    "1. Train three Gradient Boosting Classifiers on the breast cancer data with:\n",
    "   - subsample=1.0 (use all samples)\n",
    "   - subsample=0.8\n",
    "   - subsample=0.5\n",
    "2. Keep other parameters constant: n_estimators=100, learning_rate=0.1, max_depth=3\n",
    "3. Compare training and test accuracies\n",
    "4. Which subsample value helps reduce overfitting the most?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Step 1: Create a list of subsample values to test\n",
    "# Step 2: Train models with different subsample values\n",
    "# Step 3: Calculate train and test accuracies\n",
    "# Step 4: Visualize results with a bar plot\n",
    "# Step 5: Analyze which value best reduces overfitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Feature Importance Analysis\n",
    "\n",
    "Using the breast cancer dataset:\n",
    "1. Train a Gradient Boosting Classifier\n",
    "2. Extract feature importances\n",
    "3. Train another model using only the top 10 most important features\n",
    "4. Compare the test accuracy of the full model vs the reduced model\n",
    "5. Does using fewer features significantly hurt performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Step 1: Train initial model and get feature importances\n",
    "# Step 2: Identify top 10 most important features\n",
    "# Step 3: Create new dataset with only these features\n",
    "# Step 4: Train model on reduced dataset\n",
    "# Step 5: Compare performances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Early Stopping Optimization\n",
    "\n",
    "Create a regression dataset and experiment with early stopping:\n",
    "1. Create a regression dataset with 1000 samples, 20 features\n",
    "2. Train a model with n_estimators=500 and early stopping enabled\n",
    "3. Record the actual number of estimators used\n",
    "4. Plot the staged predictions to show when the model stops improving\n",
    "5. What's the optimal number of trees according to your analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Step 1: Create regression dataset\n",
    "# Step 2: Train with early stopping\n",
    "# Step 3: Use staged_predict to get predictions at each iteration\n",
    "# Step 4: Calculate MSE at each stage\n",
    "# Step 5: Plot and analyze\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary\n",
    "\n",
    "In this notebook, you learned about Gradient Boosting Machines:\n",
    "\n",
    "### Key Concepts:\n",
    "1. **Gradient Boosting** builds models sequentially by fitting each new model to the residuals (errors) of the previous ensemble\n",
    "2. **Loss Functions** drive the optimization - MSE for regression, log-loss for classification\n",
    "3. **Learning Rate** controls how much each tree contributes - lower values require more trees but often generalize better\n",
    "4. **Tree Depth** controls model complexity - shallow trees (3-5 levels) often work best\n",
    "5. **Early Stopping** prevents overfitting by monitoring validation performance\n",
    "6. **Subsample** parameter adds randomness and reduces overfitting\n",
    "\n",
    "### Best Practices:\n",
    "- Start with conservative parameters: learning_rate=0.1, max_depth=3, n_estimators=100\n",
    "- Use early stopping to automatically determine the optimal number of trees\n",
    "- Lower learning rates often give better results but require more trees\n",
    "- Use cross-validation for hyperparameter tuning\n",
    "- Monitor training vs test performance to detect overfitting\n",
    "- Consider using subsample < 1.0 to add randomness and speed up training\n",
    "\n",
    "### Advantages:\n",
    "- Often achieves state-of-the-art performance on structured data\n",
    "- Handles mixed data types naturally\n",
    "- Provides feature importance scores\n",
    "- Robust to outliers (especially with robust loss functions)\n",
    "- Works well with small to medium datasets\n",
    "\n",
    "### Disadvantages:\n",
    "- Training is sequential (cannot be parallelized like Random Forest)\n",
    "- More hyperparameters to tune than Random Forest\n",
    "- Can be slow to train with many trees\n",
    "- Risk of overfitting if not carefully tuned\n",
    "- Less effective on very high-dimensional sparse data\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In the next module, we'll explore **XGBoost** (Extreme Gradient Boosting), an optimized implementation of gradient boosting that includes:\n",
    "- Regularization terms to prevent overfitting\n",
    "- Efficient tree pruning algorithms\n",
    "- Built-in cross-validation\n",
    "- Parallel processing capabilities\n",
    "- Better handling of missing values\n",
    "\n",
    "XGBoost has become the go-to algorithm for many machine learning competitions and real-world applications!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "- [Scikit-learn Gradient Boosting Documentation](https://scikit-learn.org/stable/modules/ensemble.html#gradient-boosting)\n",
    "- [Original Gradient Boosting Paper by Friedman (2001)](https://statweb.stanford.edu/~jhf/ftp/trebst.pdf)\n",
    "- [Understanding Gradient Boosting (Video)](https://www.youtube.com/watch?v=3CC4N4z3GJc)\n",
    "- [Gradient Boosting Interactive Demo](https://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
