{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 04: Gradient Boosting Machines\n",
    "\n",
    "**Difficulty**: ⭐⭐⭐ Advanced\n",
    "**Estimated Time**: 85 minutes\n",
    "**Prerequisites**: \n",
    "- Module 00: Introduction to Ensemble Learning\n",
    "- Module 03: Boosting Fundamentals and AdaBoost\n",
    "- Understanding of gradient descent\n",
    "- Basic calculus (derivatives)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Understand gradient boosting as optimization in function space\n",
    "2. Explain how gradient boosting generalizes AdaBoost to arbitrary loss functions\n",
    "3. Implement residual fitting for regression problems\n",
    "4. Configure learning rate and tree depth for optimal performance\n",
    "5. Apply regularization techniques to prevent overfitting\n",
    "6. Use early stopping to find optimal number of estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.datasets import (\n",
    "    make_regression, make_classification, \n",
    "    load_diabetes, load_breast_cancer, fetch_california_housing\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, cross_val_score, validation_curve,\n",
    "    learning_curve\n",
    ")\n",
    "from sklearn.ensemble import (\n",
    "    GradientBoostingClassifier, GradientBoostingRegressor,\n",
    "    RandomForestRegressor, AdaBoostRegressor\n",
    ")\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, mean_absolute_error, r2_score,\n",
    "    accuracy_score, log_loss, roc_auc_score\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 4)\n",
    "\n",
    "print(\"Setup complete! All libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gradient Boosting: The Big Picture\n",
    "\n",
    "### From AdaBoost to Gradient Boosting\n",
    "\n",
    "**AdaBoost limitation**: Specifically designed for classification with exponential loss\n",
    "- Hard to extend to other loss functions\n",
    "- Not easily adaptable to regression\n",
    "\n",
    "**Gradient Boosting breakthrough**: Unified framework for any differentiable loss function\n",
    "- Works for classification, regression, ranking, etc.\n",
    "- More flexible and powerful\n",
    "\n",
    "### Key Insight: Optimization in Function Space\n",
    "\n",
    "**Traditional gradient descent**: Optimize parameters\n",
    "```\n",
    "θ ← θ - learning_rate × ∇L(θ)\n",
    "```\n",
    "\n",
    "**Gradient boosting**: Optimize function itself\n",
    "```\n",
    "F(x) ← F(x) + learning_rate × new_tree(x)\n",
    "```\n",
    "\n",
    "### The Algorithm (Simplified)\n",
    "\n",
    "1. **Initialize** with constant prediction (e.g., mean for regression)\n",
    "   $$F_0(x) = \\text{argmin}_c \\sum_{i=1}^n L(y_i, c)$$\n",
    "\n",
    "2. **For each iteration** m = 1 to M:\n",
    "   \n",
    "   a. **Compute pseudo-residuals** (negative gradient of loss):\n",
    "   $$r_{im} = -\\left[\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}\\right]_{F=F_{m-1}}$$\n",
    "   \n",
    "   b. **Fit tree** to pseudo-residuals:\n",
    "   $$h_m(x) = \\text{DecisionTree}(X, r_m)$$\n",
    "   \n",
    "   c. **Update model**:\n",
    "   $$F_m(x) = F_{m-1}(x) + \\nu \\cdot h_m(x)$$\n",
    "   \n",
    "   where $\\nu$ is the learning rate\n",
    "\n",
    "3. **Final model**: $F_M(x) = F_0(x) + \\nu \\sum_{m=1}^M h_m(x)$\n",
    "\n",
    "### Why \"Gradient\"?\n",
    "\n",
    "- Pseudo-residuals are the negative gradient of the loss function\n",
    "- Each tree fits the gradient direction\n",
    "- We're doing gradient descent, but adding functions instead of adjusting parameters!\n",
    "\n",
    "### Common Loss Functions\n",
    "\n",
    "**Regression**:\n",
    "- Squared loss: $L(y, F) = \\frac{1}{2}(y - F)^2$ → residual = $y - F$\n",
    "- Absolute loss: $L(y, F) = |y - F|$ → residual = $\\text{sign}(y - F)$\n",
    "- Huber loss: Robust to outliers (combines squared and absolute)\n",
    "\n",
    "**Classification**:\n",
    "- Log loss (deviance): $L(y, F) = -\\sum [y\\log(p) + (1-y)\\log(1-p)]$\n",
    "- Exponential loss: $L(y, F) = e^{-yF}$ (equivalent to AdaBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize different loss functions and their gradients\n",
    "# For regression: compare squared vs absolute loss\n",
    "\n",
    "residuals = np.linspace(-3, 3, 100)\n",
    "\n",
    "# Loss functions\n",
    "squared_loss = 0.5 * residuals**2\n",
    "absolute_loss = np.abs(residuals)\n",
    "huber_loss = np.where(\n",
    "    np.abs(residuals) <= 1,\n",
    "    0.5 * residuals**2,\n",
    "    np.abs(residuals) - 0.5\n",
    ")\n",
    "\n",
    "# Gradients (pseudo-residuals for squared loss)\n",
    "squared_grad = residuals  # d/dF of 0.5(y-F)^2 = -(y-F) = -residual\n",
    "absolute_grad = np.sign(residuals)\n",
    "huber_grad = np.where(\n",
    "    np.abs(residuals) <= 1,\n",
    "    residuals,\n",
    "    np.sign(residuals)\n",
    ")\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss functions\n",
    "axes[0].plot(residuals, squared_loss, linewidth=2, label='Squared Loss')\n",
    "axes[0].plot(residuals, absolute_loss, linewidth=2, label='Absolute Loss')\n",
    "axes[0].plot(residuals, huber_loss, linewidth=2, label='Huber Loss')\n",
    "axes[0].set_xlabel('Residual (y - F)', fontsize=11)\n",
    "axes[0].set_ylabel('Loss', fontsize=11)\n",
    "axes[0].set_title('Loss Functions for Regression', fontsize=12, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Gradients\n",
    "axes[1].plot(residuals, -squared_grad, linewidth=2, label='Squared (linear)')\n",
    "axes[1].plot(residuals, -absolute_grad, linewidth=2, label='Absolute (constant)')\n",
    "axes[1].plot(residuals, -huber_grad, linewidth=2, label='Huber (robust)')\n",
    "axes[1].set_xlabel('Residual (y - F)', fontsize=11)\n",
    "axes[1].set_ylabel('Pseudo-Residual (-gradient)', fontsize=11)\n",
    "axes[1].set_title('What Trees Fit (Pseudo-Residuals)', fontsize=12, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Loss Function Comparison:\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Squared Loss:\")\n",
    "print(\"  - Smooth, differentiable everywhere\")\n",
    "print(\"  - Large errors get LARGE penalties (quadratic)\")\n",
    "print(\"  - Sensitive to outliers\")\n",
    "print(\"\\nAbsolute Loss:\")\n",
    "print(\"  - Linear penalty for all errors\")\n",
    "print(\"  - Robust to outliers\")\n",
    "print(\"  - Gradient is constant (just sign)\")\n",
    "print(\"\\nHuber Loss:\")\n",
    "print(\"  - Best of both worlds\")\n",
    "print(\"  - Squared for small errors (smooth)\")\n",
    "print(\"  - Linear for large errors (robust)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Residual Fitting Demonstration\n",
    "\n",
    "Let's build intuition by manually implementing gradient boosting for regression with squared loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simple regression dataset\n",
    "np.random.seed(RANDOM_STATE)\n",
    "X_simple = np.linspace(0, 10, 100).reshape(-1, 1)\n",
    "y_simple = np.sin(X_simple).ravel() + np.random.normal(0, 0.1, X_simple.shape[0])\n",
    "\n",
    "# Manual gradient boosting with 5 trees\n",
    "n_trees = 5\n",
    "learning_rate = 0.3\n",
    "max_depth = 3\n",
    "\n",
    "# Initialize with mean\n",
    "F = np.full(len(y_simple), y_simple.mean())\n",
    "trees = []\n",
    "predictions_history = [F.copy()]\n",
    "\n",
    "print(\"Gradient Boosting Step-by-Step:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Initial prediction (mean): {F[0]:.4f}\")\n",
    "print(f\"Initial MSE: {mean_squared_error(y_simple, F):.4f}\\n\")\n",
    "\n",
    "# Fit trees sequentially\n",
    "for i in range(n_trees):\n",
    "    # Compute residuals (negative gradient for squared loss)\n",
    "    residuals = y_simple - F\n",
    "    \n",
    "    # Fit tree to residuals\n",
    "    tree = DecisionTreeRegressor(max_depth=max_depth, random_state=RANDOM_STATE + i)\n",
    "    tree.fit(X_simple, residuals)\n",
    "    \n",
    "    # Update predictions\n",
    "    tree_pred = tree.predict(X_simple)\n",
    "    F = F + learning_rate * tree_pred\n",
    "    \n",
    "    trees.append(tree)\n",
    "    predictions_history.append(F.copy())\n",
    "    \n",
    "    mse = mean_squared_error(y_simple, F)\n",
    "    print(f\"Tree {i+1}:\")\n",
    "    print(f\"  Mean absolute residual: {np.mean(np.abs(residuals)):.4f}\")\n",
    "    print(f\"  Updated MSE: {mse:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the sequential improvement\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(6):\n",
    "    axes[i].scatter(X_simple, y_simple, alpha=0.5, s=20, color='black', label='True data')\n",
    "    axes[i].plot(X_simple, predictions_history[i], color='red', linewidth=2, \n",
    "                label=f'Prediction (MSE={mean_squared_error(y_simple, predictions_history[i]):.3f})')\n",
    "    \n",
    "    if i == 0:\n",
    "        axes[i].set_title('Initial: F₀(x) = mean', fontsize=11, fontweight='bold')\n",
    "    else:\n",
    "        axes[i].set_title(f'After Tree {i}', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    axes[i].set_xlabel('X')\n",
    "    axes[i].set_ylabel('y')\n",
    "    axes[i].legend(fontsize=9)\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Observation:\")\n",
    "print(\"Each tree corrects errors from previous predictions.\")\n",
    "print(\"The model gradually learns the complex sinusoidal pattern!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gradient Boosting with Scikit-learn\n",
    "\n",
    "Now let's use sklearn's optimized implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load real regression dataset\n",
    "housing = fetch_california_housing()\n",
    "X, y = housing.data, housing.target\n",
    "\n",
    "# Take a subset for faster training\n",
    "X = X[:5000]\n",
    "y = y[:5000]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"Dataset: {len(X_train)} training samples, {len(X_test)} test samples\")\n",
    "print(f\"Features: {X.shape[1]}\")\n",
    "print(f\"Target: {housing.target_names}\")\n",
    "print(f\"Target range: [{y.min():.2f}, {y.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Gradient Boosting Regressor\n",
    "gbr = GradientBoostingRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=RANDOM_STATE,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "gbr.fit(X_train, y_train)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = gbr.predict(X_train)\n",
    "y_test_pred = gbr.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"\\nGradient Boosting Regressor Performance:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Training time: {train_time:.2f} seconds\")\n",
    "print(f\"\\nTrain MSE: {train_mse:.4f}\")\n",
    "print(f\"Test MSE:  {test_mse:.4f}\")\n",
    "print(f\"\\nTrain R²: {train_r2:.4f}\")\n",
    "print(f\"Test R²:  {test_r2:.4f}\")\n",
    "\n",
    "# Compare with other models\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_test_r2 = rf.score(X_test, y_test)\n",
    "\n",
    "print(f\"\\nRandom Forest R² (baseline): {rf_test_r2:.4f}\")\n",
    "print(f\"Gradient Boosting improvement: {(test_r2 - rf_test_r2) / rf_test_r2 * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Predicted vs Actual\n",
    "axes[0].scatter(y_test, y_test_pred, alpha=0.5, s=20)\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "             'r--', linewidth=2, label='Perfect prediction')\n",
    "axes[0].set_xlabel('Actual Price', fontsize=11)\n",
    "axes[0].set_ylabel('Predicted Price', fontsize=11)\n",
    "axes[0].set_title(f'Predictions vs Actual (R²={test_r2:.3f})', fontsize=12, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals\n",
    "residuals = y_test - y_test_pred\n",
    "axes[1].scatter(y_test_pred, residuals, alpha=0.5, s=20)\n",
    "axes[1].axhline(0, color='red', linestyle='--', linewidth=2)\n",
    "axes[1].set_xlabel('Predicted Price', fontsize=11)\n",
    "axes[1].set_ylabel('Residuals', fontsize=11)\n",
    "axes[1].set_title('Residual Plot', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Key Hyperparameters\n",
    "\n",
    "### 4.1 Number of Estimators (n_estimators)\n",
    "- More trees → better training performance\n",
    "- But risk of overfitting!\n",
    "- Use early stopping or validation to find optimal number\n",
    "\n",
    "### 4.2 Learning Rate (learning_rate)\n",
    "- Controls contribution of each tree\n",
    "- Lower rate → needs more trees but generalizes better\n",
    "- Typical values: 0.01 to 0.3\n",
    "- **Trade-off**: learning_rate × n_estimators ≈ constant for similar performance\n",
    "\n",
    "### 4.3 Tree Depth (max_depth)\n",
    "- Controls complexity of each tree\n",
    "- Shallow trees (3-5): less prone to overfitting, need more trees\n",
    "- Deeper trees: can capture complex interactions, but risk overfitting\n",
    "- **Default**: 3 (works well in practice)\n",
    "\n",
    "### 4.4 Minimum Samples per Leaf (min_samples_leaf)\n",
    "- Regularization: prevents tiny leaf nodes\n",
    "- Higher values → simpler trees → less overfitting\n",
    "- Typical values: 1-20\n",
    "\n",
    "### 4.5 Subsample (subsample)\n",
    "- Fraction of samples to use for each tree (stochastic gradient boosting)\n",
    "- Values < 1.0 add randomness and reduce overfitting\n",
    "- Typical values: 0.5 to 1.0\n",
    "- **Benefit**: Faster training + better generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of learning rate\n",
    "learning_rates = [0.01, 0.05, 0.1, 0.2, 0.5]\n",
    "n_estimators = 200\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    gbr = GradientBoostingRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        learning_rate=lr,\n",
    "        max_depth=3,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    gbr.fit(X_train, y_train)\n",
    "    \n",
    "    train_scores.append(gbr.score(X_train, y_train))\n",
    "    test_scores.append(gbr.score(X_test, y_test))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(learning_rates, train_scores, marker='o', linewidth=2, markersize=8, label='Train R²')\n",
    "plt.plot(learning_rates, test_scores, marker='s', linewidth=2, markersize=8, label='Test R²')\n",
    "plt.xlabel('Learning Rate', fontsize=12)\n",
    "plt.ylabel('R² Score', fontsize=12)\n",
    "plt.title(f'Learning Rate Impact (n_estimators={n_estimators})', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xscale('log')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "best_lr = learning_rates[np.argmax(test_scores)]\n",
    "print(f\"\\nBest learning rate: {best_lr}\")\n",
    "print(f\"Test R²: {max(test_scores):.4f}\")\n",
    "print(\"\\nRule of thumb: Lower learning rate + more trees = better generalization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of max_depth\n",
    "max_depths = [1, 2, 3, 4, 5, 7, 10]\n",
    "\n",
    "depth_train_scores = []\n",
    "depth_test_scores = []\n",
    "\n",
    "for depth in max_depths:\n",
    "    gbr = GradientBoostingRegressor(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=depth,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    gbr.fit(X_train, y_train)\n",
    "    \n",
    "    depth_train_scores.append(gbr.score(X_train, y_train))\n",
    "    depth_test_scores.append(gbr.score(X_test, y_test))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(max_depths, depth_train_scores, marker='o', linewidth=2, markersize=8, label='Train R²')\n",
    "plt.plot(max_depths, depth_test_scores, marker='s', linewidth=2, markersize=8, label='Test R²')\n",
    "plt.axvline(3, color='red', linestyle='--', linewidth=1, alpha=0.7, label='Default (3)')\n",
    "plt.xlabel('Maximum Tree Depth', fontsize=12)\n",
    "plt.ylabel('R² Score', fontsize=12)\n",
    "plt.title('Tree Depth Impact', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "best_depth = max_depths[np.argmax(depth_test_scores)]\n",
    "print(f\"\\nBest max_depth: {best_depth}\")\n",
    "print(f\"Test R²: {max(depth_test_scores):.4f}\")\n",
    "print(\"\\nObservation: Deeper trees increase overfitting (train-test gap grows)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Regularization Techniques\n",
    "\n",
    "Gradient boosting can easily overfit. Multiple regularization strategies help:\n",
    "\n",
    "### 5.1 Learning Rate (Shrinkage)\n",
    "- Lower learning rate = stronger regularization\n",
    "- Prevents any single tree from having too much influence\n",
    "\n",
    "### 5.2 Tree Constraints\n",
    "- `max_depth`: Limit tree complexity\n",
    "- `min_samples_split`: Require minimum samples to split\n",
    "- `min_samples_leaf`: Require minimum samples in leaves\n",
    "- `max_features`: Random feature subsampling (like Random Forest)\n",
    "\n",
    "### 5.3 Subsampling (Stochastic Gradient Boosting)\n",
    "- `subsample < 1.0`: Use random subset of data for each tree\n",
    "- Adds variance → reduces overfitting\n",
    "- Also speeds up training!\n",
    "\n",
    "### 5.4 Early Stopping\n",
    "- Monitor validation performance\n",
    "- Stop when no improvement\n",
    "- Prevents wasting computation and overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate subsampling (stochastic gradient boosting)\n",
    "subsample_values = [0.3, 0.5, 0.7, 0.9, 1.0]\n",
    "\n",
    "subsample_train_scores = []\n",
    "subsample_test_scores = []\n",
    "subsample_times = []\n",
    "\n",
    "for subsample in subsample_values:\n",
    "    start = time.time()\n",
    "    gbr = GradientBoostingRegressor(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=3,\n",
    "        subsample=subsample,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    gbr.fit(X_train, y_train)\n",
    "    train_time = time.time() - start\n",
    "    \n",
    "    subsample_train_scores.append(gbr.score(X_train, y_train))\n",
    "    subsample_test_scores.append(gbr.score(X_test, y_test))\n",
    "    subsample_times.append(train_time)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy\n",
    "axes[0].plot(subsample_values, subsample_train_scores, marker='o', linewidth=2, label='Train R²')\n",
    "axes[0].plot(subsample_values, subsample_test_scores, marker='s', linewidth=2, label='Test R²')\n",
    "axes[0].set_xlabel('Subsample Ratio', fontsize=11)\n",
    "axes[0].set_ylabel('R² Score', fontsize=11)\n",
    "axes[0].set_title('Subsampling Effect on Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Training time\n",
    "axes[1].bar(range(len(subsample_values)), subsample_times, alpha=0.7, edgecolor='black')\n",
    "axes[1].set_xticks(range(len(subsample_values)))\n",
    "axes[1].set_xticklabels(subsample_values)\n",
    "axes[1].set_xlabel('Subsample Ratio', fontsize=11)\n",
    "axes[1].set_ylabel('Training Time (seconds)', fontsize=11)\n",
    "axes[1].set_title('Subsampling Effect on Speed', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "best_subsample = subsample_values[np.argmax(subsample_test_scores)]\n",
    "print(f\"\\nBest subsample: {best_subsample}\")\n",
    "print(f\"Test R²: {max(subsample_test_scores):.4f}\")\n",
    "print(f\"\\nSpeed improvement with subsample=0.5: {(subsample_times[-1]/subsample_times[1] - 1) * 100:.1f}%\")\n",
    "print(\"Subsampling often improves both speed AND generalization!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Early Stopping\n",
    "\n",
    "Early stopping monitors validation performance and stops training when it stops improving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training data into train/validation for early stopping\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Train with many estimators and track validation performance\n",
    "gbr_es = GradientBoostingRegressor(\n",
    "    n_estimators=500,  # Train many\n",
    "    learning_rate=0.05,\n",
    "    max_depth=3,\n",
    "    subsample=0.8,\n",
    "    random_state=RANDOM_STATE,\n",
    "    validation_fraction=0.2,  # Use 20% for validation\n",
    "    n_iter_no_change=10,  # Stop if no improvement for 10 iterations\n",
    "    tol=1e-4\n",
    ")\n",
    "\n",
    "gbr_es.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Early stopping results:\")\n",
    "print(f\"Estimators trained: {gbr_es.n_estimators_}\")\n",
    "print(f\"Training stopped early at iteration: {len(gbr_es.train_score_)}\")\n",
    "print(f\"\\nFinal test R²: {gbr_es.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize learning curves with early stopping\n",
    "# Train without early stopping to see full curve\n",
    "gbr_full = GradientBoostingRegressor(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=3,\n",
    "    subsample=0.8,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "gbr_full.fit(X_train_split, y_train_split)\n",
    "\n",
    "# Compute validation scores for each iteration\n",
    "val_scores = []\n",
    "for i, pred in enumerate(gbr_full.staged_predict(X_val_split)):\n",
    "    val_scores.append(r2_score(y_val_split, pred))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, len(gbr_full.train_score_) + 1), gbr_full.train_score_, \n",
    "         linewidth=2, label='Train Score', alpha=0.8)\n",
    "plt.plot(range(1, len(val_scores) + 1), val_scores, \n",
    "         linewidth=2, label='Validation Score', alpha=0.8)\n",
    "\n",
    "# Mark best validation score\n",
    "best_iter = np.argmax(val_scores) + 1\n",
    "plt.axvline(best_iter, color='red', linestyle='--', linewidth=2, \n",
    "            label=f'Best iteration: {best_iter}')\n",
    "\n",
    "plt.xlabel('Number of Boosting Iterations', fontsize=12)\n",
    "plt.ylabel('R² Score', fontsize=12)\n",
    "plt.title('Learning Curves: When to Stop?', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nOptimal stopping point: iteration {best_iter}\")\n",
    "print(f\"Best validation R²: {max(val_scores):.4f}\")\n",
    "print(f\"\\nContinuing to train beyond this point leads to overfitting!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Loss Function Comparison\n",
    "\n",
    "Create a regression dataset with 10% outliers (extreme values). Train Gradient Boosting models with different loss functions:\n",
    "- `loss='squared_error'` (default, sensitive to outliers)\n",
    "- `loss='absolute_error'` (robust to outliers)\n",
    "- `loss='huber'` (balanced)\n",
    "\n",
    "Compare:\n",
    "1. Performance metrics (MSE, MAE)\n",
    "2. Prediction errors on outliers vs normal points\n",
    "3. Visual comparison of predictions\n",
    "\n",
    "**Which loss function handles outliers best?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Hyperparameter Grid Search\n",
    "\n",
    "Perform a systematic grid search over:\n",
    "- `learning_rate`: [0.01, 0.05, 0.1]\n",
    "- `n_estimators`: [50, 100, 200]\n",
    "- `max_depth`: [3, 5, 7]\n",
    "- `subsample`: [0.8, 1.0]\n",
    "\n",
    "Use cross-validation to find the best combination. Create a visualization showing:\n",
    "1. Top 10 parameter combinations\n",
    "2. Parameter importance (which matters most?)\n",
    "3. Trade-off between n_estimators and learning_rate\n",
    "\n",
    "**Bonus**: Include training time in your analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Feature Importance Analysis\n",
    "\n",
    "Train a Gradient Boosting model on a classification dataset and analyze feature importance:\n",
    "\n",
    "1. Get impurity-based importance from the model\n",
    "2. Calculate permutation importance\n",
    "3. Visualize both and compare\n",
    "4. Remove the bottom 50% of features and retrain\n",
    "5. Compare performance and training time\n",
    "\n",
    "**Can you achieve similar performance with fewer features?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Learning Rate Schedule\n",
    "\n",
    "Implement a custom learning rate schedule (not built into sklearn):\n",
    "\n",
    "1. Start with high learning rate (e.g., 0.3)\n",
    "2. Train in stages, decreasing learning rate each stage:\n",
    "   - Stage 1: 50 estimators at lr=0.3\n",
    "   - Stage 2: 50 estimators at lr=0.1  \n",
    "   - Stage 3: 50 estimators at lr=0.03\n",
    "3. Use `warm_start=True` to continue training\n",
    "4. Compare with constant learning rate\n",
    "\n",
    "**Does this learning rate schedule improve performance?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Gradient Boosting Framework**:\n",
    "   - Generalization of AdaBoost to arbitrary loss functions\n",
    "   - Optimization in function space via gradient descent\n",
    "   - Each tree fits negative gradient (pseudo-residuals)\n",
    "   - Flexible: works for classification, regression, ranking, etc.\n",
    "\n",
    "2. **Algorithm**:\n",
    "   - Initialize with constant prediction\n",
    "   - For each iteration:\n",
    "     - Compute pseudo-residuals (negative gradient)\n",
    "     - Fit tree to residuals\n",
    "     - Update model with scaled tree prediction\n",
    "   - Final model: sum of all trees\n",
    "\n",
    "3. **Loss Functions**:\n",
    "   - Squared loss: Smooth, sensitive to outliers\n",
    "   - Absolute loss: Robust, constant gradient\n",
    "   - Huber loss: Best of both worlds\n",
    "   - Log loss: For classification\n",
    "\n",
    "4. **Key Hyperparameters**:\n",
    "   - `n_estimators`: Number of trees (use early stopping)\n",
    "   - `learning_rate`: Shrinkage (lower is better but needs more trees)\n",
    "   - `max_depth`: Tree complexity (3-5 works well)\n",
    "   - `subsample`: Stochastic gradient boosting (0.5-0.8 reduces overfitting)\n",
    "   - `min_samples_leaf`: Regularization\n",
    "\n",
    "5. **Regularization**:\n",
    "   - Learning rate (shrinkage)\n",
    "   - Tree constraints (depth, samples)\n",
    "   - Subsampling (stochastic GB)\n",
    "   - Early stopping\n",
    "   - **Multiple strategies recommended**: combine them!\n",
    "\n",
    "### Strengths\n",
    "\n",
    "- Often best performance on tabular data\n",
    "- Handles mixed feature types naturally\n",
    "- Built-in feature importance\n",
    "- Flexible loss functions\n",
    "- Robust to irrelevant features\n",
    "\n",
    "### Weaknesses\n",
    "\n",
    "- Sequential training (can't parallelize like Random Forest)\n",
    "- Sensitive to hyperparameters (needs tuning)\n",
    "- Can overfit easily without regularization\n",
    "- Slower training than Random Forest\n",
    "- Harder to interpret than single trees\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Start with these defaults**:\n",
    "   - `n_estimators=100-500` (use early stopping)\n",
    "   - `learning_rate=0.1`\n",
    "   - `max_depth=3-5`\n",
    "   - `subsample=0.8`\n",
    "   - `min_samples_leaf=5-10`\n",
    "\n",
    "2. **Tuning strategy**:\n",
    "   - First tune tree parameters (max_depth, min_samples_leaf)\n",
    "   - Then tune learning_rate and n_estimators together\n",
    "   - Finally tune subsample\n",
    "   - Use cross-validation throughout\n",
    "\n",
    "3. **For production**:\n",
    "   - Lower learning rate (0.01-0.05) for better generalization\n",
    "   - Use early stopping to save computation\n",
    "   - Monitor for overfitting with validation set\n",
    "   - Consider modern variants (XGBoost, LightGBM, CatBoost)\n",
    "\n",
    "### Comparison Summary\n",
    "\n",
    "| Method | Training | Bias/Variance | Regularization | Speed |\n",
    "|--------|----------|---------------|----------------|-------|\n",
    "| Random Forest | Parallel | Reduces variance | Natural (averaging) | Fast |\n",
    "| AdaBoost | Sequential | Reduces bias | Limited options | Medium |\n",
    "| Gradient Boosting | Sequential | Reduces bias | Many options | Medium |\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In **Module 05: XGBoost**, we'll explore:\n",
    "- Modern optimized gradient boosting\n",
    "- Advanced regularization (L1, L2, gamma)\n",
    "- Handling missing values\n",
    "- Built-in cross-validation\n",
    "- SHAP values for interpretability\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- **Original Paper**: \"Greedy Function Approximation: A Gradient Boosting Machine\" (Friedman, 2001)\n",
    "- **Tutorial**: \"Gradient Boosting from Scratch\" on StatQuest\n",
    "- **Documentation**: [Sklearn Gradient Boosting](https://scikit-learn.org/stable/modules/ensemble.html#gradient-boosting)\n",
    "- **Book**: \"The Elements of Statistical Learning\" Chapter 10 (Hastie et al.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
