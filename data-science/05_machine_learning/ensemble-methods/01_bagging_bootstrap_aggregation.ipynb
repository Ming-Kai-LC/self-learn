{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 01: Bagging and Bootstrap Aggregation\n",
    "\n",
    "**Difficulty**: ‚≠ê‚≠ê‚≠ê Advanced\n",
    "\n",
    "**Estimated Time**: 75 minutes\n",
    "\n",
    "**Prerequisites**: \n",
    "- Module 00: Introduction to Ensemble Learning\n",
    "- Understanding of decision trees\n",
    "- Basic probability and sampling concepts\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Understand bootstrap sampling and why it creates model diversity\n",
    "2. Implement bagging from scratch and with scikit-learn\n",
    "3. Explain how bagging reduces variance without increasing bias\n",
    "4. Use out-of-bag (OOB) error estimation for model evaluation\n",
    "5. Determine optimal number of estimators for bagging ensembles\n",
    "6. Apply bagging to both classification and regression problems\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "# Scikit-learn models and utilities\n",
    "from sklearn.datasets import make_classification, make_regression, load_wine\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, \n",
    "    cross_val_score,\n",
    "    learning_curve\n",
    ")\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingClassifier, BaggingRegressor\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 4)\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding Bootstrap Sampling\n",
    "\n",
    "### What is Bootstrap?\n",
    "\n",
    "**Bootstrap sampling** is a resampling technique where we:\n",
    "1. Randomly sample n observations from dataset of size n\n",
    "2. Sample **with replacement** (same observation can appear multiple times)\n",
    "3. Create multiple different training sets from the same original data\n",
    "\n",
    "### Why Bootstrap?\n",
    "\n",
    "**Problem**: We want to train multiple models for ensemble, but we only have one dataset.\n",
    "\n",
    "**Solution**: Create multiple \"synthetic\" datasets through bootstrap sampling.\n",
    "\n",
    "### Mathematical Properties\n",
    "\n",
    "**Key insight**: Each bootstrap sample contains approximately **63.2%** unique observations from original data.\n",
    "\n",
    "Probability an observation is **selected** in one draw: $\\frac{1}{n}$\n",
    "\n",
    "Probability it's **NOT selected** in one draw: $1 - \\frac{1}{n}$\n",
    "\n",
    "Probability it's **NOT selected** in n draws: $\\left(1 - \\frac{1}{n}\\right)^n$\n",
    "\n",
    "As $n \\to \\infty$: $\\left(1 - \\frac{1}{n}\\right)^n \\to \\frac{1}{e} \\approx 0.368$\n",
    "\n",
    "Therefore, probability observation **IS selected**: $1 - 0.368 = 0.632$ ‚ú®\n",
    "\n",
    "### Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate bootstrap sampling\n",
    "original_data = np.array(['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'])\n",
    "n_samples = len(original_data)\n",
    "\n",
    "# Create 5 bootstrap samples\n",
    "n_bootstraps = 5\n",
    "bootstrap_samples = []\n",
    "\n",
    "print(f\"Original dataset: {original_data}\\n\")\n",
    "print(\"Bootstrap Samples:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i in range(n_bootstraps):\n",
    "    # Sample with replacement\n",
    "    bootstrap_idx = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "    bootstrap_sample = original_data[bootstrap_idx]\n",
    "    bootstrap_samples.append(bootstrap_sample)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    unique_items = np.unique(bootstrap_sample)\n",
    "    unique_pct = len(unique_items) / n_samples * 100\n",
    "    \n",
    "    # Show sample composition\n",
    "    counter = Counter(bootstrap_sample)\n",
    "    \n",
    "    print(f\"\\nSample {i+1}: {bootstrap_sample}\")\n",
    "    print(f\"  Unique items: {len(unique_items)}/10 ({unique_pct:.1f}%)\")\n",
    "    print(f\"  Item counts: {dict(counter)}\")\n",
    "    print(f\"  Missing from original: {set(original_data) - set(unique_items)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\\nüìä Key Observations:\")\n",
    "print(\"  1. Each sample has same size as original (10)\")\n",
    "print(\"  2. Some items appear multiple times (duplication)\")\n",
    "print(\"  3. Some original items are missing (out-of-bag samples)\")\n",
    "print(\"  4. Each sample is different (diversity!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify the 63.2% Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empirically verify the 63.2% rule\n",
    "def calculate_unique_percentage(n_samples, n_bootstraps=1000):\n",
    "    \"\"\"\n",
    "    Calculate average percentage of unique samples in bootstrap.\n",
    "    \n",
    "    Args:\n",
    "        n_samples: Size of original dataset\n",
    "        n_bootstraps: Number of bootstrap samples to create\n",
    "    \n",
    "    Returns:\n",
    "        Average percentage of unique samples\n",
    "    \"\"\"\n",
    "    unique_percentages = []\n",
    "    \n",
    "    for _ in range(n_bootstraps):\n",
    "        # Create bootstrap sample\n",
    "        bootstrap_idx = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "        unique_count = len(np.unique(bootstrap_idx))\n",
    "        unique_pct = unique_count / n_samples * 100\n",
    "        unique_percentages.append(unique_pct)\n",
    "    \n",
    "    return np.mean(unique_percentages)\n",
    "\n",
    "# Test with different dataset sizes\n",
    "dataset_sizes = [10, 50, 100, 500, 1000, 5000]\n",
    "results = []\n",
    "\n",
    "for size in dataset_sizes:\n",
    "    avg_pct = calculate_unique_percentage(size)\n",
    "    results.append(avg_pct)\n",
    "    print(f\"n={size:5d}: {avg_pct:.2f}% unique (expected: 63.2%)\")\n",
    "\n",
    "# Visualize convergence to 63.2%\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(dataset_sizes, results, marker='o', linewidth=2, markersize=8, label='Empirical')\n",
    "plt.axhline(y=63.2, color='red', linestyle='--', linewidth=2, label='Theoretical (63.2%)')\n",
    "plt.xlabel('Dataset Size (n)', fontsize=12)\n",
    "plt.ylabel('% Unique Samples in Bootstrap', fontsize=12)\n",
    "plt.title('Bootstrap Sampling: Convergence to 63.2% Rule', fontsize=14, fontweight='bold')\n",
    "plt.xscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ As dataset size increases, percentage converges to 63.2%!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Exercise 1: Bootstrap Sampling Properties\n",
    "\n",
    "Explore bootstrap sampling behavior:\n",
    "\n",
    "1. Create a dataset of 100 samples\n",
    "2. Generate 50 bootstrap samples\n",
    "3. For each original sample, count how many times it appears across all bootstrap samples\n",
    "4. Plot the distribution of counts\n",
    "5. What distribution does this follow? (Hint: Think about repeated independent trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bagging: Bootstrap Aggregating\n",
    "\n",
    "### The Bagging Algorithm\n",
    "\n",
    "**Bootstrap Aggregating (Bagging)** combines bootstrap sampling with model aggregation:\n",
    "\n",
    "1. **Bootstrap**: Create B bootstrap samples from training data\n",
    "2. **Train**: Train a model on each bootstrap sample\n",
    "3. **Aggregate**: Combine predictions\n",
    "   - Classification: Majority vote\n",
    "   - Regression: Average predictions\n",
    "\n",
    "### Why Bagging Reduces Variance\n",
    "\n",
    "**Single Model Variance**: $\\sigma^2$\n",
    "\n",
    "**Averaged Model Variance** (if models are independent): $\\frac{\\sigma^2}{B}$\n",
    "\n",
    "**Key**: Even if models are partially correlated, averaging still reduces variance!\n",
    "\n",
    "If correlation is $\\rho$:\n",
    "\n",
    "$$\\text{Ensemble Variance} = \\rho\\sigma^2 + \\frac{1-\\rho}{B}\\sigma^2$$\n",
    "\n",
    "### Bagging from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBaggingClassifier:\n",
    "    \"\"\"\n",
    "    Simple implementation of Bagging Classifier from scratch.\n",
    "    \n",
    "    This helps understand the core algorithm before using sklearn.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_estimator, n_estimators=10, random_state=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            base_estimator: Base model to use (e.g., DecisionTreeClassifier)\n",
    "            n_estimators: Number of models in ensemble\n",
    "            random_state: Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.base_estimator = base_estimator\n",
    "        self.n_estimators = n_estimators\n",
    "        self.random_state = random_state\n",
    "        self.estimators_ = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train ensemble on data.\n",
    "        \n",
    "        Args:\n",
    "            X: Feature matrix (n_samples, n_features)\n",
    "            y: Target vector (n_samples,)\n",
    "        \"\"\"\n",
    "        # Set random seed if provided\n",
    "        if self.random_state is not None:\n",
    "            np.random.seed(self.random_state)\n",
    "        \n",
    "        n_samples = X.shape[0]\n",
    "        self.estimators_ = []\n",
    "        \n",
    "        for i in range(self.n_estimators):\n",
    "            # Create bootstrap sample\n",
    "            bootstrap_idx = np.random.choice(\n",
    "                n_samples, \n",
    "                size=n_samples, \n",
    "                replace=True\n",
    "            )\n",
    "            X_bootstrap = X[bootstrap_idx]\n",
    "            y_bootstrap = y[bootstrap_idx]\n",
    "            \n",
    "            # Train model on bootstrap sample\n",
    "            # Note: We need to clone the base estimator to avoid retraining same model\n",
    "            from sklearn.base import clone\n",
    "            estimator = clone(self.base_estimator)\n",
    "            estimator.fit(X_bootstrap, y_bootstrap)\n",
    "            \n",
    "            self.estimators_.append(estimator)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict using majority voting.\n",
    "        \n",
    "        Args:\n",
    "            X: Feature matrix (n_samples, n_features)\n",
    "        \n",
    "        Returns:\n",
    "            predictions: Predicted classes (n_samples,)\n",
    "        \"\"\"\n",
    "        # Get predictions from all estimators\n",
    "        predictions = np.array([\n",
    "            estimator.predict(X) \n",
    "            for estimator in self.estimators_\n",
    "        ])\n",
    "        \n",
    "        # Majority vote: take mode along axis 0 (across estimators)\n",
    "        from scipy import stats\n",
    "        majority_vote = stats.mode(predictions, axis=0, keepdims=True)[0].flatten()\n",
    "        \n",
    "        return majority_vote\n",
    "\n",
    "# Test our implementation\n",
    "X, y = make_classification(\n",
    "    n_samples=500, \n",
    "    n_features=10, \n",
    "    n_informative=8,\n",
    "    random_state=42\n",
    ")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Compare single tree vs our bagging implementation\n",
    "single_tree = DecisionTreeClassifier(max_depth=10, random_state=42)\n",
    "single_tree.fit(X_train, y_train)\n",
    "single_pred = single_tree.predict(X_test)\n",
    "single_acc = accuracy_score(y_test, single_pred)\n",
    "\n",
    "bagging = SimpleBaggingClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=10),\n",
    "    n_estimators=10,\n",
    "    random_state=42\n",
    ")\n",
    "bagging.fit(X_train, y_train)\n",
    "bagging_pred = bagging.predict(X_test)\n",
    "bagging_acc = accuracy_score(y_test, bagging_pred)\n",
    "\n",
    "print(\"üìä Performance Comparison:\")\n",
    "print(f\"Single Decision Tree: {single_acc:.4f}\")\n",
    "print(f\"Bagging (10 trees):   {bagging_acc:.4f}\")\n",
    "print(f\"Improvement:          {bagging_acc - single_acc:.4f}\")\n",
    "print(f\"\\n‚úÖ Bagging reduces overfitting and improves generalization!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Scikit-learn BaggingClassifier\n",
    "\n",
    "Now let's use the professional implementation from scikit-learn, which offers:\n",
    "- Feature subsampling (in addition to sample subsampling)\n",
    "- Out-of-bag score estimation\n",
    "- Parallel processing\n",
    "- More efficient implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a real dataset\n",
    "wine = load_wine()\n",
    "X_wine = wine.data\n",
    "y_wine = wine.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_wine, y_wine, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Dataset: {wine.DESCR.split('**')[1].split('**')[0].strip()}\")\n",
    "print(f\"Samples: {len(X_wine)}\")\n",
    "print(f\"Features: {len(wine.feature_names)}\")\n",
    "print(f\"Classes: {wine.target_names}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "for i, name in enumerate(wine.target_names):\n",
    "    count = np.sum(y_wine == i)\n",
    "    print(f\"  {name}: {count} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train BaggingClassifier\n",
    "bagging_clf = BaggingClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=10),\n",
    "    n_estimators=50,\n",
    "    max_samples=1.0,  # Use 100% of samples for each bootstrap\n",
    "    max_features=1.0,  # Use 100% of features\n",
    "    bootstrap=True,  # Sample with replacement\n",
    "    bootstrap_features=False,  # Don't bootstrap features\n",
    "    oob_score=True,  # Calculate out-of-bag score\n",
    "    random_state=42,\n",
    "    n_jobs=-1  # Use all CPU cores\n",
    ")\n",
    "\n",
    "# Train\n",
    "bagging_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "train_acc = bagging_clf.score(X_train, y_train)\n",
    "test_acc = bagging_clf.score(X_test, y_test)\n",
    "oob_acc = bagging_clf.oob_score_\n",
    "\n",
    "print(\"\\nüìä Bagging Performance:\")\n",
    "print(f\"Training Accuracy:   {train_acc:.4f}\")\n",
    "print(f\"Test Accuracy:       {test_acc:.4f}\")\n",
    "print(f\"OOB Accuracy:        {oob_acc:.4f}\")\n",
    "print(f\"\\nNote: OOB score is close to test score without needing a validation set!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Exercise 2: Hyperparameter Exploration\n",
    "\n",
    "Experiment with BaggingClassifier hyperparameters:\n",
    "\n",
    "1. **max_samples**: Try [0.5, 0.7, 1.0] - what happens with smaller bootstrap samples?\n",
    "2. **max_features**: Try [0.5, 0.7, 1.0] - does feature subsampling help?\n",
    "3. **base estimator depth**: Try max_depth=[3, 5, 10, 20] - which works best?\n",
    "\n",
    "Create a visualization comparing different configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Out-of-Bag (OOB) Error Estimation\n",
    "\n",
    "### What is OOB Error?\n",
    "\n",
    "Remember: Each bootstrap sample contains ~63.2% of original data. The remaining ~36.8% are **out-of-bag (OOB) samples**.\n",
    "\n",
    "**OOB Score Calculation**:\n",
    "1. For each training sample, find all models that didn't use it during training\n",
    "2. Get predictions from only those models (like a validation set)\n",
    "3. Calculate accuracy using these OOB predictions\n",
    "\n",
    "**Benefits**:\n",
    "- Free validation score without needing separate validation set\n",
    "- More training data (no need to hold out validation set)\n",
    "- Unbiased estimate of generalization error\n",
    "\n",
    "### OOB vs Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare OOB score with cross-validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import time\n",
    "\n",
    "# Create bagging model with OOB\n",
    "bagging_oob = BaggingClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=8),\n",
    "    n_estimators=100,\n",
    "    oob_score=True,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Measure OOB time\n",
    "start = time.time()\n",
    "bagging_oob.fit(X_train, y_train)\n",
    "oob_time = time.time() - start\n",
    "oob_score = bagging_oob.oob_score_\n",
    "\n",
    "# Create bagging model for CV\n",
    "bagging_cv = BaggingClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=8),\n",
    "    n_estimators=100,\n",
    "    oob_score=False,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Measure CV time\n",
    "start = time.time()\n",
    "cv_scores = cross_val_score(bagging_cv, X_train, y_train, cv=5, n_jobs=-1)\n",
    "cv_time = time.time() - start\n",
    "cv_score = cv_scores.mean()\n",
    "\n",
    "# Compare\n",
    "print(\"\\n‚ö° OOB vs Cross-Validation Comparison:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nOOB Score:\")\n",
    "print(f\"  Accuracy: {oob_score:.4f}\")\n",
    "print(f\"  Time:     {oob_time:.2f}s\")\n",
    "print(f\"\\n5-Fold CV Score:\")\n",
    "print(f\"  Accuracy: {cv_score:.4f} (¬±{cv_scores.std():.4f})\")\n",
    "print(f\"  Time:     {cv_time:.2f}s\")\n",
    "print(f\"\\nüí° OOB is {cv_time/oob_time:.1f}√ó faster and gives similar accuracy!\")\n",
    "\n",
    "# Visualize\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Accuracy comparison\n",
    "methods = ['OOB', '5-Fold CV']\n",
    "scores = [oob_score, cv_score]\n",
    "ax1.bar(methods, scores, color=['skyblue', 'lightcoral'])\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_title('Accuracy Comparison')\n",
    "ax1.set_ylim(0.8, 1.0)\n",
    "for i, v in enumerate(scores):\n",
    "    ax1.text(i, v + 0.01, f'{v:.4f}', ha='center', fontweight='bold')\n",
    "\n",
    "# Time comparison\n",
    "times = [oob_time, cv_time]\n",
    "ax2.bar(methods, times, color=['skyblue', 'lightcoral'])\n",
    "ax2.set_ylabel('Time (seconds)')\n",
    "ax2.set_title('Computation Time Comparison')\n",
    "for i, v in enumerate(times):\n",
    "    ax2.text(i, v + 0.1, f'{v:.2f}s', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Exercise 3: OOB Score Stability\n",
    "\n",
    "Investigate how OOB score changes with number of estimators:\n",
    "\n",
    "1. Train bagging models with n_estimators = [10, 25, 50, 100, 200, 500]\n",
    "2. Record OOB score and test score for each\n",
    "3. Plot both scores vs number of estimators\n",
    "4. At what point does OOB score stabilize?\n",
    "5. How well does OOB score predict test score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Optimal Number of Estimators\n",
    "\n",
    "### How Many Models Should We Bag?\n",
    "\n",
    "Trade-offs:\n",
    "- **More estimators** ‚Üí Better performance (up to a point)\n",
    "- **More estimators** ‚Üí Longer training and prediction time\n",
    "- **More estimators** ‚Üí Higher memory usage\n",
    "\n",
    "**General rule**: Performance improvement plateaus after a certain point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Study effect of number of estimators\n",
    "n_estimators_range = [1, 5, 10, 25, 50, 100, 150, 200, 300, 500]\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "oob_scores = []\n",
    "training_times = []\n",
    "\n",
    "for n_est in n_estimators_range:\n",
    "    # Train model\n",
    "    start = time.time()\n",
    "    model = BaggingClassifier(\n",
    "        estimator=DecisionTreeClassifier(max_depth=10),\n",
    "        n_estimators=n_est,\n",
    "        oob_score=True,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    # Record scores\n",
    "    train_scores.append(model.score(X_train, y_train))\n",
    "    test_scores.append(model.score(X_test, y_test))\n",
    "    oob_scores.append(model.oob_score_)\n",
    "    training_times.append(elapsed)\n",
    "    \n",
    "    print(f\"n={n_est:3d}: Train={train_scores[-1]:.4f}, Test={test_scores[-1]:.4f}, \"\n",
    "          f\"OOB={oob_scores[-1]:.4f}, Time={elapsed:.2f}s\")\n",
    "\n",
    "# Visualize\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy vs number of estimators\n",
    "ax1.plot(n_estimators_range, train_scores, marker='o', label='Training', linewidth=2)\n",
    "ax1.plot(n_estimators_range, test_scores, marker='s', label='Test', linewidth=2)\n",
    "ax1.plot(n_estimators_range, oob_scores, marker='^', label='OOB', linewidth=2, linestyle='--')\n",
    "ax1.set_xlabel('Number of Estimators', fontsize=12)\n",
    "ax1.set_ylabel('Accuracy', fontsize=12)\n",
    "ax1.set_title('Performance vs Ensemble Size', fontsize=13, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xscale('log')\n",
    "\n",
    "# Training time vs number of estimators\n",
    "ax2.plot(n_estimators_range, training_times, marker='o', color='red', linewidth=2)\n",
    "ax2.set_xlabel('Number of Estimators', fontsize=12)\n",
    "ax2.set_ylabel('Training Time (seconds)', fontsize=12)\n",
    "ax2.set_title('Computational Cost vs Ensemble Size', fontsize=13, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find optimal point (elbow)\n",
    "best_idx = np.argmax(test_scores)\n",
    "best_n = n_estimators_range[best_idx]\n",
    "best_score = test_scores[best_idx]\n",
    "\n",
    "print(f\"\\n‚úÖ Optimal configuration:\")\n",
    "print(f\"  n_estimators: {best_n}\")\n",
    "print(f\"  Test accuracy: {best_score:.4f}\")\n",
    "print(f\"\\nüí° Beyond {best_n} estimators, improvement is marginal!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Bagging for Regression\n",
    "\n",
    "Bagging works equally well for regression problems:\n",
    "- Bootstrap sampling stays the same\n",
    "- Aggregation uses **averaging** instead of voting\n",
    "- Reduces variance in predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create regression dataset\n",
    "X_reg, y_reg = make_regression(\n",
    "    n_samples=400,\n",
    "    n_features=10,\n",
    "    n_informative=8,\n",
    "    noise=20,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Single decision tree regressor\n",
    "single_tree_reg = DecisionTreeRegressor(max_depth=10, random_state=42)\n",
    "single_tree_reg.fit(X_train_reg, y_train_reg)\n",
    "single_pred = single_tree_reg.predict(X_test_reg)\n",
    "single_mse = mean_squared_error(y_test_reg, single_pred)\n",
    "single_r2 = r2_score(y_test_reg, single_pred)\n",
    "\n",
    "# Bagging regressor\n",
    "bagging_reg = BaggingRegressor(\n",
    "    estimator=DecisionTreeRegressor(max_depth=10),\n",
    "    n_estimators=100,\n",
    "    oob_score=True,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "bagging_reg.fit(X_train_reg, y_train_reg)\n",
    "bagging_pred = bagging_reg.predict(X_test_reg)\n",
    "bagging_mse = mean_squared_error(y_test_reg, bagging_pred)\n",
    "bagging_r2 = r2_score(y_test_reg, bagging_pred)\n",
    "\n",
    "# Compare\n",
    "print(\"\\nüìä Regression Performance:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nSingle Decision Tree:\")\n",
    "print(f\"  MSE: {single_mse:.2f}\")\n",
    "print(f\"  R¬≤:  {single_r2:.4f}\")\n",
    "print(f\"\\nBagging (100 trees):\")\n",
    "print(f\"  MSE: {bagging_mse:.2f}\")\n",
    "print(f\"  R¬≤:  {bagging_r2:.4f}\")\n",
    "print(f\"  OOB R¬≤: {bagging_reg.oob_score_:.4f}\")\n",
    "print(f\"\\n‚úÖ MSE reduction: {(1 - bagging_mse/single_mse)*100:.1f}%\")\n",
    "\n",
    "# Visualize predictions\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Single tree\n",
    "ax1.scatter(y_test_reg, single_pred, alpha=0.6, edgecolors='black')\n",
    "ax1.plot([y_test_reg.min(), y_test_reg.max()], \n",
    "         [y_test_reg.min(), y_test_reg.max()], \n",
    "         'r--', linewidth=2, label='Perfect Prediction')\n",
    "ax1.set_xlabel('True Values', fontsize=12)\n",
    "ax1.set_ylabel('Predicted Values', fontsize=12)\n",
    "ax1.set_title(f'Single Tree (R¬≤={single_r2:.4f})', fontsize=13, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Bagging\n",
    "ax2.scatter(y_test_reg, bagging_pred, alpha=0.6, edgecolors='black', color='green')\n",
    "ax2.plot([y_test_reg.min(), y_test_reg.max()], \n",
    "         [y_test_reg.min(), y_test_reg.max()], \n",
    "         'r--', linewidth=2, label='Perfect Prediction')\n",
    "ax2.set_xlabel('True Values', fontsize=12)\n",
    "ax2.set_ylabel('Predicted Values', fontsize=12)\n",
    "ax2.set_title(f'Bagging (R¬≤={bagging_r2:.4f})', fontsize=13, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Exercise 4: Variance Reduction in Regression\n",
    "\n",
    "Empirically demonstrate variance reduction:\n",
    "\n",
    "1. Train 20 different single decision trees (different random_state)\n",
    "2. Get predictions from all 20 trees on test set\n",
    "3. Calculate variance of predictions for each test sample\n",
    "4. Train bagging model with 20 estimators\n",
    "5. Compare prediction variance between individual trees and bagging\n",
    "6. Visualize the variance reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Next Steps\n",
    "\n",
    "### üéì Key Takeaways\n",
    "\n",
    "1. **Bootstrap Sampling**:\n",
    "   - Samples with replacement create diverse training sets\n",
    "   - Each bootstrap contains ~63.2% unique samples\n",
    "   - Remaining ~36.8% are out-of-bag (OOB) samples\n",
    "\n",
    "2. **Bagging Algorithm**:\n",
    "   - Train multiple models on bootstrap samples\n",
    "   - Aggregate via voting (classification) or averaging (regression)\n",
    "   - Reduces variance without increasing bias\n",
    "\n",
    "3. **Out-of-Bag Estimation**:\n",
    "   - Free validation score using OOB samples\n",
    "   - Faster than cross-validation\n",
    "   - Provides unbiased generalization estimate\n",
    "\n",
    "4. **Hyperparameters**:\n",
    "   - `n_estimators`: More is better (with diminishing returns)\n",
    "   - `max_samples`: Usually 1.0 works well\n",
    "   - `max_features`: Can help with very high-dimensional data\n",
    "   - Base estimator: Use high-variance models (deep trees)\n",
    "\n",
    "5. **When to Use Bagging**:\n",
    "   - ‚úÖ Base model has high variance (overfitting)\n",
    "   - ‚úÖ Sufficient training data available\n",
    "   - ‚úÖ Want to reduce overfitting without changing base model\n",
    "   - ‚ùå Base model has high bias (underfitting) - use boosting instead\n",
    "\n",
    "### üìö What's Next?\n",
    "\n",
    "- **Module 02**: Random Forests (specialized bagging with feature randomness)\n",
    "- **Module 03**: AdaBoost (sequential ensemble that reduces bias)\n",
    "- **Module 04**: Gradient Boosting (more powerful sequential ensemble)\n",
    "\n",
    "### üéØ Practice Recommendations\n",
    "\n",
    "1. Apply bagging to your own dataset\n",
    "2. Compare single model vs bagging performance\n",
    "3. Use OOB score for model selection\n",
    "4. Experiment with different base estimators (SVM, KNN, etc.)\n",
    "\n",
    "### üìñ Additional Resources\n",
    "\n",
    "- **Original Paper**: Breiman, L. (1996). \"Bagging Predictors\"\n",
    "- **Sklearn User Guide**: https://scikit-learn.org/stable/modules/ensemble.html#bagging\n",
    "- **Elements of Statistical Learning**: Chapter 8 (Model Averaging)\n",
    "\n",
    "---\n",
    "\n",
    "**üöÄ Ready for Random Forests? Let's move to Module 02!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
