{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 02: Data Preparation and Train/Test Split\n",
    "\n",
    "**Difficulty**: ⭐ Beginner  \n",
    "**Estimated Time**: 55 minutes  \n",
    "**Prerequisites**: \n",
    "- [Module 00: Introduction to ML and scikit-learn](00_introduction_to_ml_and_sklearn.ipynb)\n",
    "- [Module 01: Supervised vs Unsupervised Learning](01_supervised_vs_unsupervised_learning.ipynb)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Understand why we split data into training and testing sets\n",
    "2. Properly split data using train_test_split()\n",
    "3. Handle missing values and categorical data\n",
    "4. Scale and normalize features appropriately\n",
    "5. Avoid data leakage - one of the most common ML mistakes\n",
    "6. Prepare data following best practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Split Data?\n",
    "\n",
    "### The Golden Rule of Machine Learning\n",
    "**Never test your model on the same data you used to train it!**\n",
    "\n",
    "### Analogy: Studying for an Exam\n",
    "- **Training data** = Practice problems you study\n",
    "- **Testing data** = Actual exam questions (different from practice)\n",
    "- **Goal** = Perform well on new, unseen questions\n",
    "\n",
    "If the exam had the exact same questions you practiced, you'd get 100% but wouldn't prove you learned the concepts!\n",
    "\n",
    "### The Problem: Overfitting\n",
    "If we evaluate on training data, the model might just **memorize** the answers instead of learning general patterns. This is called **overfitting**.\n",
    "\n",
    "**Training Data Performance** ≠ **Real-World Performance**\n",
    "\n",
    "We need to simulate real-world conditions by holding out some data for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Visualization settings\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"✓ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Train/Test Split\n",
    "\n",
    "### Common Split Ratios\n",
    "- **70/30 split**: 70% training, 30% testing (common)\n",
    "- **80/20 split**: 80% training, 20% testing (also common)\n",
    "- **60/20/20 split**: 60% train, 20% validation, 20% test (for hyperparameter tuning)\n",
    "\n",
    "### Choosing the Right Ratio\n",
    "- **More training data** → Better model learning\n",
    "- **More testing data** → More reliable evaluation\n",
    "- **Large datasets (>10,000 samples)** → Can use 90/10 or 95/5\n",
    "- **Small datasets (<1,000 samples)** → Use 70/30 or cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris_df = pd.read_csv('data/sample/iris.csv')\n",
    "\n",
    "# Separate features and target\n",
    "feature_cols = ['sepal length (cm)', 'sepal width (cm)', \n",
    "                'petal length (cm)', 'petal width (cm)']\n",
    "X = iris_df[feature_cols]\n",
    "y = iris_df['species']\n",
    "\n",
    "print(f\"Total dataset size: {len(X)} samples\")\n",
    "print(f\"Number of features: {X.shape[1]}\")\n",
    "print(f\"Number of classes: {y.nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a basic train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.3,  # 30% for testing\n",
    "    random_state=42  # For reproducibility\n",
    ")\n",
    "\n",
    "print(\"Data Split Results:\")\n",
    "print(f\"Training set: {len(X_train)} samples ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"Testing set: {len(X_test)} samples ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTraining features shape: {X_train.shape}\")\n",
    "print(f\"Testing features shape: {X_test.shape}\")\n",
    "print(f\"Training target shape: {y_train.shape}\")\n",
    "print(f\"Testing target shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Stratified Split for Classification\n",
    "\n",
    "When dealing with classification, especially with **imbalanced classes**, we want to ensure each split has the same proportion of each class.\n",
    "\n",
    "**Problem**: Random split might give different class distributions in train/test\n",
    "\n",
    "**Solution**: Use `stratify` parameter to maintain class proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class distribution in original data\n",
    "print(\"Original class distribution:\")\n",
    "print(y.value_counts(normalize=True).sort_index())\n",
    "\n",
    "# Regular split (without stratification)\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(\"\\nRegular split - Training set distribution:\")\n",
    "print(y_train_reg.value_counts(normalize=True).sort_index())\n",
    "print(\"\\nRegular split - Testing set distribution:\")\n",
    "print(y_test_reg.value_counts(normalize=True).sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified split (maintaining class proportions)\n",
    "X_train_strat, X_test_strat, y_train_strat, y_test_strat = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.3, \n",
    "    random_state=42,\n",
    "    stratify=y  # This ensures proportional class distribution\n",
    ")\n",
    "\n",
    "print(\"Stratified split - Training set distribution:\")\n",
    "print(y_train_strat.value_counts(normalize=True).sort_index())\n",
    "print(\"\\nStratified split - Testing set distribution:\")\n",
    "print(y_test_strat.value_counts(normalize=True).sort_index())\n",
    "\n",
    "print(\"\\n✓ Notice: Stratified split maintains the same proportions in both sets!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Handling Missing Values\n",
    "\n",
    "Real-world data often has missing values. We need to handle them before training.\n",
    "\n",
    "### Common Strategies\n",
    "1. **Drop rows** with missing values (if few)\n",
    "2. **Impute** with mean, median, or mode\n",
    "3. **Forward/backward fill** for time series\n",
    "4. **Use algorithms** that handle missing values (e.g., XGBoost)\n",
    "\n",
    "### WARNING: The Order Matters!\n",
    "**ALWAYS split data BEFORE imputing to avoid data leakage!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset with missing values\n",
    "# Load diabetes dataset and artificially introduce missing values\n",
    "diabetes_df = pd.read_csv('data/sample/diabetes.csv')\n",
    "\n",
    "# Randomly set 10% of values to NaN\n",
    "diabetes_missing = diabetes_df.copy()\n",
    "n_missing = int(0.1 * diabetes_missing.shape[0] * diabetes_missing.shape[1])\n",
    "np.random.seed(42)\n",
    "for _ in range(n_missing):\n",
    "    row = np.random.randint(0, len(diabetes_missing))\n",
    "    col = np.random.choice(diabetes_missing.columns[:-1])  # Don't make target missing\n",
    "    diabetes_missing.loc[row, col] = np.nan\n",
    "\n",
    "print(f\"Missing values per column:\")\n",
    "print(diabetes_missing.isnull().sum())\n",
    "print(f\"\\nTotal missing values: {diabetes_missing.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORRECT WAY: Split THEN impute\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Separate features and target\n",
    "X_missing = diabetes_missing.drop('progression', axis=1)\n",
    "y_missing = diabetes_missing['progression']\n",
    "\n",
    "# Step 1: Split the data FIRST\n",
    "X_train_miss, X_test_miss, y_train_miss, y_test_miss = train_test_split(\n",
    "    X_missing, y_missing, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Step 1: Split complete\")\n",
    "print(f\"Training set missing values: {X_train_miss.isnull().sum().sum()}\")\n",
    "print(f\"Testing set missing values: {X_test_miss.isnull().sum().sum()}\")\n",
    "\n",
    "# Step 2: Fit imputer on training data ONLY\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "imputer.fit(X_train_miss)\n",
    "\n",
    "print(\"\\nStep 2: Imputer fitted on training data\")\n",
    "print(f\"Learned means: {imputer.statistics_[:3]}...\")  # Show first 3\n",
    "\n",
    "# Step 3: Transform both sets using the same imputer\n",
    "X_train_imputed = pd.DataFrame(\n",
    "    imputer.transform(X_train_miss),\n",
    "    columns=X_train_miss.columns\n",
    ")\n",
    "X_test_imputed = pd.DataFrame(\n",
    "    imputer.transform(X_test_miss),\n",
    "    columns=X_test_miss.columns\n",
    ")\n",
    "\n",
    "print(\"\\nStep 3: Imputation complete\")\n",
    "print(f\"Training set missing values: {X_train_imputed.isnull().sum().sum()}\")\n",
    "print(f\"Testing set missing values: {X_test_imputed.isnull().sum().sum()}\")\n",
    "print(\"\\n✓ All missing values handled correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Scaling\n",
    "\n",
    "Many ML algorithms perform better when features are on similar scales.\n",
    "\n",
    "### Why Scale?\n",
    "- Features with large ranges can dominate the model\n",
    "- Distance-based algorithms (KNN, SVM) are sensitive to scale\n",
    "- Gradient descent converges faster with scaled features\n",
    "\n",
    "### Two Common Methods\n",
    "1. **Standardization (Z-score)**: Mean=0, Std=1\n",
    "   - Formula: (x - mean) / std\n",
    "   - Use when: Data is normally distributed\n",
    "   \n",
    "2. **Normalization (Min-Max)**: Scale to [0, 1]\n",
    "   - Formula: (x - min) / (max - min)\n",
    "   - Use when: Data has bounds or you need [0, 1] range\n",
    "\n",
    "### CRITICAL: Fit on Training, Transform on Both\n",
    "Calculate scaling parameters from **training data only** to avoid data leakage!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate why scaling matters\n",
    "# Load housing data with features of different scales\n",
    "housing_df = pd.read_csv('data/sample/california_housing.csv')\n",
    "\n",
    "print(\"Feature Statistics (Different Scales):\")\n",
    "print(housing_df.describe()[['MedInc', 'HouseAge', 'Population']].T)\n",
    "\n",
    "print(\"\\nNotice: Features have vastly different ranges!\")\n",
    "print(\"- MedInc: 0.5 to 15\")\n",
    "print(\"- HouseAge: 1 to 52\")\n",
    "print(\"- Population: 100 to 5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORRECT WAY: Scale after splitting\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# Prepare data\n",
    "X_housing = housing_df.drop('median_house_value', axis=1)\n",
    "y_housing = housing_df['median_house_value']\n",
    "\n",
    "# Step 1: Split first\n",
    "X_train_h, X_test_h, y_train_h, y_test_h = train_test_split(\n",
    "    X_housing, y_housing, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Step 2: Fit scaler on training data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_h)\n",
    "\n",
    "# Step 3: Transform both sets\n",
    "X_train_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_train_h),\n",
    "    columns=X_train_h.columns\n",
    ")\n",
    "X_test_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_test_h),\n",
    "    columns=X_test_h.columns\n",
    ")\n",
    "\n",
    "print(\"After Standardization:\")\n",
    "print(X_train_scaled[['MedInc', 'HouseAge', 'Population']].describe().T)\n",
    "print(\"\\n✓ All features now have mean ≈ 0 and std ≈ 1!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the effect of scaling\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Before scaling\n",
    "X_train_h[['MedInc', 'HouseAge', 'Population']].boxplot(ax=axes[0])\n",
    "axes[0].set_title('Before Scaling\\n(Different ranges)', fontsize=13, fontweight='bold')\n",
    "axes[0].set_ylabel('Value', fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# After scaling\n",
    "X_train_scaled[['MedInc', 'HouseAge', 'Population']].boxplot(ax=axes[1])\n",
    "axes[1].set_title('After Standardization\\n(Same scale)', fontsize=13, fontweight='bold')\n",
    "axes[1].set_ylabel('Standardized Value', fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key Insight: Scaling puts all features on the same scale!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Leakage - The Silent Killer\n",
    "\n",
    "**Data leakage** occurs when information from outside the training set influences the model.\n",
    "\n",
    "### Common Causes\n",
    "1. **Scaling before splitting** - Test data influences the scaler\n",
    "2. **Imputing before splitting** - Test data influences the imputation\n",
    "3. **Feature engineering using all data** - Creates unrealistic features\n",
    "4. **Using future information** - Including data not available at prediction time\n",
    "\n",
    "### The Golden Rule\n",
    "**ANY transformation that \"learns\" from data must be fit ONLY on training data!**\n",
    "\n",
    "This includes:\n",
    "- Scalers (StandardScaler, MinMaxScaler)\n",
    "- Imputers (SimpleImputer)\n",
    "- Encoders (LabelEncoder, OneHotEncoder)\n",
    "- Feature selectors\n",
    "- Dimensionality reducers (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRONG WAY - Causes data leakage!\n",
    "print(\"❌ WRONG: Scaling before splitting\\n\")\n",
    "\n",
    "# DON'T DO THIS!\n",
    "scaler_wrong = StandardScaler()\n",
    "X_scaled_wrong = scaler_wrong.fit_transform(X_housing)  # Uses ALL data\n",
    "\n",
    "# Then split\n",
    "X_train_wrong, X_test_wrong, y_train_wrong, y_test_wrong = train_test_split(\n",
    "    X_scaled_wrong, y_housing, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Problem: The scaler saw the test data!\")\n",
    "print(\"This gives unrealistically good results and won't work in production.\")\n",
    "print(\"\")\n",
    "\n",
    "# CORRECT WAY\n",
    "print(\"\\n✓ CORRECT: Split first, then scale\\n\")\n",
    "\n",
    "# Split first\n",
    "X_train_right, X_test_right, y_train_right, y_test_right = train_test_split(\n",
    "    X_housing, y_housing, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Fit scaler on training only\n",
    "scaler_right = StandardScaler()\n",
    "X_train_scaled_right = scaler_right.fit_transform(X_train_right)\n",
    "X_test_scaled_right = scaler_right.transform(X_test_right)  # Only transform test\n",
    "\n",
    "print(\"Correct: The scaler was fit on training data only!\")\n",
    "print(\"Test data is transformed using training statistics.\")\n",
    "print(\"This simulates real-world deployment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Complete Data Preparation Pipeline\n",
    "\n",
    "Let's put it all together in the correct order:\n",
    "\n",
    "1. **Load data**\n",
    "2. **Explore and understand** (EDA)\n",
    "3. **Separate features and target**\n",
    "4. **Split into train/test sets**\n",
    "5. **Handle missing values** (fit on train, transform both)\n",
    "6. **Scale features** (fit on train, transform both)\n",
    "7. **Train model**\n",
    "8. **Evaluate on test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete pipeline example\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# 1. Load data\n",
    "data = pd.read_csv('data/sample/california_housing.csv')\n",
    "\n",
    "# 2. Separate features and target\n",
    "X = data.drop('median_house_value', axis=1)\n",
    "y = data['median_house_value']\n",
    "\n",
    "# 3. Split data (70/30)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "print(\"Step 1: Data split\")\n",
    "print(f\"  Training: {len(X_train)} samples\")\n",
    "print(f\"  Testing: {len(X_test)} samples\")\n",
    "\n",
    "# 4. Handle missing values (if any)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_clean = imputer.fit_transform(X_train)\n",
    "X_test_clean = imputer.transform(X_test)\n",
    "print(\"\\nStep 2: Missing values handled\")\n",
    "\n",
    "# 5. Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_clean)\n",
    "X_test_scaled = scaler.transform(X_test_clean)\n",
    "print(\"Step 3: Features scaled\")\n",
    "\n",
    "# 6. Train model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "print(\"Step 4: Model trained\")\n",
    "\n",
    "# 7. Evaluate\n",
    "train_score = model.score(X_train_scaled, y_train)\n",
    "test_score = model.score(X_test_scaled, y_test)\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(\"\\nStep 5: Evaluation\")\n",
    "print(f\"  Training R²: {train_score:.3f}\")\n",
    "print(f\"  Testing R²: {test_score:.3f}\")\n",
    "print(f\"  RMSE: ${rmse:,.2f}\")\n",
    "print(\"\\n✓ Complete pipeline executed correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Identify Data Leakage\n",
    "\n",
    "Which of these code snippets cause data leakage? Mark them as CORRECT or WRONG:\n",
    "\n",
    "```python\n",
    "# Snippet A\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_train, X_test = train_test_split(X_scaled)\n",
    "\n",
    "# Snippet B\n",
    "X_train, X_test = train_test_split(X)\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Snippet C\n",
    "X_train, X_test = train_test_split(X)\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.fit_transform(X_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answers:\n",
    "# Snippet A: \n",
    "# Snippet B: \n",
    "# Snippet C: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Proper Train/Test Split\n",
    "\n",
    "Load the breast cancer dataset and perform a stratified 80/20 train/test split.\n",
    "\n",
    "Steps:\n",
    "1. Load data from `data/sample/breast_cancer.csv`\n",
    "2. Separate features (all columns except 'target' and 'diagnosis') and target ('target')\n",
    "3. Perform stratified split (80% train, 20% test)\n",
    "4. Verify that class proportions are maintained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Complete Preprocessing Pipeline\n",
    "\n",
    "Create a complete preprocessing pipeline for the diabetes dataset:\n",
    "\n",
    "1. Load the diabetes dataset\n",
    "2. Create 15% artificial missing values (use the code from Section 4)\n",
    "3. Split data (70/30)\n",
    "4. Impute missing values (mean strategy)\n",
    "5. Scale features using StandardScaler\n",
    "6. Verify no missing values remain and features are scaled\n",
    "\n",
    "Print the shape and statistics of your final training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Impact of Proper Splitting\n",
    "\n",
    "Demonstrate the importance of proper train/test splitting:\n",
    "\n",
    "1. Use the Iris dataset\n",
    "2. Build two models:\n",
    "   - Model A: Evaluate on training data (no split)\n",
    "   - Model B: Evaluate on test data (proper split)\n",
    "3. Compare the accuracies\n",
    "4. Explain which one is more realistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You now understand proper data preparation - a critical skill in machine learning.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Train/Test Split**:\n",
    "   - Never test on training data - causes overfitting\n",
    "   - Common splits: 70/30, 80/20, 60/20/20\n",
    "   - Use stratified split for classification\n",
    "   - Always set random_state for reproducibility\n",
    "\n",
    "2. **Data Preparation Order**:\n",
    "   ```\n",
    "   Load → Explore → Separate X/y → SPLIT → Preprocess → Train → Evaluate\n",
    "   ```\n",
    "\n",
    "3. **Handling Missing Values**:\n",
    "   - Common strategies: drop, impute (mean/median/mode)\n",
    "   - Fit imputer on training data only\n",
    "   - Transform both train and test with same imputer\n",
    "\n",
    "4. **Feature Scaling**:\n",
    "   - Standardization: mean=0, std=1 (preferred for most cases)\n",
    "   - Normalization: scale to [0, 1]\n",
    "   - Required for distance-based algorithms (KNN, SVM)\n",
    "   - Fit scaler on training data only\n",
    "\n",
    "5. **Avoiding Data Leakage**:\n",
    "   - Golden Rule: Split FIRST, preprocess SECOND\n",
    "   - Fit transformers on training data only\n",
    "   - Transform both sets with the same fitted transformer\n",
    "   - Never let test data influence the model\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In **Module 03: Linear Regression**, you'll learn:\n",
    "- The theory behind linear regression\n",
    "- Simple and multiple linear regression\n",
    "- Interpreting coefficients and predictions\n",
    "- Evaluating regression models\n",
    "- Making predictions on new data\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [Train/Test Split - scikit-learn](https://scikit-learn.org/stable/modules/cross_validation.html)\n",
    "- [Data Leakage in Machine Learning](https://machinelearningmastery.com/data-leakage-machine-learning/)\n",
    "- [Feature Scaling - Why and How](https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
