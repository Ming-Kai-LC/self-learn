{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 04: Logistic Regression\n",
    "\n",
    "**Difficulty**: ⭐⭐ Intermediate  \n",
    "**Estimated Time**: 90 minutes  \n",
    "**Prerequisites**: \n",
    "- [Module 03: Linear Regression](03_linear_regression.ipynb)\n",
    "- Understanding of probability and odds\n",
    "- Basic calculus (derivatives)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Understand logistic regression for binary classification\n",
    "2. Explain the sigmoid function and its properties\n",
    "3. Interpret odds, log-odds, and probabilities\n",
    "4. Visualize and understand decision boundaries\n",
    "5. Implement multi-class classification (OvR and OvO)\n",
    "6. Evaluate classification models using accuracy and confusion matrices\n",
    "7. Apply logistic regression to real-world datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.special import expit\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix, classification_report,\n",
    "    roc_curve, roc_auc_score, precision_recall_curve\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. From Linear to Logistic Regression\n",
    "\n",
    "### Why Not Use Linear Regression for Classification?\n",
    "\n",
    "**Problem**: Linear regression predicts continuous values, but classification needs discrete categories (0 or 1).\n",
    "\n",
    "```\n",
    "Linear Regression: y = β₀ + β₁x₁ + β₂x₂ + ...\n",
    "Output: Any real number (-∞ to +∞)\n",
    "```\n",
    "\n",
    "**What we need**: Probabilities (0 to 1) that can be converted to class labels.\n",
    "\n",
    "### Solution: The Sigmoid Function\n",
    "\n",
    "**Logistic Regression** uses the **sigmoid function** to map any real number to (0, 1):\n",
    "\n",
    "```\n",
    "σ(z) = 1 / (1 + e^(-z))\n",
    "```\n",
    "\n",
    "Where z = β₀ + β₁x₁ + β₂x₂ + ... (same as linear regression)\n",
    "\n",
    "**Properties**:\n",
    "- Output always between 0 and 1\n",
    "- S-shaped curve\n",
    "- σ(0) = 0.5 (midpoint)\n",
    "- σ(+∞) = 1\n",
    "- σ(-∞) = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the sigmoid function\n",
    "z = np.linspace(-10, 10, 200)\n",
    "sigmoid = expit(z)  # 1 / (1 + np.exp(-z))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(z, sigmoid, 'b-', linewidth=2, label='Sigmoid: σ(z) = 1/(1+e^-z)')\n",
    "plt.axhline(y=0.5, color='r', linestyle='--', alpha=0.7, label='Decision threshold')\n",
    "plt.axvline(x=0, color='g', linestyle='--', alpha=0.7)\n",
    "plt.xlabel('z (linear combination of features)', fontsize=12)\n",
    "plt.ylabel('Probability P(y=1)', fontsize=12)\n",
    "plt.title('Sigmoid Function: Mapping Real Numbers to Probabilities', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(fontsize=11)\n",
    "plt.ylim(-0.1, 1.1)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key Points:\")\n",
    "print(f\"  σ(-10) = {expit(-10):.6f} ≈ 0\")\n",
    "print(f\"  σ(-2)  = {expit(-2):.6f}\")\n",
    "print(f\"  σ(0)   = {expit(0):.6f} = 0.5\")\n",
    "print(f\"  σ(2)   = {expit(2):.6f}\")\n",
    "print(f\"  σ(10)  = {expit(10):.6f} ≈ 1\")\n",
    "print(\"\\nIf P(y=1) ≥ 0.5 → predict class 1\")\n",
    "print(\"If P(y=1) < 0.5 → predict class 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Understanding Odds and Log-Odds\n",
    "\n",
    "### Probability vs Odds\n",
    "\n",
    "**Probability**: P(event) = successes / total attempts\n",
    "- Range: [0, 1]\n",
    "- Example: P(rain) = 0.7 = 70%\n",
    "\n",
    "**Odds**: Odds(event) = P(event) / P(not event)\n",
    "- Range: [0, ∞)\n",
    "- Example: Odds(rain) = 0.7 / 0.3 = 2.33 (\"7 to 3\")\n",
    "\n",
    "### Log-Odds (Logit)\n",
    "\n",
    "**Log-Odds** = log(Odds) = log(P / (1-P))\n",
    "- Range: (-∞, +∞)\n",
    "- This is what logistic regression actually models!\n",
    "\n",
    "```\n",
    "log(P/(1-P)) = β₀ + β₁x₁ + β₂x₂ + ...\n",
    "```\n",
    "\n",
    "**Interpretation**: A unit increase in x increases log-odds by β."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate probability ↔ odds ↔ log-odds conversions\n",
    "probabilities = np.array([0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99])\n",
    "odds = probabilities / (1 - probabilities)\n",
    "log_odds = np.log(odds)\n",
    "\n",
    "conversion_df = pd.DataFrame({\n",
    "    'Probability': probabilities,\n",
    "    'Odds': odds,\n",
    "    'Log-Odds': log_odds\n",
    "})\n",
    "\n",
    "print(\"Probability ↔ Odds ↔ Log-Odds Conversion Table:\")\n",
    "print(conversion_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nKey Insights:\")\n",
    "print(\"  P = 0.5 → Odds = 1.0 (50-50 chance) → Log-Odds = 0\")\n",
    "print(\"  P > 0.5 → Odds > 1.0 → Log-Odds > 0\")\n",
    "print(\"  P < 0.5 → Odds < 1.0 → Log-Odds < 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Binary Classification Example\n",
    "\n",
    "Let's build a binary classifier to predict if a tumor is malignant or benign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load breast cancer dataset\n",
    "cancer = datasets.load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target  # 0 = malignant, 1 = benign\n",
    "\n",
    "# Create DataFrame for exploration\n",
    "cancer_df = pd.DataFrame(X, columns=cancer.feature_names)\n",
    "cancer_df['diagnosis'] = y\n",
    "cancer_df['diagnosis_name'] = cancer_df['diagnosis'].map({0: 'malignant', 1: 'benign'})\n",
    "\n",
    "print(\"Breast Cancer Dataset:\")\n",
    "print(f\"Shape: {cancer_df.shape}\")\n",
    "print(f\"\\nFeatures (first 10): {list(cancer.feature_names[:10])}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(cancer_df['diagnosis_name'].value_counts())\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(cancer_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize two features\n",
    "plt.figure(figsize=(10, 6))\n",
    "for diagnosis, name in [(0, 'Malignant'), (1, 'Benign')]:\n",
    "    mask = y == diagnosis\n",
    "    plt.scatter(\n",
    "        X[mask, 0], X[mask, 1],\n",
    "        label=name, s=50, alpha=0.6, edgecolors='k'\n",
    "    )\n",
    "\n",
    "plt.xlabel(cancer.feature_names[0], fontsize=12)\n",
    "plt.ylabel(cancer.feature_names[1], fontsize=12)\n",
    "plt.title('Tumor Classification: Mean Radius vs Mean Texture', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice some separation between classes - good for classification!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and scale data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"\\nClass distribution in training:\")\n",
    "print(pd.Series(y_train).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train logistic regression model\n",
    "log_reg = LogisticRegression(max_iter=10000, random_state=42)\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = log_reg.predict(X_train_scaled)\n",
    "y_test_pred = log_reg.predict(X_test_scaled)\n",
    "\n",
    "# Get probability predictions\n",
    "y_test_proba = log_reg.predict_proba(X_test_scaled)[:, 1]  # Probability of class 1\n",
    "\n",
    "# Evaluate\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"Logistic Regression Results:\")\n",
    "print(f\"Training Accuracy: {train_acc:.2%}\")\n",
    "print(f\"Test Accuracy: {test_acc:.2%}\")\n",
    "print(f\"\\nModel Coefficients (first 5 features):\")\n",
    "for feature, coef in zip(cancer.feature_names[:5], log_reg.coef_[0][:5]):\n",
    "    print(f\"  {feature:30s}: {coef:8.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "           xticklabels=['Malignant', 'Benign'],\n",
    "           yticklabels=['Malignant', 'Benign'])\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.ylabel('Actual', fontsize=12)\n",
    "plt.title('Confusion Matrix', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Confusion Matrix Interpretation:\")\n",
    "print(f\"  True Negatives (TN):  {cm[0,0]} (correctly predicted malignant)\")\n",
    "print(f\"  False Positives (FP): {cm[0,1]} (malignant predicted as benign)\")\n",
    "print(f\"  False Negatives (FN): {cm[1,0]} (benign predicted as malignant)\")\n",
    "print(f\"  True Positives (TP):  {cm[1,1]} (correctly predicted benign)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report\n",
    "print(\"Detailed Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred, \n",
    "                          target_names=['Malignant', 'Benign']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Decision Boundaries\n",
    "\n",
    "The **decision boundary** is where the model switches predictions (P=0.5).\n",
    "\n",
    "For logistic regression, this boundary is **linear** (a straight line in 2D, a plane in 3D, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize decision boundary using first two features\n",
    "# Train a simpler model with just 2 features for visualization\n",
    "X_2d = X[:, :2]  # Use first 2 features only\n",
    "X_train_2d, X_test_2d, y_train_2d, y_test_2d = train_test_split(\n",
    "    X_2d, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "scaler_2d = StandardScaler()\n",
    "X_train_2d_scaled = scaler_2d.fit_transform(X_train_2d)\n",
    "X_test_2d_scaled = scaler_2d.transform(X_test_2d)\n",
    "\n",
    "log_reg_2d = LogisticRegression(max_iter=10000)\n",
    "log_reg_2d.fit(X_train_2d_scaled, y_train_2d)\n",
    "\n",
    "# Create mesh for decision boundary\n",
    "x_min, x_max = X_train_2d_scaled[:, 0].min() - 1, X_train_2d_scaled[:, 0].max() + 1\n",
    "y_min, y_max = X_train_2d_scaled[:, 1].min() - 1, X_train_2d_scaled[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                    np.linspace(y_min, y_max, 200))\n",
    "\n",
    "Z = log_reg_2d.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Contour plot showing probabilities\n",
    "contour = plt.contourf(xx, yy, Z, levels=20, cmap='RdYlBu', alpha=0.7)\n",
    "plt.colorbar(contour, label='P(Benign)')\n",
    "\n",
    "# Decision boundary (P=0.5)\n",
    "plt.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2)\n",
    "\n",
    "# Plot data points\n",
    "for diagnosis, name, color in [(0, 'Malignant', 'red'), (1, 'Benign', 'blue')]:\n",
    "    mask = y_train_2d == diagnosis\n",
    "    plt.scatter(X_train_2d_scaled[mask, 0], X_train_2d_scaled[mask, 1],\n",
    "              c=color, label=name, s=30, alpha=0.7, edgecolors='k')\n",
    "\n",
    "plt.xlabel(f'{cancer.feature_names[0]} (scaled)', fontsize=12)\n",
    "plt.ylabel(f'{cancer.feature_names[1]} (scaled)', fontsize=12)\n",
    "plt.title('Logistic Regression Decision Boundary', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key Observations:\")\n",
    "print(\"  • Black line = decision boundary (P = 0.5)\")\n",
    "print(\"  • Blue region = model predicts Benign (P > 0.5)\")\n",
    "print(\"  • Red region = model predicts Malignant (P < 0.5)\")\n",
    "print(\"  • Boundary is LINEAR (straight line)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Probability Interpretation\n",
    "\n",
    "Logistic regression gives **calibrated probabilities** - not just class predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show probability predictions for first 10 test samples\n",
    "sample_df = pd.DataFrame({\n",
    "    'Actual': y_test[:10],\n",
    "    'Predicted': y_test_pred[:10],\n",
    "    'P(Benign)': y_test_proba[:10],\n",
    "    'P(Malignant)': 1 - y_test_proba[:10],\n",
    "    'Correct': y_test[:10] == y_test_pred[:10]\n",
    "})\n",
    "\n",
    "print(\"Sample Predictions with Probabilities:\")\n",
    "print(sample_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nNote: Higher probability = more confident prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize probability distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram of predicted probabilities\n",
    "axes[0].hist(y_test_proba[y_test == 0], bins=20, alpha=0.7, \n",
    "            label='Actual Malignant', color='red', edgecolor='k')\n",
    "axes[0].hist(y_test_proba[y_test == 1], bins=20, alpha=0.7, \n",
    "            label='Actual Benign', color='blue', edgecolor='k')\n",
    "axes[0].axvline(x=0.5, color='black', linestyle='--', linewidth=2, label='Threshold')\n",
    "axes[0].set_xlabel('Predicted Probability of Benign', fontsize=12)\n",
    "axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0].set_title('Distribution of Predicted Probabilities', fontsize=14)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Scatter plot: actual vs probability\n",
    "jitter = np.random.normal(0, 0.02, len(y_test))\n",
    "axes[1].scatter(y_test_proba, y_test + jitter, alpha=0.5, edgecolors='k')\n",
    "axes[1].axvline(x=0.5, color='r', linestyle='--', linewidth=2, label='Threshold')\n",
    "axes[1].set_xlabel('Predicted Probability of Benign', fontsize=12)\n",
    "axes[1].set_ylabel('Actual Class (0=Malignant, 1=Benign)', fontsize=12)\n",
    "axes[1].set_title('Actual Class vs Predicted Probability', fontsize=14)\n",
    "axes[1].set_yticks([0, 1])\n",
    "axes[1].set_yticklabels(['Malignant', 'Benign'])\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Good separation → Confident predictions (close to 0 or 1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Multi-Class Classification\n",
    "\n",
    "Logistic regression is naturally binary, but can handle multiple classes using:\n",
    "\n",
    "### 1. One-vs-Rest (OvR) / One-vs-All\n",
    "- Train N binary classifiers (one per class)\n",
    "- Each classifier: \"this class vs all others\"\n",
    "- Prediction: class with highest probability\n",
    "- **Default in scikit-learn**\n",
    "\n",
    "### 2. One-vs-One (OvO)\n",
    "- Train N(N-1)/2 binary classifiers\n",
    "- Each classifier: \"class A vs class B\"\n",
    "- Prediction: class that wins most pairwise comparisons\n",
    "- More classifiers, but each is simpler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-class example: Iris dataset (3 classes)\n",
    "iris = datasets.load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "\n",
    "# Split and scale\n",
    "X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(\n",
    "    X_iris, y_iris, test_size=0.2, random_state=42, stratify=y_iris\n",
    ")\n",
    "\n",
    "scaler_iris = StandardScaler()\n",
    "X_train_iris_scaled = scaler_iris.fit_transform(X_train_iris)\n",
    "X_test_iris_scaled = scaler_iris.transform(X_test_iris)\n",
    "\n",
    "print(\"Iris Dataset for Multi-Class Classification:\")\n",
    "print(f\"Classes: {iris.target_names}\")\n",
    "print(f\"Training samples: {X_train_iris.shape[0]}\")\n",
    "print(f\"Test samples: {X_test_iris.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with One-vs-Rest (default)\n",
    "log_reg_ovr = LogisticRegression(multi_class='ovr', max_iter=10000, random_state=42)\n",
    "log_reg_ovr.fit(X_train_iris_scaled, y_train_iris)\n",
    "y_pred_ovr = log_reg_ovr.predict(X_test_iris_scaled)\n",
    "acc_ovr = accuracy_score(y_test_iris, y_pred_ovr)\n",
    "\n",
    "print(\"One-vs-Rest (OvR) Results:\")\n",
    "print(f\"Accuracy: {acc_ovr:.2%}\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_iris, y_pred_ovr, target_names=iris.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for multi-class\n",
    "cm_iris = confusion_matrix(y_test_iris, y_pred_ovr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_iris, annot=True, fmt='d', cmap='Blues',\n",
    "           xticklabels=iris.target_names,\n",
    "           yticklabels=iris.target_names)\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.ylabel('Actual', fontsize=12)\n",
    "plt.title('Multi-Class Confusion Matrix (Iris)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Diagonal = correct predictions\")\n",
    "print(\"Off-diagonal = misclassifications\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show probability predictions for multi-class\n",
    "y_proba_iris = log_reg_ovr.predict_proba(X_test_iris_scaled)\n",
    "\n",
    "print(\"Multi-Class Probability Predictions (first 5 samples):\")\n",
    "proba_df = pd.DataFrame(\n",
    "    y_proba_iris[:5],\n",
    "    columns=[f'P({name})' for name in iris.target_names]\n",
    ")\n",
    "proba_df['Predicted'] = [iris.target_names[i] for i in y_pred_ovr[:5]]\n",
    "proba_df['Actual'] = [iris.target_names[i] for i in y_test_iris[:5]]\n",
    "print(proba_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nNote: Sum of probabilities for each row = 1.0\")\n",
    "print(\"Predicted class = highest probability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Practice Exercises\n",
    "\n",
    "### Exercise 1: Adjust Decision Threshold\n",
    "\n",
    "Using the breast cancer model:\n",
    "1. Change the decision threshold from 0.5 to 0.3\n",
    "2. How does this affect predictions?\n",
    "3. Which errors increase/decrease (false positives vs false negatives)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Feature Importance\n",
    "\n",
    "For the breast cancer model:\n",
    "1. Find the 5 features with largest coefficient magnitudes\n",
    "2. What do these coefficients tell you about their importance?\n",
    "3. Train a model using only these 5 features - how does accuracy compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Wine Dataset Classification\n",
    "\n",
    "Load the wine dataset (`datasets.load_wine()`):\n",
    "1. Train a multi-class logistic regression model\n",
    "2. Calculate accuracy and create confusion matrix\n",
    "3. Which classes are most confused with each other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Coefficient Interpretation\n",
    "\n",
    "Create a simple logistic regression with one feature.\n",
    "If the coefficient is 2.5:\n",
    "1. What happens to log-odds when feature increases by 1?\n",
    "2. What happens to odds?\n",
    "3. Demonstrate with actual predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "### Key Concepts Learned\n",
    "\n",
    "1. **Logistic Regression Fundamentals**:\n",
    "   - Binary classification algorithm\n",
    "   - Uses sigmoid function: σ(z) = 1/(1+e^(-z))\n",
    "   - Outputs probabilities between 0 and 1\n",
    "   - Linear decision boundary\n",
    "\n",
    "2. **Sigmoid Function**:\n",
    "   - Maps any real number to (0, 1)\n",
    "   - S-shaped curve\n",
    "   - Threshold at 0.5 for binary decisions\n",
    "\n",
    "3. **Probability Concepts**:\n",
    "   - **Probability**: P ∈ [0, 1]\n",
    "   - **Odds**: P/(1-P) ∈ [0, ∞)\n",
    "   - **Log-Odds**: log(P/(1-P)) ∈ (-∞, ∞)\n",
    "   - Logistic regression models log-odds linearly\n",
    "\n",
    "4. **Classification Metrics**:\n",
    "   - Accuracy: overall correctness\n",
    "   - Confusion matrix: detailed error breakdown\n",
    "   - Precision, recall, F1-score (from classification_report)\n",
    "\n",
    "5. **Multi-Class Classification**:\n",
    "   - **One-vs-Rest (OvR)**: N binary classifiers\n",
    "   - **One-vs-One (OvO)**: N(N-1)/2 pairwise classifiers\n",
    "   - OvR is default in scikit-learn\n",
    "\n",
    "6. **Model Outputs**:\n",
    "   - `predict()`: Class labels (0/1 or category)\n",
    "   - `predict_proba()`: Probability estimates\n",
    "   - `decision_function()`: Raw scores (before sigmoid)\n",
    "\n",
    "### When to Use Logistic Regression\n",
    "\n",
    "✅ **Good for**:\n",
    "- Binary classification (spam/not spam, fraud/legitimate)\n",
    "- Multi-class classification (3+ categories)\n",
    "- When you need probability estimates\n",
    "- Interpretable models (coefficient = feature importance)\n",
    "- Baseline classification model\n",
    "- Linearly separable classes\n",
    "\n",
    "❌ **Not ideal for**:\n",
    "- Non-linear decision boundaries (use kernel methods or trees)\n",
    "- Highly imbalanced datasets (adjust class_weight)\n",
    "- Many correlated features (regularization needed)\n",
    "\n",
    "### Quick Reference: Scikit-learn\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Binary classification\n",
    "model = LogisticRegression(max_iter=10000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)  # Class labels\n",
    "y_proba = model.predict_proba(X_test)  # Probabilities\n",
    "\n",
    "# Multi-class\n",
    "model_multi = LogisticRegression(multi_class='ovr')  # or 'multinomial'\n",
    "\n",
    "# Important parameters:\n",
    "# - C: Inverse of regularization (higher = less regularization)\n",
    "# - penalty: 'l1', 'l2', 'elasticnet', 'none'\n",
    "# - solver: 'lbfgs', 'liblinear', 'saga'\n",
    "# - class_weight: 'balanced' for imbalanced data\n",
    "```\n",
    "\n",
    "### Decision Tree: Linear vs Logistic Regression\n",
    "\n",
    "| Aspect | Linear Regression | Logistic Regression |\n",
    "|--------|------------------|--------------------|\n",
    "| **Task** | Regression | Classification |\n",
    "| **Output** | Continuous | Probability (0-1) |\n",
    "| **Equation** | y = β₀ + β₁x | P = 1/(1+e^(-z)) |\n",
    "| **Loss** | MSE | Log-loss |\n",
    "| **Use Case** | Predict prices | Predict categories |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the next module, we'll explore:\n",
    "- **Decision Trees** for non-linear boundaries\n",
    "- Tree visualization and interpretation\n",
    "- Handling overfitting with pruning\n",
    "- Feature importance analysis\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [Scikit-learn Logistic Regression](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)\n",
    "- [StatQuest: Logistic Regression](https://www.youtube.com/watch?v=yIYKR4sgzI8)\n",
    "- [Andrew Ng: Logistic Regression](https://www.coursera.org/learn/machine-learning)\n",
    "- [Understanding the Sigmoid Function](https://towardsdatascience.com/understanding-the-sigmoid-function-f0e6e0a7eca)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
