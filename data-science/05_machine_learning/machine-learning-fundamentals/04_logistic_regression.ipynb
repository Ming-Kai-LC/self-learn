{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 04: Logistic Regression\n",
    "\n",
    "**Difficulty**: ⭐ Beginner  \n",
    "**Estimated Time**: 60 minutes  \n",
    "**Prerequisites**: \n",
    "- [Module 00: Introduction to ML and scikit-learn](00_introduction_to_ml_and_sklearn.ipynb)\n",
    "- [Module 01: Supervised vs Unsupervised Learning](01_supervised_vs_unsupervised_learning.ipynb)\n",
    "- [Module 02: Data Preparation and Train/Test Split](02_data_preparation_train_test_split.ipynb)\n",
    "- [Module 03: Linear Regression](03_linear_regression.ipynb)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Understand how logistic regression adapts linear regression for classification\n",
    "2. Explain the sigmoid function and its role in converting to probabilities\n",
    "3. Build binary classification models\n",
    "4. Build multiclass classification models using One-vs-Rest strategy\n",
    "5. Interpret probability predictions and decision thresholds\n",
    "6. Evaluate classification models using accuracy and other metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. From Linear to Logistic Regression\n",
    "\n",
    "### The Problem with Linear Regression for Classification\n",
    "\n",
    "Linear regression predicts continuous values, but classification needs discrete categories:\n",
    "- Email: Spam (1) or Not Spam (0)\n",
    "- Disease: Present (1) or Absent (0)\n",
    "- Transaction: Fraud (1) or Legitimate (0)\n",
    "\n",
    "**Problem**: Linear regression can predict values like 1.5, -0.3, or 100, which don't make sense for categories!\n",
    "\n",
    "### The Solution: Logistic Regression\n",
    "\n",
    "**Logistic Regression** uses the **sigmoid function** to transform linear regression output into probabilities between 0 and 1.\n",
    "\n",
    "### The Sigmoid Function\n",
    "\n",
    "```\n",
    "σ(z) = 1 / (1 + e^(-z))\n",
    "```\n",
    "\n",
    "Where:\n",
    "- **z** = linear combination: β₀ + β₁x₁ + β₂x₂ + ...\n",
    "- **σ(z)** = probability between 0 and 1\n",
    "\n",
    "**Key Properties**:\n",
    "- Output always between 0 and 1\n",
    "- S-shaped curve\n",
    "- When z = 0, σ(z) = 0.5\n",
    "- As z → ∞, σ(z) → 1\n",
    "- As z → -∞, σ(z) → 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Visualization settings\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"✓ Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the sigmoid function\n",
    "def sigmoid(z):\n",
    "    \"\"\"Calculate sigmoid function\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Create range of z values\n",
    "z_values = np.linspace(-10, 10, 200)\n",
    "sigmoid_values = sigmoid(z_values)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(z_values, sigmoid_values, linewidth=3, color='blue')\n",
    "plt.axhline(y=0.5, color='red', linestyle='--', linewidth=2, label='Decision Threshold (0.5)')\n",
    "plt.axvline(x=0, color='green', linestyle='--', linewidth=2, alpha=0.5)\n",
    "plt.xlabel('z (linear combination)', fontsize=12)\n",
    "plt.ylabel('σ(z) - Probability', fontsize=12)\n",
    "plt.title('The Sigmoid Function\\nConverts any value to probability [0, 1]', \n",
    "         fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(fontsize=10)\n",
    "plt.ylim(-0.1, 1.1)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key Properties:\")\n",
    "print(f\"- When z = -5: σ(z) = {sigmoid(-5):.4f} (very unlikely)\")\n",
    "print(f\"- When z = 0: σ(z) = {sigmoid(0):.4f} (neutral)\")\n",
    "print(f\"- When z = 5: σ(z) = {sigmoid(5):.4f} (very likely)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Binary Classification with Logistic Regression\n",
    "\n",
    "Let's predict whether a tumor is malignant (cancerous) or benign using the breast cancer dataset.\n",
    "\n",
    "**Target**:\n",
    "- 0 = Malignant (cancerous)\n",
    "- 1 = Benign (non-cancerous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load breast cancer dataset\n",
    "cancer_df = pd.read_csv('data/sample/breast_cancer.csv')\n",
    "\n",
    "print(\"Breast Cancer Dataset Overview:\")\n",
    "print(f\"Shape: {cancer_df.shape}\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(cancer_df['diagnosis'].value_counts())\n",
    "print(f\"\\nClass balance:\")\n",
    "print(cancer_df['diagnosis'].value_counts(normalize=True))\n",
    "print(f\"\\nFirst few rows:\")\n",
    "cancer_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "# Drop non-feature columns\n",
    "X = cancer_df.drop(['target', 'diagnosis'], axis=1)\n",
    "y = cancer_df['target']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nNumber of features: {X.shape[1]}\")\n",
    "print(f\"Feature names (first 5): {list(X.columns[:5])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and scale the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Split data (stratified to maintain class balance)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Scale features (important for logistic regression)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Data Preparation Complete:\")\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Testing samples: {len(X_test)}\")\n",
    "print(f\"Features scaled: ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create and train the model\n",
    "log_reg = LogisticRegression(random_state=42, max_iter=10000)\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"✓ Logistic Regression model trained!\")\n",
    "print(f\"\\nNumber of coefficients: {len(log_reg.coef_[0])}\")\n",
    "print(f\"Intercept: {log_reg.intercept_[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = log_reg.predict(X_test_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Model Performance:\")\n",
    "print(f\"Accuracy: {accuracy:.1%}\")\n",
    "print(f\"\\nThis means the model correctly classifies {accuracy:.1%} of tumors!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Probability Predictions\n",
    "\n",
    "Unlike other classifiers, logistic regression provides **probability estimates** for each class. This is very useful in real-world applications!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get probability predictions\n",
    "y_proba = log_reg.predict_proba(X_test_scaled)\n",
    "\n",
    "# Display first 10 predictions with probabilities\n",
    "predictions_df = pd.DataFrame({\n",
    "    'Actual': y_test.values[:10],\n",
    "    'Predicted': y_pred[:10],\n",
    "    'Prob_Malignant': y_proba[:10, 0],\n",
    "    'Prob_Benign': y_proba[:10, 1],\n",
    "    'Confidence': np.max(y_proba[:10], axis=1)\n",
    "})\n",
    "\n",
    "print(\"Prediction Examples with Probabilities:\")\n",
    "print(predictions_df.to_string(index=False))\n",
    "print(\"\\nNote: Confidence = probability of predicted class\")\n",
    "print(\"Higher confidence = more certain prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize probability distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram of probabilities for class 1 (benign)\n",
    "axes[0].hist(y_proba[y_test == 0, 1], bins=30, alpha=0.7, label='Actual: Malignant', color='red')\n",
    "axes[0].hist(y_proba[y_test == 1, 1], bins=30, alpha=0.7, label='Actual: Benign', color='blue')\n",
    "axes[0].axvline(x=0.5, color='black', linestyle='--', linewidth=2, label='Decision Threshold')\n",
    "axes[0].set_xlabel('Predicted Probability (Benign)', fontsize=12)\n",
    "axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0].set_title('Distribution of Predicted Probabilities', fontsize=13, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Confidence distribution\n",
    "confidence = np.max(y_proba, axis=1)\n",
    "axes[1].hist(confidence, bins=30, color='green', alpha=0.7, edgecolor='black')\n",
    "axes[1].set_xlabel('Prediction Confidence', fontsize=12)\n",
    "axes[1].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1].set_title('Model Confidence Distribution', fontsize=13, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average confidence: {confidence.mean():.1%}\")\n",
    "print(f\"Min confidence: {confidence.min():.1%}\")\n",
    "print(f\"Max confidence: {confidence.max():.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Decision Threshold\n",
    "\n",
    "By default, logistic regression uses a **threshold of 0.5**:\n",
    "- If probability ≥ 0.5 → Predict class 1\n",
    "- If probability < 0.5 → Predict class 0\n",
    "\n",
    "But we can adjust this threshold based on the problem requirements!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate different thresholds\n",
    "thresholds = [0.3, 0.5, 0.7]\n",
    "\n",
    "print(\"Effect of Different Decision Thresholds:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for threshold in thresholds:\n",
    "    # Apply threshold manually\n",
    "    y_pred_threshold = (y_proba[:, 1] >= threshold).astype(int)\n",
    "    accuracy_threshold = accuracy_score(y_test, y_pred_threshold)\n",
    "    \n",
    "    # Count predictions\n",
    "    n_positive = np.sum(y_pred_threshold == 1)\n",
    "    n_negative = np.sum(y_pred_threshold == 0)\n",
    "    \n",
    "    print(f\"\\nThreshold: {threshold}\")\n",
    "    print(f\"  Accuracy: {accuracy_threshold:.1%}\")\n",
    "    print(f\"  Predicted as Benign (1): {n_positive}\")\n",
    "    print(f\"  Predicted as Malignant (0): {n_negative}\")\n",
    "\n",
    "print(\"\\nKey Insight:\")\n",
    "print(\"- Lower threshold → More predictions of class 1 (more sensitive)\")\n",
    "print(\"- Higher threshold → Fewer predictions of class 1 (more specific)\")\n",
    "print(\"- Choose threshold based on the cost of false positives vs false negatives\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Confusion Matrix\n",
    "\n",
    "A confusion matrix shows the breakdown of correct and incorrect predictions:\n",
    "\n",
    "```\n",
    "                 Predicted\n",
    "              Negative  Positive\n",
    "Actual Negative   TN       FP\n",
    "       Positive   FN       TP\n",
    "```\n",
    "\n",
    "Where:\n",
    "- **TN** (True Negative): Correctly predicted negative\n",
    "- **TP** (True Positive): Correctly predicted positive\n",
    "- **FN** (False Negative): Incorrectly predicted negative (missed)\n",
    "- **FP** (False Positive): Incorrectly predicted positive (false alarm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True,\n",
    "           xticklabels=['Malignant', 'Benign'],\n",
    "           yticklabels=['Malignant', 'Benign'])\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.ylabel('Actual', fontsize=12)\n",
    "plt.title('Confusion Matrix\\nBreakdown of Predictions', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Extract values\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print(\"Confusion Matrix Breakdown:\")\n",
    "print(f\"True Negatives (TN): {tn} - Correctly identified malignant\")\n",
    "print(f\"True Positives (TP): {tp} - Correctly identified benign\")\n",
    "print(f\"False Negatives (FN): {fn} - Malignant classified as benign (dangerous!)\")\n",
    "print(f\"False Positives (FP): {fp} - Benign classified as malignant (unnecessary worry)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Malignant', 'Benign']))\n",
    "\n",
    "print(\"\\nMetrics Explanation:\")\n",
    "print(\"- Precision: Of all predicted positive, how many were correct?\")\n",
    "print(\"- Recall: Of all actual positive, how many did we find?\")\n",
    "print(\"- F1-Score: Harmonic mean of precision and recall\")\n",
    "print(\"- Support: Number of samples in each class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Multiclass Classification\n",
    "\n",
    "Logistic regression can also handle **multiple classes** using the **One-vs-Rest (OvR)** strategy:\n",
    "- Train one binary classifier for each class\n",
    "- Class A vs (B and C)\n",
    "- Class B vs (A and C)\n",
    "- Class C vs (A and B)\n",
    "- Choose the class with highest probability\n",
    "\n",
    "Let's classify Iris species (3 classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Iris dataset\n",
    "iris_df = pd.read_csv('data/sample/iris.csv')\n",
    "\n",
    "# Prepare features and target\n",
    "feature_cols = ['sepal length (cm)', 'sepal width (cm)', \n",
    "                'petal length (cm)', 'petal width (cm)']\n",
    "X_iris = iris_df[feature_cols]\n",
    "y_iris = iris_df['species']\n",
    "\n",
    "print(\"Iris Dataset - Multiclass Classification:\")\n",
    "print(f\"Number of samples: {len(X_iris)}\")\n",
    "print(f\"Number of features: {X_iris.shape[1]}\")\n",
    "print(f\"Number of classes: {y_iris.nunique()}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(y_iris.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and scale\n",
    "X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(\n",
    "    X_iris, y_iris, test_size=0.3, random_state=42, stratify=y_iris\n",
    ")\n",
    "\n",
    "scaler_iris = StandardScaler()\n",
    "X_train_iris_scaled = scaler_iris.fit_transform(X_train_iris)\n",
    "X_test_iris_scaled = scaler_iris.transform(X_test_iris)\n",
    "\n",
    "# Train multiclass logistic regression\n",
    "log_reg_multi = LogisticRegression(random_state=42, max_iter=10000, multi_class='ovr')\n",
    "log_reg_multi.fit(X_train_iris_scaled, y_train_iris)\n",
    "\n",
    "print(\"✓ Multiclass Logistic Regression trained!\")\n",
    "print(f\"\\nNumber of classes: {len(log_reg_multi.classes_)}\")\n",
    "print(f\"Classes: {log_reg_multi.classes_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_iris = log_reg_multi.predict(X_test_iris_scaled)\n",
    "y_proba_iris = log_reg_multi.predict_proba(X_test_iris_scaled)\n",
    "\n",
    "# Evaluate\n",
    "accuracy_iris = accuracy_score(y_test_iris, y_pred_iris)\n",
    "\n",
    "print(f\"Multiclass Classification Accuracy: {accuracy_iris:.1%}\")\n",
    "\n",
    "# Show example predictions with probabilities for all classes\n",
    "multi_pred_df = pd.DataFrame({\n",
    "    'Actual': y_test_iris.values[:8],\n",
    "    'Predicted': y_pred_iris[:8],\n",
    "    'Prob_Class_0': y_proba_iris[:8, 0],\n",
    "    'Prob_Class_1': y_proba_iris[:8, 1],\n",
    "    'Prob_Class_2': y_proba_iris[:8, 2]\n",
    "})\n",
    "\n",
    "print(\"\\nExample Multiclass Predictions:\")\n",
    "print(multi_pred_df.to_string(index=False))\n",
    "print(\"\\nNote: Each sample has probability for all 3 classes (sum = 1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for multiclass\n",
    "cm_iris = confusion_matrix(y_test_iris, y_pred_iris)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_iris, annot=True, fmt='d', cmap='Greens', cbar=True,\n",
    "           xticklabels=['Setosa', 'Versicolor', 'Virginica'],\n",
    "           yticklabels=['Setosa', 'Versicolor', 'Virginica'])\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.ylabel('Actual', fontsize=12)\n",
    "plt.title('Multiclass Confusion Matrix\\nIris Species Classification', \n",
    "         fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_iris, y_pred_iris, \n",
    "                          target_names=['Setosa', 'Versicolor', 'Virginica']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Practice building and evaluating logistic regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Binary Classification on Wine Quality\n",
    "\n",
    "Using the wine dataset, create a binary classification problem:\n",
    "\n",
    "Steps:\n",
    "1. Load the wine dataset from 'data/sample/wine.csv'\n",
    "2. Create a binary target: class 0 vs (class 1 and 2 combined)\n",
    "3. Split data (70/30, stratified)\n",
    "4. Scale features\n",
    "5. Train a LogisticRegression model\n",
    "6. Calculate and print accuracy\n",
    "7. Display the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Probability Interpretation\n",
    "\n",
    "Using the breast cancer model (log_reg) we trained:\n",
    "\n",
    "1. Find the sample with the HIGHEST confidence prediction\n",
    "2. Find the sample with the LOWEST confidence prediction\n",
    "3. Print both samples with their probabilities\n",
    "4. What do these confidence levels tell us?\n",
    "\n",
    "Hint: Use y_proba and np.max() to find confidence scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Threshold Tuning\n",
    "\n",
    "For the breast cancer model, experiment with different thresholds:\n",
    "\n",
    "1. Try thresholds: [0.2, 0.4, 0.5, 0.6, 0.8]\n",
    "2. For each threshold, calculate:\n",
    "   - Number of false negatives (FN)\n",
    "   - Number of false positives (FP)\n",
    "3. Create a plot showing FN and FP vs threshold\n",
    "4. Which threshold would you choose for cancer detection and why?\n",
    "\n",
    "Think: In cancer detection, which is worse - FN or FP?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Feature Importance in Logistic Regression\n",
    "\n",
    "Examine which features are most important for predicting breast cancer:\n",
    "\n",
    "1. Get the coefficients from log_reg.coef_[0]\n",
    "2. Create a DataFrame with feature names and coefficients\n",
    "3. Sort by absolute value of coefficients\n",
    "4. Visualize the top 10 most important features (horizontal bar plot)\n",
    "5. Interpret: What do positive/negative coefficients mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You've mastered logistic regression for classification tasks.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Logistic Regression**:\n",
    "   - Adapts linear regression for classification\n",
    "   - Uses sigmoid function to convert to probabilities\n",
    "   - Output always between 0 and 1\n",
    "   - Default threshold: 0.5 for binary classification\n",
    "\n",
    "2. **Sigmoid Function**:\n",
    "   - Formula: σ(z) = 1 / (1 + e^(-z))\n",
    "   - S-shaped curve\n",
    "   - Maps any value to probability [0, 1]\n",
    "   - Critical for converting linear output to probabilities\n",
    "\n",
    "3. **Binary Classification**:\n",
    "   - Two classes: 0 and 1\n",
    "   - Predicts probability of class 1\n",
    "   - Threshold determines final class\n",
    "   - Examples: spam detection, disease diagnosis, fraud detection\n",
    "\n",
    "4. **Probability Predictions**:\n",
    "   - predict_proba() returns probabilities for each class\n",
    "   - Useful for risk assessment and ranking\n",
    "   - Confidence = max probability\n",
    "   - Can adjust threshold based on cost of errors\n",
    "\n",
    "5. **Multiclass Classification**:\n",
    "   - One-vs-Rest (OvR) strategy\n",
    "   - One binary classifier per class\n",
    "   - Choose class with highest probability\n",
    "   - Probabilities sum to 1 across all classes\n",
    "\n",
    "6. **Evaluation Metrics**:\n",
    "   - **Accuracy**: Overall correctness\n",
    "   - **Confusion Matrix**: Breakdown of predictions (TP, TN, FP, FN)\n",
    "   - **Precision**: Of predicted positive, how many correct?\n",
    "   - **Recall**: Of actual positive, how many found?\n",
    "   - **F1-Score**: Balance of precision and recall\n",
    "\n",
    "7. **Best Practices**:\n",
    "   - Always scale features for logistic regression\n",
    "   - Use stratified split for classification\n",
    "   - Examine probability distributions\n",
    "   - Consider cost of FP vs FN when choosing threshold\n",
    "   - Use confusion matrix to understand errors\n",
    "\n",
    "### When to Use Logistic Regression\n",
    "\n",
    "**Good for:**\n",
    "- Binary or multiclass classification\n",
    "- Need probability estimates\n",
    "- Interpretable models (coefficients show feature importance)\n",
    "- Linearly separable classes\n",
    "- Baseline classification models\n",
    "\n",
    "**Not good for:**\n",
    "- Non-linear decision boundaries (use SVM with kernels or neural networks)\n",
    "- Very large feature spaces (consider regularization)\n",
    "- Regression problems (use linear regression instead)\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In **Module 05: Decision Trees**, you'll learn:\n",
    "- How decision trees make classifications\n",
    "- Understanding tree depth and complexity\n",
    "- Visualizing decision trees\n",
    "- Feature importance in tree-based models\n",
    "- Advantages and disadvantages of trees\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [Logistic Regression - StatQuest](https://www.youtube.com/watch?v=yIYKR4sgzI8)\n",
    "- [scikit-learn Logistic Regression](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)\n",
    "- [Understanding the Sigmoid Function](https://towardsdatascience.com/derivative-of-the-sigmoid-function-536880cf918e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
