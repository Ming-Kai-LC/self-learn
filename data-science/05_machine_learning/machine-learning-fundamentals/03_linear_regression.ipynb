{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 03: Linear Regression\n",
    "\n",
    "**Difficulty**: ⭐⭐ Intermediate  \n",
    "**Estimated Time**: 90 minutes  \n",
    "**Prerequisites**: \n",
    "- [Module 00: Introduction to Machine Learning](00_introduction_to_machine_learning.ipynb)\n",
    "- [Module 01: Supervised vs Unsupervised Learning](01_supervised_vs_unsupervised_learning.ipynb)\n",
    "- [Module 02: Data Preparation and Splitting](02_data_preparation_and_splitting.ipynb)\n",
    "- Basic calculus and linear algebra\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Understand the mathematical foundation of linear regression (OLS)\n",
    "2. Distinguish between simple and multiple linear regression\n",
    "3. Implement linear regression using scikit-learn\n",
    "4. Create and use polynomial features for non-linear relationships\n",
    "5. Evaluate regression models using R², MSE, RMSE, and MAE\n",
    "6. Interpret coefficients and analyze residuals\n",
    "7. Recognize and validate linear regression assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Scikit-learn for ML\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, mean_absolute_error, r2_score,\n",
    "    mean_absolute_percentage_error\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What is Linear Regression?\n",
    "\n",
    "### Definition\n",
    "\n",
    "**Linear Regression** is a supervised learning algorithm that models the relationship between:\n",
    "- **Independent variables (X)**: Features/predictors\n",
    "- **Dependent variable (y)**: Target/outcome\n",
    "\n",
    "By finding the **best-fitting straight line** (or hyperplane in multiple dimensions) through the data.\n",
    "\n",
    "### The Linear Equation\n",
    "\n",
    "**Simple Linear Regression** (one feature):\n",
    "```\n",
    "y = β₀ + β₁x + ε\n",
    "```\n",
    "\n",
    "**Multiple Linear Regression** (many features):\n",
    "```\n",
    "y = β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ + ε\n",
    "```\n",
    "\n",
    "Where:\n",
    "- **β₀** (beta zero): Intercept (value when all x = 0)\n",
    "- **β₁, β₂, ...** (beta coefficients): Slopes (effect of each feature)\n",
    "- **ε** (epsilon): Error term (what the model can't explain)\n",
    "\n",
    "### Goal: Minimize Error\n",
    "\n",
    "Find the coefficients (β values) that **minimize the sum of squared errors** between predictions and actual values.\n",
    "\n",
    "This is called **Ordinary Least Squares (OLS)** estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Simple Linear Regression\n",
    "\n",
    "Let's start with the simplest case: predicting one variable from another.\n",
    "\n",
    "### Example: Years of Experience → Salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple dataset\n",
    "np.random.seed(42)\n",
    "years_experience = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "salary = 30000 + 5000 * years_experience + np.random.normal(0, 3000, 10)\n",
    "\n",
    "# Create DataFrame for better visualization\n",
    "simple_df = pd.DataFrame({\n",
    "    'Years_Experience': years_experience,\n",
    "    'Salary': salary\n",
    "})\n",
    "\n",
    "print(\"Simple Linear Regression Dataset:\")\n",
    "print(simple_df)\n",
    "print(f\"\\nCorrelation: {simple_df.corr().iloc[0, 1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the relationship\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(years_experience, salary, s=100, alpha=0.7, edgecolors='k')\n",
    "plt.xlabel('Years of Experience', fontsize=12)\n",
    "plt.ylabel('Salary ($)', fontsize=12)\n",
    "plt.title('Relationship: Experience vs Salary', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice the roughly linear relationship!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for sklearn (needs 2D arrays)\n",
    "X_simple = years_experience.reshape(-1, 1)  # Reshape to column vector\n",
    "y_simple = salary\n",
    "\n",
    "# Create and train the model\n",
    "simple_model = LinearRegression()\n",
    "simple_model.fit(X_simple, y_simple)\n",
    "\n",
    "# Get the learned parameters\n",
    "intercept = simple_model.intercept_\n",
    "slope = simple_model.coef_[0]\n",
    "\n",
    "print(\"Learned Linear Equation:\")\n",
    "print(f\"Salary = {intercept:.2f} + {slope:.2f} × Years\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"- Base salary (no experience): ${intercept:.2f}\")\n",
    "print(f\"- Salary increase per year: ${slope:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_simple = simple_model.predict(X_simple)\n",
    "\n",
    "# Visualize the fitted line\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(years_experience, salary, s=100, alpha=0.7, \n",
    "           edgecolors='k', label='Actual Data')\n",
    "plt.plot(years_experience, y_pred_simple, 'r-', linewidth=2, \n",
    "        label=f'Fitted Line: y = {intercept:.0f} + {slope:.0f}x')\n",
    "\n",
    "# Show residuals (errors)\n",
    "for i in range(len(years_experience)):\n",
    "    plt.plot([years_experience[i], years_experience[i]], \n",
    "            [salary[i], y_pred_simple[i]], \n",
    "            'g--', alpha=0.5, linewidth=1)\n",
    "\n",
    "plt.xlabel('Years of Experience', fontsize=12)\n",
    "plt.ylabel('Salary ($)', fontsize=12)\n",
    "plt.title('Simple Linear Regression: Best-Fit Line', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Green dashed lines show residuals (prediction errors)\")\n",
    "print(\"OLS minimizes the sum of squared residuals!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multiple Linear Regression\n",
    "\n",
    "Real-world problems usually have multiple features. Let's use the California housing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load California housing dataset\n",
    "housing = datasets.fetch_california_housing()\n",
    "X = housing.data\n",
    "y = housing.target  # Median house value (in $100,000s)\n",
    "\n",
    "# Use subset for faster computation\n",
    "X = X[:1000]\n",
    "y = y[:1000]\n",
    "\n",
    "# Create DataFrame for exploration\n",
    "housing_df = pd.DataFrame(X, columns=housing.feature_names)\n",
    "housing_df['MedHouseValue'] = y\n",
    "\n",
    "print(\"California Housing Dataset:\")\n",
    "print(f\"Shape: {housing_df.shape}\")\n",
    "print(f\"\\nFeatures: {list(housing.feature_names)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(housing_df.head())\n",
    "print(f\"\\nTarget range: ${y.min()*100000:.0f} - ${y.max()*100000:.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore correlations with target\n",
    "correlations = housing_df.corr()['MedHouseValue'].sort_values(ascending=False)\n",
    "\n",
    "print(\"Feature Correlations with House Value:\")\n",
    "print(correlations)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "correlations[1:].plot(kind='barh')\n",
    "plt.xlabel('Correlation with Median House Value')\n",
    "plt.title('Feature Importance (Correlation)', fontsize=14)\n",
    "plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nMedInc (median income) has strongest positive correlation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data (BEFORE preprocessing!)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features (fit on training data only!)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train multiple linear regression model\n",
    "multi_model = LinearRegression()\n",
    "multi_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Multiple Linear Regression Model Trained!\")\n",
    "print(f\"\\nIntercept: {multi_model.intercept_:.3f}\")\n",
    "print(f\"\\nCoefficients:\")\n",
    "for feature, coef in zip(housing.feature_names, multi_model.coef_):\n",
    "    print(f\"  {feature:12s}: {coef:8.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_train_pred = multi_model.predict(X_train_scaled)\n",
    "y_test_pred = multi_model.predict(X_test_scaled)\n",
    "\n",
    "# Visualize predictions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Training set\n",
    "axes[0].scatter(y_train, y_train_pred, alpha=0.6, edgecolors='k')\n",
    "axes[0].plot([y_train.min(), y_train.max()], \n",
    "            [y_train.min(), y_train.max()], \n",
    "            'r--', lw=2, label='Perfect Prediction')\n",
    "axes[0].set_xlabel('Actual Price ($100,000s)')\n",
    "axes[0].set_ylabel('Predicted Price ($100,000s)')\n",
    "axes[0].set_title('Training Set Predictions')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Test set\n",
    "axes[1].scatter(y_test, y_test_pred, alpha=0.6, edgecolors='k')\n",
    "axes[1].plot([y_test.min(), y_test.max()], \n",
    "            [y_test.min(), y_test.max()], \n",
    "            'r--', lw=2, label='Perfect Prediction')\n",
    "axes[1].set_xlabel('Actual Price ($100,000s)')\n",
    "axes[1].set_ylabel('Predicted Price ($100,000s)')\n",
    "axes[1].set_title('Test Set Predictions')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Points closer to red line = better predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Regression Metrics\n",
    "\n",
    "How do we measure regression model performance?\n",
    "\n",
    "### 1. R² (R-squared) - Coefficient of Determination\n",
    "\n",
    "**Formula**: R² = 1 - (SS_res / SS_tot)\n",
    "\n",
    "**Interpretation**:\n",
    "- Ranges from 0 to 1 (can be negative for bad models)\n",
    "- **0.7** = model explains 70% of variance\n",
    "- Higher is better\n",
    "- **Perfect score**: 1.0\n",
    "\n",
    "### 2. MSE (Mean Squared Error)\n",
    "\n",
    "**Formula**: MSE = (1/n) Σ(y_actual - y_pred)²\n",
    "\n",
    "**Interpretation**:\n",
    "- Average squared difference\n",
    "- Penalizes large errors heavily (squared term)\n",
    "- Lower is better\n",
    "- **Perfect score**: 0.0\n",
    "\n",
    "### 3. RMSE (Root Mean Squared Error)\n",
    "\n",
    "**Formula**: RMSE = √MSE\n",
    "\n",
    "**Interpretation**:\n",
    "- Same units as target variable\n",
    "- More interpretable than MSE\n",
    "- Lower is better\n",
    "\n",
    "### 4. MAE (Mean Absolute Error)\n",
    "\n",
    "**Formula**: MAE = (1/n) Σ|y_actual - y_pred|\n",
    "\n",
    "**Interpretation**:\n",
    "- Average absolute difference\n",
    "- Less sensitive to outliers than MSE\n",
    "- More robust\n",
    "- Lower is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate all metrics\n",
    "def evaluate_regression(y_true, y_pred, set_name=\"\"):\n",
    "    \"\"\"Calculate and display regression metrics\"\"\"\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n",
    "    \n",
    "    print(f\"\\n{set_name} Metrics:\")\n",
    "    print(f\"  R² Score:  {r2:.4f}  (Higher is better, max=1.0)\")\n",
    "    print(f\"  MSE:       {mse:.4f}\")\n",
    "    print(f\"  RMSE:      ${rmse*100000:.2f}  (in actual dollars)\")\n",
    "    print(f\"  MAE:       ${mae*100000:.2f}  (average error)\")\n",
    "    print(f\"  MAPE:      {mape:.2f}%  (percentage error)\")\n",
    "    \n",
    "    return {'R2': r2, 'MSE': mse, 'RMSE': rmse, 'MAE': mae, 'MAPE': mape}\n",
    "\n",
    "# Evaluate on both sets\n",
    "train_metrics = evaluate_regression(y_train, y_train_pred, \"Training Set\")\n",
    "test_metrics = evaluate_regression(y_test, y_test_pred, \"Test Set\")\n",
    "\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"Model explains {test_metrics['R2']:.1%} of variance in house prices\")\n",
    "print(f\"Average prediction error: ${test_metrics['MAE']*100000:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Residual Analysis\n",
    "\n",
    "**Residuals** are the differences between actual and predicted values.\n",
    "\n",
    "Good residuals should:\n",
    "1. **Be randomly distributed** (no patterns)\n",
    "2. **Have mean ≈ 0**\n",
    "3. **Have constant variance** (homoscedasticity)\n",
    "4. **Follow normal distribution**\n",
    "\n",
    "Patterns in residuals indicate model problems!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate residuals\n",
    "residuals = y_test - y_test_pred\n",
    "\n",
    "# Comprehensive residual analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Residuals vs Predicted Values\n",
    "axes[0, 0].scatter(y_test_pred, residuals, alpha=0.6, edgecolors='k')\n",
    "axes[0, 0].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "axes[0, 0].set_xlabel('Predicted Values')\n",
    "axes[0, 0].set_ylabel('Residuals')\n",
    "axes[0, 0].set_title('Residual Plot (should be random around 0)')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Distribution of Residuals\n",
    "axes[0, 1].hist(residuals, bins=30, edgecolor='k', alpha=0.7)\n",
    "axes[0, 1].axvline(x=0, color='r', linestyle='--', lw=2)\n",
    "axes[0, 1].set_xlabel('Residuals')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title(f'Residual Distribution (mean={residuals.mean():.4f})')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Q-Q Plot (normality check)\n",
    "stats.probplot(residuals, dist=\"norm\", plot=axes[1, 0])\n",
    "axes[1, 0].set_title('Q-Q Plot (should follow red line for normality)')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Scale-Location Plot\n",
    "standardized_residuals = (residuals - residuals.mean()) / residuals.std()\n",
    "axes[1, 1].scatter(y_test_pred, np.sqrt(np.abs(standardized_residuals)), \n",
    "                  alpha=0.6, edgecolors='k')\n",
    "axes[1, 1].set_xlabel('Predicted Values')\n",
    "axes[1, 1].set_ylabel('√|Standardized Residuals|')\n",
    "axes[1, 1].set_title('Scale-Location Plot (check for constant variance)')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nResidual Analysis Checklist:\")\n",
    "print(\"✓ Top-left: Should show no clear pattern (random scatter)\")\n",
    "print(\"✓ Top-right: Should be roughly bell-shaped (normal distribution)\")\n",
    "print(\"✓ Bottom-left: Points should follow red line (normality)\")\n",
    "print(\"✓ Bottom-right: Should show constant spread (homoscedasticity)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Polynomial Regression\n",
    "\n",
    "What if the relationship isn't linear? Use **polynomial features**!\n",
    "\n",
    "Transform: x → [x, x², x³, ...]\n",
    "\n",
    "Still uses linear regression, but on transformed features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create non-linear dataset\n",
    "np.random.seed(42)\n",
    "X_poly_raw = np.linspace(0, 3, 50)\n",
    "y_poly_raw = 0.5 * X_poly_raw**2 + X_poly_raw + 2 + np.random.normal(0, 0.5, 50)\n",
    "\n",
    "X_poly = X_poly_raw.reshape(-1, 1)\n",
    "\n",
    "# Visualize the non-linear relationship\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_poly, y_poly_raw, s=50, alpha=0.7, edgecolors='k')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Non-Linear Relationship (Quadratic)', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"This data has a quadratic (curved) relationship!\")\n",
    "print(\"Simple linear regression won't fit well.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare linear vs polynomial regression\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# 1. Linear (degree=1)\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_poly, y_poly_raw)\n",
    "y_linear_pred = linear_model.predict(X_poly)\n",
    "\n",
    "axes[0].scatter(X_poly, y_poly_raw, s=50, alpha=0.7, edgecolors='k')\n",
    "axes[0].plot(X_poly, y_linear_pred, 'r-', linewidth=2)\n",
    "axes[0].set_title(f'Linear (R²={r2_score(y_poly_raw, y_linear_pred):.3f})')\n",
    "axes[0].set_xlabel('X')\n",
    "axes[0].set_ylabel('y')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Polynomial (degree=2)\n",
    "poly_features_2 = PolynomialFeatures(degree=2)\n",
    "X_poly_2 = poly_features_2.fit_transform(X_poly)\n",
    "poly_model_2 = LinearRegression()\n",
    "poly_model_2.fit(X_poly_2, y_poly_raw)\n",
    "y_poly_pred_2 = poly_model_2.predict(X_poly_2)\n",
    "\n",
    "axes[1].scatter(X_poly, y_poly_raw, s=50, alpha=0.7, edgecolors='k')\n",
    "axes[1].plot(X_poly, y_poly_pred_2, 'g-', linewidth=2)\n",
    "axes[1].set_title(f'Polynomial deg=2 (R²={r2_score(y_poly_raw, y_poly_pred_2):.3f})')\n",
    "axes[1].set_xlabel('X')\n",
    "axes[1].set_ylabel('y')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Polynomial (degree=5) - Overfitting!\n",
    "poly_features_5 = PolynomialFeatures(degree=5)\n",
    "X_poly_5 = poly_features_5.fit_transform(X_poly)\n",
    "poly_model_5 = LinearRegression()\n",
    "poly_model_5.fit(X_poly_5, y_poly_raw)\n",
    "y_poly_pred_5 = poly_model_5.predict(X_poly_5)\n",
    "\n",
    "axes[2].scatter(X_poly, y_poly_raw, s=50, alpha=0.7, edgecolors='k')\n",
    "axes[2].plot(X_poly, y_poly_pred_5, 'purple', linewidth=2)\n",
    "axes[2].set_title(f'Polynomial deg=5 (R²={r2_score(y_poly_raw, y_poly_pred_5):.3f})')\n",
    "axes[2].set_xlabel('X')\n",
    "axes[2].set_ylabel('y')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(\"• Degree=1 (linear): Underfits - too simple\")\n",
    "print(\"• Degree=2 (quadratic): Perfect fit for this data!\")\n",
    "print(\"• Degree=5: Overfits - follows noise too closely\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show what PolynomialFeatures does\n",
    "sample_X = np.array([[2], [3]])\n",
    "poly_transformer = PolynomialFeatures(degree=3)\n",
    "sample_X_poly = poly_transformer.fit_transform(sample_X)\n",
    "\n",
    "print(\"Polynomial Features Transformation:\")\n",
    "print(\"\\nOriginal features:\")\n",
    "print(sample_X)\n",
    "print(\"\\nPolynomial features (degree=3):\")\n",
    "print(sample_X_poly)\n",
    "print(\"\\nFeature names:\")\n",
    "print(poly_transformer.get_feature_names_out())\n",
    "print(\"\\nFor x=2: [1, x, x², x³] = [1, 2, 4, 8]\")\n",
    "print(\"For x=3: [1, x, x², x³] = [1, 3, 9, 27]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Linear Regression Assumptions\n",
    "\n",
    "For linear regression to work well, data should satisfy:\n",
    "\n",
    "### 1. Linearity\n",
    "- Relationship between X and y should be linear\n",
    "- **Check**: Scatter plots, residual plots\n",
    "\n",
    "### 2. Independence\n",
    "- Observations should be independent\n",
    "- **Check**: Domain knowledge, avoid time series without proper handling\n",
    "\n",
    "### 3. Homoscedasticity\n",
    "- Residuals should have constant variance\n",
    "- **Check**: Scale-location plot\n",
    "\n",
    "### 4. Normality of Residuals\n",
    "- Residuals should follow normal distribution\n",
    "- **Check**: Q-Q plot, histogram of residuals\n",
    "\n",
    "### 5. No Multicollinearity\n",
    "- Features shouldn't be highly correlated with each other\n",
    "- **Check**: Correlation matrix, VIF (Variance Inflation Factor)\n",
    "\n",
    "**Note**: Violations don't always break the model, but reduce reliability!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Practice Exercises\n",
    "\n",
    "### Exercise 1: Diabetes Dataset Regression\n",
    "\n",
    "Load the diabetes dataset (`datasets.load_diabetes()`), build a linear regression model, and:\n",
    "1. Calculate R², MSE, RMSE, and MAE\n",
    "2. Identify the 3 most important features (highest coefficient magnitudes)\n",
    "3. Create a residual plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Feature Engineering Impact\n",
    "\n",
    "Using the California housing dataset:\n",
    "1. Create a new feature: `rooms_per_household = AveRooms / AveOccup`\n",
    "2. Train two models: one with original features, one with the new feature added\n",
    "3. Compare R² scores - did the new feature help?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Polynomial Degree Selection\n",
    "\n",
    "Create a synthetic dataset with a cubic relationship (y = x³ + noise).\n",
    "Test polynomial regression with degrees 1, 2, 3, 4, and 5.\n",
    "Which degree gives the best test set performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Interpretation Challenge\n",
    "\n",
    "Train a linear regression model on the Boston housing dataset (or California housing).\n",
    "Answer these questions:\n",
    "1. What does a coefficient of 2.5 for feature \"X\" mean?\n",
    "2. If you increase \"X\" by 1 unit, how much does the prediction change?\n",
    "3. Which feature has the strongest effect on price?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "### Key Concepts Learned\n",
    "\n",
    "1. **Linear Regression Fundamentals**:\n",
    "   - Models linear relationship: y = β₀ + β₁x₁ + ... + βₙxₙ\n",
    "   - Uses Ordinary Least Squares (OLS) to minimize errors\n",
    "   - Simple (1 feature) vs Multiple (many features)\n",
    "\n",
    "2. **Regression Metrics**:\n",
    "   - **R²**: Proportion of variance explained (0-1, higher better)\n",
    "   - **MSE**: Mean squared error (lower better, penalizes large errors)\n",
    "   - **RMSE**: Root MSE (same units as target, more interpretable)\n",
    "   - **MAE**: Mean absolute error (robust to outliers)\n",
    "\n",
    "3. **Residual Analysis**:\n",
    "   - Residuals = actual - predicted\n",
    "   - Should be randomly distributed around 0\n",
    "   - No patterns = good model fit\n",
    "   - Patterns indicate model problems\n",
    "\n",
    "4. **Polynomial Regression**:\n",
    "   - Handles non-linear relationships\n",
    "   - Transforms features: x → [x, x², x³, ...]\n",
    "   - Still uses linear regression on transformed features\n",
    "   - Careful: high degrees can overfit!\n",
    "\n",
    "5. **Model Assumptions**:\n",
    "   - Linearity (or use polynomial features)\n",
    "   - Independence of observations\n",
    "   - Homoscedasticity (constant variance)\n",
    "   - Normality of residuals\n",
    "   - No multicollinearity\n",
    "\n",
    "### When to Use Linear Regression\n",
    "\n",
    "✅ **Good for**:\n",
    "- Predicting continuous values\n",
    "- Understanding feature importance\n",
    "- Interpretable models (coefficients have meaning)\n",
    "- Fast training and prediction\n",
    "- Baseline model for comparison\n",
    "\n",
    "❌ **Not ideal for**:\n",
    "- Complex non-linear relationships (use polynomial or other models)\n",
    "- Classification tasks (use logistic regression instead)\n",
    "- High-dimensional data with many correlated features\n",
    "- Data with many outliers\n",
    "\n",
    "### Quick Reference: Scikit-learn\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Simple linear regression\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Polynomial regression\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(X)\n",
    "model.fit(X_poly, y)\n",
    "\n",
    "# Evaluate\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "```\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the next module, we'll explore:\n",
    "- **Logistic Regression** for classification\n",
    "- Sigmoid function and decision boundaries\n",
    "- Binary and multi-class classification\n",
    "- Classification metrics\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [Scikit-learn Linear Regression](https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares)\n",
    "- [StatQuest: Linear Regression](https://www.youtube.com/watch?v=nk2CQITm_eo)\n",
    "- [Andrew Ng's ML Course - Linear Regression](https://www.coursera.org/learn/machine-learning)\n",
    "- [Polynomial Regression in Depth](https://towardsdatascience.com/polynomial-regression-bbe8b9d97491)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
