{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 13: Dimensionality Reduction - PCA and t-SNE\n",
    "\n",
    "**Difficulty**: ‚≠ê‚≠ê Intermediate  \n",
    "**Estimated Time**: 75 minutes  \n",
    "**Prerequisites**: [Module 02 - Data Preparation](02_data_preparation_train_test_split.ipynb), [Module 12 - Clustering](12_clustering_kmeans_dbscan.ipynb)\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Understand why dimensionality reduction is important\n",
    "2. Explain the curse of dimensionality and its effects\n",
    "3. Apply Principal Component Analysis (PCA) for dimensionality reduction\n",
    "4. Understand eigenvalues, eigenvectors, and explained variance\n",
    "5. Choose the optimal number of principal components\n",
    "6. Use PCA for data visualization and preprocessing\n",
    "7. Apply t-SNE for 2D/3D visualization of high-dimensional data\n",
    "8. Understand when to use PCA vs t-SNE\n",
    "9. Use PCA to improve machine learning model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction: Why Reduce Dimensions?\n",
    "\n",
    "### What is Dimensionality Reduction?\n",
    "\n",
    "**Dimensionality reduction** transforms high-dimensional data into lower dimensions while preserving important information.\n",
    "\n",
    "**Example**: 100 features ‚Üí 10 features (keeping 95% of information)\n",
    "\n",
    "### The Curse of Dimensionality (Revisited)\n",
    "\n",
    "As dimensions increase:\n",
    "- Data becomes **sparse** (points are far apart)\n",
    "- **Distances become meaningless** (all points roughly equidistant)\n",
    "- **Computational cost explodes** (exponentially more data needed)\n",
    "- **Visualization becomes impossible** (can't plot 100D data!)\n",
    "- **Overfitting risk increases** (more parameters to learn)\n",
    "\n",
    "### Benefits of Dimensionality Reduction\n",
    "\n",
    "**1. Visualization** üé®\n",
    "- Humans can only see 2D or 3D\n",
    "- Project high-D data to 2D/3D for exploration\n",
    "- Discover patterns, clusters, outliers\n",
    "\n",
    "**2. Speed** ‚ö°\n",
    "- Fewer features ‚Üí faster training\n",
    "- Fewer features ‚Üí faster predictions\n",
    "- Critical for large datasets\n",
    "\n",
    "**3. Remove Redundancy** üîÑ\n",
    "- Many features are correlated\n",
    "- Height in cm vs height in inches (redundant!)\n",
    "- Combine correlated features\n",
    "\n",
    "**4. Avoid Overfitting** üìâ\n",
    "- Fewer dimensions ‚Üí simpler models\n",
    "- Less risk of fitting noise\n",
    "- Better generalization\n",
    "\n",
    "**5. Storage** üíæ\n",
    "- Smaller data = less storage\n",
    "- Faster I/O operations\n",
    "\n",
    "### Two Main Approaches\n",
    "\n",
    "**Feature Selection**:\n",
    "- Choose subset of original features\n",
    "- Keep: [feature_1, feature_5, feature_10]\n",
    "- Discard others\n",
    "\n",
    "**Feature Extraction** (PCA, t-SNE):\n",
    "- Create new features from combinations of original features\n",
    "- PC1 = 0.5√ófeature_1 + 0.3√ófeature_2 + ...\n",
    "- More powerful but less interpretable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import warnings\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "%matplotlib inline\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('Set2')\n",
    "\n",
    "print('‚úì All libraries imported successfully!')\n",
    "print(f'‚úì Random seed set to 42 for reproducibility')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets for dimensionality reduction\n",
    "\n",
    "# Dataset 1: Breast Cancer (30 features - good for PCA)\n",
    "bc_df = pd.read_csv('data/sample/breast_cancer.csv')\n",
    "print(\"Breast Cancer Dataset (High-dimensional):\")\n",
    "print(f\"Shape: {bc_df.shape}\")\n",
    "print(f\"Features: {bc_df.shape[1] - 1} (excluding target)\")\n",
    "\n",
    "# Dataset 2: Digits (64 features - pixel intensities)\n",
    "digits_df = pd.read_csv('data/sample/digits.csv')\n",
    "print(\"\\nDigits Dataset (8x8 pixel images):\")\n",
    "print(f\"Shape: {digits_df.shape}\")\n",
    "print(f\"Features: {digits_df.shape[1] - 1} (pixel values)\")\n",
    "\n",
    "# Dataset 3: Wine (13 features)\n",
    "wine_df = pd.read_csv('data/sample/wine.csv')\n",
    "print(\"\\nWine Dataset (Chemical properties):\")\n",
    "print(f\"Shape: {wine_df.shape}\")\n",
    "print(f\"Features: {wine_df.shape[1] - 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Principal Component Analysis (PCA): The Concept\n",
    "\n",
    "### What is PCA?\n",
    "\n",
    "**PCA** = Principal Component Analysis\n",
    "\n",
    "**Goal**: Find new axes (principal components) that capture maximum variance in data\n",
    "\n",
    "**Key idea**: Data often varies more in some directions than others\n",
    "- **PC1** (1st principal component): Direction of maximum variance\n",
    "- **PC2** (2nd principal component): Direction of 2nd-most variance (perpendicular to PC1)\n",
    "- **PC3**, **PC4**, ... : Continue finding perpendicular directions\n",
    "\n",
    "### Simple Analogy: Shadow on a Wall\n",
    "\n",
    "Imagine a 3D object casting a shadow on a 2D wall:\n",
    "- Original: 3D object (high-dimensional data)\n",
    "- Shadow: 2D projection (reduced dimensions)\n",
    "- PCA finds the **best angle** to orient the wall so the shadow shows the most detail\n",
    "\n",
    "### How PCA Works (Simplified)\n",
    "\n",
    "**Step 1: Standardize data**\n",
    "- Center data (mean = 0)\n",
    "- Scale to unit variance\n",
    "- Important: PCA is sensitive to scale!\n",
    "\n",
    "**Step 2: Compute covariance matrix**\n",
    "- Shows how features vary together\n",
    "- High covariance = features are correlated\n",
    "\n",
    "**Step 3: Find eigenvectors and eigenvalues**\n",
    "- **Eigenvectors**: Directions of principal components\n",
    "- **Eigenvalues**: Amount of variance in each direction\n",
    "- Sort by eigenvalue (largest first)\n",
    "\n",
    "**Step 4: Project data**\n",
    "- Keep top K eigenvectors (K = desired dimensions)\n",
    "- Transform original data using these vectors\n",
    "- Result: Data in K dimensions\n",
    "\n",
    "### Eigenvalues and Eigenvectors (Don't Panic!)\n",
    "\n",
    "**Eigenvector**: A direction in space\n",
    "- Think: Arrow pointing in a specific direction\n",
    "- For PCA: Direction where data varies\n",
    "\n",
    "**Eigenvalue**: Magnitude of variance in that direction\n",
    "- Large eigenvalue = data spreads a lot in this direction\n",
    "- Small eigenvalue = data doesn't vary much in this direction\n",
    "\n",
    "**In PCA terms**:\n",
    "- Eigenvector with largest eigenvalue = PC1 (most important)\n",
    "- Eigenvector with 2nd largest eigenvalue = PC2\n",
    "- And so on..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Applying PCA: Step by Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare breast cancer data\n",
    "X_bc = bc_df.drop('target', axis=1).values\n",
    "y_bc = bc_df['target'].values\n",
    "\n",
    "print(f\"Original data shape: {X_bc.shape}\")\n",
    "print(f\"Number of features: {X_bc.shape[1]}\")\n",
    "print(f\"\\nFeature ranges (before scaling):\")\n",
    "print(f\"Min: {X_bc.min():.2f}\")\n",
    "print(f\"Max: {X_bc.max():.2f}\")\n",
    "print(f\"\\nThese features have very different scales - scaling is critical!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Standardize the data\n",
    "# CRITICAL: PCA is very sensitive to feature scales!\n",
    "scaler = StandardScaler()\n",
    "X_bc_scaled = scaler.fit_transform(X_bc)\n",
    "\n",
    "print(\"After standardization:\")\n",
    "print(f\"Mean of each feature: ~0 (actually {X_bc_scaled.mean():.10f})\")\n",
    "print(f\"Std of each feature: ~1 (actually {X_bc_scaled.std():.10f})\")\n",
    "print(\"\\n‚úì Data is now ready for PCA!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Apply PCA (keep all components initially to analyze)\n",
    "pca_full = PCA()\n",
    "X_bc_pca_full = pca_full.fit_transform(X_bc_scaled)\n",
    "\n",
    "print(\"PCA Results:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Original dimensions: {X_bc_scaled.shape[1]}\")\n",
    "print(f\"Number of components: {pca_full.n_components_}\")\n",
    "print(f\"Transformed data shape: {X_bc_pca_full.shape}\")\n",
    "print(\"\\nExplained variance ratio per component:\")\n",
    "print(\"(What % of variance each PC captures)\\n\")\n",
    "\n",
    "for i, var_ratio in enumerate(pca_full.explained_variance_ratio_[:10], 1):\n",
    "    print(f\"PC{i:2d}: {var_ratio:.4f} ({var_ratio*100:.2f}%)\")\n",
    "    \n",
    "print(\"\\n‚úì PC1 alone captures {:.1f}% of total variance!\".format(\n",
    "    pca_full.explained_variance_ratio_[0] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Explained Variance: How Much Information Do We Keep?\n",
    "\n",
    "### Explained Variance Ratio\n",
    "\n",
    "**Measures**: What proportion of total variance each PC captures\n",
    "\n",
    "**Interpretation**:\n",
    "- PC1 = 45% ‚Üí First component captures 45% of data's variance\n",
    "- PC2 = 20% ‚Üí Second component captures 20% more\n",
    "- Cumulative = 65% ‚Üí Together they capture 65% of variance\n",
    "\n",
    "### The 90% Rule\n",
    "\n",
    "**Common practice**: Keep enough PCs to capture 90-95% of variance\n",
    "- Preserves most information\n",
    "- Reduces dimensions significantly\n",
    "- Good balance between compression and information retention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cumulative explained variance\n",
    "cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "\n",
    "print(\"Cumulative Explained Variance:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Components':<15} {'Individual':<15} {'Cumulative':<15}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i in range(min(15, len(cumulative_variance))):\n",
    "    print(f\"PC1-PC{i+1:<10} {pca_full.explained_variance_ratio_[i]:>10.4f} {cumulative_variance[i]:>15.4f}\")\n",
    "\n",
    "# Find number of components for 90% variance\n",
    "n_components_90 = np.argmax(cumulative_variance >= 0.90) + 1\n",
    "n_components_95 = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"\\nüìä Key Findings:\")\n",
    "print(f\"   - {n_components_90} components needed for 90% variance\")\n",
    "print(f\"   - {n_components_95} components needed for 95% variance\")\n",
    "print(f\"   - Original: {X_bc_scaled.shape[1]} dimensions\")\n",
    "print(f\"   - Reduction: {X_bc_scaled.shape[1]} ‚Üí {n_components_90} dimensions (90% info retained)\")\n",
    "print(f\"   - Compression ratio: {(1 - n_components_90/X_bc_scaled.shape[1])*100:.1f}% reduction!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize explained variance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Individual explained variance (Scree plot)\n",
    "axes[0].bar(range(1, len(pca_full.explained_variance_ratio_) + 1),\n",
    "           pca_full.explained_variance_ratio_,\n",
    "           alpha=0.7)\n",
    "axes[0].set_xlabel('Principal Component', fontsize=12)\n",
    "axes[0].set_ylabel('Explained Variance Ratio', fontsize=12)\n",
    "axes[0].set_title('Scree Plot: Individual Explained Variance', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlim(0, 20)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 2: Cumulative explained variance\n",
    "axes[1].plot(range(1, len(cumulative_variance) + 1),\n",
    "            cumulative_variance,\n",
    "            'o-',\n",
    "            linewidth=2,\n",
    "            markersize=6)\n",
    "axes[1].axhline(0.90, color='red', linestyle='--', label='90% threshold', linewidth=2)\n",
    "axes[1].axhline(0.95, color='orange', linestyle='--', label='95% threshold', linewidth=2)\n",
    "axes[1].axvline(n_components_90, color='red', linestyle=':', alpha=0.5)\n",
    "axes[1].axvline(n_components_95, color='orange', linestyle=':', alpha=0.5)\n",
    "axes[1].set_xlabel('Number of Components', fontsize=12)\n",
    "axes[1].set_ylabel('Cumulative Explained Variance', fontsize=12)\n",
    "axes[1].set_title('Cumulative Explained Variance', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_xlim(0, 20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Interpretation:\")\n",
    "print(\"- Left: First few PCs capture most variance (elbow around PC5-7)\")\n",
    "print(\"- Right: Sharp rise initially, then plateau\")\n",
    "print(f\"- {n_components_90} PCs give 90% of information (instead of {X_bc_scaled.shape[1]})!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. PCA for Visualization: 2D and 3D Projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA with just 2 components for visualization\n",
    "pca_2d = PCA(n_components=2)\n",
    "X_bc_2d = pca_2d.fit_transform(X_bc_scaled)\n",
    "\n",
    "print(\"PCA 2D Projection:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Original dimensions: {X_bc_scaled.shape[1]}\")\n",
    "print(f\"Reduced to: 2 dimensions\")\n",
    "print(f\"Variance preserved: {pca_2d.explained_variance_ratio_.sum():.1%}\")\n",
    "print(f\"\\nPC1 explains: {pca_2d.explained_variance_ratio_[0]:.1%}\")\n",
    "print(f\"PC2 explains: {pca_2d.explained_variance_ratio_[1]:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize 2D PCA projection\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "# Plot points colored by class\n",
    "scatter = plt.scatter(\n",
    "    X_bc_2d[:, 0],\n",
    "    X_bc_2d[:, 1],\n",
    "    c=y_bc,\n",
    "    cmap='coolwarm',\n",
    "    alpha=0.6,\n",
    "    s=50,\n",
    "    edgecolors='black',\n",
    "    linewidth=0.5\n",
    ")\n",
    "\n",
    "plt.xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]:.1%} variance)', fontsize=12)\n",
    "plt.ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]:.1%} variance)', fontsize=12)\n",
    "plt.title('PCA 2D Projection of Breast Cancer Data\\n(30D ‚Üí 2D)', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(scatter, label='Class (0=Malignant, 1=Benign)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Success! We can now visualize 30D data in 2D!\")\n",
    "print(\"   Notice: Classes are reasonably well-separated even in just 2D.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D visualization\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "pca_3d = PCA(n_components=3)\n",
    "X_bc_3d = pca_3d.fit_transform(X_bc_scaled)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 9))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "scatter = ax.scatter(\n",
    "    X_bc_3d[:, 0],\n",
    "    X_bc_3d[:, 1],\n",
    "    X_bc_3d[:, 2],\n",
    "    c=y_bc,\n",
    "    cmap='coolwarm',\n",
    "    alpha=0.6,\n",
    "    s=50,\n",
    "    edgecolors='black',\n",
    "    linewidth=0.5\n",
    ")\n",
    "\n",
    "ax.set_xlabel(f'PC1 ({pca_3d.explained_variance_ratio_[0]:.1%})', fontsize=11)\n",
    "ax.set_ylabel(f'PC2 ({pca_3d.explained_variance_ratio_[1]:.1%})', fontsize=11)\n",
    "ax.set_zlabel(f'PC3 ({pca_3d.explained_variance_ratio_[2]:.1%})', fontsize=11)\n",
    "ax.set_title('PCA 3D Projection\\n(30D ‚Üí 3D)', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(scatter, label='Class', shrink=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n3D projection preserves {pca_3d.explained_variance_ratio_.sum():.1%} of variance.\")\n",
    "print(\"Even more separation visible with third dimension!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. PCA for Preprocessing: Improving Model Performance\n",
    "\n",
    "### Why Use PCA Before Training?\n",
    "\n",
    "**Benefits:**\n",
    "1. **Faster training**: Fewer features = faster computation\n",
    "2. **Reduced overfitting**: Simpler model, less noise\n",
    "3. **Remove multicollinearity**: Correlated features combined\n",
    "4. **Handle curse of dimensionality**: Especially for KNN, Naive Bayes\n",
    "\n",
    "**Workflow:**\n",
    "1. Split data into train/test\n",
    "2. Fit PCA on training data only\n",
    "3. Transform both train and test using fitted PCA\n",
    "4. Train model on reduced features\n",
    "5. Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performance: Original vs PCA features\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_bc_scaled, y_bc, test_size=0.3, random_state=42, stratify=y_bc\n",
    ")\n",
    "\n",
    "print(\"Comparing Models: Original Features vs PCA Features\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test with different numbers of components\n",
    "n_components_list = [2, 5, 10, 15, 20, X_bc_scaled.shape[1]]\n",
    "results = []\n",
    "\n",
    "for n_comp in n_components_list:\n",
    "    if n_comp < X_bc_scaled.shape[1]:\n",
    "        # Apply PCA\n",
    "        pca = PCA(n_components=n_comp)\n",
    "        X_train_pca = pca.fit_transform(X_train)\n",
    "        X_test_pca = pca.transform(X_test)\n",
    "        var_retained = pca.explained_variance_ratio_.sum()\n",
    "    else:\n",
    "        # Use all original features\n",
    "        X_train_pca = X_train\n",
    "        X_test_pca = X_test\n",
    "        var_retained = 1.0\n",
    "    \n",
    "    # Train logistic regression\n",
    "    lr = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    lr.fit(X_train_pca, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    train_score = lr.score(X_train_pca, y_train)\n",
    "    test_score = lr.score(X_test_pca, y_test)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(lr, X_train_pca, y_train, cv=5)\n",
    "    cv_mean = cv_scores.mean()\n",
    "    \n",
    "    results.append({\n",
    "        'n_components': n_comp,\n",
    "        'variance': var_retained,\n",
    "        'train': train_score,\n",
    "        'test': test_score,\n",
    "        'cv': cv_mean\n",
    "    })\n",
    "    \n",
    "    print(f\"Components: {n_comp:2d} | Variance: {var_retained:.3f} | \"\n",
    "          f\"Train: {train_score:.3f} | Test: {test_score:.3f} | CV: {cv_mean:.3f}\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(results_df['n_components'], results_df['train'], 'o-', label='Train', linewidth=2)\n",
    "plt.plot(results_df['n_components'], results_df['test'], 's-', label='Test', linewidth=2)\n",
    "plt.plot(results_df['n_components'], results_df['cv'], '^-', label='CV', linewidth=2)\n",
    "plt.xlabel('Number of Components', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('Model Performance vs Number of PCA Components', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(results_df['variance'], results_df['test'], 'o-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Variance Retained', fontsize=12)\n",
    "plt.ylabel('Test Accuracy', fontsize=12)\n",
    "plt.title('Test Accuracy vs Variance Retained', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find best number of components\n",
    "best_idx = results_df['test'].idxmax()\n",
    "best_result = results_df.iloc[best_idx]\n",
    "\n",
    "print(\"\\nüìä Key Findings:\")\n",
    "print(f\"   Best performance: {best_result['n_components']:.0f} components\")\n",
    "print(f\"   Test accuracy: {best_result['test']:.3f}\")\n",
    "print(f\"   Variance retained: {best_result['variance']:.1%}\")\n",
    "print(f\"\\n‚úÖ Using PCA for preprocessing can match or exceed full-feature performance!\")\n",
    "print(f\"   Benefit: {X_bc_scaled.shape[1] - best_result['n_components']:.0f} fewer features = faster training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. t-SNE: Advanced Visualization\n",
    "\n",
    "### What is t-SNE?\n",
    "\n",
    "**t-SNE** = t-Distributed Stochastic Neighbor Embedding\n",
    "\n",
    "**Purpose**: Visualize high-dimensional data in 2D or 3D\n",
    "\n",
    "### How t-SNE Differs from PCA\n",
    "\n",
    "**PCA**:\n",
    "- Linear transformation\n",
    "- Preserves large distances (global structure)\n",
    "- Fast and deterministic\n",
    "- Good for feature extraction\n",
    "\n",
    "**t-SNE**:\n",
    "- Non-linear transformation\n",
    "- Preserves small distances (local structure)\n",
    "- Slower and stochastic (different results each run)\n",
    "- **ONLY for visualization** (not for preprocessing!)\n",
    "- Better at separating clusters visually\n",
    "\n",
    "### t-SNE Parameters\n",
    "\n",
    "**perplexity**:\n",
    "- Balances local vs global structure\n",
    "- Think: \"How many neighbors to consider?\"\n",
    "- Range: 5-50 (typically 30)\n",
    "- Larger dataset ‚Üí larger perplexity\n",
    "\n",
    "**learning_rate**:\n",
    "- Controls optimization speed\n",
    "- Range: 10-1000 (typically 200)\n",
    "- Too low: slow convergence\n",
    "- Too high: unstable, poor results\n",
    "\n",
    "**n_iter**:\n",
    "- Number of optimization iterations\n",
    "- At least 1000 (typically 1000-5000)\n",
    "- More iterations = better quality (but slower)\n",
    "\n",
    "### Important t-SNE Caveats\n",
    "\n",
    "‚ö†Ô∏è **Don't use t-SNE for preprocessing!**\n",
    "- Only for visualization\n",
    "- Non-deterministic (changes each run)\n",
    "- No \"inverse transform\" to original space\n",
    "\n",
    "‚ö†Ô∏è **Don't interpret distances directly**\n",
    "- Cluster sizes don't mean anything\n",
    "- Distances between clusters don't mean anything\n",
    "- Only within-cluster groupings are meaningful\n",
    "\n",
    "‚ö†Ô∏è **Reduce dimensions with PCA first**\n",
    "- t-SNE is slow on high dimensions\n",
    "- PCA to 50D, then t-SNE to 2D (common workflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply t-SNE to digits dataset\n",
    "# First, prepare the data\n",
    "X_digits = digits_df.drop('target', axis=1).values\n",
    "y_digits = digits_df['target'].values\n",
    "\n",
    "# Scale features\n",
    "scaler_digits = StandardScaler()\n",
    "X_digits_scaled = scaler_digits.fit_transform(X_digits)\n",
    "\n",
    "# Reduce to 50D with PCA first (speeds up t-SNE)\n",
    "pca_50 = PCA(n_components=50)\n",
    "X_digits_pca = pca_50.fit_transform(X_digits_scaled)\n",
    "\n",
    "print(f\"Original digits data: {X_digits_scaled.shape}\")\n",
    "print(f\"After PCA: {X_digits_pca.shape}\")\n",
    "print(f\"PCA variance retained: {pca_50.explained_variance_ratio_.sum():.1%}\")\n",
    "print(\"\\nApplying t-SNE (this may take a minute)...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply t-SNE\n",
    "tsne = TSNE(\n",
    "    n_components=2,\n",
    "    perplexity=30,\n",
    "    learning_rate=200,\n",
    "    n_iter=1000,\n",
    "    random_state=42,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "X_digits_tsne = tsne.fit_transform(X_digits_pca)\n",
    "\n",
    "print(\"‚úì t-SNE complete!\")\n",
    "print(f\"Final shape: {X_digits_tsne.shape}\")\n",
    "print(f\"KL divergence (lower is better): {tsne.kl_divergence_:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare PCA vs t-SNE visualization\n",
    "# First get PCA 2D projection\n",
    "pca_2d_digits = PCA(n_components=2)\n",
    "X_digits_pca_2d = pca_2d_digits.fit_transform(X_digits_scaled)\n",
    "\n",
    "# Create comparison plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# PCA visualization\n",
    "scatter1 = axes[0].scatter(\n",
    "    X_digits_pca_2d[:, 0],\n",
    "    X_digits_pca_2d[:, 1],\n",
    "    c=y_digits,\n",
    "    cmap='tab10',\n",
    "    alpha=0.6,\n",
    "    s=30,\n",
    "    edgecolors='black',\n",
    "    linewidth=0.3\n",
    ")\n",
    "axes[0].set_xlabel('PC1', fontsize=12)\n",
    "axes[0].set_ylabel('PC2', fontsize=12)\n",
    "axes[0].set_title('PCA Projection\\n(Linear, preserves global structure)', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter1, ax=axes[0], label='Digit Class')\n",
    "\n",
    "# t-SNE visualization\n",
    "scatter2 = axes[1].scatter(\n",
    "    X_digits_tsne[:, 0],\n",
    "    X_digits_tsne[:, 1],\n",
    "    c=y_digits,\n",
    "    cmap='tab10',\n",
    "    alpha=0.6,\n",
    "    s=30,\n",
    "    edgecolors='black',\n",
    "    linewidth=0.3\n",
    ")\n",
    "axes[1].set_xlabel('t-SNE 1', fontsize=12)\n",
    "axes[1].set_ylabel('t-SNE 2', fontsize=12)\n",
    "axes[1].set_title('t-SNE Projection\\n(Non-linear, preserves local structure)',\n",
    "                 fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter2, ax=axes[1], label='Digit Class')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Comparison:\")\n",
    "print(\"- PCA (left): Clusters overlap, some separation\")\n",
    "print(\"- t-SNE (right): Much better cluster separation!\")\n",
    "print(\"- t-SNE creates distinct 'islands' for each digit class\")\n",
    "print(\"\\n‚úÖ t-SNE is excellent for visualizing cluster structure!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. PCA vs t-SNE: When to Use Each\n",
    "\n",
    "### Use PCA When:\n",
    "\n",
    "‚úÖ **Feature extraction for ML models**\n",
    "   - Preprocessing step before training\n",
    "   - Reduces overfitting\n",
    "   - Speeds up training\n",
    "\n",
    "‚úÖ **Need interpretable components**\n",
    "   - Can examine loadings (feature contributions)\n",
    "   - Understand what each PC represents\n",
    "\n",
    "‚úÖ **Need deterministic results**\n",
    "   - Same input ‚Üí same output\n",
    "   - Important for reproducibility\n",
    "\n",
    "‚úÖ **Large datasets**\n",
    "   - PCA is very fast\n",
    "   - Scales well\n",
    "\n",
    "‚úÖ **Need inverse transform**\n",
    "   - Can go back to original space\n",
    "   - Useful for reconstruction\n",
    "\n",
    "### Use t-SNE When:\n",
    "\n",
    "‚úÖ **Only want visualization**\n",
    "   - Not for preprocessing!\n",
    "   - Final step for exploration\n",
    "\n",
    "‚úÖ **Complex cluster structures**\n",
    "   - Non-linear relationships\n",
    "   - Intertwined clusters\n",
    "\n",
    "‚úÖ **Want better visual separation**\n",
    "   - Creates more distinct clusters\n",
    "   - Easier to interpret visually\n",
    "\n",
    "‚úÖ **Small to medium datasets**\n",
    "   - t-SNE is slow (O(n¬≤))\n",
    "   - Not practical for millions of points\n",
    "\n",
    "### Common Workflow\n",
    "\n",
    "**Best practice**: Use both!\n",
    "1. **PCA to 50D** (fast dimensionality reduction)\n",
    "2. **Train ML models** on PCA features\n",
    "3. **t-SNE to 2D** (for visualization only)\n",
    "4. **Plot results** to understand data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Now it's your turn to practice! Complete these exercises to reinforce your understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: PCA on Wine Dataset\n",
    "\n",
    "Apply PCA to the wine dataset and determine optimal number of components.\n",
    "\n",
    "**Tasks:**\n",
    "1. Load `wine.csv` and separate features from target\n",
    "2. Standardize features\n",
    "3. Apply PCA with all components\n",
    "4. Create a scree plot showing explained variance\n",
    "5. How many components needed for 90% variance?\n",
    "6. Create 2D PCA visualization colored by wine class\n",
    "7. Are the wine classes well-separated in 2D?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: Follow the PCA workflow from sections 4-6\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: PCA vs Original Features Performance\n",
    "\n",
    "Compare classification performance using original features vs PCA features.\n",
    "\n",
    "**Tasks:**\n",
    "1. Use breast cancer dataset\n",
    "2. Split into train/test (70/30)\n",
    "3. Train Random Forest classifier with original 30 features\n",
    "4. Train Random Forest with PCA features (5, 10, 15, 20 components)\n",
    "5. Compare: Training time, Test accuracy, Number of features\n",
    "6. Create a plot showing accuracy vs number of PCA components\n",
    "7. What's the optimal number of components? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: import time; start = time.time(); ... ; elapsed = time.time() - start\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: t-SNE Parameter Exploration\n",
    "\n",
    "Experiment with different t-SNE parameters to see their effects.\n",
    "\n",
    "**Tasks:**\n",
    "1. Use the digits dataset (reduce to 50D with PCA first)\n",
    "2. Apply t-SNE with perplexity values: [5, 30, 50]\n",
    "3. For each perplexity, create a 2D scatter plot colored by digit class\n",
    "4. Which perplexity value gives the best separation?\n",
    "5. What happens with perplexity=5 (too small)?\n",
    "6. What happens with perplexity=50 (large)?\n",
    "7. Why is it important to reduce dimensions with PCA before t-SNE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: Create a 1x3 subplot to compare different perplexity values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Feature Contribution Analysis\n",
    "\n",
    "Analyze which original features contribute most to principal components.\n",
    "\n",
    "**Tasks:**\n",
    "1. Load breast cancer dataset and apply PCA with 5 components\n",
    "2. Get the PCA components (loadings) - these show feature contributions\n",
    "3. For PC1 and PC2, identify the top 5 most influential features\n",
    "4. Create a heatmap showing all feature contributions to first 5 PCs\n",
    "5. Which features are most important for PC1?\n",
    "6. Are any features consistently important across multiple PCs?\n",
    "7. What does this tell you about the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: pca.components_ contains the loadings\n",
    "# Use sns.heatmap() for visualization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts Learned\n",
    "\n",
    "1. **Why Dimensionality Reduction?**\n",
    "   - Visualization (2D/3D from high-D)\n",
    "   - Speed (faster training and prediction)\n",
    "   - Avoid overfitting (simpler models)\n",
    "   - Remove redundancy (correlated features)\n",
    "   - Combat curse of dimensionality\n",
    "\n",
    "2. **Principal Component Analysis (PCA)**\n",
    "   - Linear transformation to new axes\n",
    "   - Finds directions of maximum variance\n",
    "   - Orthogonal components (uncorrelated)\n",
    "   - Fast and deterministic\n",
    "   - Good for preprocessing and visualization\n",
    "\n",
    "3. **Explained Variance**\n",
    "   - Measures information retained\n",
    "   - Common threshold: 90-95%\n",
    "   - Scree plot shows variance per component\n",
    "   - Cumulative plot guides component selection\n",
    "\n",
    "4. **PCA Workflow**\n",
    "   - Standardize data (critical!)\n",
    "   - Fit PCA on training data\n",
    "   - Transform train and test data\n",
    "   - Train model on reduced features\n",
    "   - Benefit: Speed + performance\n",
    "\n",
    "5. **t-SNE (t-Distributed Stochastic Neighbor Embedding)**\n",
    "   - Non-linear dimensionality reduction\n",
    "   - Preserves local structure (nearby points)\n",
    "   - Excellent for visualization\n",
    "   - **ONLY for visualization** (not preprocessing!)\n",
    "   - Stochastic (different results each run)\n",
    "\n",
    "6. **PCA vs t-SNE**\n",
    "   - **PCA**: Feature extraction, fast, linear, deterministic\n",
    "   - **t-SNE**: Visualization only, slow, non-linear, stochastic\n",
    "   - **Workflow**: PCA ‚Üí train models, t-SNE ‚Üí visualize\n",
    "\n",
    "7. **Best Practices**\n",
    "   - Always standardize before PCA\n",
    "   - Use PCA to ~50D before t-SNE (speed)\n",
    "   - Choose components retaining 90-95% variance\n",
    "   - Visualize with scree plots\n",
    "   - Validate with cross-validation\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "- **Always scale features** before PCA (mean=0, std=1)\n",
    "- **Fit PCA on training data only** (avoid data leakage)\n",
    "- **Use scree plot** to choose number of components\n",
    "- **Start with 90% variance** as reasonable default\n",
    "- **Validate performance** with cross-validation\n",
    "- **Use PCA + model** rather than just PCA or just model\n",
    "- **Reduce to 50D before t-SNE** for speed\n",
    "- **Don't use t-SNE for ML** (only visualization)\n",
    "\n",
    "### Common Pitfalls to Avoid\n",
    "\n",
    "- ‚ùå Forgetting to scale features before PCA\n",
    "- ‚ùå Fitting PCA on entire dataset (including test data)\n",
    "- ‚ùå Using t-SNE for preprocessing (it's non-invertible!)\n",
    "- ‚ùå Interpreting t-SNE distances/sizes literally\n",
    "- ‚ùå Choosing too few components (losing important information)\n",
    "- ‚ùå Running t-SNE on high-dimensional data directly\n",
    "- ‚ùå Expecting PCA to work on categorical data\n",
    "\n",
    "### What's Next\n",
    "\n",
    "In **Module 14: Final Project - End-to-End ML Pipeline**, you'll:\n",
    "- Apply everything you've learned in a complete project\n",
    "- Build a full machine learning pipeline from scratch\n",
    "- Handle real-world data with all its challenges\n",
    "- Compare multiple algorithms systematically\n",
    "- Perform hyperparameter tuning\n",
    "- Create production-ready models\n",
    "- Learn ML best practices and deployment considerations\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "**Videos:**\n",
    "- [StatQuest: PCA Step-by-Step](https://www.youtube.com/watch?v=FgakZw6K1QQ)\n",
    "- [StatQuest: PCA in Python](https://www.youtube.com/watch?v=Lsue2gEM9D0)\n",
    "- [t-SNE Explained](https://www.youtube.com/watch?v=NEaUSP4YerM)\n",
    "\n",
    "**Documentation:**\n",
    "- [scikit-learn PCA Guide](https://scikit-learn.org/stable/modules/decomposition.html#pca)\n",
    "- [PCA API](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)\n",
    "- [t-SNE API](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)\n",
    "\n",
    "**Articles:**\n",
    "- [PCA for Machine Learning](https://machinelearningmastery.com/principal-component-analysis-for-dimensionality-reduction/)\n",
    "- [How to Use t-SNE Effectively](https://distill.pub/2016/misread-tsne/)\n",
    "- [PCA vs t-SNE](https://towardsdatascience.com/pca-vs-t-sne-17bcd882bf3d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
