{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 12: Clustering - K-Means and DBSCAN\n",
    "\n",
    "**Difficulty**: â­â­ Intermediate  \n",
    "**Estimated Time**: 75 minutes  \n",
    "**Prerequisites**: [Module 01 - Supervised vs Unsupervised Learning](01_supervised_vs_unsupervised_learning.ipynb), [Module 02 - Data Preparation](02_data_preparation_train_test_split.ipynb)\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Understand unsupervised learning and clustering concepts\n",
    "2. Implement K-Means clustering and understand how it works\n",
    "3. Use the elbow method to choose the optimal number of clusters (K)\n",
    "4. Evaluate clustering quality with silhouette score and inertia\n",
    "5. Recognize K-Means limitations (spherical clusters, sensitivity to initialization)\n",
    "6. Apply DBSCAN for density-based clustering\n",
    "7. Tune DBSCAN parameters (eps and min_samples)\n",
    "8. Compare K-Means and DBSCAN on different data shapes\n",
    "9. Know when to use each clustering algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction: What is Clustering?\n",
    "\n",
    "### Unsupervised Learning Refresher\n",
    "\n",
    "**Supervised Learning** (what we've done so far):\n",
    "- We have labels (target variable)\n",
    "- Goal: Predict labels for new data\n",
    "- Examples: Classification, regression\n",
    "\n",
    "**Unsupervised Learning** (clustering):\n",
    "- We DON'T have labels\n",
    "- Goal: Find patterns, structure, or groupings in data\n",
    "- Examples: Clustering, dimensionality reduction\n",
    "\n",
    "### What is Clustering?\n",
    "\n",
    "**Clustering** is the task of grouping similar data points together.\n",
    "\n",
    "**Real-world examples:**\n",
    "- **Customer segmentation**: Group customers by buying behavior\n",
    "- **Document organization**: Group similar articles or papers\n",
    "- **Image segmentation**: Group pixels into regions\n",
    "- **Anomaly detection**: Points that don't fit any cluster\n",
    "- **Gene expression analysis**: Group genes with similar patterns\n",
    "\n",
    "### Key Questions in Clustering\n",
    "\n",
    "1. **How many clusters?** (K in K-Means)\n",
    "2. **What makes points similar?** (Distance metric)\n",
    "3. **How to evaluate quality?** (No labels to check against!)\n",
    "4. **What shape are clusters?** (Spherical, arbitrary, density-based)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from sklearn.metrics import davies_bouldin_score, calinski_harabasz_score\n",
    "import warnings\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "%matplotlib inline\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print('âœ“ All libraries imported successfully!')\n",
    "print(f'âœ“ Random seed set to 42 for reproducibility')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load clustering datasets\n",
    "# Dataset 1: Blob-like clusters (perfect for K-Means)\n",
    "blobs_df = pd.read_csv('data/sample/blobs_clustering.csv')\n",
    "\n",
    "print(\"Blobs Dataset (Well-separated spherical clusters):\")\n",
    "print(f\"Shape: {blobs_df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(blobs_df.head())\n",
    "\n",
    "# Dataset 2: Moon-shaped clusters (challenging for K-Means, good for DBSCAN)\n",
    "moons_df = pd.read_csv('data/sample/moons_nonlinear.csv')\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"\\nMoons Dataset (Non-linear, crescent-shaped clusters):\")\n",
    "print(f\"Shape: {moons_df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(moons_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize both datasets\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Blobs dataset\n",
    "axes[0].scatter(blobs_df['feature_1'], blobs_df['feature_2'], alpha=0.6, s=50)\n",
    "axes[0].set_xlabel('Feature 1', fontsize=12)\n",
    "axes[0].set_ylabel('Feature 2', fontsize=12)\n",
    "axes[0].set_title('Blobs Dataset\\n(Spherical clusters)', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Moons dataset\n",
    "axes[1].scatter(moons_df['feature_1'], moons_df['feature_2'], alpha=0.6, s=50)\n",
    "axes[1].set_xlabel('Feature 1', fontsize=12)\n",
    "axes[1].set_ylabel('Feature 2', fontsize=12)\n",
    "axes[1].set_title('Moons Dataset\\n(Non-linear clusters)', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ Notice:\")\n",
    "print(\"- Left: Clear, spherical clusters - perfect for K-Means!\")\n",
    "print(\"- Right: Crescent shapes - challenging for K-Means, better for DBSCAN!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. K-Means Clustering: The Algorithm\n",
    "\n",
    "### How K-Means Works\n",
    "\n",
    "K-Means is an iterative algorithm that partitions data into K clusters:\n",
    "\n",
    "**Step 1: Initialization**\n",
    "- Choose K (number of clusters)\n",
    "- Randomly place K cluster centers (centroids)\n",
    "\n",
    "**Step 2: Assignment**\n",
    "- Assign each point to the nearest centroid\n",
    "- Forms K clusters\n",
    "\n",
    "**Step 3: Update**\n",
    "- Move each centroid to the mean of its assigned points\n",
    "- Centroids shift to cluster centers\n",
    "\n",
    "**Step 4: Repeat**\n",
    "- Repeat steps 2-3 until centroids stop moving (convergence)\n",
    "- Or reach maximum iterations\n",
    "\n",
    "### Objective Function\n",
    "\n",
    "K-Means minimizes **within-cluster sum of squares (WCSS)** or **inertia**:\n",
    "\n",
    "$$\\text{Inertia} = \\sum_{i=1}^{K} \\sum_{x \\in C_i} ||x - \\mu_i||^2$$\n",
    "\n",
    "Where:\n",
    "- K = number of clusters\n",
    "- C_i = points in cluster i\n",
    "- Î¼_i = centroid of cluster i\n",
    "- || || = Euclidean distance\n",
    "\n",
    "**In words**: Sum of squared distances from each point to its cluster center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply K-Means to blobs dataset\n",
    "# First, let's try K=3 (we can see 3 clusters visually)\n",
    "\n",
    "X_blobs = blobs_df[['feature_1', 'feature_2']].values\n",
    "\n",
    "# Scale features (important for K-Means!)\n",
    "scaler = StandardScaler()\n",
    "X_blobs_scaled = scaler.fit_transform(X_blobs)\n",
    "\n",
    "# Create and fit K-Means model\n",
    "kmeans = KMeans(\n",
    "    n_clusters=3,           # Number of clusters\n",
    "    init='k-means++',       # Smart initialization (better than random)\n",
    "    n_init=10,              # Number of times to run with different initializations\n",
    "    max_iter=300,           # Maximum iterations\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit and predict cluster labels\n",
    "cluster_labels = kmeans.fit_predict(X_blobs_scaled)\n",
    "\n",
    "# Get cluster centers\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "# Get inertia (lower is better - tighter clusters)\n",
    "inertia = kmeans.inertia_\n",
    "\n",
    "print(\"K-Means Clustering Results (K=3):\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Number of iterations: {kmeans.n_iter_}\")\n",
    "print(f\"Inertia (WCSS): {inertia:.2f}\")\n",
    "print(f\"\\nCluster centers (scaled coordinates):\")\n",
    "print(centers)\n",
    "print(f\"\\nCluster sizes:\")\n",
    "unique, counts = np.unique(cluster_labels, return_counts=True)\n",
    "for cluster, count in zip(unique, counts):\n",
    "    print(f\"  Cluster {cluster}: {count} points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize K-Means results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot 1: Original data\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_blobs[:, 0], X_blobs[:, 1], alpha=0.6, s=50, c='gray')\n",
    "plt.xlabel('Feature 1', fontsize=12)\n",
    "plt.ylabel('Feature 2', fontsize=12)\n",
    "plt.title('Before Clustering', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Clustered data\n",
    "plt.subplot(1, 2, 2)\n",
    "# Plot points colored by cluster\n",
    "scatter = plt.scatter(\n",
    "    X_blobs[:, 0], X_blobs[:, 1],\n",
    "    c=cluster_labels,\n",
    "    cmap='viridis',\n",
    "    alpha=0.6,\n",
    "    s=50\n",
    ")\n",
    "\n",
    "# Plot cluster centers\n",
    "centers_original = scaler.inverse_transform(centers)\n",
    "plt.scatter(\n",
    "    centers_original[:, 0], centers_original[:, 1],\n",
    "    c='red',\n",
    "    marker='X',\n",
    "    s=300,\n",
    "    edgecolors='black',\n",
    "    linewidth=2,\n",
    "    label='Centroids'\n",
    ")\n",
    "\n",
    "plt.xlabel('Feature 1', fontsize=12)\n",
    "plt.ylabel('Feature 2', fontsize=12)\n",
    "plt.title('After K-Means Clustering (K=3)', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Perfect! K-Means correctly identified the 3 spherical clusters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Choosing K: The Elbow Method\n",
    "\n",
    "### The Challenge\n",
    "\n",
    "**Problem**: How do we choose K (number of clusters)?\n",
    "- Too few clusters: Underfitting, miss patterns\n",
    "- Too many clusters: Overfitting, meaningless groups\n",
    "\n",
    "### The Elbow Method\n",
    "\n",
    "**Idea**: Plot inertia (WCSS) vs K\n",
    "- As K increases, inertia decreases (points get closer to centroids)\n",
    "- Eventually, gains diminish (diminishing returns)\n",
    "- Choose K at the \"elbow\" - where the curve bends sharply\n",
    "\n",
    "**Interpretation**:\n",
    "- Sharp drop before elbow: Adding clusters helps a lot\n",
    "- Gradual decline after elbow: Adding clusters doesn't help much\n",
    "- Elbow point: Optimal tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply elbow method: Test K from 1 to 10\n",
    "K_range = range(1, 11)\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "\n",
    "print(\"Testing different values of K...\\n\")\n",
    "\n",
    "for k in K_range:\n",
    "    # Train K-Means with k clusters\n",
    "    kmeans_test = KMeans(n_clusters=k, init='k-means++', n_init=10, random_state=42)\n",
    "    kmeans_test.fit(X_blobs_scaled)\n",
    "    \n",
    "    # Store inertia\n",
    "    inertias.append(kmeans_test.inertia_)\n",
    "    \n",
    "    # Calculate silhouette score (requires at least 2 clusters)\n",
    "    if k > 1:\n",
    "        labels = kmeans_test.labels_\n",
    "        silhouette_avg = silhouette_score(X_blobs_scaled, labels)\n",
    "        silhouette_scores.append(silhouette_avg)\n",
    "    else:\n",
    "        silhouette_scores.append(0)\n",
    "    \n",
    "    print(f\"K={k:2d} | Inertia: {kmeans_test.inertia_:7.2f} | Silhouette: {silhouette_scores[-1]:.3f}\")\n",
    "\n",
    "print(\"\\nâœ“ Testing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the elbow method\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Elbow curve (Inertia)\n",
    "axes[0].plot(K_range, inertias, 'o-', linewidth=2, markersize=8)\n",
    "axes[0].axvline(3, color='red', linestyle='--', label='Optimal K=3', alpha=0.7)\n",
    "axes[0].set_xlabel('Number of Clusters (K)', fontsize=12)\n",
    "axes[0].set_ylabel('Inertia (WCSS)', fontsize=12)\n",
    "axes[0].set_title('Elbow Method: Inertia vs K', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add annotation for elbow\n",
    "axes[0].annotate('Elbow Point', xy=(3, inertias[2]), xytext=(5, inertias[2] + 100),\n",
    "                arrowprops=dict(arrowstyle='->', color='red', lw=2),\n",
    "                fontsize=11, color='red')\n",
    "\n",
    "# Plot 2: Silhouette score\n",
    "axes[1].plot(K_range, silhouette_scores, 's-', linewidth=2, markersize=8, color='green')\n",
    "axes[1].axvline(3, color='red', linestyle='--', label='Optimal K=3', alpha=0.7)\n",
    "axes[1].set_xlabel('Number of Clusters (K)', fontsize=12)\n",
    "axes[1].set_ylabel('Silhouette Score', fontsize=12)\n",
    "axes[1].set_title('Silhouette Score vs K', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ“Š Interpretation:\")\n",
    "print(\"- Left: Inertia drops sharply until K=3, then levels off (elbow at K=3)\")\n",
    "print(\"- Right: Silhouette score is highest at K=3 (best cluster separation)\")\n",
    "print(\"\\nâœ… Both methods agree: K=3 is optimal!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluating Cluster Quality: Silhouette Score\n",
    "\n",
    "### What is Silhouette Score?\n",
    "\n",
    "**Measures how well each point fits its cluster** compared to other clusters.\n",
    "\n",
    "For each point:\n",
    "- **a**: Mean distance to other points in same cluster (cohesion)\n",
    "- **b**: Mean distance to points in nearest different cluster (separation)\n",
    "- **Silhouette coefficient**: (b - a) / max(a, b)\n",
    "\n",
    "**Range**: -1 to +1\n",
    "- **+1**: Point is far from other clusters, close to its own (perfect)\n",
    "- **0**: Point is on the border between clusters\n",
    "- **-1**: Point might be in wrong cluster\n",
    "\n",
    "**Average silhouette score**: Mean across all points\n",
    "- **> 0.7**: Strong clustering structure\n",
    "- **0.5 - 0.7**: Reasonable structure\n",
    "- **0.25 - 0.5**: Weak structure\n",
    "- **< 0.25**: No substantial structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate detailed silhouette scores\n",
    "silhouette_vals = silhouette_samples(X_blobs_scaled, cluster_labels)\n",
    "silhouette_avg = silhouette_score(X_blobs_scaled, cluster_labels)\n",
    "\n",
    "print(\"Silhouette Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Average Silhouette Score: {silhouette_avg:.3f}\")\n",
    "print(\"\\nPer-cluster Silhouette Scores:\")\n",
    "\n",
    "for cluster in range(3):\n",
    "    cluster_silhouette_vals = silhouette_vals[cluster_labels == cluster]\n",
    "    print(f\"  Cluster {cluster}: {cluster_silhouette_vals.mean():.3f} (n={len(cluster_silhouette_vals)})\")\n",
    "\n",
    "print(\"\\nâœ… Score > 0.7 indicates strong, well-separated clusters!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize silhouette plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "y_lower = 10\n",
    "for cluster in range(3):\n",
    "    # Get silhouette values for this cluster\n",
    "    cluster_silhouette_vals = silhouette_vals[cluster_labels == cluster]\n",
    "    cluster_silhouette_vals.sort()\n",
    "    \n",
    "    size_cluster = len(cluster_silhouette_vals)\n",
    "    y_upper = y_lower + size_cluster\n",
    "    \n",
    "    color = plt.cm.viridis(float(cluster) / 3)\n",
    "    ax.fill_betweenx(\n",
    "        np.arange(y_lower, y_upper),\n",
    "        0,\n",
    "        cluster_silhouette_vals,\n",
    "        facecolor=color,\n",
    "        edgecolor=color,\n",
    "        alpha=0.7,\n",
    "        label=f'Cluster {cluster}'\n",
    "    )\n",
    "    \n",
    "    # Label the clusters\n",
    "    ax.text(-0.05, y_lower + 0.5 * size_cluster, str(cluster), fontsize=12)\n",
    "    \n",
    "    y_lower = y_upper + 10\n",
    "\n",
    "# Add average line\n",
    "ax.axvline(silhouette_avg, color='red', linestyle='--', linewidth=2, label=f'Average: {silhouette_avg:.3f}')\n",
    "\n",
    "ax.set_xlabel('Silhouette Coefficient', fontsize=12)\n",
    "ax.set_ylabel('Cluster', fontsize=12)\n",
    "ax.set_title('Silhouette Plot for K-Means Clustering', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='upper right')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ Interpretation:\")\n",
    "print(\"- Width of each colored region: Number of points in cluster\")\n",
    "print(\"- Thickness: How far points extend to the right (higher = better fit)\")\n",
    "print(\"- All clusters exceed average line: All clusters are well-formed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. K-Means Limitations\n",
    "\n",
    "### When K-Means Struggles\n",
    "\n",
    "**1. Non-spherical clusters**\n",
    "- K-Means assumes spherical clusters\n",
    "- Fails on elongated, crescent, or irregular shapes\n",
    "\n",
    "**2. Different sized clusters**\n",
    "- K-Means tries to make equal-sized clusters\n",
    "- Struggles when clusters have very different sizes\n",
    "\n",
    "**3. Different densities**\n",
    "- K-Means assumes similar density\n",
    "- Fails when some clusters are dense, others sparse\n",
    "\n",
    "**4. Sensitive to initialization**\n",
    "- Random initialization can lead to different results\n",
    "- Solution: Use k-means++ initialization, run multiple times\n",
    "\n",
    "**5. Requires K in advance**\n",
    "- Must specify number of clusters beforehand\n",
    "- Not always obvious what K should be\n",
    "\n",
    "**6. Outliers affect centroids**\n",
    "- Outliers pull centroids away from true centers\n",
    "- Sensitive to noise\n",
    "\n",
    "Let's demonstrate these limitations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate K-Means failure on non-spherical data (moons)\n",
    "X_moons = moons_df[['feature_1', 'feature_2']].values\n",
    "\n",
    "# Scale features\n",
    "scaler_moons = StandardScaler()\n",
    "X_moons_scaled = scaler_moons.fit_transform(X_moons)\n",
    "\n",
    "# Apply K-Means with K=2 (we can see 2 crescent shapes)\n",
    "kmeans_moons = KMeans(n_clusters=2, init='k-means++', n_init=10, random_state=42)\n",
    "labels_moons = kmeans_moons.fit_predict(X_moons_scaled)\n",
    "\n",
    "# Calculate silhouette score\n",
    "silhouette_moons = silhouette_score(X_moons_scaled, labels_moons)\n",
    "\n",
    "# Visualize K-Means on moons\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_moons[:, 0], X_moons[:, 1], alpha=0.6, s=50, c='gray')\n",
    "plt.xlabel('Feature 1', fontsize=12)\n",
    "plt.ylabel('Feature 2', fontsize=12)\n",
    "plt.title('Moons Dataset\\n(Two crescent-shaped clusters)', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "scatter = plt.scatter(X_moons[:, 0], X_moons[:, 1], c=labels_moons, cmap='viridis', alpha=0.6, s=50)\n",
    "centers_moons = scaler_moons.inverse_transform(kmeans_moons.cluster_centers_)\n",
    "plt.scatter(centers_moons[:, 0], centers_moons[:, 1], c='red', marker='X', s=300, \n",
    "           edgecolors='black', linewidth=2, label='Centroids')\n",
    "plt.xlabel('Feature 1', fontsize=12)\n",
    "plt.ylabel('Feature 2', fontsize=12)\n",
    "plt.title(f'K-Means Result (K=2)\\nSilhouette: {silhouette_moons:.3f}', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâŒ K-Means Failure:\")\n",
    "print(f\"   Silhouette Score: {silhouette_moons:.3f} (poor!)\")\n",
    "print(\"   K-Means splits each crescent in half instead of separating the crescents.\")\n",
    "print(\"   This is because K-Means assumes spherical clusters!\")\n",
    "print(\"\\nğŸ’¡ Solution: Use density-based clustering like DBSCAN!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. DBSCAN: Density-Based Clustering\n",
    "\n",
    "### What is DBSCAN?\n",
    "\n",
    "**DBSCAN** = Density-Based Spatial Clustering of Applications with Noise\n",
    "\n",
    "**Key idea**: Clusters are dense regions separated by sparse regions\n",
    "\n",
    "### How DBSCAN Works\n",
    "\n",
    "**Core concepts:**\n",
    "1. **Core point**: Has at least min_samples neighbors within distance eps\n",
    "2. **Border point**: Within eps of a core point, but not a core point itself\n",
    "3. **Noise point**: Neither core nor border (outlier)\n",
    "\n",
    "**Algorithm:**\n",
    "1. For each point, count neighbors within distance eps\n",
    "2. Mark as core point if it has â‰¥ min_samples neighbors\n",
    "3. Form clusters by connecting core points that are neighbors\n",
    "4. Add border points to nearby clusters\n",
    "5. Mark remaining points as noise (cluster label = -1)\n",
    "\n",
    "### Parameters\n",
    "\n",
    "**eps (epsilon)**:\n",
    "- Maximum distance between two points to be neighbors\n",
    "- Larger eps â†’ fewer, larger clusters\n",
    "- Smaller eps â†’ more, smaller clusters (more noise)\n",
    "\n",
    "**min_samples**:\n",
    "- Minimum points needed to form a dense region (core point)\n",
    "- Larger min_samples â†’ fewer core points, more noise\n",
    "- Rule of thumb: min_samples = 2 Ã— dimensions\n",
    "\n",
    "### Advantages of DBSCAN\n",
    "\n",
    "âœ… **No need to specify K** (number of clusters)  \n",
    "âœ… **Handles arbitrary shapes** (non-spherical clusters)  \n",
    "âœ… **Robust to outliers** (labels them as noise)  \n",
    "âœ… **Handles different densities** (to some extent)  \n",
    "âœ… **Deterministic** (same result every run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply DBSCAN to moons dataset\n",
    "# Start with reasonable parameters\n",
    "dbscan = DBSCAN(\n",
    "    eps=0.3,              # Maximum distance between neighbors\n",
    "    min_samples=5         # Minimum points to form a dense region\n",
    ")\n",
    "\n",
    "# Fit and predict cluster labels\n",
    "labels_dbscan = dbscan.fit_predict(X_moons_scaled)\n",
    "\n",
    "# Count clusters (excluding noise points labeled -1)\n",
    "n_clusters = len(set(labels_dbscan)) - (1 if -1 in labels_dbscan else 0)\n",
    "n_noise = list(labels_dbscan).count(-1)\n",
    "\n",
    "print(\"DBSCAN Clustering Results:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Parameters: eps={dbscan.eps}, min_samples={dbscan.min_samples}\")\n",
    "print(f\"\\nNumber of clusters found: {n_clusters}\")\n",
    "print(f\"Number of noise points: {n_noise}\")\n",
    "print(f\"\\nCluster sizes:\")\n",
    "unique, counts = np.unique(labels_dbscan, return_counts=True)\n",
    "for cluster, count in zip(unique, counts):\n",
    "    if cluster == -1:\n",
    "        print(f\"  Noise: {count} points\")\n",
    "    else:\n",
    "        print(f\"  Cluster {cluster}: {count} points\")\n",
    "\n",
    "# Calculate silhouette score (excluding noise points)\n",
    "if n_clusters > 1 and n_noise < len(labels_dbscan):\n",
    "    mask = labels_dbscan != -1\n",
    "    silhouette_dbscan = silhouette_score(X_moons_scaled[mask], labels_dbscan[mask])\n",
    "    print(f\"\\nSilhouette Score (excluding noise): {silhouette_dbscan:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize DBSCAN results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot 1: K-Means (poor result)\n",
    "plt.subplot(1, 2, 1)\n",
    "scatter1 = plt.scatter(X_moons[:, 0], X_moons[:, 1], c=labels_moons, cmap='viridis', alpha=0.6, s=50)\n",
    "plt.xlabel('Feature 1', fontsize=12)\n",
    "plt.ylabel('Feature 2', fontsize=12)\n",
    "plt.title(f'K-Means (K=2)\\nSilhouette: {silhouette_moons:.3f}', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(scatter1, label='Cluster')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: DBSCAN (excellent result)\n",
    "plt.subplot(1, 2, 2)\n",
    "# Plot noise points in gray\n",
    "noise_mask = labels_dbscan == -1\n",
    "plt.scatter(X_moons[noise_mask, 0], X_moons[noise_mask, 1], \n",
    "           c='gray', alpha=0.3, s=50, label='Noise')\n",
    "# Plot clustered points\n",
    "plt.scatter(X_moons[~noise_mask, 0], X_moons[~noise_mask, 1], \n",
    "           c=labels_dbscan[~noise_mask], cmap='viridis', alpha=0.6, s=50)\n",
    "plt.xlabel('Feature 1', fontsize=12)\n",
    "plt.ylabel('Feature 2', fontsize=12)\n",
    "plt.title(f'DBSCAN (eps={dbscan.eps}, min_samples={dbscan.min_samples})\\nSilhouette: {silhouette_dbscan:.3f}', \n",
    "         fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… DBSCAN Success:\")\n",
    "print(\"   Correctly identified the two crescent-shaped clusters!\")\n",
    "print(\"   Much better silhouette score than K-Means.\")\n",
    "print(\"   DBSCAN can handle non-spherical cluster shapes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Tuning DBSCAN Parameters\n",
    "\n",
    "### Choosing eps (epsilon)\n",
    "\n",
    "**Method**: K-distance plot\n",
    "1. For each point, calculate distance to its K-th nearest neighbor\n",
    "2. Sort these distances in ascending order\n",
    "3. Plot the sorted distances\n",
    "4. Look for \"elbow\" - sharp increase indicates noise\n",
    "5. Choose eps at the elbow\n",
    "\n",
    "**Rules of thumb**:\n",
    "- Start with eps = distance to 4th or 5th nearest neighbor\n",
    "- Too small â†’ many small clusters and noise\n",
    "- Too large â†’ everything in one cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-distance plot to find optimal eps\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Calculate distances to 5th nearest neighbor for each point\n",
    "k = 5\n",
    "neighbors = NearestNeighbors(n_neighbors=k)\n",
    "neighbors.fit(X_moons_scaled)\n",
    "distances, indices = neighbors.kneighbors(X_moons_scaled)\n",
    "\n",
    "# Get distance to k-th nearest neighbor for each point\n",
    "k_distances = distances[:, k-1]\n",
    "k_distances_sorted = np.sort(k_distances)\n",
    "\n",
    "# Plot k-distance graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_distances_sorted, linewidth=2)\n",
    "plt.xlabel('Points sorted by distance', fontsize=12)\n",
    "plt.ylabel(f'Distance to {k}-th nearest neighbor', fontsize=12)\n",
    "plt.title('K-Distance Plot for Finding Optimal eps', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add horizontal line at elbow\n",
    "optimal_eps = 0.3\n",
    "plt.axhline(optimal_eps, color='red', linestyle='--', label=f'Suggested eps={optimal_eps}', linewidth=2)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ How to read this plot:\")\n",
    "print(\"- Flat region: Dense areas (points close together)\")\n",
    "print(\"- Sharp increase: Transition to sparse areas/noise\")\n",
    "print(f\"- Elbow point around {optimal_eps}: Good choice for eps\")\n",
    "print(\"\\nâœ… Choose eps where the curve starts to rise sharply!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different eps values\n",
    "eps_values = [0.1, 0.2, 0.3, 0.5]\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, eps in enumerate(eps_values):\n",
    "    # Apply DBSCAN with this eps\n",
    "    dbscan_test = DBSCAN(eps=eps, min_samples=5)\n",
    "    labels_test = dbscan_test.fit_predict(X_moons_scaled)\n",
    "    \n",
    "    # Count clusters and noise\n",
    "    n_clusters_test = len(set(labels_test)) - (1 if -1 in labels_test else 0)\n",
    "    n_noise_test = list(labels_test).count(-1)\n",
    "    \n",
    "    # Calculate silhouette if possible\n",
    "    if n_clusters_test > 1 and n_noise_test < len(labels_test):\n",
    "        mask = labels_test != -1\n",
    "        silhouette_test = silhouette_score(X_moons_scaled[mask], labels_test[mask])\n",
    "    else:\n",
    "        silhouette_test = -1\n",
    "    \n",
    "    # Plot\n",
    "    noise_mask_test = labels_test == -1\n",
    "    axes[idx].scatter(X_moons[noise_mask_test, 0], X_moons[noise_mask_test, 1],\n",
    "                     c='gray', alpha=0.3, s=30, label='Noise')\n",
    "    axes[idx].scatter(X_moons[~noise_mask_test, 0], X_moons[~noise_mask_test, 1],\n",
    "                     c=labels_test[~noise_mask_test], cmap='viridis', alpha=0.6, s=30)\n",
    "    axes[idx].set_xlabel('Feature 1', fontsize=10)\n",
    "    axes[idx].set_ylabel('Feature 2', fontsize=10)\n",
    "    axes[idx].set_title(f'eps={eps}\\nClusters: {n_clusters_test}, Noise: {n_noise_test}\\nSilhouette: {silhouette_test:.3f}',\n",
    "                       fontsize=12, fontweight='bold')\n",
    "    axes[idx].legend(loc='upper right', fontsize=8)\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ“Š Effect of eps:\")\n",
    "print(\"- eps=0.1: Too small â†’ many clusters, lots of noise\")\n",
    "print(\"- eps=0.2: Better, but still some fragmentation\")\n",
    "print(\"- eps=0.3: âœ… Optimal - correctly identifies 2 clusters\")\n",
    "print(\"- eps=0.5: Too large â†’ merges clusters together\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. K-Means vs DBSCAN: When to Use Each\n",
    "\n",
    "### Use K-Means When:\n",
    "\n",
    "âœ… **Clusters are roughly spherical** (circular, blob-like)  \n",
    "âœ… **Similar cluster sizes** (roughly equal number of points)  \n",
    "âœ… **Similar densities** across clusters  \n",
    "âœ… **Know K in advance** (or can estimate it)  \n",
    "âœ… **Large datasets** (K-Means is very fast)  \n",
    "âœ… **Want guaranteed K clusters** (no noise points)\n",
    "\n",
    "**Examples**: Customer segmentation, image compression, document clustering\n",
    "\n",
    "### Use DBSCAN When:\n",
    "\n",
    "âœ… **Arbitrary cluster shapes** (crescents, elongated, irregular)  \n",
    "âœ… **Don't know K** (number of clusters)  \n",
    "âœ… **Varying cluster sizes** (some big, some small)  \n",
    "âœ… **Varying densities** (some dense, some sparse)  \n",
    "âœ… **Want to detect outliers** (noise points)  \n",
    "âœ… **Spatial/geographic data** (location-based clustering)\n",
    "\n",
    "**Examples**: Anomaly detection, geographic data, image segmentation, astronomy\n",
    "\n",
    "### Summary Comparison Table\n",
    "\n",
    "| Feature | K-Means | DBSCAN |\n",
    "|---------|---------|--------|\n",
    "| Cluster shape | Spherical only | Arbitrary |\n",
    "| Specify K? | Yes | No |\n",
    "| Handles outliers? | No (assigns to nearest) | Yes (marks as noise) |\n",
    "| Speed | Very fast | Slower |\n",
    "| Deterministic? | No (depends on init) | Yes |\n",
    "| Different densities? | No | Somewhat |\n",
    "| Different sizes? | No | Yes |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Now it's your turn to practice! Complete these exercises to reinforce your understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: K-Means on Iris Dataset\n",
    "\n",
    "Apply K-Means clustering to the Iris dataset (without using species labels).\n",
    "\n",
    "**Tasks:**\n",
    "1. Load `iris.csv` and extract only the feature columns\n",
    "2. Scale the features\n",
    "3. Use the elbow method to find optimal K (test K from 2 to 8)\n",
    "4. Apply K-Means with optimal K\n",
    "5. Calculate silhouette score\n",
    "6. Visualize the clusters using the first 2 features\n",
    "7. Compare your clusters to the true species labels (how well did it do?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: Follow the K-Means workflow from sections 3-5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: DBSCAN Parameter Tuning\n",
    "\n",
    "Find optimal DBSCAN parameters for the blobs dataset.\n",
    "\n",
    "**Tasks:**\n",
    "1. Use the blobs dataset\n",
    "2. Create a K-distance plot to estimate optimal eps\n",
    "3. Test different combinations of eps (0.3, 0.5, 0.7, 1.0) and min_samples (3, 5, 10)\n",
    "4. For each combination, count clusters and noise points\n",
    "5. Calculate silhouette scores\n",
    "6. Create a heatmap showing silhouette scores for each parameter combination\n",
    "7. Which combination works best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: Use nested loops to test all combinations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Comparing Algorithms on Wine Dataset\n",
    "\n",
    "Compare K-Means and DBSCAN on the wine dataset.\n",
    "\n",
    "**Tasks:**\n",
    "1. Load `wine.csv` (drop target column for unsupervised learning)\n",
    "2. Scale features\n",
    "3. Apply K-Means with K=3\n",
    "4. Apply DBSCAN with reasonable parameters\n",
    "5. Calculate silhouette scores for both\n",
    "6. Calculate Davies-Bouldin score for both (lower is better)\n",
    "7. Visualize both results using first 2 features\n",
    "8. Which algorithm performs better? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: from sklearn.metrics import davies_bouldin_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Outlier Detection with DBSCAN\n",
    "\n",
    "Use DBSCAN to detect outliers in the breast cancer dataset.\n",
    "\n",
    "**Tasks:**\n",
    "1. Load `breast_cancer.csv` (drop target column)\n",
    "2. Scale features\n",
    "3. Apply DBSCAN with eps=0.5, min_samples=10\n",
    "4. How many outliers (noise points) were detected?\n",
    "5. What percentage of total data is considered noise?\n",
    "6. Visualize using first 2 PCA components (hint: use PCA for dimensionality reduction)\n",
    "7. Try different eps values (0.3, 0.5, 1.0) - how does it affect outlier detection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: Outliers have cluster label = -1\n",
    "# For visualization: from sklearn.decomposition import PCA\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts Learned\n",
    "\n",
    "1. **Clustering Fundamentals**\n",
    "   - Unsupervised learning: finding patterns without labels\n",
    "   - Goal: Group similar points together\n",
    "   - Applications: segmentation, organization, anomaly detection\n",
    "\n",
    "2. **K-Means Clustering**\n",
    "   - Iterative algorithm: assign â†’ update â†’ repeat\n",
    "   - Minimizes within-cluster sum of squares (inertia)\n",
    "   - Fast and scalable\n",
    "   - Best for spherical, similarly-sized clusters\n",
    "\n",
    "3. **Choosing K (Elbow Method)**\n",
    "   - Plot inertia vs K\n",
    "   - Look for \"elbow\" where gains diminish\n",
    "   - Combine with silhouette score for validation\n",
    "\n",
    "4. **Cluster Evaluation**\n",
    "   - **Silhouette score**: Measures cluster separation (-1 to +1)\n",
    "   - **Inertia**: Within-cluster sum of squares (lower is better)\n",
    "   - **Davies-Bouldin**: Average similarity between clusters (lower is better)\n",
    "\n",
    "5. **K-Means Limitations**\n",
    "   - Assumes spherical clusters\n",
    "   - Sensitive to initialization\n",
    "   - Requires K in advance\n",
    "   - Affected by outliers\n",
    "\n",
    "6. **DBSCAN Clustering**\n",
    "   - Density-based: clusters are dense regions\n",
    "   - No need to specify K\n",
    "   - Handles arbitrary shapes\n",
    "   - Identifies outliers automatically\n",
    "\n",
    "7. **DBSCAN Parameters**\n",
    "   - **eps**: Maximum distance between neighbors\n",
    "   - **min_samples**: Minimum points for dense region\n",
    "   - Use K-distance plot to find optimal eps\n",
    "\n",
    "8. **Algorithm Selection**\n",
    "   - **K-Means**: Fast, spherical clusters, known K\n",
    "   - **DBSCAN**: Arbitrary shapes, unknown K, outlier detection\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "- **Always scale features** before clustering (especially for K-Means)\n",
    "- **Use multiple evaluation metrics** (silhouette, inertia, visual inspection)\n",
    "- **Try multiple K values** and compare results\n",
    "- **Visualize clusters** when possible (2D or PCA projection)\n",
    "- **Consider data characteristics** when choosing algorithm\n",
    "- **Run K-Means multiple times** (n_init parameter) due to random initialization\n",
    "- **Tune DBSCAN parameters** using K-distance plot\n",
    "\n",
    "### Common Pitfalls to Avoid\n",
    "\n",
    "- âŒ Forgetting to scale features\n",
    "- âŒ Choosing K without validation (elbow method, silhouette)\n",
    "- âŒ Using K-Means on non-spherical clusters\n",
    "- âŒ Ignoring outliers in the data\n",
    "- âŒ Trusting clustering results without domain knowledge\n",
    "- âŒ Using inappropriate distance metrics\n",
    "\n",
    "### What's Next\n",
    "\n",
    "In **Module 13: Dimensionality Reduction (PCA, t-SNE)**, you'll learn:\n",
    "- Why reduce dimensions (curse of dimensionality, visualization, speed)\n",
    "- Principal Component Analysis (PCA) explained\n",
    "- Choosing number of components (explained variance)\n",
    "- t-SNE for 2D/3D visualization\n",
    "- When to use PCA vs t-SNE\n",
    "- PCA for preprocessing before machine learning\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "**Videos:**\n",
    "- [StatQuest: K-Means Clustering](https://www.youtube.com/watch?v=4b5d3muPQmA)\n",
    "- [StatQuest: DBSCAN](https://www.youtube.com/watch?v=RDZUdRSDOok)\n",
    "- [Visualizing K-Means](https://www.youtube.com/watch?v=IuRb3y8qKX4)\n",
    "\n",
    "**Documentation:**\n",
    "- [scikit-learn Clustering Guide](https://scikit-learn.org/stable/modules/clustering.html)\n",
    "- [KMeans API](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)\n",
    "- [DBSCAN API](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html)\n",
    "\n",
    "**Articles:**\n",
    "- [K-Means Clustering: Algorithm, Applications, Evaluation](https://towardsdatascience.com/k-means-clustering-algorithm-applications-evaluation-methods-and-drawbacks-aa03e644b48a)\n",
    "- [DBSCAN Python Example](https://towardsdatascience.com/dbscan-clustering-explained-97556a2ad556)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
