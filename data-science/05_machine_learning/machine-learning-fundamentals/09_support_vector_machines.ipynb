{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 09: Support Vector Machines\n",
    "\n",
    "**Difficulty**: ⭐⭐ Intermediate  \n",
    "**Estimated Time**: 75 minutes  \n",
    "**Prerequisites**: \n",
    "- [Module 04: Logistic Regression](04_logistic_regression.ipynb)\n",
    "- [Module 07: Cross-Validation and Hyperparameter Tuning](07_cross_validation_hyperparameter_tuning.ipynb)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Understand the intuition behind Support Vector Machines (maximum margin)\n",
    "2. Apply Linear SVM for classification problems\n",
    "3. Use the kernel trick (RBF, polynomial) for non-linear problems\n",
    "4. Tune hyperparameters: C (regularization) and gamma (kernel coefficient)\n",
    "5. Distinguish between SVC and SVR (classification vs regression)\n",
    "6. Visualize decision boundaries in 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. SVM Intuition: Maximum Margin Classifier\n",
    "\n",
    "### The Problem\n",
    "\n",
    "**Given**: Two classes of data points  \n",
    "**Goal**: Find the \"best\" line to separate them\n",
    "\n",
    "### Many Lines Can Separate!\n",
    "\n",
    "```\n",
    "    O  O  O         Different lines:\n",
    "   ________         Line A: Close to O's\n",
    "    X  X  X         Line B: Close to X's\n",
    "                    Line C: In the middle ✓\n",
    "```\n",
    "\n",
    "**Question**: Which line is best?\n",
    "\n",
    "### SVM's Answer: Maximum Margin!\n",
    "\n",
    "**Margin**: Distance from decision boundary to nearest data points\n",
    "\n",
    "**SVM Strategy**: Find the line with the LARGEST margin\n",
    "- Maximizes distance to both classes\n",
    "- More robust to new data\n",
    "- Better generalization\n",
    "\n",
    "### Key Terms\n",
    "\n",
    "**Decision Boundary**: The line (or hyperplane) that separates classes  \n",
    "**Support Vectors**: Data points closest to decision boundary  \n",
    "**Margin**: Distance from decision boundary to support vectors  \n",
    "\n",
    "### Real-World Analogy\n",
    "\n",
    "**Building a road between two cities**:\n",
    "- Many possible routes\n",
    "- Best route: Maximizes distance from obstacles on both sides\n",
    "- Support vectors = closest obstacles that determine the route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Visualization settings\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"✓ Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simple 2D dataset for visualization\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X_simple, y_simple = make_classification(\n",
    "    n_samples=100, n_features=2, n_redundant=0, n_informative=2,\n",
    "    n_clusters_per_class=1, random_state=42\n",
    ")\n",
    "\n",
    "# Visualize data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_simple[y_simple==0, 0], X_simple[y_simple==0, 1], \n",
    "           c='blue', marker='o', s=50, alpha=0.7, label='Class 0')\n",
    "plt.scatter(X_simple[y_simple==1, 0], X_simple[y_simple==1, 1], \n",
    "           c='red', marker='s', s=50, alpha=0.7, label='Class 1')\n",
    "plt.xlabel('Feature 1', fontsize=12)\n",
    "plt.ylabel('Feature 2', fontsize=12)\n",
    "plt.title('Sample Dataset\\nGoal: Find best separating line', fontsize=13, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Two linearly separable classes\")\n",
    "print(\"SVM will find the line with maximum margin!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Linear SVM\n",
    "\n",
    "### How Linear SVM Works\n",
    "\n",
    "**Step 1**: Find decision boundary (hyperplane)  \n",
    "**Step 2**: Maximize margin to nearest points  \n",
    "**Step 3**: Only support vectors matter!\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "```\n",
    "Minimize: (1/2)||w||² + C × Σ(slack_i)\n",
    "            ↑              ↑\n",
    "      Maximize margin   Allow some errors\n",
    "```\n",
    "\n",
    "### C Parameter (Regularization)\n",
    "\n",
    "**C**: Controls tradeoff between margin size and misclassification\n",
    "\n",
    "- **Small C** (e.g., 0.1):\n",
    "  - Large margin\n",
    "  - More errors allowed\n",
    "  - Simple decision boundary\n",
    "  - May underfit\n",
    "\n",
    "- **Large C** (e.g., 100):\n",
    "  - Small margin\n",
    "  - Few errors allowed\n",
    "  - Complex decision boundary\n",
    "  - May overfit\n",
    "\n",
    "**Default**: C=1 (balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Linear SVM\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Scale features (IMPORTANT for SVM!)\n",
    "scaler_simple = StandardScaler()\n",
    "X_simple_scaled = scaler_simple.fit_transform(X_simple)\n",
    "\n",
    "# Train Linear SVM\n",
    "svm_linear = SVC(kernel='linear', C=1.0, random_state=42)\n",
    "svm_linear.fit(X_simple_scaled, y_simple)\n",
    "\n",
    "# Get support vectors\n",
    "support_vectors = svm_linear.support_vectors_\n",
    "n_support = svm_linear.n_support_\n",
    "\n",
    "print(f\"Number of support vectors: {len(support_vectors)}\")\n",
    "print(f\"  Class 0: {n_support[0]}\")\n",
    "print(f\"  Class 1: {n_support[1]}\")\n",
    "print(f\"\\nTraining accuracy: {svm_linear.score(X_simple_scaled, y_simple):.3f}\")\n",
    "print(\"\\nOnly these support vectors determine the decision boundary!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Linear SVM decision boundary\n",
    "def plot_svm_boundary(model, X, y, title):\n",
    "    \"\"\"Helper function to plot SVM decision boundary and margins\"\"\"\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    \n",
    "    # Create mesh for decision boundary\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                         np.linspace(y_min, y_max, 200))\n",
    "    \n",
    "    # Predict for mesh\n",
    "    Z = model.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot decision boundary and margins\n",
    "    plt.contourf(xx, yy, Z, levels=[-999, -1, 1, 999], \n",
    "                colors=['lightblue', 'white', 'lightcoral'], alpha=0.3)\n",
    "    plt.contour(xx, yy, Z, levels=[-1, 0, 1], \n",
    "               linestyles=['--', '-', '--'], linewidths=[2, 3, 2], \n",
    "               colors=['blue', 'black', 'red'])\n",
    "    \n",
    "    # Plot data points\n",
    "    plt.scatter(X[y==0, 0], X[y==0, 1], c='blue', marker='o', s=50, \n",
    "               alpha=0.7, edgecolors='k', label='Class 0')\n",
    "    plt.scatter(X[y==1, 0], X[y==1, 1], c='red', marker='s', s=50, \n",
    "               alpha=0.7, edgecolors='k', label='Class 1')\n",
    "    \n",
    "    # Highlight support vectors\n",
    "    plt.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1],\n",
    "               s=200, linewidth=2, facecolors='none', edgecolors='green',\n",
    "               label='Support Vectors')\n",
    "    \n",
    "    plt.xlabel('Feature 1 (scaled)', fontsize=12)\n",
    "    plt.ylabel('Feature 2 (scaled)', fontsize=12)\n",
    "    plt.title(title, fontsize=13, fontweight='bold')\n",
    "    plt.legend(fontsize=11)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_svm_boundary(svm_linear, X_simple_scaled, y_simple, \n",
    "                 'Linear SVM\\nSolid line = Decision boundary, Dashed lines = Margins')\n",
    "\n",
    "print(\"Green circles = Support vectors (determine the boundary)\")\n",
    "print(\"Dashed lines = Margin boundaries\")\n",
    "print(\"Solid black line = Decision boundary (maximum margin!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate effect of C parameter\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "C_values = [0.1, 1, 100]\n",
    "\n",
    "for idx, C in enumerate(C_values):\n",
    "    model = SVC(kernel='linear', C=C, random_state=42)\n",
    "    model.fit(X_simple_scaled, y_simple)\n",
    "    \n",
    "    # Create mesh\n",
    "    x_min, x_max = X_simple_scaled[:, 0].min() - 1, X_simple_scaled[:, 0].max() + 1\n",
    "    y_min, y_max = X_simple_scaled[:, 1].min() - 1, X_simple_scaled[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                         np.linspace(y_min, y_max, 200))\n",
    "    Z = model.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot\n",
    "    axes[idx].contourf(xx, yy, Z, levels=20, cmap='RdBu', alpha=0.3)\n",
    "    axes[idx].contour(xx, yy, Z, levels=[0], linewidths=3, colors='black')\n",
    "    axes[idx].scatter(X_simple_scaled[y_simple==0, 0], X_simple_scaled[y_simple==0, 1],\n",
    "                     c='blue', marker='o', s=50, alpha=0.7, edgecolors='k')\n",
    "    axes[idx].scatter(X_simple_scaled[y_simple==1, 0], X_simple_scaled[y_simple==1, 1],\n",
    "                     c='red', marker='s', s=50, alpha=0.7, edgecolors='k')\n",
    "    axes[idx].scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1],\n",
    "                     s=200, linewidth=2, facecolors='none', edgecolors='green')\n",
    "    \n",
    "    title = f\"C = {C}\\n\"\n",
    "    if C == 0.1:\n",
    "        title += \"Large margin\\n(Few support vectors)\"\n",
    "    elif C == 1:\n",
    "        title += \"Balanced\\n(Moderate support vectors)\"\n",
    "    else:\n",
    "        title += \"Small margin\\n(Many support vectors)\"\n",
    "    \n",
    "    axes[idx].set_title(title, fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Feature 1', fontsize=10)\n",
    "    axes[idx].set_ylabel('Feature 2', fontsize=10)\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observation: Larger C → More complex boundary → More support vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Kernel Trick\n",
    "\n",
    "### The Problem: Non-Linear Data\n",
    "\n",
    "**Linear SVM fails when data is not linearly separable!**\n",
    "\n",
    "Example:\n",
    "```\n",
    "    O O O\n",
    "   O X X O\n",
    "    O O O\n",
    "```\n",
    "No straight line can separate X's from O's!\n",
    "\n",
    "### The Solution: Kernel Trick\n",
    "\n",
    "**Key Idea**: Transform data to higher dimension where it becomes linearly separable!\n",
    "\n",
    "**Magic**: SVM can do this efficiently without explicitly computing the transformation!\n",
    "\n",
    "### Common Kernels\n",
    "\n",
    "1. **Linear**: `kernel='linear'`\n",
    "   - No transformation\n",
    "   - Use when: Data is linearly separable\n",
    "\n",
    "2. **RBF (Radial Basis Function)**: `kernel='rbf'` (default)\n",
    "   - Gaussian kernel\n",
    "   - Can model any shape\n",
    "   - Most popular choice\n",
    "   - Use when: Non-linear patterns\n",
    "\n",
    "3. **Polynomial**: `kernel='poly'`\n",
    "   - Polynomial transformation\n",
    "   - Degree parameter\n",
    "   - Use when: Polynomial relationships\n",
    "\n",
    "4. **Sigmoid**: `kernel='sigmoid'`\n",
    "   - Similar to neural network\n",
    "   - Rarely used\n",
    "\n",
    "### Gamma Parameter (for RBF kernel)\n",
    "\n",
    "**Gamma**: Defines how far the influence of a single training point reaches\n",
    "\n",
    "- **Small gamma** (e.g., 0.01):\n",
    "  - Far reach\n",
    "  - Smooth decision boundary\n",
    "  - May underfit\n",
    "\n",
    "- **Large gamma** (e.g., 10):\n",
    "  - Close reach\n",
    "  - Wiggly decision boundary\n",
    "  - May overfit\n",
    "\n",
    "**Default**: gamma='scale' (1 / (n_features × variance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load non-linear dataset (moons)\n",
    "moons_df = pd.read_csv('data/sample/moons_nonlinear.csv')\n",
    "\n",
    "X_moons = moons_df[['feature_1', 'feature_2']].values\n",
    "y_moons = moons_df['target'].values\n",
    "\n",
    "# Scale features\n",
    "scaler_moons = StandardScaler()\n",
    "X_moons_scaled = scaler_moons.fit_transform(X_moons)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_moons_scaled[y_moons==0, 0], X_moons_scaled[y_moons==0, 1],\n",
    "           c='blue', marker='o', s=50, alpha=0.7, label='Class 0')\n",
    "plt.scatter(X_moons_scaled[y_moons==1, 0], X_moons_scaled[y_moons==1, 1],\n",
    "           c='red', marker='s', s=50, alpha=0.7, label='Class 1')\n",
    "plt.xlabel('Feature 1 (scaled)', fontsize=12)\n",
    "plt.ylabel('Feature 2 (scaled)', fontsize=12)\n",
    "plt.title('Non-Linear Dataset (Moons)\\nNo straight line can separate these!', \n",
    "         fontsize=13, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"This dataset requires non-linear decision boundary!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Linear vs RBF kernel\n",
    "svm_linear_moons = SVC(kernel='linear', C=1, random_state=42)\n",
    "svm_rbf_moons = SVC(kernel='rbf', C=1, gamma='scale', random_state=42)\n",
    "\n",
    "svm_linear_moons.fit(X_moons_scaled, y_moons)\n",
    "svm_rbf_moons.fit(X_moons_scaled, y_moons)\n",
    "\n",
    "linear_score = svm_linear_moons.score(X_moons_scaled, y_moons)\n",
    "rbf_score = svm_rbf_moons.score(X_moons_scaled, y_moons)\n",
    "\n",
    "print(\"Performance on Non-Linear Data:\\n\")\n",
    "print(f\"Linear Kernel: {linear_score:.3f} accuracy\")\n",
    "print(f\"RBF Kernel:    {rbf_score:.3f} accuracy\")\n",
    "print(f\"\\nRBF is much better! (+{(rbf_score-linear_score)*100:.1f} percentage points)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Linear vs RBF\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "models = [svm_linear_moons, svm_rbf_moons]\n",
    "titles = [f'Linear Kernel\\nAccuracy: {linear_score:.3f}', \n",
    "         f'RBF Kernel\\nAccuracy: {rbf_score:.3f}']\n",
    "\n",
    "for idx, (model, title) in enumerate(zip(models, titles)):\n",
    "    # Create mesh\n",
    "    x_min, x_max = X_moons_scaled[:, 0].min() - 0.5, X_moons_scaled[:, 0].max() + 0.5\n",
    "    y_min, y_max = X_moons_scaled[:, 1].min() - 0.5, X_moons_scaled[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                         np.linspace(y_min, y_max, 200))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot decision boundary\n",
    "    axes[idx].contourf(xx, yy, Z, alpha=0.3, cmap='RdBu')\n",
    "    axes[idx].scatter(X_moons_scaled[y_moons==0, 0], X_moons_scaled[y_moons==0, 1],\n",
    "                     c='blue', marker='o', s=50, alpha=0.7, edgecolors='k', label='Class 0')\n",
    "    axes[idx].scatter(X_moons_scaled[y_moons==1, 0], X_moons_scaled[y_moons==1, 1],\n",
    "                     c='red', marker='s', s=50, alpha=0.7, edgecolors='k', label='Class 1')\n",
    "    axes[idx].set_xlabel('Feature 1', fontsize=11)\n",
    "    axes[idx].set_ylabel('Feature 2', fontsize=11)\n",
    "    axes[idx].set_title(title, fontsize=12, fontweight='bold')\n",
    "    axes[idx].legend(fontsize=10)\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"RBF kernel captures the curved boundary perfectly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate effect of gamma parameter\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "gamma_values = [0.1, 1, 10]\n",
    "\n",
    "for idx, gamma in enumerate(gamma_values):\n",
    "    model = SVC(kernel='rbf', C=1, gamma=gamma, random_state=42)\n",
    "    model.fit(X_moons_scaled, y_moons)\n",
    "    \n",
    "    # Create mesh\n",
    "    x_min, x_max = X_moons_scaled[:, 0].min() - 0.5, X_moons_scaled[:, 0].max() + 0.5\n",
    "    y_min, y_max = X_moons_scaled[:, 1].min() - 0.5, X_moons_scaled[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                         np.linspace(y_min, y_max, 200))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot\n",
    "    axes[idx].contourf(xx, yy, Z, alpha=0.3, cmap='RdBu')\n",
    "    axes[idx].scatter(X_moons_scaled[y_moons==0, 0], X_moons_scaled[y_moons==0, 1],\n",
    "                     c='blue', marker='o', s=30, alpha=0.7, edgecolors='k')\n",
    "    axes[idx].scatter(X_moons_scaled[y_moons==1, 0], X_moons_scaled[y_moons==1, 1],\n",
    "                     c='red', marker='s', s=30, alpha=0.7, edgecolors='k')\n",
    "    \n",
    "    score = model.score(X_moons_scaled, y_moons)\n",
    "    title = f\"gamma = {gamma}\\nAccuracy: {score:.3f}\\n\"\n",
    "    if gamma == 0.1:\n",
    "        title += \"Smooth boundary\"\n",
    "    elif gamma == 1:\n",
    "        title += \"Balanced\"\n",
    "    else:\n",
    "        title += \"Overfitting!\"\n",
    "    \n",
    "    axes[idx].set_title(title, fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Feature 1', fontsize=10)\n",
    "    axes[idx].set_ylabel('Feature 2', fontsize=10)\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observation: Large gamma → Complex boundary → Risk of overfitting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. SVM on Real Dataset\n",
    "\n",
    "### Breast Cancer Classification\n",
    "\n",
    "Let's apply SVM to a real medical dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load breast cancer dataset\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "cancer_df = pd.read_csv('data/sample/breast_cancer.csv')\n",
    "\n",
    "X_cancer = cancer_df.drop(['target', 'diagnosis'], axis=1)\n",
    "y_cancer = cancer_df['target']\n",
    "\n",
    "# Split data\n",
    "X_train_cancer, X_test_cancer, y_train_cancer, y_test_cancer = train_test_split(\n",
    "    X_cancer, y_cancer, test_size=0.3, random_state=42, stratify=y_cancer\n",
    ")\n",
    "\n",
    "# Scale features (CRUCIAL for SVM!)\n",
    "scaler_cancer = StandardScaler()\n",
    "X_train_cancer_scaled = scaler_cancer.fit_transform(X_train_cancer)\n",
    "X_test_cancer_scaled = scaler_cancer.transform(X_test_cancer)\n",
    "\n",
    "print(f\"Dataset shape: {X_cancer.shape}\")\n",
    "print(f\"Class distribution:\\n{y_cancer.value_counts()}\")\n",
    "print(\"\\n0 = Malignant (cancer), 1 = Benign (no cancer)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM with default parameters\n",
    "svm_cancer = SVC(kernel='rbf', random_state=42)\n",
    "svm_cancer.fit(X_train_cancer_scaled, y_train_cancer)\n",
    "\n",
    "# Evaluate\n",
    "train_score_cancer = svm_cancer.score(X_train_cancer_scaled, y_train_cancer)\n",
    "test_score_cancer = svm_cancer.score(X_test_cancer_scaled, y_test_cancer)\n",
    "\n",
    "print(\"SVM with Default Parameters:\")\n",
    "print(f\"Train accuracy: {train_score_cancer:.3f}\")\n",
    "print(f\"Test accuracy:  {test_score_cancer:.3f}\")\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(svm_cancer, X_train_cancer_scaled, y_train_cancer, cv=5)\n",
    "print(f\"\\nCross-validation: {cv_scores.mean():.3f} ± {cv_scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning with GridSearchCV\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],\n",
    "    'kernel': ['rbf', 'linear']\n",
    "}\n",
    "\n",
    "print(\"Performing Grid Search...\")\n",
    "print(f\"Testing {np.prod([len(v) for v in param_grid.values()])} combinations\\n\")\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    SVC(random_state=42),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_cancer_scaled, y_train_cancer)\n",
    "\n",
    "print(\"Best hyperparameters:\")\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "print(f\"\\nBest cross-validation score: {grid_search.best_score_:.3f}\")\n",
    "print(f\"Test score: {grid_search.score(X_test_cancer_scaled, y_test_cancer):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed evaluation of best model\n",
    "best_svm = grid_search.best_estimator_\n",
    "y_pred_cancer = best_svm.predict(X_test_cancer_scaled)\n",
    "\n",
    "print(\"Classification Report:\\n\")\n",
    "print(classification_report(y_test_cancer, y_pred_cancer, \n",
    "                           target_names=['Malignant', 'Benign']))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test_cancer, y_pred_cancer)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "           xticklabels=['Malignant', 'Benign'],\n",
    "           yticklabels=['Malignant', 'Benign'])\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.ylabel('Actual', fontsize=12)\n",
    "plt.title('Confusion Matrix - Optimized SVM', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFalse Negatives (missed cancers): {cm[0, 1]}\")\n",
    "print(f\"False Positives (false alarms): {cm[1, 0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. SVR (Support Vector Regression)\n",
    "\n",
    "### SVM for Regression!\n",
    "\n",
    "**SVR**: Support Vector Regression (same idea, different goal)\n",
    "\n",
    "**Goal**: Fit as many points as possible within a margin (epsilon-tube)\n",
    "\n",
    "### Key Differences\n",
    "\n",
    "**SVC (Classification)**:\n",
    "- Maximize margin between classes\n",
    "- Output: Class labels\n",
    "\n",
    "**SVR (Regression)**:\n",
    "- Fit within epsilon margin\n",
    "- Output: Continuous values\n",
    "\n",
    "### Additional Parameter\n",
    "\n",
    "**epsilon**: Width of margin (default=0.1)\n",
    "- Points within epsilon: No penalty\n",
    "- Points outside epsilon: Penalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVR example on California housing\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "housing_df = pd.read_csv('data/sample/california_housing.csv')\n",
    "\n",
    "X_housing = housing_df.drop('median_house_value', axis=1)\n",
    "y_housing = housing_df['median_house_value']\n",
    "\n",
    "# Split and scale\n",
    "X_train_h, X_test_h, y_train_h, y_test_h = train_test_split(\n",
    "    X_housing, y_housing, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "scaler_h = StandardScaler()\n",
    "X_train_h_scaled = scaler_h.fit_transform(X_train_h)\n",
    "X_test_h_scaled = scaler_h.transform(X_test_h)\n",
    "\n",
    "# Train SVR\n",
    "svr_model = SVR(kernel='rbf', C=100, gamma='scale', epsilon=0.1)\n",
    "svr_model.fit(X_train_h_scaled, y_train_h)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_h = svr_model.predict(X_test_h_scaled)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test_h, y_pred_h))\n",
    "r2 = r2_score(y_test_h, y_pred_h)\n",
    "\n",
    "print(\"SVR Performance:\")\n",
    "print(f\"RMSE: ${rmse:,.2f}\")\n",
    "print(f\"R²:   {r2:.3f}\")\n",
    "print(f\"\\nPredictions are off by about ${rmse:,.0f} on average\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test_h, y_pred_h, alpha=0.3, s=20)\n",
    "plt.plot([y_test_h.min(), y_test_h.max()], \n",
    "        [y_test_h.min(), y_test_h.max()], \n",
    "        'r--', linewidth=2, label='Perfect predictions')\n",
    "plt.xlabel('Actual House Value', fontsize=12)\n",
    "plt.ylabel('Predicted House Value', fontsize=12)\n",
    "plt.title(f'SVR Predictions\\nR² = {r2:.3f}', fontsize=13, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Kernel Comparison\n",
    "\n",
    "Compare different kernels on the moons dataset:\n",
    "1. Train SVM with kernels: linear, rbf, poly (degree=3), sigmoid\n",
    "2. Use same C=1 for all\n",
    "3. Evaluate with cross-validation (5-fold)\n",
    "4. Create a bar chart comparing performance\n",
    "5. Visualize decision boundaries for each kernel\n",
    "6. Which kernel works best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: C and Gamma Interaction\n",
    "\n",
    "Explore how C and gamma interact:\n",
    "1. Use wine dataset\n",
    "2. Create a grid: C = [0.1, 1, 10, 100], gamma = [0.001, 0.01, 0.1, 1]\n",
    "3. For each combination, compute cross-validation score\n",
    "4. Create a heatmap showing performance\n",
    "5. What patterns do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Multiclass SVM\n",
    "\n",
    "Apply SVM to iris dataset (3 classes):\n",
    "1. Load iris dataset\n",
    "2. Split into train/test (70/30)\n",
    "3. Scale features\n",
    "4. Use GridSearchCV to find best C and gamma\n",
    "5. Train final model and evaluate\n",
    "6. Create confusion matrix\n",
    "7. Which classes are most confused?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: SVR Hyperparameter Tuning\n",
    "\n",
    "Optimize SVR for California housing:\n",
    "1. Use RandomizedSearchCV (50 iterations)\n",
    "2. Search over:\n",
    "   - C: log-uniform from 0.1 to 1000\n",
    "   - gamma: ['scale', 'auto'] + log-uniform from 0.0001 to 1\n",
    "   - epsilon: uniform from 0.01 to 1\n",
    "3. Use R² as scoring metric\n",
    "4. Print best parameters\n",
    "5. Compare with default SVR\n",
    "6. How much improvement did you get?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **SVM Intuition**:\n",
    "   - Find decision boundary with **maximum margin**\n",
    "   - Only **support vectors** matter (points closest to boundary)\n",
    "   - Robust and generalizes well\n",
    "\n",
    "2. **Linear SVM**:\n",
    "   - Works when data is linearly separable\n",
    "   - Simple and interpretable\n",
    "   - Controlled by C parameter\n",
    "\n",
    "3. **Kernel Trick**:\n",
    "   - Transforms data to higher dimension\n",
    "   - Enables non-linear decision boundaries\n",
    "   - Computationally efficient (no explicit transformation)\n",
    "\n",
    "4. **Common Kernels**:\n",
    "   - **Linear**: For linearly separable data\n",
    "   - **RBF**: Most versatile (default choice)\n",
    "   - **Polynomial**: For polynomial relationships\n",
    "   - **Sigmoid**: Rarely used\n",
    "\n",
    "5. **Hyperparameters**:\n",
    "   - **C**: Regularization (small C = large margin, large C = small margin)\n",
    "   - **gamma**: RBF kernel width (small = smooth, large = wiggly)\n",
    "   - Both affect overfitting/underfitting tradeoff\n",
    "\n",
    "6. **SVC vs SVR**:\n",
    "   - **SVC**: Classification (predict class labels)\n",
    "   - **SVR**: Regression (predict continuous values)\n",
    "   - Same principles, different objectives\n",
    "\n",
    "7. **Best Practices**:\n",
    "   - **Always scale features!** (SVM is sensitive to scale)\n",
    "   - Start with RBF kernel (good default)\n",
    "   - Use GridSearchCV or RandomizedSearchCV for tuning\n",
    "   - Cross-validate to prevent overfitting\n",
    "   - Consider computation time (SVM slow on large datasets)\n",
    "\n",
    "### When to Use SVM\n",
    "\n",
    "✓ **Good for**:\n",
    "- Small to medium datasets\n",
    "- High-dimensional data (many features)\n",
    "- Non-linear patterns\n",
    "- Clear margin of separation\n",
    "\n",
    "✗ **Not ideal for**:\n",
    "- Very large datasets (slow training)\n",
    "- Many overlapping classes\n",
    "- When interpretability is crucial\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In **Module 10: K-Nearest Neighbors**, you'll learn:\n",
    "- Distance-based classification\n",
    "- Different distance metrics (Euclidean, Manhattan)\n",
    "- Choosing optimal K value\n",
    "- Weighted vs uniform neighbors\n",
    "- Impact of feature scaling\n",
    "- Curse of dimensionality\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [SVM Explained - StatQuest](https://www.youtube.com/watch?v=efR1C6CvhmE)\n",
    "- [Kernel Trick - Andrew Ng](https://www.youtube.com/watch?v=XUj5JbQihlU)\n",
    "- [scikit-learn SVM Guide](https://scikit-learn.org/stable/modules/svm.html)\n",
    "- [SVM Tutorial](https://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
