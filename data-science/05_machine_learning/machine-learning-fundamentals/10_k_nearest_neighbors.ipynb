{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 10: K-Nearest Neighbors\n",
    "\n",
    "**Difficulty**: ⭐ Beginner  \n",
    "**Estimated Time**: 60 minutes  \n",
    "**Prerequisites**: [Module 02 - Data Preparation](02_data_preparation_train_test_split.ipynb), [Module 06 - Model Evaluation](06_model_evaluation_metrics.ipynb)\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Understand how the K-Nearest Neighbors (KNN) algorithm works\n",
    "2. Apply different distance metrics (Euclidean, Manhattan, Minkowski)\n",
    "3. Choose the optimal K value using cross-validation\n",
    "4. Understand the critical importance of feature scaling for KNN\n",
    "5. Compare weighted vs uniform neighbor voting\n",
    "6. Recognize the curse of dimensionality and its impact on KNN\n",
    "7. Know when to use KNN vs other algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction: The KNN Intuition\n",
    "\n",
    "### The Core Idea\n",
    "\n",
    "**\"You are the average of your K closest neighbors\"**\n",
    "\n",
    "K-Nearest Neighbors is one of the simplest machine learning algorithms. The idea is beautifully intuitive:\n",
    "\n",
    "- **Real-world analogy**: If you want to predict whether someone likes a movie, look at the preferences of people similar to them. If most similar people liked it, they probably will too!\n",
    "\n",
    "### How KNN Works\n",
    "\n",
    "**For Classification:**\n",
    "1. Choose a value for K (number of neighbors)\n",
    "2. Find the K closest training examples to your new data point\n",
    "3. Take a majority vote among those K neighbors\n",
    "4. Assign the most common class\n",
    "\n",
    "**For Regression:**\n",
    "1. Choose a value for K\n",
    "2. Find the K closest training examples\n",
    "3. Take the average of their target values\n",
    "4. That average is your prediction\n",
    "\n",
    "### The \"Lazy Learner\" Concept\n",
    "\n",
    "KNN is called a **lazy learner** or **instance-based learner** because:\n",
    "- **No training phase**: It simply stores the training data\n",
    "- **All computation at prediction time**: When you want a prediction, it calculates distances to all training points\n",
    "- **Memory-intensive**: Must keep entire training set in memory\n",
    "\n",
    "This is different from algorithms like logistic regression that learn parameters during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import warnings\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "%matplotlib inline\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print('✓ All libraries imported successfully!')\n",
    "print(f'✓ Random seed set to 42 for reproducibility')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "# This classic dataset contains measurements of iris flowers from 3 different species\n",
    "iris_df = pd.read_csv('data/sample/iris.csv')\n",
    "\n",
    "print(\"Iris Dataset Shape:\", iris_df.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(iris_df.head())\n",
    "print(\"\\nSpecies distribution:\")\n",
    "print(iris_df['species'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualizing the KNN Concept\n",
    "\n",
    "Let's visualize how KNN makes predictions using 2 features so we can plot it easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only 2 features for visualization\n",
    "# Petal length and petal width are the most discriminative features for iris species\n",
    "X_visual = iris_df[['petal length (cm)', 'petal width (cm)']].values\n",
    "y_visual = iris_df['species'].values\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "# Plot each species with a different color\n",
    "for species in iris_df['species'].unique():\n",
    "    mask = iris_df['species'] == species\n",
    "    plt.scatter(\n",
    "        iris_df.loc[mask, 'petal length (cm)'],\n",
    "        iris_df.loc[mask, 'petal width (cm)'],\n",
    "        label=species,\n",
    "        alpha=0.7,\n",
    "        s=100\n",
    "    )\n",
    "\n",
    "# Add a new point to classify\n",
    "new_point = [4.5, 1.5]\n",
    "plt.scatter(new_point[0], new_point[1], \n",
    "           color='red', marker='*', s=500, \n",
    "           label='New Point to Classify',\n",
    "           edgecolors='black', linewidth=2)\n",
    "\n",
    "plt.xlabel('Petal Length (cm)', fontsize=12)\n",
    "plt.ylabel('Petal Width (cm)', fontsize=12)\n",
    "plt.title('KNN Classification: Finding Nearest Neighbors', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# Show decision boundaries\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Create a mesh to plot decision boundaries\n",
    "h = 0.02  # Step size in the mesh\n",
    "x_min, x_max = X_visual[:, 0].min() - 0.5, X_visual[:, 0].max() + 0.5\n",
    "y_min, y_max = X_visual[:, 1].min() - 0.5, X_visual[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "# Train KNN with K=5\n",
    "knn_visual = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_visual.fit(X_visual, y_visual)\n",
    "\n",
    "# Predict for all points in the mesh\n",
    "Z = knn_visual.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = pd.factorize(Z)[0].reshape(xx.shape)\n",
    "\n",
    "# Plot decision boundaries\n",
    "plt.contourf(xx, yy, Z, alpha=0.3, cmap='viridis')\n",
    "\n",
    "# Plot training points\n",
    "for species in iris_df['species'].unique():\n",
    "    mask = iris_df['species'] == species\n",
    "    plt.scatter(\n",
    "        iris_df.loc[mask, 'petal length (cm)'],\n",
    "        iris_df.loc[mask, 'petal width (cm)'],\n",
    "        label=species,\n",
    "        alpha=0.7,\n",
    "        s=100,\n",
    "        edgecolors='black',\n",
    "        linewidth=0.5\n",
    "    )\n",
    "\n",
    "plt.xlabel('Petal Length (cm)', fontsize=12)\n",
    "plt.ylabel('Petal Width (cm)', fontsize=12)\n",
    "plt.title('KNN Decision Boundaries (K=5)', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Left plot: Shows the data points and a new point (red star) to classify\")\n",
    "print(\"- Right plot: Shows decision boundaries - regions where KNN predicts each class\")\n",
    "print(\"- KNN creates non-linear, flexible boundaries based on nearby points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Distance Metrics: How to Measure \"Closeness\"\n",
    "\n",
    "KNN needs to calculate distance between points. Different distance metrics can give different results!\n",
    "\n",
    "### Common Distance Metrics\n",
    "\n",
    "**1. Euclidean Distance (default)** - \"As the crow flies\"\n",
    "- Formula: $\\sqrt{(x_1-x_2)^2 + (y_1-y_2)^2 + ...}$\n",
    "- Straight-line distance\n",
    "- Most commonly used\n",
    "\n",
    "**2. Manhattan Distance** - \"City block distance\"\n",
    "- Formula: $|x_1-x_2| + |y_1-y_2| + ...$\n",
    "- Distance along axis-aligned paths (like navigating city streets)\n",
    "- Useful when features are not continuous\n",
    "\n",
    "**3. Minkowski Distance** - Generalization\n",
    "- Formula: $(\\sum |x_i-y_i|^p)^{1/p}$\n",
    "- p=1: Manhattan distance\n",
    "- p=2: Euclidean distance\n",
    "- p>2: Increasingly emphasizes larger differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for all features\n",
    "X = iris_df.drop('species', axis=1).values\n",
    "y = iris_df['species'].values\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# IMPORTANT: Scale features for KNN (we'll explain why soon)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training set: {X_train_scaled.shape}\")\n",
    "print(f\"Test set: {X_test_scaled.shape}\")\n",
    "print(\"\\n✓ Data prepared and scaled!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different distance metrics\n",
    "distance_metrics = {\n",
    "    'Euclidean (p=2)': {'p': 2, 'metric': 'minkowski'},\n",
    "    'Manhattan (p=1)': {'p': 1, 'metric': 'minkowski'},\n",
    "    'Minkowski (p=3)': {'p': 3, 'metric': 'minkowski'}\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(\"Comparing Distance Metrics (K=5):\\n\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for name, params in distance_metrics.items():\n",
    "    # Train KNN with this distance metric\n",
    "    knn = KNeighborsClassifier(n_neighbors=5, **params)\n",
    "    knn.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    train_score = knn.score(X_train_scaled, y_train)\n",
    "    test_score = knn.score(X_test_scaled, y_test)\n",
    "    \n",
    "    results[name] = {'train': train_score, 'test': test_score}\n",
    "    \n",
    "    print(f\"{name:20} | Train: {train_score:.3f} | Test: {test_score:.3f}\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"\\nInsight: Different distance metrics can lead to different performance.\")\n",
    "print(\"Euclidean distance is usually a good default choice.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Choosing the Optimal K Value\n",
    "\n",
    "**K is the most important hyperparameter in KNN.** How do we choose it?\n",
    "\n",
    "### Effect of K on Model Complexity\n",
    "\n",
    "**Small K (e.g., K=1, K=3)**:\n",
    "- ✅ Captures fine patterns in data\n",
    "- ❌ Very sensitive to noise and outliers\n",
    "- ❌ Can overfit - complex, wiggly decision boundaries\n",
    "- Example: K=1 predicts exactly like the single nearest neighbor\n",
    "\n",
    "**Large K (e.g., K=50, K=100)**:\n",
    "- ✅ More robust to noise\n",
    "- ✅ Smoother decision boundaries\n",
    "- ❌ May underfit - too simplistic\n",
    "- ❌ Loses local patterns\n",
    "\n",
    "**Finding Optimal K**: Use cross-validation to test different K values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test K values from 1 to 30\n",
    "k_values = range(1, 31)\n",
    "train_scores = []\n",
    "cv_scores = []\n",
    "\n",
    "print(\"Testing different K values...\\n\")\n",
    "\n",
    "for k in k_values:\n",
    "    # Create KNN classifier with K neighbors\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    \n",
    "    # Fit on training data\n",
    "    knn.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Training accuracy\n",
    "    train_score = knn.score(X_train_scaled, y_train)\n",
    "    train_scores.append(train_score)\n",
    "    \n",
    "    # Cross-validation accuracy (5-fold)\n",
    "    # This is more reliable than simple train/test split\n",
    "    cv_score = cross_val_score(knn, X_train_scaled, y_train, cv=5).mean()\n",
    "    cv_scores.append(cv_score)\n",
    "\n",
    "# Find best K\n",
    "best_k = k_values[np.argmax(cv_scores)]\n",
    "best_cv_score = max(cv_scores)\n",
    "\n",
    "print(f\"✓ Testing complete!\")\n",
    "print(f\"\\nBest K value: {best_k}\")\n",
    "print(f\"Best CV score: {best_cv_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the effect of K on performance\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(k_values, train_scores, 'o-', label='Training Accuracy', linewidth=2)\n",
    "plt.plot(k_values, cv_scores, 's-', label='CV Accuracy', linewidth=2)\n",
    "plt.axvline(best_k, color='red', linestyle='--', label=f'Best K={best_k}', alpha=0.7)\n",
    "plt.xlabel('K (Number of Neighbors)', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('Finding Optimal K with Cross-Validation', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# Show the bias-variance tradeoff\n",
    "plt.plot(k_values, train_scores, 'o-', linewidth=2, label='Train (Lower = More Variance)')\n",
    "plt.plot(k_values, cv_scores, 's-', linewidth=2, label='CV (Lower = More Bias)')\n",
    "plt.axvline(best_k, color='red', linestyle='--', label=f'Optimal Tradeoff K={best_k}', alpha=0.7)\n",
    "plt.xlabel('K (Number of Neighbors)', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('Bias-Variance Tradeoff in KNN', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(\"- As K increases, training accuracy decreases (model becomes simpler)\")\n",
    "print(\"- CV accuracy is highest at intermediate K values\")\n",
    "print(\"- Very small K (1-3): Risk of overfitting\")\n",
    "print(\"- Very large K (25+): Risk of underfitting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Scaling: CRITICAL for KNN!\n",
    "\n",
    "**Why is feature scaling essential for KNN?**\n",
    "\n",
    "KNN uses distances to find neighbors. If features are on different scales, the feature with the largest range will dominate the distance calculation!\n",
    "\n",
    "### Example Problem\n",
    "\n",
    "Imagine predicting house prices using:\n",
    "- Number of bedrooms: 1-5 (small range)\n",
    "- Square footage: 500-5000 (large range)\n",
    "\n",
    "Without scaling:\n",
    "- Distance will be dominated by square footage\n",
    "- Number of bedrooms will have almost no effect\n",
    "- Even if bedrooms is more predictive!\n",
    "\n",
    "**Solution**: Scale all features to similar ranges (e.g., StandardScaler, MinMaxScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the impact of feature scaling\n",
    "print(\"Feature Ranges BEFORE Scaling:\")\n",
    "print(\"=\" * 50)\n",
    "for i, col in enumerate(['sepal length', 'sepal width', 'petal length', 'petal width']):\n",
    "    print(f\"{col:15} : [{X_train[:, i].min():.2f}, {X_train[:, i].max():.2f}]\")\n",
    "\n",
    "print(\"\\nFeature Ranges AFTER Scaling:\")\n",
    "print(\"=\" * 50)\n",
    "for i, col in enumerate(['sepal length', 'sepal width', 'petal length', 'petal width']):\n",
    "    print(f\"{col:15} : [{X_train_scaled[:, i].min():.2f}, {X_train_scaled[:, i].max():.2f}]\")\n",
    "\n",
    "print(\"\\nNotice: After scaling, all features are on similar scales (roughly -2 to +2)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare performance with and without scaling\n",
    "print(\"Performance Comparison:\\n\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Without scaling\n",
    "knn_unscaled = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_unscaled.fit(X_train, y_train)\n",
    "unscaled_score = knn_unscaled.score(X_test, y_test)\n",
    "\n",
    "# With scaling\n",
    "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_scaled.fit(X_train_scaled, y_train)\n",
    "scaled_score = knn_scaled.score(X_test_scaled, y_test)\n",
    "\n",
    "print(f\"WITHOUT scaling: {unscaled_score:.3f}\")\n",
    "print(f\"WITH scaling:    {scaled_score:.3f}\")\n",
    "print(f\"\\nImprovement:     {(scaled_score - unscaled_score):.3f}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\n⚠️  ALWAYS scale features when using KNN!\")\n",
    "print(\"   - Use StandardScaler for normally distributed features\")\n",
    "print(\"   - Use MinMaxScaler for bounded features\")\n",
    "print(\"   - Use RobustScaler for features with outliers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Weighted vs Uniform Neighbors\n",
    "\n",
    "When making predictions, should all K neighbors have equal influence?\n",
    "\n",
    "**Uniform Weighting** (default):\n",
    "- All K neighbors vote equally\n",
    "- Majority class wins (classification)\n",
    "- Simple average (regression)\n",
    "\n",
    "**Distance Weighting**:\n",
    "- Closer neighbors have more influence\n",
    "- Weight = 1 / distance\n",
    "- More intuitive: trust closer neighbors more\n",
    "- Often performs better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare uniform vs distance weighting\n",
    "print(\"Comparing Voting Schemes:\\n\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Uniform weights\n",
    "knn_uniform = KNeighborsClassifier(n_neighbors=5, weights='uniform')\n",
    "knn_uniform.fit(X_train_scaled, y_train)\n",
    "uniform_score = knn_uniform.score(X_test_scaled, y_test)\n",
    "\n",
    "# Distance weights\n",
    "knn_distance = KNeighborsClassifier(n_neighbors=5, weights='distance')\n",
    "knn_distance.fit(X_train_scaled, y_train)\n",
    "distance_score = knn_distance.score(X_test_scaled, y_test)\n",
    "\n",
    "print(f\"Uniform weighting:   {uniform_score:.3f}\")\n",
    "print(f\"Distance weighting:  {distance_score:.3f}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\nInsight: Distance weighting often (but not always) performs better.\")\n",
    "print(\"Try both and use cross-validation to choose!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. The Curse of Dimensionality\n",
    "\n",
    "**Critical limitation of KNN**: Performance degrades in high-dimensional spaces!\n",
    "\n",
    "### Why?\n",
    "\n",
    "As the number of dimensions (features) increases:\n",
    "1. **Distance becomes meaningless**: All points become roughly equidistant\n",
    "2. **Data becomes sparse**: Points are very far apart\n",
    "3. **Need exponentially more data**: To maintain the same density\n",
    "\n",
    "**Rule of thumb**: KNN works best with < 20-30 features. Beyond that, consider:\n",
    "- Dimensionality reduction (PCA, feature selection)\n",
    "- Other algorithms (tree-based methods, neural networks)\n",
    "\n",
    "Let's demonstrate this phenomenon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate curse of dimensionality\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "print(\"Testing KNN performance across different numbers of dimensions...\\n\")\n",
    "\n",
    "dimensions = [2, 5, 10, 20, 50, 100]\n",
    "scores = []\n",
    "\n",
    "# Fixed sample size\n",
    "n_samples = 500\n",
    "\n",
    "for n_features in dimensions:\n",
    "    # Create synthetic dataset with n_features dimensions\n",
    "    X_dim, y_dim = make_classification(\n",
    "        n_samples=n_samples,\n",
    "        n_features=n_features,\n",
    "        n_informative=min(n_features, 5),  # Only 5 features are actually useful\n",
    "        n_redundant=0,\n",
    "        n_classes=3,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Split and scale\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X_dim, y_dim, test_size=0.3, random_state=42)\n",
    "    scaler_dim = StandardScaler()\n",
    "    X_tr_scaled = scaler_dim.fit_transform(X_tr)\n",
    "    X_te_scaled = scaler_dim.transform(X_te)\n",
    "    \n",
    "    # Train KNN\n",
    "    knn_dim = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn_dim.fit(X_tr_scaled, y_tr)\n",
    "    score = knn_dim.score(X_te_scaled, y_te)\n",
    "    scores.append(score)\n",
    "    \n",
    "    print(f\"Dimensions: {n_features:3d} | Accuracy: {score:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the curse of dimensionality\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(dimensions, scores, 'o-', linewidth=2, markersize=10)\n",
    "plt.xlabel('Number of Dimensions (Features)', fontsize=12)\n",
    "plt.ylabel('Test Accuracy', fontsize=12)\n",
    "plt.title('The Curse of Dimensionality in KNN', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=scores[0], color='red', linestyle='--', alpha=0.5, label='Performance with 2D')\n",
    "plt.legend()\n",
    "\n",
    "# Add annotation\n",
    "plt.annotate('Performance degrades\\nin high dimensions', \n",
    "             xy=(50, scores[-2]), xytext=(30, 0.55),\n",
    "             arrowprops=dict(arrowstyle='->', color='red', lw=2),\n",
    "             fontsize=11, color='red')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Insight:\")\n",
    "print(\"- Performance is good with few dimensions (2-10)\")\n",
    "print(\"- Performance drops significantly with many dimensions (50+)\")\n",
    "print(\"- This is the 'curse of dimensionality' - distances become less meaningful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. When to Use KNN\n",
    "\n",
    "### KNN is Great When:\n",
    "\n",
    "✅ **Small to medium-sized datasets** (< 10,000 samples)\n",
    "   - Prediction time is O(n) per prediction\n",
    "   - Can be slow with millions of samples\n",
    "\n",
    "✅ **Low to moderate dimensions** (< 20-30 features)\n",
    "   - Suffers from curse of dimensionality\n",
    "\n",
    "✅ **Non-linear decision boundaries**\n",
    "   - Can capture complex patterns\n",
    "   - No assumptions about data distribution\n",
    "\n",
    "✅ **No training time constraints**\n",
    "   - Zero training time (just stores data)\n",
    "   - Great for online learning\n",
    "\n",
    "✅ **Multi-class problems**\n",
    "   - Naturally handles multiple classes\n",
    "   - No need for one-vs-rest\n",
    "\n",
    "### Avoid KNN When:\n",
    "\n",
    "❌ **Large datasets** (millions of samples)\n",
    "   - Consider tree-based methods or neural networks\n",
    "\n",
    "❌ **High-dimensional data** (hundreds/thousands of features)\n",
    "   - Use dimensionality reduction first, or choose different algorithm\n",
    "\n",
    "❌ **Prediction speed is critical**\n",
    "   - Each prediction requires scanning all training data\n",
    "   - Consider logistic regression or SVM\n",
    "\n",
    "❌ **Imbalanced datasets**\n",
    "   - Majority class can dominate voting\n",
    "   - Need to handle class imbalance carefully\n",
    "\n",
    "❌ **Mixed data types** (categorical + numerical)\n",
    "   - Distance metrics work best with numerical data\n",
    "   - Need careful preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Final Evaluation with Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model with optimal hyperparameters\n",
    "final_knn = KNeighborsClassifier(\n",
    "    n_neighbors=best_k,\n",
    "    weights='distance',  # Use distance weighting\n",
    "    metric='minkowski',\n",
    "    p=2  # Euclidean distance\n",
    ")\n",
    "\n",
    "# Fit on training data\n",
    "final_knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = final_knn.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Final Model Configuration:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"K (neighbors):      {best_k}\")\n",
    "print(f\"Weights:            distance\")\n",
    "print(f\"Distance metric:    Euclidean\")\n",
    "print(f\"Feature scaling:    StandardScaler\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nTest Accuracy:      {test_accuracy:.3f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=np.unique(y_test),\n",
    "            yticklabels=np.unique(y_test))\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.ylabel('Actual', fontsize=12)\n",
    "plt.title(f'Confusion Matrix - KNN (K={best_k})', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Excellent! The model correctly classifies nearly all test samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Now it's your turn to practice! Complete these exercises to reinforce your understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Distance Metric Comparison on Wine Dataset\n",
    "\n",
    "Load the wine dataset and compare the performance of Euclidean, Manhattan, and Minkowski (p=3) distance metrics.\n",
    "\n",
    "**Tasks:**\n",
    "1. Load `wine.csv` dataset\n",
    "2. Split into train/test (70/30)\n",
    "3. Scale the features\n",
    "4. Train KNN with K=5 using each distance metric\n",
    "5. Compare test accuracies\n",
    "6. Which distance metric works best for this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: Use similar code structure as the iris example\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Finding Optimal K with GridSearchCV\n",
    "\n",
    "Use GridSearchCV to find the optimal K value for the wine dataset.\n",
    "\n",
    "**Tasks:**\n",
    "1. Create a parameter grid testing K from 1 to 20\n",
    "2. Test both 'uniform' and 'distance' weights\n",
    "3. Use 5-fold cross-validation\n",
    "4. Report the best parameters and score\n",
    "5. Visualize how CV score changes with K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: from sklearn.model_selection import GridSearchCV\n",
    "# param_grid = {'n_neighbors': [...], 'weights': [...]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Impact of Feature Scaling\n",
    "\n",
    "Demonstrate the importance of feature scaling using the breast cancer dataset.\n",
    "\n",
    "**Tasks:**\n",
    "1. Load `breast_cancer.csv`\n",
    "2. Train KNN (K=5) WITHOUT scaling\n",
    "3. Train KNN (K=5) WITH StandardScaler\n",
    "4. Train KNN (K=5) WITH MinMaxScaler\n",
    "5. Compare test accuracies\n",
    "6. Explain why scaling matters (look at feature ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: KNN for Regression\n",
    "\n",
    "Apply KNN to a regression problem using the California housing dataset.\n",
    "\n",
    "**Tasks:**\n",
    "1. Load `california_housing.csv` (first 1000 rows only for speed)\n",
    "2. Target variable: 'MedHouseVal'\n",
    "3. Split into train/test and scale features\n",
    "4. Train KNeighborsRegressor with K=5\n",
    "5. Calculate RMSE and R² score\n",
    "6. Compare with K=10 and K=20\n",
    "7. Which K works best for regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: from sklearn.neighbors import KNeighborsRegressor\n",
    "# Use mean_squared_error and r2_score for evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts Learned\n",
    "\n",
    "1. **KNN Algorithm Basics**\n",
    "   - \"You are the average of your K nearest neighbors\"\n",
    "   - Classification: majority vote | Regression: average\n",
    "   - Lazy learner: no training phase, all work at prediction time\n",
    "\n",
    "2. **Distance Metrics**\n",
    "   - Euclidean: straight-line distance (default, usually best)\n",
    "   - Manhattan: city-block distance\n",
    "   - Minkowski: generalization (p=1 Manhattan, p=2 Euclidean)\n",
    "\n",
    "3. **Choosing K Value**\n",
    "   - Small K: complex boundaries, risk of overfitting\n",
    "   - Large K: smooth boundaries, risk of underfitting\n",
    "   - Use cross-validation to find optimal K\n",
    "\n",
    "4. **Feature Scaling is CRITICAL**\n",
    "   - KNN is distance-based\n",
    "   - Features on different scales will dominate distance\n",
    "   - ALWAYS use StandardScaler or MinMaxScaler\n",
    "\n",
    "5. **Weighted vs Uniform Neighbors**\n",
    "   - Uniform: all K neighbors vote equally\n",
    "   - Distance: closer neighbors have more influence\n",
    "   - Distance weighting often performs better\n",
    "\n",
    "6. **Curse of Dimensionality**\n",
    "   - Performance degrades with many features (>20-30)\n",
    "   - Distances become less meaningful in high dimensions\n",
    "   - Use dimensionality reduction or choose different algorithm\n",
    "\n",
    "7. **When to Use KNN**\n",
    "   - ✅ Small-medium datasets, low dimensions, non-linear patterns\n",
    "   - ❌ Large datasets, high dimensions, need fast predictions\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "- **ALWAYS scale features** before using KNN\n",
    "- **Use cross-validation** to choose K\n",
    "- **Start with K=5** as a reasonable default\n",
    "- **Try distance weighting** - often better than uniform\n",
    "- **Check feature count** - if >30 features, consider dimensionality reduction\n",
    "- **Monitor prediction time** - can be slow with large training sets\n",
    "\n",
    "### Common Pitfalls to Avoid\n",
    "\n",
    "- ❌ Forgetting to scale features\n",
    "- ❌ Using KNN with hundreds of features\n",
    "- ❌ Not using cross-validation to choose K\n",
    "- ❌ Applying KNN to huge datasets without considering speed\n",
    "- ❌ Ignoring class imbalance (majority class can dominate)\n",
    "\n",
    "### What's Next\n",
    "\n",
    "In **Module 11: Naive Bayes**, you'll learn:\n",
    "- Probability-based classification\n",
    "- Bayes' theorem and the \"naive\" assumption\n",
    "- When Naive Bayes excels (text classification, spam detection)\n",
    "- Gaussian, Multinomial, and Bernoulli variants\n",
    "- Handling zero probabilities with Laplace smoothing\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "**Videos:**\n",
    "- [StatQuest: KNN Explained](https://www.youtube.com/watch?v=HVXime0nQeI)\n",
    "- [KNN Algorithm - Step by Step](https://www.youtube.com/watch?v=4HKqjENq9OU)\n",
    "\n",
    "**Documentation:**\n",
    "- [scikit-learn KNN User Guide](https://scikit-learn.org/stable/modules/neighbors.html)\n",
    "- [KNeighborsClassifier API](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n",
    "\n",
    "**Articles:**\n",
    "- [K-Nearest Neighbors: Dangerously Simple](https://mathbabe.org/2013/04/04/k-nearest-neighbors-dangerously-simple/)\n",
    "- [The Curse of Dimensionality](https://www.kdnuggets.com/2017/04/curse-dimensionality-explained.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
