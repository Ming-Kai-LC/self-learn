{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 11: Naive Bayes\n",
    "\n",
    "**Difficulty**: ‚≠ê‚≠ê Intermediate  \n",
    "**Estimated Time**: 60 minutes  \n",
    "**Prerequisites**: [Module 04 - Logistic Regression](04_logistic_regression.ipynb), [Module 06 - Model Evaluation](06_model_evaluation_metrics.ipynb)\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Understand Bayes' theorem and its application to classification\n",
    "2. Explain the \"naive\" conditional independence assumption\n",
    "3. Apply Gaussian Naive Bayes to continuous features\n",
    "4. Use Multinomial Naive Bayes for count/frequency data\n",
    "5. Apply Bernoulli Naive Bayes to binary features\n",
    "6. Recognize when Naive Bayes excels (text classification, spam detection)\n",
    "7. Handle zero probabilities using Laplace smoothing\n",
    "8. Appreciate the speed and efficiency advantages of Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction: The Power of Probability\n",
    "\n",
    "### What is Naive Bayes?\n",
    "\n",
    "Naive Bayes is a **probability-based classification algorithm** built on Bayes' theorem. Despite its simplicity (and the \"naive\" assumption), it works surprisingly well in many real-world applications!\n",
    "\n",
    "### Real-World Example: Email Spam Detection\n",
    "\n",
    "Imagine you're building a spam filter:\n",
    "- You receive an email containing the word \"FREE\"\n",
    "- Question: Is this email spam?\n",
    "- Naive Bayes answers: \"What's the probability this is spam, given it contains 'FREE'?\"\n",
    "\n",
    "**Key insight**: Instead of learning complex decision boundaries, Naive Bayes calculates probabilities!\n",
    "\n",
    "### Why \"Naive\"?\n",
    "\n",
    "The algorithm assumes that **all features are independent** given the class label. \n",
    "\n",
    "Example with spam:\n",
    "- It assumes the probability of seeing \"FREE\" is independent of seeing \"MONEY\"\n",
    "- In reality, these words often appear together in spam\n",
    "- **But**: This \"naive\" assumption simplifies calculations enormously\n",
    "- **And**: It works well in practice despite being unrealistic!\n",
    "\n",
    "### Advantages of Naive Bayes\n",
    "\n",
    "‚úÖ **Extremely fast** training and prediction  \n",
    "‚úÖ **Works well with high dimensions** (thousands of features)  \n",
    "‚úÖ **Requires little training data** compared to other algorithms  \n",
    "‚úÖ **Handles multi-class classification** naturally  \n",
    "‚úÖ **Probabilistic predictions** (not just class labels)  \n",
    "‚úÖ **Great for text classification** and categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "import warnings\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "%matplotlib inline\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('Set2')\n",
    "\n",
    "print('‚úì All libraries imported successfully!')\n",
    "print(f'‚úì Random seed set to 42 for reproducibility')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bayes' Theorem: The Foundation\n",
    "\n",
    "### The Formula\n",
    "\n",
    "Bayes' theorem tells us how to update our beliefs based on new evidence:\n",
    "\n",
    "$$P(Class|Features) = \\frac{P(Features|Class) \\times P(Class)}{P(Features)}$$\n",
    "\n",
    "**In words:**\n",
    "- **P(Class|Features)**: Probability of class given features (what we want!)\n",
    "- **P(Features|Class)**: Probability of seeing these features if it's this class\n",
    "- **P(Class)**: Prior probability of the class (how common is it?)\n",
    "- **P(Features)**: Probability of seeing these features (normalization constant)\n",
    "\n",
    "### Intuitive Example: Medical Diagnosis\n",
    "\n",
    "**Scenario**: Testing for a rare disease\n",
    "- Disease affects 1% of population: P(Disease) = 0.01\n",
    "- Test is 99% accurate: P(Positive|Disease) = 0.99\n",
    "- Test has 1% false positive rate: P(Positive|Healthy) = 0.01\n",
    "\n",
    "**Question**: If you test positive, what's the probability you have the disease?\n",
    "\n",
    "**Naive guess**: 99% (the test accuracy)\n",
    "\n",
    "**Bayes' theorem answer**: Much lower! Let's calculate..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Medical diagnosis example with Bayes' theorem\n",
    "# Given information\n",
    "P_disease = 0.01          # 1% of people have the disease (prior)\n",
    "P_healthy = 0.99          # 99% are healthy\n",
    "P_pos_given_disease = 0.99  # Test correctly identifies disease 99% of the time\n",
    "P_pos_given_healthy = 0.01  # Test incorrectly says positive 1% of the time\n",
    "\n",
    "# Calculate P(Positive) - probability of testing positive overall\n",
    "# This happens if: (you have disease AND test positive) OR (you're healthy AND false positive)\n",
    "P_positive = (P_pos_given_disease * P_disease) + (P_pos_given_healthy * P_healthy)\n",
    "\n",
    "# Apply Bayes' theorem: P(Disease|Positive)\n",
    "P_disease_given_positive = (P_pos_given_disease * P_disease) / P_positive\n",
    "\n",
    "print(\"Medical Diagnosis Example:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Prior probability of disease: {P_disease:.1%}\")\n",
    "print(f\"Test accuracy (sensitivity):  {P_pos_given_disease:.1%}\")\n",
    "print(f\"False positive rate:          {P_pos_given_healthy:.1%}\")\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(f\"\\nProbability of disease GIVEN positive test: {P_disease_given_positive:.1%}\")\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "\n",
    "print(\"\\nüîç Key Insight:\")\n",
    "print(\"Even with a 99% accurate test, if the disease is rare,\")\n",
    "print(\"a positive test only means ~50% chance of actually having it!\")\n",
    "print(\"This is because false positives outnumber true positives.\")\n",
    "print(\"\\nThis is the power of Bayes' theorem - it accounts for base rates!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Naive Assumption Explained\n",
    "\n",
    "### Feature Independence\n",
    "\n",
    "For classification with multiple features, we need:\n",
    "\n",
    "$$P(x_1, x_2, ..., x_n | Class)$$\n",
    "\n",
    "**Problem**: Computing this joint probability is computationally expensive!\n",
    "\n",
    "**Naive assumption**: Features are conditionally independent given the class\n",
    "\n",
    "$$P(x_1, x_2, ..., x_n | Class) = P(x_1|Class) \\times P(x_2|Class) \\times ... \\times P(x_n|Class)$$\n",
    "\n",
    "**Example**: Email spam with features \"FREE\" and \"MONEY\"\n",
    "- Reality: If \"FREE\" appears, \"MONEY\" is more likely (they're correlated)\n",
    "- Naive Bayes: Assumes they're independent\n",
    "- P(FREE, MONEY | Spam) = P(FREE | Spam) √ó P(MONEY | Spam)\n",
    "\n",
    "### Why Does This Work?\n",
    "\n",
    "Even though the assumption is \"naive\" (usually violated), Naive Bayes often works because:\n",
    "1. We only need to rank classes, not get exact probabilities\n",
    "2. The relative ordering is often correct even if absolute values aren't\n",
    "3. The bias introduced often helps prevent overfitting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Gaussian Naive Bayes: For Continuous Features\n",
    "\n",
    "**Use when**: Features are continuous and approximately normally distributed\n",
    "\n",
    "**How it works**:\n",
    "1. For each feature and each class, calculate mean (Œº) and variance (œÉ¬≤)\n",
    "2. Assume feature values follow a Gaussian (normal) distribution\n",
    "3. Calculate P(feature|class) using the Gaussian probability density function\n",
    "\n",
    "**Best for**: Iris classification, medical measurements, sensor data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Iris dataset for Gaussian Naive Bayes\n",
    "iris_df = pd.read_csv('data/sample/iris.csv')\n",
    "\n",
    "print(\"Iris Dataset for Gaussian Naive Bayes:\")\n",
    "print(f\"Shape: {iris_df.shape}\")\n",
    "print(f\"\\nFeatures: Continuous measurements (perfect for Gaussian NB!)\")\n",
    "print(iris_df.head())\n",
    "print(\"\\nClass distribution:\")\n",
    "print(iris_df['species'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for Gaussian Naive Bayes\n",
    "X_iris = iris_df.drop('species', axis=1).values\n",
    "y_iris = iris_df['species'].values\n",
    "\n",
    "# Split into train and test\n",
    "X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(\n",
    "    X_iris, y_iris, test_size=0.3, random_state=42, stratify=y_iris\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train_iris.shape}\")\n",
    "print(f\"Test set: {X_test_iris.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Gaussian Naive Bayes\n",
    "# Note: Unlike KNN, we don't need to scale features for Naive Bayes!\n",
    "# (But it doesn't hurt either)\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Fit the model - this is extremely fast!\n",
    "gnb.fit(X_train_iris, y_train_iris)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_iris = gnb.predict(X_test_iris)\n",
    "\n",
    "# Get probability estimates\n",
    "y_prob_iris = gnb.predict_proba(X_test_iris)\n",
    "\n",
    "# Evaluate\n",
    "accuracy_iris = accuracy_score(y_test_iris, y_pred_iris)\n",
    "\n",
    "print(\"Gaussian Naive Bayes Results:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Accuracy: {accuracy_iris:.3f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_iris, y_pred_iris))\n",
    "\n",
    "print(\"\\n‚úì Training was instant! Naive Bayes is very fast.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize probability predictions\n",
    "# Show the first 10 test samples with their predicted probabilities\n",
    "print(\"Probability Predictions for First 10 Test Samples:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'True Label':<15} {'Predicted':<15} {'Probabilities':<40}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "classes = gnb.classes_\n",
    "for i in range(10):\n",
    "    true_label = y_test_iris[i]\n",
    "    pred_label = y_pred_iris[i]\n",
    "    probs = y_prob_iris[i]\n",
    "    \n",
    "    prob_str = \", \".join([f\"{cls}: {p:.2%}\" for cls, p in zip(classes, probs)])\n",
    "    \n",
    "    match = \"‚úì\" if true_label == pred_label else \"‚úó\"\n",
    "    print(f\"{true_label:<15} {pred_label:<15} {prob_str:<40} {match}\")\n",
    "\n",
    "print(\"\\nüí° Insight: Naive Bayes gives probability estimates, not just predictions!\")\n",
    "print(\"   This allows you to set custom confidence thresholds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm_iris = confusion_matrix(y_test_iris, y_pred_iris)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_iris, annot=True, fmt='d', cmap='Greens',\n",
    "            xticklabels=classes,\n",
    "            yticklabels=classes)\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.ylabel('Actual', fontsize=12)\n",
    "plt.title('Confusion Matrix - Gaussian Naive Bayes on Iris', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Excellent performance! Gaussian NB works well with continuous features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Multinomial Naive Bayes: For Count Data\n",
    "\n",
    "**Use when**: Features represent counts or frequencies\n",
    "\n",
    "**How it works**:\n",
    "- Features are counts (non-negative integers)\n",
    "- Example: Word counts in text documents\n",
    "- Assumes features follow a multinomial distribution\n",
    "\n",
    "**Best for**: Text classification, document categorization, word frequency analysis\n",
    "\n",
    "**Common applications**:\n",
    "- Spam detection\n",
    "- Sentiment analysis\n",
    "- Topic classification\n",
    "- Language detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate text classification data (word counts)\n",
    "# In practice, you'd use CountVectorizer or TfidfVectorizer on real text\n",
    "\n",
    "# For demonstration, let's use the synthetic classification dataset\n",
    "# We'll treat it as \"word count\" data\n",
    "text_df = pd.read_csv('data/sample/synthetic_classification.csv')\n",
    "\n",
    "print(\"Synthetic Data for Multinomial Naive Bayes:\")\n",
    "print(f\"Shape: {text_df.shape}\")\n",
    "print(f\"\\nFirst few rows (imagine these are word counts):\")\n",
    "print(text_df.head())\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(text_df['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for Multinomial Naive Bayes\n",
    "X_text = text_df.drop('target', axis=1).values\n",
    "y_text = text_df['target'].values\n",
    "\n",
    "# Multinomial NB requires non-negative features\n",
    "# Let's shift features to be non-negative (as if they were counts)\n",
    "X_text_pos = X_text - X_text.min() + 1\n",
    "\n",
    "# Split data\n",
    "X_train_text, X_test_text, y_train_text, y_test_text = train_test_split(\n",
    "    X_text_pos, y_text, test_size=0.3, random_state=42, stratify=y_text\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train_text.shape}\")\n",
    "print(f\"Test set: {X_test_text.shape}\")\n",
    "print(f\"\\n‚úì Features are now non-negative (required for Multinomial NB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Multinomial Naive Bayes\n",
    "# alpha parameter: Laplace smoothing (more on this later!)\n",
    "mnb = MultinomialNB(alpha=1.0)  # alpha=1.0 is additive smoothing\n",
    "\n",
    "# Fit the model\n",
    "mnb.fit(X_train_text, y_train_text)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_text = mnb.predict(X_test_text)\n",
    "\n",
    "# Evaluate\n",
    "accuracy_text = accuracy_score(y_test_text, y_pred_text)\n",
    "\n",
    "print(\"Multinomial Naive Bayes Results:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Accuracy: {accuracy_text:.3f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_text, y_pred_text))\n",
    "\n",
    "print(\"\\nüí° In practice, Multinomial NB is THE go-to algorithm for text classification!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Bernoulli Naive Bayes: For Binary Features\n",
    "\n",
    "**Use when**: Features are binary (0 or 1, True or False, present or absent)\n",
    "\n",
    "**How it works**:\n",
    "- Each feature is either present (1) or absent (0)\n",
    "- Example: Word appears in document (1) or doesn't (0)\n",
    "- Assumes features follow a Bernoulli distribution\n",
    "\n",
    "**Difference from Multinomial**:\n",
    "- **Multinomial**: \"How many times does word X appear?\" (counts)\n",
    "- **Bernoulli**: \"Does word X appear at all?\" (presence/absence)\n",
    "\n",
    "**Best for**: Binary feature vectors, short text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary features from breast cancer dataset\n",
    "bc_df = pd.read_csv('data/sample/breast_cancer.csv')\n",
    "\n",
    "print(\"Breast Cancer Dataset for Bernoulli Naive Bayes:\")\n",
    "print(f\"Shape: {bc_df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(bc_df.head())\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(bc_df['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data and binarize features\n",
    "X_bc = bc_df.drop('target', axis=1).values\n",
    "y_bc = bc_df['target'].values\n",
    "\n",
    "# For Bernoulli NB, we need binary features\n",
    "# Let's binarize: 1 if above median, 0 if below\n",
    "X_bc_binary = (X_bc > np.median(X_bc, axis=0)).astype(int)\n",
    "\n",
    "# Split data\n",
    "X_train_bc, X_test_bc, y_train_bc, y_test_bc = train_test_split(\n",
    "    X_bc_binary, y_bc, test_size=0.3, random_state=42, stratify=y_bc\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train_bc.shape}\")\n",
    "print(f\"Test set: {X_test_bc.shape}\")\n",
    "print(f\"\\nFeatures binarized: {np.unique(X_train_bc)}\")\n",
    "print(\"(0 = below median, 1 = above median)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Bernoulli Naive Bayes\n",
    "bnb = BernoulliNB(alpha=1.0)\n",
    "\n",
    "# Fit the model\n",
    "bnb.fit(X_train_bc, y_train_bc)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_bc = bnb.predict(X_test_bc)\n",
    "y_prob_bc = bnb.predict_proba(X_test_bc)\n",
    "\n",
    "# Evaluate\n",
    "accuracy_bc = accuracy_score(y_test_bc, y_pred_bc)\n",
    "\n",
    "print(\"Bernoulli Naive Bayes Results:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Accuracy: {accuracy_bc:.3f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_bc, y_pred_bc))\n",
    "\n",
    "print(\"\\nüí° Note: Binarizing continuous features can lose information,\")\n",
    "print(\"   but Bernoulli NB is great for truly binary features!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comparing the Three Variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all three Naive Bayes variants on the same dataset (breast cancer)\n",
    "# Using original continuous features for fair comparison\n",
    "\n",
    "X_compare = bc_df.drop('target', axis=1).values\n",
    "y_compare = bc_df['target'].values\n",
    "\n",
    "X_train_cmp, X_test_cmp, y_train_cmp, y_test_cmp = train_test_split(\n",
    "    X_compare, y_compare, test_size=0.3, random_state=42, stratify=y_compare\n",
    ")\n",
    "\n",
    "# Prepare different feature versions\n",
    "X_train_binary = (X_train_cmp > np.median(X_train_cmp, axis=0)).astype(int)\n",
    "X_test_binary = (X_test_cmp > np.median(X_train_cmp, axis=0)).astype(int)\n",
    "\n",
    "X_train_pos = X_train_cmp - X_train_cmp.min() + 1\n",
    "X_test_pos = X_test_cmp - X_train_cmp.min() + 1\n",
    "\n",
    "# Train all three variants\n",
    "models = {\n",
    "    'Gaussian NB': (GaussianNB(), X_train_cmp, X_test_cmp),\n",
    "    'Multinomial NB': (MultinomialNB(), X_train_pos, X_test_pos),\n",
    "    'Bernoulli NB': (BernoulliNB(), X_train_binary, X_test_binary)\n",
    "}\n",
    "\n",
    "print(\"Comparing Naive Bayes Variants on Breast Cancer Data:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Model':<20} {'Accuracy':<15} {'CV Score (mean)':<20}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results = {}\n",
    "for name, (model, X_tr, X_te) in models.items():\n",
    "    # Train and evaluate\n",
    "    model.fit(X_tr, y_train_cmp)\n",
    "    accuracy = model.score(X_te, y_test_cmp)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(model, X_tr, y_train_cmp, cv=5)\n",
    "    cv_mean = cv_scores.mean()\n",
    "    \n",
    "    results[name] = {'accuracy': accuracy, 'cv_mean': cv_mean}\n",
    "    \n",
    "    print(f\"{name:<20} {accuracy:<15.3f} {cv_mean:<20.3f}\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nüìä Insights:\")\n",
    "print(\"- Gaussian NB: Best for continuous features (our case!)\")\n",
    "print(\"- Multinomial NB: Better for count data (word frequencies)\")\n",
    "print(\"- Bernoulli NB: Better for binary features (word presence/absence)\")\n",
    "print(\"\\n‚úÖ Always choose the variant that matches your data type!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Laplace Smoothing: Handling Zero Probabilities\n",
    "\n",
    "### The Zero Probability Problem\n",
    "\n",
    "**Problem**: What if a feature value never appears in training data for a class?\n",
    "- P(feature|class) = 0\n",
    "- Since we multiply probabilities: 0 √ó anything = 0\n",
    "- The entire probability becomes 0!\n",
    "- Model can't make sensible predictions\n",
    "\n",
    "**Example**:\n",
    "- Training spam: Never saw the word \"quantum\"\n",
    "- Test email contains \"quantum\"\n",
    "- P(\"quantum\" | spam) = 0\n",
    "- P(spam | email) = 0 (even if all other words suggest spam!)\n",
    "\n",
    "### Laplace Smoothing Solution\n",
    "\n",
    "**Add a small constant (Œ±) to all counts**:\n",
    "- Œ± = 0: No smoothing (can have zero probabilities)\n",
    "- Œ± = 1: Laplace smoothing (add-one smoothing)\n",
    "- Œ± < 1: Less smoothing\n",
    "- Œ± > 1: More smoothing\n",
    "\n",
    "**Effect**: \n",
    "- Prevents zero probabilities\n",
    "- Gives unseen features a small probability\n",
    "- Acts as regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the effect of smoothing parameter (alpha)\n",
    "alpha_values = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "smoothing_results = []\n",
    "\n",
    "print(\"Effect of Smoothing Parameter (alpha) on Multinomial NB:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'Alpha':<10} {'Train Accuracy':<20} {'Test Accuracy':<20}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    # Train with different alpha values\n",
    "    mnb_smooth = MultinomialNB(alpha=alpha)\n",
    "    mnb_smooth.fit(X_train_text, y_train_text)\n",
    "    \n",
    "    train_acc = mnb_smooth.score(X_train_text, y_train_text)\n",
    "    test_acc = mnb_smooth.score(X_test_text, y_test_text)\n",
    "    \n",
    "    smoothing_results.append({'alpha': alpha, 'train': train_acc, 'test': test_acc})\n",
    "    \n",
    "    print(f\"{alpha:<10} {train_acc:<20.3f} {test_acc:<20.3f}\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\nüìä Observations:\")\n",
    "print(\"- Small alpha (< 1): Less smoothing, may overfit\")\n",
    "print(\"- Alpha = 1: Standard Laplace smoothing (good default)\")\n",
    "print(\"- Large alpha (> 10): Heavy smoothing, may underfit\")\n",
    "print(\"\\n‚úÖ Alpha is a regularization parameter - tune it with cross-validation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the effect of alpha\n",
    "alphas = [r['alpha'] for r in smoothing_results]\n",
    "train_accs = [r['train'] for r in smoothing_results]\n",
    "test_accs = [r['test'] for r in smoothing_results]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.semilogx(alphas, train_accs, 'o-', label='Training Accuracy', linewidth=2, markersize=8)\n",
    "plt.semilogx(alphas, test_accs, 's-', label='Test Accuracy', linewidth=2, markersize=8)\n",
    "plt.xlabel('Alpha (Smoothing Parameter)', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('Effect of Laplace Smoothing on Model Performance', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The gap between train and test narrows with more smoothing (regularization effect).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. When Naive Bayes Excels\n",
    "\n",
    "### Perfect Use Cases\n",
    "\n",
    "**1. Text Classification** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê\n",
    "- Spam detection\n",
    "- Sentiment analysis\n",
    "- Topic categorization\n",
    "- Language detection\n",
    "- Why: High dimensions (thousands of words), independence assumption works reasonably\n",
    "\n",
    "**2. Real-Time Prediction**\n",
    "- Need instant predictions\n",
    "- Training and prediction are extremely fast\n",
    "- O(n√ód) training, O(d) prediction\n",
    "\n",
    "**3. Small Training Datasets**\n",
    "- Works well with limited data\n",
    "- Less prone to overfitting than complex models\n",
    "- Good baseline model\n",
    "\n",
    "**4. High-Dimensional Data**\n",
    "- Handles thousands of features well\n",
    "- Doesn't suffer from curse of dimensionality like KNN\n",
    "- No feature scaling needed\n",
    "\n",
    "**5. Multi-Class Problems**\n",
    "- Naturally handles multiple classes\n",
    "- No need for one-vs-rest strategy\n",
    "\n",
    "### When to Avoid Naive Bayes\n",
    "\n",
    "‚ùå **Features are highly correlated**\n",
    "   - Independence assumption is severely violated\n",
    "   - Consider logistic regression or decision trees\n",
    "\n",
    "‚ùå **Need precise probability estimates**\n",
    "   - Naive Bayes probabilities are often poorly calibrated\n",
    "   - Rankings are good, absolute values aren't\n",
    "\n",
    "‚ùå **Complex feature interactions**\n",
    "   - Can't capture feature combinations\n",
    "   - Use tree-based methods or neural networks\n",
    "\n",
    "‚ùå **Numerical data with non-linear patterns**\n",
    "   - Gaussian assumption may not fit\n",
    "   - Try SVM or random forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Now it's your turn to practice! Complete these exercises to reinforce your understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Manual Bayes' Theorem Calculation\n",
    "\n",
    "Given this information about email classification:\n",
    "- 30% of emails are spam: P(Spam) = 0.3\n",
    "- The word \"FREE\" appears in 80% of spam emails: P(\"FREE\"|Spam) = 0.8\n",
    "- The word \"FREE\" appears in 10% of legitimate emails: P(\"FREE\"|Legitimate) = 0.1\n",
    "\n",
    "**Tasks:**\n",
    "1. Calculate P(\"FREE\") - probability of seeing \"FREE\" overall\n",
    "2. Calculate P(Spam|\"FREE\") - probability email is spam given it contains \"FREE\"\n",
    "3. If an email contains \"FREE\", should you classify it as spam?\n",
    "4. Verify your answer by coding the calculation below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: Use Bayes' theorem: P(A|B) = P(B|A) * P(A) / P(B)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Comparing Naive Bayes Variants on Wine Dataset\n",
    "\n",
    "Apply all three Naive Bayes variants to the wine classification dataset.\n",
    "\n",
    "**Tasks:**\n",
    "1. Load `wine.csv` dataset\n",
    "2. Split into train/test (70/30)\n",
    "3. Train GaussianNB on original features\n",
    "4. Train MultinomialNB on non-negative features\n",
    "5. Train BernoulliNB on binarized features\n",
    "6. Compare accuracies and cross-validation scores\n",
    "7. Which variant works best for this dataset? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: Follow the comparison pattern from section 8\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Hyperparameter Tuning with GridSearchCV\n",
    "\n",
    "Find the optimal smoothing parameter (alpha) for MultinomialNB on synthetic data.\n",
    "\n",
    "**Tasks:**\n",
    "1. Use the synthetic classification dataset\n",
    "2. Create a parameter grid testing alpha values: [0.001, 0.01, 0.1, 1.0, 10.0]\n",
    "3. Use GridSearchCV with 5-fold cross-validation\n",
    "4. Report the best alpha and best score\n",
    "5. Visualize how performance changes with alpha\n",
    "6. Does the optimal alpha prevent overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: from sklearn.model_selection import GridSearchCV\n",
    "# param_grid = {'alpha': [...]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Speed Comparison\n",
    "\n",
    "Compare training and prediction speed of Naive Bayes vs other algorithms.\n",
    "\n",
    "**Tasks:**\n",
    "1. Load the breast cancer dataset\n",
    "2. Time the training of: GaussianNB, Logistic Regression, KNN (K=5), Decision Tree\n",
    "3. Time the prediction on test set for each model\n",
    "4. Compare accuracies\n",
    "5. Create a table showing: Model, Train Time, Predict Time, Accuracy\n",
    "6. Which model is fastest? Which is most accurate?\n",
    "7. When would you choose Naive Bayes despite lower accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: import time; start = time.time(); ... ; elapsed = time.time() - start\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts Learned\n",
    "\n",
    "1. **Bayes' Theorem**\n",
    "   - Foundation of probabilistic classification\n",
    "   - P(Class|Features) = P(Features|Class) √ó P(Class) / P(Features)\n",
    "   - Updates beliefs based on new evidence\n",
    "\n",
    "2. **The Naive Assumption**\n",
    "   - Features are conditionally independent given the class\n",
    "   - Simplifies computation enormously\n",
    "   - Works surprisingly well despite being \"naive\"\n",
    "\n",
    "3. **Three Naive Bayes Variants**\n",
    "   - **Gaussian NB**: Continuous features (normal distribution)\n",
    "   - **Multinomial NB**: Count/frequency data (text classification)\n",
    "   - **Bernoulli NB**: Binary features (presence/absence)\n",
    "\n",
    "4. **Laplace Smoothing**\n",
    "   - Prevents zero probability problem\n",
    "   - Alpha parameter controls smoothing strength\n",
    "   - Acts as regularization\n",
    "\n",
    "5. **Advantages**\n",
    "   - Extremely fast training and prediction\n",
    "   - Works well with high dimensions\n",
    "   - Requires little training data\n",
    "   - Provides probability estimates\n",
    "   - No feature scaling needed\n",
    "\n",
    "6. **Best Use Cases**\n",
    "   - Text classification (spam, sentiment, topics)\n",
    "   - Real-time prediction systems\n",
    "   - Baseline model for comparison\n",
    "   - Multi-class classification\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "- **Choose the right variant** for your data type\n",
    "- **Use alpha=1.0** as starting point (Laplace smoothing)\n",
    "- **Tune alpha with cross-validation** for optimal performance\n",
    "- **Consider feature independence** - if violated, try other algorithms\n",
    "- **Use for baseline** - always try NB as a quick first model\n",
    "- **Don't trust absolute probabilities** - rankings are reliable, values aren't\n",
    "\n",
    "### Common Pitfalls to Avoid\n",
    "\n",
    "- ‚ùå Using wrong variant for data type\n",
    "- ‚ùå Forgetting about zero probability problem\n",
    "- ‚ùå Trusting probability estimates for threshold decisions\n",
    "- ‚ùå Applying to data with strong feature correlations\n",
    "- ‚ùå Using Multinomial/Bernoulli NB with negative features\n",
    "\n",
    "### What's Next\n",
    "\n",
    "In **Module 12: Clustering (K-Means, DBSCAN)**, you'll learn:\n",
    "- Unsupervised learning for finding patterns\n",
    "- K-Means algorithm and choosing optimal K\n",
    "- Density-based clustering with DBSCAN\n",
    "- Cluster evaluation metrics\n",
    "- When to use different clustering algorithms\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "**Videos:**\n",
    "- [StatQuest: Naive Bayes](https://www.youtube.com/watch?v=O2L2Uv9pdDA)\n",
    "- [Bayes Theorem Explained](https://www.youtube.com/watch?v=HZGCoVF3YvM)\n",
    "\n",
    "**Documentation:**\n",
    "- [scikit-learn Naive Bayes Guide](https://scikit-learn.org/stable/modules/naive_bayes.html)\n",
    "- [GaussianNB API](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html)\n",
    "\n",
    "**Articles:**\n",
    "- [Naive Bayes for Machine Learning](https://machinelearningmastery.com/naive-bayes-for-machine-learning/)\n",
    "- [Why Naive Bayes Works So Well](https://www.cs.cmu.edu/~tom/mlbook/NBayesLogReg.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
