{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 05: Decision Trees\n",
    "\n",
    "**Difficulty**: ⭐ Beginner  \n",
    "**Estimated Time**: 60 minutes  \n",
    "**Prerequisites**: \n",
    "- [Module 00: Introduction to ML and scikit-learn](00_introduction_to_ml_and_sklearn.ipynb)\n",
    "- [Module 01: Supervised vs Unsupervised Learning](01_supervised_vs_unsupervised_learning.ipynb)\n",
    "- [Module 03: Linear Regression](03_linear_regression.ipynb)\n",
    "- [Module 04: Logistic Regression](04_logistic_regression.ipynb)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Understand how decision trees make predictions using if-then rules\n",
    "2. Build classification and regression trees\n",
    "3. Control tree complexity with max_depth and other hyperparameters\n",
    "4. Visualize decision trees to understand their logic\n",
    "5. Interpret feature importance from decision trees\n",
    "6. Recognize and prevent overfitting in trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What are Decision Trees?\n",
    "\n",
    "**Decision Trees** are a popular machine learning algorithm that makes predictions by learning simple decision rules from data.\n",
    "\n",
    "### The Big Idea\n",
    "Think of a flowchart or a game of \"20 Questions\":\n",
    "- Ask a series of yes/no questions\n",
    "- Each answer leads to another question\n",
    "- Eventually reach a decision\n",
    "\n",
    "### Real-World Example: Should I Play Tennis?\n",
    "```\n",
    "Is it sunny?\n",
    "├── Yes: Is humidity high?\n",
    "│   ├── Yes: Don't play (too hot)\n",
    "│   └── No: Play!\n",
    "└── No: Is it raining?\n",
    "    ├── Yes: Is it windy?\n",
    "    │   ├── Yes: Don't play\n",
    "    │   └── No: Play!\n",
    "    └── No: Play!\n",
    "```\n",
    "\n",
    "### Key Terminology\n",
    "- **Root Node**: Top of tree (first question)\n",
    "- **Internal Nodes**: Decision points (questions)\n",
    "- **Leaf Nodes**: Final predictions (answers)\n",
    "- **Branches**: Connections between nodes\n",
    "- **Depth**: Longest path from root to leaf\n",
    "\n",
    "### Advantages\n",
    "- Easy to understand and interpret\n",
    "- Visual representation\n",
    "- Handles both numerical and categorical data\n",
    "- No need for feature scaling\n",
    "- Captures non-linear relationships\n",
    "\n",
    "### Disadvantages\n",
    "- Can easily overfit (memorize training data)\n",
    "- Sensitive to small changes in data\n",
    "- Biased toward features with more values\n",
    "- Not always the most accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Visualization settings\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"✓ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classification Trees\n",
    "\n",
    "Let's build a decision tree to classify Iris species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Iris dataset\n",
    "iris_df = pd.read_csv('data/sample/iris.csv')\n",
    "\n",
    "# Prepare features and target\n",
    "feature_cols = ['sepal length (cm)', 'sepal width (cm)', \n",
    "                'petal length (cm)', 'petal width (cm)']\n",
    "X = iris_df[feature_cols]\n",
    "y = iris_df['species']\n",
    "\n",
    "print(\"Iris Dataset:\")\n",
    "print(f\"Samples: {len(X)}\")\n",
    "print(f\"Features: {X.shape[1]}\")\n",
    "print(f\"Classes: {y.nunique()}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(y.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Testing samples: {len(X_test)}\")\n",
    "print(\"\\nNote: Decision trees don't require feature scaling!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a simple decision tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create a tree with max_depth=3 (to prevent overfitting)\n",
    "tree_clf = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"✓ Decision Tree trained!\")\n",
    "print(f\"\\nTree depth: {tree_clf.get_depth()}\")\n",
    "print(f\"Number of leaves: {tree_clf.get_n_leaves()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "train_accuracy = tree_clf.score(X_train, y_train)\n",
    "test_accuracy = tree_clf.score(X_test, y_test)\n",
    "\n",
    "print(\"Model Performance:\")\n",
    "print(f\"Training Accuracy: {train_accuracy:.1%}\")\n",
    "print(f\"Testing Accuracy: {test_accuracy:.1%}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred = tree_clf.predict(X_test)\n",
    "\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualizing Decision Trees\n",
    "\n",
    "One of the best features of decision trees is that we can visualize them to understand exactly how they make decisions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the decision tree\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(tree_clf, \n",
    "         feature_names=feature_cols,\n",
    "         class_names=['Class 0', 'Class 1', 'Class 2'],\n",
    "         filled=True,\n",
    "         rounded=True,\n",
    "         fontsize=10)\n",
    "plt.title('Decision Tree Visualization\\nEach box shows: condition, samples, values, class', \n",
    "         fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"How to read the tree:\")\n",
    "print(\"- Top box: Root node (first decision)\")\n",
    "print(\"- Each box shows:\")\n",
    "print(\"  * Decision rule (e.g., petal length <= 2.45)\")\n",
    "print(\"  * samples: Number of samples at this node\")\n",
    "print(\"  * value: Distribution across classes\")\n",
    "print(\"  * class: Predicted class (majority)\")\n",
    "print(\"- Color intensity: Confidence (darker = more confident)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Importance\n",
    "\n",
    "Decision trees automatically calculate which features are most important for making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "importances = tree_clf.feature_importances_\n",
    "\n",
    "# Create DataFrame for better visualization\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Importance': importances\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importance:\")\n",
    "print(feature_importance_df.to_string(index=False))\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'], \n",
    "        color='steelblue', alpha=0.7)\n",
    "plt.xlabel('Importance Score', fontsize=12)\n",
    "plt.ylabel('Feature', fontsize=12)\n",
    "plt.title('Feature Importance in Decision Tree\\n(Higher = More Important)', \n",
    "         fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"- {feature_importance_df.iloc[0]['Feature']} is the most important feature\")\n",
    "print(f\"- Accounts for {feature_importance_df.iloc[0]['Importance']:.1%} of decisions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tree Depth and Overfitting\n",
    "\n",
    "**Key Concept**: Deeper trees can memorize training data (overfit) instead of learning general patterns.\n",
    "\n",
    "Let's compare trees of different depths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train trees with different depths\n",
    "depths = [1, 2, 3, 5, 10, None]  # None = no limit\n",
    "results = []\n",
    "\n",
    "for depth in depths:\n",
    "    tree = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
    "    tree.fit(X_train, y_train)\n",
    "    \n",
    "    train_acc = tree.score(X_train, y_train)\n",
    "    test_acc = tree.score(X_test, y_test)\n",
    "    \n",
    "    results.append({\n",
    "        'Max Depth': str(depth),\n",
    "        'Actual Depth': tree.get_depth(),\n",
    "        'Num Leaves': tree.get_n_leaves(),\n",
    "        'Train Accuracy': f\"{train_acc:.1%}\",\n",
    "        'Test Accuracy': f\"{test_acc:.1%}\"\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"Impact of Tree Depth on Performance:\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"- Shallow trees (depth 1-2): Underfit (low training accuracy)\")\n",
    "print(\"- Medium trees (depth 3-5): Good balance\")\n",
    "print(\"- Deep trees (depth 10+): Overfit (perfect training, lower test)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the effect of depth\n",
    "depths_numeric = [1, 2, 3, 5, 10, 20]\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "for depth in depths_numeric:\n",
    "    tree = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
    "    tree.fit(X_train, y_train)\n",
    "    train_accuracies.append(tree.score(X_train, y_train))\n",
    "    test_accuracies.append(tree.score(X_test, y_test))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(depths_numeric, train_accuracies, 'o-', linewidth=2, label='Training Accuracy', \n",
    "        markersize=8)\n",
    "plt.plot(depths_numeric, test_accuracies, 's-', linewidth=2, label='Testing Accuracy', \n",
    "        markersize=8)\n",
    "plt.xlabel('Maximum Tree Depth', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('Accuracy vs Tree Depth\\n(Gap indicates overfitting)', \n",
    "         fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The gap between training and testing accuracy shows overfitting!\")\n",
    "print(\"Deeper trees memorize training data instead of learning patterns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Regression Trees\n",
    "\n",
    "Decision trees can also be used for regression (predicting continuous values)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load California housing dataset\n",
    "housing_df = pd.read_csv('data/sample/california_housing.csv')\n",
    "\n",
    "# Prepare data\n",
    "X_housing = housing_df.drop('median_house_value', axis=1)\n",
    "y_housing = housing_df['median_house_value']\n",
    "\n",
    "# Split data\n",
    "X_train_h, X_test_h, y_train_h, y_test_h = train_test_split(\n",
    "    X_housing, y_housing, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(\"California Housing Dataset:\")\n",
    "print(f\"Training samples: {len(X_train_h)}\")\n",
    "print(f\"Testing samples: {len(X_test_h)}\")\n",
    "print(f\"Features: {X_housing.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a regression tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "tree_reg = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
    "tree_reg.fit(X_train_h, y_train_h)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_h = tree_reg.predict(X_test_h)\n",
    "\n",
    "# Evaluate\n",
    "rmse = np.sqrt(mean_squared_error(y_test_h, y_pred_h))\n",
    "r2 = r2_score(y_test_h, y_pred_h)\n",
    "\n",
    "print(\"Regression Tree Performance:\")\n",
    "print(f\"RMSE: ${rmse:,.2f}\")\n",
    "print(f\"R² Score: {r2:.3f}\")\n",
    "print(f\"\\nTree depth: {tree_reg.get_depth()}\")\n",
    "print(f\"Number of leaves: {tree_reg.get_n_leaves()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test_h, y_pred_h, alpha=0.3, s=20)\n",
    "plt.plot([y_test_h.min(), y_test_h.max()], [y_test_h.min(), y_test_h.max()], \n",
    "        'r--', linewidth=2, label='Perfect Prediction')\n",
    "plt.xlabel('Actual House Value ($)', fontsize=12)\n",
    "plt.ylabel('Predicted House Value ($)', fontsize=12)\n",
    "plt.title(f'Regression Tree Predictions\\nR² = {r2:.3f}', \n",
    "         fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for regression tree\n",
    "importance_df_reg = pd.DataFrame({\n",
    "    'Feature': X_housing.columns,\n",
    "    'Importance': tree_reg.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df_reg['Feature'], importance_df_reg['Importance'], \n",
    "        color='green', alpha=0.7)\n",
    "plt.xlabel('Importance Score', fontsize=12)\n",
    "plt.ylabel('Feature', fontsize=12)\n",
    "plt.title('Feature Importance for House Price Prediction', \n",
    "         fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Top 3 Most Important Features:\")\n",
    "print(importance_df_reg.head(3).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Hyperparameters for Controlling Trees\n",
    "\n",
    "Decision trees have several hyperparameters to control their complexity and prevent overfitting:\n",
    "\n",
    "### Important Hyperparameters\n",
    "\n",
    "1. **max_depth**: Maximum depth of the tree\n",
    "   - Lower = simpler tree, less overfitting\n",
    "   - Typical values: 3-10\n",
    "\n",
    "2. **min_samples_split**: Minimum samples required to split a node\n",
    "   - Higher = fewer splits, simpler tree\n",
    "   - Typical values: 2-20\n",
    "\n",
    "3. **min_samples_leaf**: Minimum samples in a leaf node\n",
    "   - Higher = smoother predictions\n",
    "   - Typical values: 1-10\n",
    "\n",
    "4. **max_features**: Number of features to consider when splitting\n",
    "   - Lower = more diversity, less overfitting\n",
    "   - Options: 'sqrt', 'log2', or integer\n",
    "\n",
    "5. **max_leaf_nodes**: Maximum number of leaf nodes\n",
    "   - Limits tree size directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different hyperparameter settings\n",
    "configs = [\n",
    "    {'max_depth': 3, 'name': 'Shallow (max_depth=3)'},\n",
    "    {'max_depth': None, 'min_samples_split': 20, 'name': 'Min Split=20'},\n",
    "    {'max_depth': None, 'min_samples_leaf': 10, 'name': 'Min Leaf=10'},\n",
    "    {'max_depth': None, 'max_leaf_nodes': 20, 'name': 'Max Leaves=20'},\n",
    "    {'max_depth': None, 'name': 'Unlimited (will overfit)'}\n",
    "]\n",
    "\n",
    "comparison_results = []\n",
    "\n",
    "for config in configs:\n",
    "    name = config.pop('name')\n",
    "    tree = DecisionTreeClassifier(random_state=42, **config)\n",
    "    tree.fit(X_train, y_train)\n",
    "    \n",
    "    comparison_results.append({\n",
    "        'Configuration': name,\n",
    "        'Depth': tree.get_depth(),\n",
    "        'Leaves': tree.get_n_leaves(),\n",
    "        'Train Acc': f\"{tree.score(X_train, y_train):.1%}\",\n",
    "        'Test Acc': f\"{tree.score(X_test, y_test):.1%}\"\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_results)\n",
    "print(\"Comparing Different Tree Configurations:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nBest Practice: Use constraints to prevent overfitting!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Practice building and analyzing decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Build a Decision Tree for Wine Classification\n",
    "\n",
    "Steps:\n",
    "1. Load the wine dataset from 'data/sample/wine.csv'\n",
    "2. Separate features and target\n",
    "3. Split data (70/30, stratified)\n",
    "4. Train a DecisionTreeClassifier with max_depth=4\n",
    "5. Calculate and print training and testing accuracy\n",
    "6. Visualize the feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Finding Optimal Tree Depth\n",
    "\n",
    "Using the breast cancer dataset:\n",
    "1. Load data from 'data/sample/breast_cancer.csv'\n",
    "2. Prepare features (drop 'target' and 'diagnosis') and target\n",
    "3. Split data (70/30, stratified)\n",
    "4. Train trees with max_depth from 1 to 15\n",
    "5. Plot training and testing accuracy vs depth\n",
    "6. Identify the optimal depth (best test accuracy without overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Regression Tree vs Linear Regression\n",
    "\n",
    "Compare a DecisionTreeRegressor with LinearRegression on the diabetes dataset:\n",
    "\n",
    "1. Load 'data/sample/diabetes.csv'\n",
    "2. Split data (70/30)\n",
    "3. Train both models:\n",
    "   - LinearRegression\n",
    "   - DecisionTreeRegressor (max_depth=5)\n",
    "4. Calculate RMSE and R² for both\n",
    "5. Which performs better? Why might this be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Visualizing a Simple Tree\n",
    "\n",
    "Create a very simple, interpretable decision tree:\n",
    "\n",
    "1. Use the Iris dataset (first two features only for simplicity)\n",
    "2. Train a DecisionTreeClassifier with max_depth=2\n",
    "3. Visualize the tree using plot_tree()\n",
    "4. Write out the decision rules in plain English\n",
    "5. Example: \"If petal length <= 2.45, predict Setosa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You've mastered decision trees, an intuitive and powerful ML algorithm.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Decision Trees**:\n",
    "   - Make predictions using series of if-then rules\n",
    "   - Like a flowchart or game of 20 questions\n",
    "   - Work for both classification and regression\n",
    "   - No need for feature scaling\n",
    "\n",
    "2. **Tree Structure**:\n",
    "   - **Root node**: First decision\n",
    "   - **Internal nodes**: Decision points (questions)\n",
    "   - **Leaf nodes**: Final predictions\n",
    "   - **Depth**: How many questions in longest path\n",
    "\n",
    "3. **Feature Importance**:\n",
    "   - Trees automatically calculate feature importance\n",
    "   - Shows which features are most useful for predictions\n",
    "   - Sum to 1.0 across all features\n",
    "   - Higher value = more important feature\n",
    "\n",
    "4. **Overfitting in Trees**:\n",
    "   - Deep trees memorize training data\n",
    "   - Perfect training accuracy but poor test accuracy\n",
    "   - Gap between train/test accuracy indicates overfitting\n",
    "   - Solution: Limit tree complexity\n",
    "\n",
    "5. **Hyperparameters**:\n",
    "   - **max_depth**: Limit tree depth (most important)\n",
    "   - **min_samples_split**: Minimum samples to split node\n",
    "   - **min_samples_leaf**: Minimum samples in leaf\n",
    "   - **max_leaf_nodes**: Limit total leaves\n",
    "   - All help prevent overfitting\n",
    "\n",
    "6. **Advantages**:\n",
    "   - Easy to understand and visualize\n",
    "   - Handles non-linear relationships\n",
    "   - No feature scaling needed\n",
    "   - Works with mixed data types\n",
    "   - Provides feature importance\n",
    "\n",
    "7. **Disadvantages**:\n",
    "   - Prone to overfitting\n",
    "   - Sensitive to small data changes\n",
    "   - Can create biased trees\n",
    "   - Often less accurate than ensemble methods\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Always limit tree depth** (start with 3-5)\n",
    "2. **Monitor train vs test accuracy** for overfitting\n",
    "3. **Visualize trees** to understand decisions\n",
    "4. **Check feature importance** for insights\n",
    "5. **Consider ensemble methods** (Random Forests, Gradient Boosting) for better accuracy\n",
    "\n",
    "### When to Use Decision Trees\n",
    "\n",
    "**Good for:**\n",
    "- Need interpretable models\n",
    "- Non-linear relationships\n",
    "- Mixed data types (numerical + categorical)\n",
    "- Quick baseline models\n",
    "- Feature importance analysis\n",
    "\n",
    "**Not good for:**\n",
    "- Need highest accuracy (use ensembles instead)\n",
    "- Linear relationships (use linear models)\n",
    "- Very small datasets (prone to overfitting)\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In **Module 06: Model Evaluation Metrics**, you'll learn:\n",
    "- Comprehensive classification metrics (precision, recall, F1)\n",
    "- When to use each metric\n",
    "- ROC curves and AUC\n",
    "- Regression metrics in depth\n",
    "- Confusion matrix analysis\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [Decision Trees - StatQuest](https://www.youtube.com/watch?v=7VeUPuFGJHk)\n",
    "- [scikit-learn Decision Trees](https://scikit-learn.org/stable/modules/tree.html)\n",
    "- [Visual Introduction to Decision Trees](https://www.r2d3.us/visual-intro-to-machine-learning-part-1/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
