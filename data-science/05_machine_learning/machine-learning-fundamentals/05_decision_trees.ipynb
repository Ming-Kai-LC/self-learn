{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 05: Decision Trees\n",
    "\n",
    "**Difficulty**: ⭐⭐ Intermediate  \n",
    "**Estimated Time**: 90 minutes  \n",
    "**Prerequisites**: \n",
    "- [Module 03: Linear Regression](03_linear_regression.ipynb)\n",
    "- [Module 04: Logistic Regression](04_logistic_regression.ipynb)\n",
    "- Understanding of information theory basics\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Understand how decision trees make splits (Gini impurity, entropy)\n",
    "2. Build and visualize decision trees for classification and regression\n",
    "3. Interpret tree structure and decision rules\n",
    "4. Identify and prevent overfitting through pruning parameters\n",
    "5. Analyze feature importance from decision trees\n",
    "6. Compare decision trees to linear models\n",
    "7. Choose appropriate hyperparameters for tree-based models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.tree import plot_tree, export_text\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix, classification_report,\n",
    "    mean_squared_error, r2_score\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What are Decision Trees?\n",
    "\n",
    "### Intuition\n",
    "\n",
    "A **decision tree** is a flowchart-like structure where:\n",
    "- **Internal nodes**: Questions about features (e.g., \"Is age > 30?\")\n",
    "- **Branches**: Answers to questions (yes/no)\n",
    "- **Leaf nodes**: Final predictions\n",
    "\n",
    "### Example: Should I play tennis?\n",
    "\n",
    "```\n",
    "Outlook sunny?\n",
    "├─ Yes → Humidity high?\n",
    "│         ├─ Yes → Don't play\n",
    "│         └─ No → Play\n",
    "└─ No → Wind strong?\n",
    "          ├─ Yes → Don't play\n",
    "          └─ No → Play\n",
    "```\n",
    "\n",
    "### Key Advantages\n",
    "\n",
    "✅ **Interpretable**: Easy to understand and visualize  \n",
    "✅ **Non-linear**: Can capture complex patterns  \n",
    "✅ **No scaling needed**: Works with raw features  \n",
    "✅ **Handles mixed data**: Both numerical and categorical  \n",
    "✅ **Feature importance**: Shows which features matter most  \n",
    "\n",
    "### Key Disadvantages\n",
    "\n",
    "❌ **Overfitting**: Can memorize training data  \n",
    "❌ **Instability**: Small data changes can change entire tree  \n",
    "❌ **Bias**: Greedy algorithm may not find global optimum  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. How Trees Split: Gini vs Entropy\n",
    "\n",
    "Decision trees split data to create **pure** nodes (all same class).\n",
    "\n",
    "### Gini Impurity\n",
    "\n",
    "**Formula**: Gini = 1 - Σ(p_i)²\n",
    "\n",
    "Where p_i is the proportion of class i\n",
    "\n",
    "- **Gini = 0**: Perfectly pure (all same class)\n",
    "- **Gini = 0.5**: Maximum impurity (50-50 split for binary)\n",
    "- **Default in scikit-learn**\n",
    "\n",
    "### Entropy (Information Gain)\n",
    "\n",
    "**Formula**: Entropy = -Σ(p_i × log₂(p_i))\n",
    "\n",
    "- **Entropy = 0**: Perfectly pure\n",
    "- **Entropy = 1**: Maximum impurity (for binary)\n",
    "- Based on information theory\n",
    "\n",
    "### Which to Use?\n",
    "\n",
    "- **Gini**: Faster to compute, works well in practice\n",
    "- **Entropy**: More theoretically grounded, slightly different trees\n",
    "- **In practice**: Results are usually similar!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate Gini vs Entropy\n",
    "def gini_impurity(p):\n",
    "    \"\"\"Calculate Gini impurity for probability p of class 1\"\"\"\n",
    "    return 1 - p**2 - (1-p)**2\n",
    "\n",
    "def entropy(p):\n",
    "    \"\"\"Calculate entropy for probability p of class 1\"\"\"\n",
    "    if p == 0 or p == 1:\n",
    "        return 0\n",
    "    return -(p * np.log2(p) + (1-p) * np.log2(1-p))\n",
    "\n",
    "# Calculate for different class distributions\n",
    "p_values = np.linspace(0.01, 0.99, 100)\n",
    "gini_values = [gini_impurity(p) for p in p_values]\n",
    "entropy_values = [entropy(p) for p in p_values]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(p_values, gini_values, label='Gini Impurity', linewidth=2)\n",
    "plt.plot(p_values, entropy_values, label='Entropy', linewidth=2, linestyle='--')\n",
    "plt.xlabel('Proportion of Class 1 (p)', fontsize=12)\n",
    "plt.ylabel('Impurity', fontsize=12)\n",
    "plt.title('Gini Impurity vs Entropy', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axvline(x=0.5, color='red', linestyle=':', alpha=0.5, label='Maximum impurity')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key Observations:\")\n",
    "print(\"  • Both reach maximum at p=0.5 (50-50 split)\")\n",
    "print(\"  • Both reach minimum at p=0 or p=1 (pure nodes)\")\n",
    "print(\"  • Entropy slightly higher, but shapes are similar\")\n",
    "print(\"\\nExamples:\")\n",
    "print(f\"  Pure node (100% class 1): Gini={gini_impurity(1):.3f}, Entropy={entropy(1):.3f}\")\n",
    "print(f\"  50-50 split: Gini={gini_impurity(0.5):.3f}, Entropy={entropy(0.5):.3f}\")\n",
    "print(f\"  75-25 split: Gini={gini_impurity(0.75):.3f}, Entropy={entropy(0.75):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Classification Tree Example\n",
    "\n",
    "Let's build a decision tree classifier on the Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Iris Dataset:\")\n",
    "print(f\"Features: {iris.feature_names}\")\n",
    "print(f\"Classes: {iris.target_names}\")\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Test samples: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a simple decision tree (max_depth=3 for visualization)\n",
    "tree_clf = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = tree_clf.predict(X_train)\n",
    "y_test_pred = tree_clf.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"Decision Tree Results:\")\n",
    "print(f\"Training Accuracy: {train_acc:.2%}\")\n",
    "print(f\"Test Accuracy: {test_acc:.2%}\")\n",
    "print(f\"\\nTree Depth: {tree_clf.get_depth()}\")\n",
    "print(f\"Number of Leaves: {tree_clf.get_n_leaves()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the decision tree\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(\n",
    "    tree_clf,\n",
    "    feature_names=iris.feature_names,\n",
    "    class_names=iris.target_names,\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    fontsize=10\n",
    ")\n",
    "plt.title('Decision Tree Visualization (Iris Dataset)', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nHow to Read the Tree:\")\n",
    "print(\"  • Top box (root): First split decision\")\n",
    "print(\"  • Each box shows:\")\n",
    "print(\"    - Split condition (e.g., 'petal width <= 0.8')\")\n",
    "print(\"    - Gini impurity\")\n",
    "print(\"    - Number of samples\")\n",
    "print(\"    - Class distribution\")\n",
    "print(\"  • Color indicates majority class\")\n",
    "print(\"  • Leaf nodes: Final predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text representation of the tree\n",
    "tree_rules = export_text(tree_clf, feature_names=list(iris.feature_names))\n",
    "print(\"Decision Tree Rules (Text Format):\")\n",
    "print(tree_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Importance\n",
    "\n",
    "Decision trees automatically calculate **feature importance** based on how much each feature reduces impurity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "importances = tree_clf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Create DataFrame\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': [iris.feature_names[i] for i in indices],\n",
    "    'Importance': importances[indices]\n",
    "})\n",
    "\n",
    "print(\"Feature Importances:\")\n",
    "print(importance_df.to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df['Feature'], importance_df['Importance'], edgecolor='k')\n",
    "plt.xlabel('Importance', fontsize=12)\n",
    "plt.title('Feature Importance in Decision Tree', fontsize=14)\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nMost important feature: {importance_df.iloc[0]['Feature']}\")\n",
    "print(f\"Importance sum: {importances.sum()} (should be 1.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Overfitting and Pruning\n",
    "\n",
    "**Problem**: Deep trees can overfit by memorizing training data.\n",
    "\n",
    "**Solution**: Control tree complexity with hyperparameters.\n",
    "\n",
    "### Key Hyperparameters\n",
    "\n",
    "1. **max_depth**: Maximum tree depth (most important!)\n",
    "2. **min_samples_split**: Minimum samples required to split a node\n",
    "3. **min_samples_leaf**: Minimum samples required in a leaf\n",
    "4. **max_leaf_nodes**: Maximum number of leaf nodes\n",
    "5. **min_impurity_decrease**: Minimum impurity decrease to split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare trees with different max_depth values\n",
    "depths = [1, 2, 3, 5, 10, None]  # None = no limit\n",
    "results = []\n",
    "\n",
    "for depth in depths:\n",
    "    tree = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
    "    tree.fit(X_train, y_train)\n",
    "    \n",
    "    train_acc = accuracy_score(y_train, tree.predict(X_train))\n",
    "    test_acc = accuracy_score(y_test, tree.predict(X_test))\n",
    "    \n",
    "    results.append({\n",
    "        'max_depth': depth if depth else 'unlimited',\n",
    "        'actual_depth': tree.get_depth(),\n",
    "        'n_leaves': tree.get_n_leaves(),\n",
    "        'train_acc': train_acc,\n",
    "        'test_acc': test_acc\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"Effect of max_depth on Model Performance:\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"  • Deeper trees → Higher training accuracy\")\n",
    "print(\"  • Too deep → Overfitting (gap between train and test)\")\n",
    "print(\"  • Optimal depth balances complexity and generalization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize overfitting\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Accuracy vs Depth\n",
    "depth_values = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for depth in depth_values:\n",
    "    tree = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
    "    tree.fit(X_train, y_train)\n",
    "    train_scores.append(accuracy_score(y_train, tree.predict(X_train)))\n",
    "    test_scores.append(accuracy_score(y_test, tree.predict(X_test)))\n",
    "\n",
    "axes[0].plot(depth_values, train_scores, 'o-', label='Training', linewidth=2)\n",
    "axes[0].plot(depth_values, test_scores, 's-', label='Test', linewidth=2)\n",
    "axes[0].set_xlabel('Max Depth', fontsize=12)\n",
    "axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[0].set_title('Accuracy vs Tree Depth', fontsize=14)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Complexity (number of nodes)\n",
    "n_nodes = [DecisionTreeClassifier(max_depth=d, random_state=42).fit(X_train, y_train).tree_.node_count \n",
    "          for d in depth_values]\n",
    "axes[1].plot(depth_values, n_nodes, 'o-', color='green', linewidth=2)\n",
    "axes[1].set_xlabel('Max Depth', fontsize=12)\n",
    "axes[1].set_ylabel('Number of Nodes', fontsize=12)\n",
    "axes[1].set_title('Tree Complexity vs Depth', fontsize=14)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice: Training accuracy keeps increasing, but test accuracy plateaus or drops!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Regression Trees\n",
    "\n",
    "Decision trees can also predict continuous values!\n",
    "\n",
    "**Difference**: Instead of class labels, leaf nodes contain the **mean** of target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load California housing dataset\n",
    "housing = datasets.fetch_california_housing()\n",
    "X_housing = housing.data[:1000]  # Use subset\n",
    "y_housing = housing.target[:1000]\n",
    "\n",
    "# Split\n",
    "X_train_h, X_test_h, y_train_h, y_test_h = train_test_split(\n",
    "    X_housing, y_housing, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Housing Dataset (Regression):\")\n",
    "print(f\"Features: {housing.feature_names}\")\n",
    "print(f\"Target: Median house value (in $100,000s)\")\n",
    "print(f\"Training samples: {X_train_h.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train regression tree\n",
    "tree_reg = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
    "tree_reg.fit(X_train_h, y_train_h)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_h = tree_reg.predict(X_train_h)\n",
    "y_test_pred_h = tree_reg.predict(X_test_h)\n",
    "\n",
    "# Evaluate\n",
    "train_r2 = r2_score(y_train_h, y_train_pred_h)\n",
    "test_r2 = r2_score(y_test_h, y_test_pred_h)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test_h, y_test_pred_h))\n",
    "\n",
    "print(\"Regression Tree Results:\")\n",
    "print(f\"Training R²: {train_r2:.3f}\")\n",
    "print(f\"Test R²: {test_r2:.3f}\")\n",
    "print(f\"Test RMSE: ${test_rmse*100000:.2f}\")\n",
    "print(f\"\\nTree depth: {tree_reg.get_depth()}\")\n",
    "print(f\"Number of leaves: {tree_reg.get_n_leaves()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test_h, y_test_pred_h, alpha=0.6, edgecolors='k')\n",
    "plt.plot([y_test_h.min(), y_test_h.max()], \n",
    "        [y_test_h.min(), y_test_h.max()], \n",
    "        'r--', lw=2, label='Perfect Prediction')\n",
    "plt.xlabel('Actual Price ($100,000s)', fontsize=12)\n",
    "plt.ylabel('Predicted Price ($100,000s)', fontsize=12)\n",
    "plt.title('Regression Tree: Actual vs Predicted', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice: Predictions are 'stepped' - trees make constant predictions in regions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for regression tree\n",
    "importances_reg = tree_reg.feature_importances_\n",
    "indices_reg = np.argsort(importances_reg)[::-1]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(\n",
    "    [housing.feature_names[i] for i in indices_reg],\n",
    "    importances_reg[indices_reg],\n",
    "    edgecolor='k'\n",
    ")\n",
    "plt.xlabel('Importance', fontsize=12)\n",
    "plt.title('Feature Importance (Regression Tree)', fontsize=14)\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Most important feature: {housing.feature_names[indices_reg[0]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Decision Trees vs Linear Models\n",
    "\n",
    "Let's compare decision trees to logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset with non-linear decision boundary\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X_moons, y_moons = make_moons(n_samples=300, noise=0.2, random_state=42)\n",
    "\n",
    "# Split\n",
    "X_train_m, X_test_m, y_train_m, y_test_m = train_test_split(\n",
    "    X_moons, y_moons, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train both models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Logistic Regression (linear boundary)\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train_m, y_train_m)\n",
    "log_acc = accuracy_score(y_test_m, log_reg.predict(X_test_m))\n",
    "\n",
    "# Decision Tree (non-linear boundary)\n",
    "tree_moon = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "tree_moon.fit(X_train_m, y_train_m)\n",
    "tree_acc = accuracy_score(y_test_m, tree_moon.predict(X_test_m))\n",
    "\n",
    "print(\"Non-Linear Dataset Comparison:\")\n",
    "print(f\"Logistic Regression Accuracy: {log_acc:.2%}\")\n",
    "print(f\"Decision Tree Accuracy: {tree_acc:.2%}\")\n",
    "print(f\"\\nWinner: {'Decision Tree' if tree_acc > log_acc else 'Logistic Regression'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize decision boundaries\n",
    "def plot_decision_boundary(model, X, y, title):\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                        np.linspace(y_min, y_max, 200))\n",
    "    \n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu')\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='RdYlBu', \n",
    "               edgecolors='k', s=50, alpha=0.7)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_decision_boundary(log_reg, X_test_m, y_test_m, \n",
    "                      f'Logistic Regression (Acc: {log_acc:.2%})')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_decision_boundary(tree_moon, X_test_m, y_test_m, \n",
    "                      f'Decision Tree (Acc: {tree_acc:.2%})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Insight:\")\n",
    "print(\"  • Logistic Regression: Linear boundary (straight line)\")\n",
    "print(\"  • Decision Tree: Non-linear boundary (rectangular regions)\")\n",
    "print(\"  • Trees better for complex, non-linear patterns!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Practice Exercises\n",
    "\n",
    "### Exercise 1: Optimal Tree Depth\n",
    "\n",
    "Using the wine dataset (`datasets.load_wine()`):\n",
    "1. Train decision trees with max_depth from 1 to 15\n",
    "2. Plot training and test accuracy\n",
    "3. Find the optimal depth that maximizes test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Pruning Parameters\n",
    "\n",
    "Train trees on breast cancer dataset with different pruning parameters:\n",
    "1. min_samples_split = [2, 10, 50]\n",
    "2. min_samples_leaf = [1, 5, 20]\n",
    "3. Which combination gives best test performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Feature Importance Analysis\n",
    "\n",
    "Using the diabetes dataset for regression:\n",
    "1. Train a regression tree\n",
    "2. Identify the top 3 most important features\n",
    "3. Train a new tree using only those 3 features\n",
    "4. How does performance compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Interpretability Challenge\n",
    "\n",
    "Train a shallow tree (max_depth=3) on Iris dataset:\n",
    "1. Export the tree rules as text\n",
    "2. Manually trace a prediction for a sample\n",
    "3. Verify your manual prediction matches model.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "### Key Concepts Learned\n",
    "\n",
    "1. **Decision Tree Structure**:\n",
    "   - Internal nodes: Feature-based questions\n",
    "   - Branches: Decision paths\n",
    "   - Leaf nodes: Final predictions\n",
    "   - Highly interpretable flowchart structure\n",
    "\n",
    "2. **Splitting Criteria**:\n",
    "   - **Gini Impurity**: 1 - Σ(p_i)² (faster, default)\n",
    "   - **Entropy**: -Σ(p_i × log(p_i)) (information theory)\n",
    "   - Goal: Create pure nodes (low impurity)\n",
    "\n",
    "3. **Classification vs Regression Trees**:\n",
    "   - Classification: Predict class labels (majority vote in leaf)\n",
    "   - Regression: Predict continuous values (mean in leaf)\n",
    "   - Same algorithm, different output types\n",
    "\n",
    "4. **Feature Importance**:\n",
    "   - Automatically calculated during training\n",
    "   - Based on impurity reduction\n",
    "   - Sum equals 1.0\n",
    "   - Helps identify key features\n",
    "\n",
    "5. **Overfitting Prevention (Pruning)**:\n",
    "   - **max_depth**: Limit tree depth (most important!)\n",
    "   - **min_samples_split**: Min samples to split node\n",
    "   - **min_samples_leaf**: Min samples in leaf\n",
    "   - **max_leaf_nodes**: Limit number of leaves\n",
    "\n",
    "6. **Advantages vs Disadvantages**:\n",
    "   - ✅ Interpretable, handles non-linear patterns, no scaling needed\n",
    "   - ❌ Prone to overfitting, unstable, greedy learning\n",
    "\n",
    "### When to Use Decision Trees\n",
    "\n",
    "✅ **Good for**:\n",
    "- Interpretability is important\n",
    "- Non-linear relationships\n",
    "- Mixed data types (numerical + categorical)\n",
    "- Feature importance analysis\n",
    "- Quick baseline model\n",
    "\n",
    "❌ **Not ideal for**:\n",
    "- High-dimensional data (many features)\n",
    "- Linear relationships (use linear models)\n",
    "- Need for stability (small data changes affect tree)\n",
    "- Production without ensembles (use Random Forest instead)\n",
    "\n",
    "### Comparison: Trees vs Linear Models\n",
    "\n",
    "| Aspect | Decision Trees | Linear Models |\n",
    "|--------|---------------|---------------|\n",
    "| **Boundary** | Non-linear (rectangular) | Linear (straight) |\n",
    "| **Interpretability** | Very high (rules) | High (coefficients) |\n",
    "| **Scaling** | Not needed | Required |\n",
    "| **Overfitting** | High risk | Lower risk |\n",
    "| **Stability** | Low (sensitive to data) | High |\n",
    "\n",
    "### Quick Reference: Scikit-learn\n",
    "\n",
    "```python\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.tree import plot_tree, export_text\n",
    "\n",
    "# Classification\n",
    "tree_clf = DecisionTreeClassifier(\n",
    "    max_depth=5,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    criterion='gini',  # or 'entropy'\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "tree_clf.fit(X_train, y_train)\n",
    "predictions = tree_clf.predict(X_test)\n",
    "\n",
    "# Feature importance\n",
    "importances = tree_clf.feature_importances_\n",
    "\n",
    "# Visualize\n",
    "plot_tree(tree_clf, feature_names=feature_names, filled=True)\n",
    "\n",
    "# Text rules\n",
    "rules = export_text(tree_clf, feature_names=feature_names)\n",
    "```\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the next module, we'll explore:\n",
    "- **Comprehensive evaluation metrics** for classification and regression\n",
    "- Precision, recall, F1-score deep dive\n",
    "- ROC curves and AUC\n",
    "- Choosing the right metric for your problem\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [Scikit-learn Decision Trees](https://scikit-learn.org/stable/modules/tree.html)\n",
    "- [StatQuest: Decision Trees](https://www.youtube.com/watch?v=7VeUPuFGJHk)\n",
    "- [Visualizing Decision Trees](https://explained.ai/decision-tree-viz/)\n",
    "- [Information Gain and Entropy](https://towardsdatascience.com/entropy-and-information-gain-in-decision-trees-c7db67a3a293)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
