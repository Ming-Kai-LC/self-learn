{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 14: Final Project - End-to-End ML Pipeline\n",
    "\n",
    "**Difficulty**: ‚≠ê‚≠ê‚≠ê Advanced  \n",
    "**Estimated Time**: 120 minutes  \n",
    "**Prerequisites**: All previous modules (00-13)\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Define a clear machine learning problem with success criteria\n",
    "2. Perform comprehensive exploratory data analysis (EDA)\n",
    "3. Engineer and select features for better model performance\n",
    "4. Properly split data into train/validation/test sets\n",
    "5. Establish a baseline model for comparison\n",
    "6. Compare multiple ML algorithms systematically\n",
    "7. Apply cross-validation for robust evaluation\n",
    "8. Tune hyperparameters using GridSearchCV\n",
    "9. Evaluate final model on held-out test set\n",
    "10. Interpret model predictions and feature importance\n",
    "11. Understand deployment considerations and best practices\n",
    "12. Build production-ready ML pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Overview: Breast Cancer Diagnosis Prediction\n",
    "\n",
    "### The Problem\n",
    "\n",
    "**Medical Context**:\n",
    "- Breast cancer is one of the most common cancers\n",
    "- Early detection is critical for successful treatment\n",
    "- Biopsies are analyzed to determine if tumors are benign or malignant\n",
    "- Manual diagnosis can be time-consuming and subjective\n",
    "\n",
    "**Our Goal**:\n",
    "Build a machine learning model to predict whether a breast tumor is:\n",
    "- **Malignant (0)**: Cancerous, requires immediate treatment\n",
    "- **Benign (1)**: Non-cancerous, less urgent\n",
    "\n",
    "### Success Criteria\n",
    "\n",
    "**Primary Metric**: **Recall for Malignant class** (minimize false negatives)\n",
    "- Missing a malignant tumor (false negative) is very costly!\n",
    "- False positives are less critical (better safe than sorry)\n",
    "- Target: Recall ‚â• 95% for malignant tumors\n",
    "\n",
    "**Secondary Metrics**:\n",
    "- Overall accuracy ‚â• 95%\n",
    "- Precision for malignant class ‚â• 90%\n",
    "- F1-score ‚â• 93%\n",
    "\n",
    "### Dataset\n",
    "\n",
    "**Features**: 30 numerical measurements from cell nuclei images:\n",
    "- Radius, texture, perimeter, area, smoothness\n",
    "- Compactness, concavity, symmetry, fractal dimension\n",
    "- Mean, standard error, and \"worst\" (largest) values\n",
    "\n",
    "**Target**: Binary classification (0=Malignant, 1=Benign)\n",
    "\n",
    "**Samples**: ~570 patient records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Setup and Initial Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from time import time\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Models\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_curve, auc, roc_auc_score\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "%matplotlib inline\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('Set2')\n",
    "\n",
    "print('‚úì All libraries imported successfully!')\n",
    "print(f'‚úì Random seed set to 42 for reproducibility')\n",
    "print('\\n' + '='*70)\n",
    "print('MACHINE LEARNING PIPELINE: BREAST CANCER DIAGNOSIS')\n",
    "print('='*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('data/sample/breast_cancer.csv')\n",
    "\n",
    "print(\"\\nüìä DATASET OVERVIEW\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Number of samples: {len(df)}\")\n",
    "print(f\"Number of features: {df.shape[1] - 1} (excluding target)\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(f\"\\nData types:\")\n",
    "print(df.dtypes.value_counts())\n",
    "\n",
    "print(f\"\\nMissing values:\")\n",
    "missing = df.isnull().sum().sum()\n",
    "print(f\"Total missing values: {missing}\")\n",
    "if missing == 0:\n",
    "    print(\"‚úì No missing values - excellent!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Exploratory Data Analysis (EDA)\n",
    "\n",
    "**Goals of EDA:**\n",
    "1. Understand the distribution of target variable (class balance)\n",
    "2. Examine feature distributions and identify outliers\n",
    "3. Detect correlations between features\n",
    "4. Identify patterns that differentiate classes\n",
    "5. Spot potential data quality issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine target variable distribution\n",
    "print(\"\\nüéØ TARGET VARIABLE ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "target_counts = df['target'].value_counts()\n",
    "target_pct = df['target'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"Class distribution:\")\n",
    "print(f\"  Benign (1):    {target_counts[1]} samples ({target_pct[1]:.1f}%)\")\n",
    "print(f\"  Malignant (0): {target_counts[0]} samples ({target_pct[0]:.1f}%)\")\n",
    "\n",
    "# Check for class imbalance\n",
    "imbalance_ratio = target_counts.max() / target_counts.min()\n",
    "print(f\"\\nImbalance ratio: {imbalance_ratio:.2f}:1\")\n",
    "\n",
    "if imbalance_ratio < 1.5:\n",
    "    print(\"‚úì Classes are well-balanced (< 1.5:1 ratio)\")\n",
    "elif imbalance_ratio < 3:\n",
    "    print(\"‚ö†Ô∏è  Slight imbalance (1.5-3:1 ratio) - monitor performance\")\n",
    "else:\n",
    "    print(\"‚ùå Significant imbalance (> 3:1 ratio) - consider resampling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize target distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Count plot\n",
    "axes[0].bar(['Malignant (0)', 'Benign (1)'], target_counts.values, color=['coral', 'skyblue'], alpha=0.7)\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "axes[0].set_title('Class Distribution (Counts)', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add count labels\n",
    "for i, v in enumerate(target_counts.values):\n",
    "    axes[0].text(i, v + 5, str(v), ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Pie chart\n",
    "colors = ['coral', 'skyblue']\n",
    "axes[1].pie(target_counts.values, labels=['Malignant (0)', 'Benign (1)'], \n",
    "           autopct='%1.1f%%', startangle=90, colors=colors, textprops={'fontsize': 11})\n",
    "axes[1].set_title('Class Distribution (Proportions)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Insight: Dataset is reasonably balanced - good for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "print(\"\\nüìà STATISTICAL SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "X_features = df.drop('target', axis=1)\n",
    "summary = X_features.describe()\n",
    "\n",
    "print(\"\\nFeature statistics (first 5 features):\")\n",
    "print(summary.iloc[:, :5].round(2))\n",
    "\n",
    "# Check for outliers using IQR method\n",
    "Q1 = X_features.quantile(0.25)\n",
    "Q3 = X_features.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "outliers = ((X_features < (Q1 - 1.5 * IQR)) | (X_features > (Q3 + 1.5 * IQR))).sum()\n",
    "total_outliers = outliers.sum()\n",
    "\n",
    "print(f\"\\nOutlier detection (IQR method):\")\n",
    "print(f\"Total potential outliers: {total_outliers}\")\n",
    "print(f\"Percentage of data: {(total_outliers / (len(df) * len(X_features.columns)) * 100):.2f}%\")\n",
    "print(\"\\nüí° Some outliers expected in medical data - will handle with robust scaling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature correlation analysis\n",
    "print(\"\\nüîó FEATURE CORRELATION ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = X_features.corr()\n",
    "\n",
    "# Find highly correlated features (> 0.9)\n",
    "high_corr_pairs = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.9:\n",
    "            high_corr_pairs.append((\n",
    "                correlation_matrix.columns[i],\n",
    "                correlation_matrix.columns[j],\n",
    "                correlation_matrix.iloc[i, j]\n",
    "            ))\n",
    "\n",
    "print(f\"Highly correlated feature pairs (|r| > 0.9): {len(high_corr_pairs)}\")\n",
    "if high_corr_pairs:\n",
    "    print(\"\\nTop 5 correlations:\")\n",
    "    for feat1, feat2, corr in sorted(high_corr_pairs, key=lambda x: abs(x[2]), reverse=True)[:5]:\n",
    "        print(f\"  {feat1[:20]:<20} ‚Üî {feat2[:20]:<20} : {corr:.3f}\")\n",
    "\n",
    "print(\"\\nüí° High correlations suggest redundancy - PCA could be beneficial!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize correlation heatmap\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(correlation_matrix, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8},\n",
    "            xticklabels=False, yticklabels=False)\n",
    "plt.title('Feature Correlation Matrix (30 Features)', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Red = positive correlation, Blue = negative correlation\")\n",
    "print(\"   Many features are highly correlated (redundant information)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3: Feature Engineering and Data Preparation\n",
    "\n",
    "**Steps:**\n",
    "1. Separate features and target\n",
    "2. Create train/validation/test splits (60/20/20)\n",
    "3. Standardize features (critical for most algorithms)\n",
    "4. Create PCA-transformed versions (for comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n‚öôÔ∏è  FEATURE ENGINEERING & DATA SPLITTING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('target', axis=1).values\n",
    "y = df['target'].values\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "\n",
    "# Split into train+validation (80%) and test (20%)\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Split train+validation into train (75% of 80% = 60%) and validation (25% of 80% = 20%)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_trainval, y_trainval, test_size=0.25, random_state=42, stratify=y_trainval\n",
    ")\n",
    "\n",
    "print(\"\\nüìä Data Split:\")\n",
    "print(f\"  Training set:   {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"  Validation set: {X_val.shape[0]} samples ({X_val.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"  Test set:       {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "\n",
    "# Verify stratification\n",
    "print(\"\\nClass distribution (should be similar):\")\n",
    "print(f\"  Train:      {np.bincount(y_train)[1]/len(y_train)*100:.1f}% benign\")\n",
    "print(f\"  Validation: {np.bincount(y_val)[1]/len(y_val)*100:.1f}% benign\")\n",
    "print(f\"  Test:       {np.bincount(y_test)[1]/len(y_test)*100:.1f}% benign\")\n",
    "print(\"\\n‚úì Stratification preserved class balance across splits!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "print(\"\\nüîß FEATURE SCALING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training data only (prevent data leakage!)\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Before scaling:\")\n",
    "print(f\"  Mean: {X_train.mean():.2f}, Std: {X_train.std():.2f}\")\n",
    "print(f\"  Min: {X_train.min():.2f}, Max: {X_train.max():.2f}\")\n",
    "\n",
    "print(\"\\nAfter scaling:\")\n",
    "print(f\"  Mean: {X_train_scaled.mean():.10f}, Std: {X_train_scaled.std():.10f}\")\n",
    "print(f\"  Min: {X_train_scaled.min():.2f}, Max: {X_train_scaled.max():.2f}\")\n",
    "\n",
    "print(\"\\n‚úì Features standardized (mean=0, std=1)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Create PCA-transformed versions\n",
    "print(\"\\nüîç DIMENSIONALITY REDUCTION (PCA)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Apply PCA to capture 95% variance\n",
    "pca = PCA(n_components=0.95, random_state=42)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_val_pca = pca.transform(X_val_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "print(f\"Original dimensions: {X_train_scaled.shape[1]}\")\n",
    "print(f\"PCA dimensions: {X_train_pca.shape[1]}\")\n",
    "print(f\"Variance explained: {pca.explained_variance_ratio_.sum():.1%}\")\n",
    "print(f\"\\nDimensionality reduction: {X_train_scaled.shape[1]} ‚Üí {X_train_pca.shape[1]} features\")\n",
    "print(f\"Reduction: {(1 - X_train_pca.shape[1]/X_train_scaled.shape[1])*100:.1f}%\")\n",
    "\n",
    "print(\"\\n‚úì PCA versions created (we'll compare performance later)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 4: Baseline Model\n",
    "\n",
    "**Why baseline?**\n",
    "- Establishes minimum acceptable performance\n",
    "- Provides reference for model improvements\n",
    "- Helps detect bugs (if model is worse than baseline, something's wrong!)\n",
    "\n",
    "**Simple baselines:**\n",
    "1. **Most frequent class**: Always predict majority class\n",
    "2. **Random guess**: Random predictions based on class distribution\n",
    "3. **Logistic Regression**: Simple linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìè BASELINE MODELS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Baseline 1: Most frequent class\n",
    "dummy_freq = DummyClassifier(strategy='most_frequent', random_state=42)\n",
    "dummy_freq.fit(X_train_scaled, y_train)\n",
    "y_pred_freq = dummy_freq.predict(X_val_scaled)\n",
    "acc_freq = accuracy_score(y_val, y_pred_freq)\n",
    "recall_mal_freq = recall_score(y_val, y_pred_freq, pos_label=0)  # Recall for malignant\n",
    "\n",
    "print(\"\\n1. Most Frequent Class Baseline:\")\n",
    "print(f\"   Accuracy: {acc_freq:.3f}\")\n",
    "print(f\"   Recall (Malignant): {recall_mal_freq:.3f}\")\n",
    "print(f\"   ‚ùå Always predicts {dummy_freq.classes_[0]} - useless for malignant detection!\")\n",
    "\n",
    "# Baseline 2: Logistic Regression (simple linear model)\n",
    "lr_baseline = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr_baseline.fit(X_train_scaled, y_train)\n",
    "y_pred_lr = lr_baseline.predict(X_val_scaled)\n",
    "acc_lr = accuracy_score(y_val, y_pred_lr)\n",
    "recall_mal_lr = recall_score(y_val, y_pred_lr, pos_label=0)\n",
    "f1_lr = f1_score(y_val, y_pred_lr, pos_label=0)\n",
    "\n",
    "print(\"\\n2. Logistic Regression Baseline:\")\n",
    "print(f\"   Accuracy: {acc_lr:.3f}\")\n",
    "print(f\"   Recall (Malignant): {recall_mal_lr:.3f}\")\n",
    "print(f\"   F1-score (Malignant): {f1_lr:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"\\nüéØ BASELINE TO BEAT:\")\n",
    "print(f\"   Accuracy: {acc_lr:.3f}\")\n",
    "print(f\"   Recall (Malignant): {recall_mal_lr:.3f}\")\n",
    "print(f\"   Target: Recall ‚â• 0.95 for malignant tumors\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 5: Model Selection - Compare Multiple Algorithms\n",
    "\n",
    "**Models to compare:**\n",
    "1. Logistic Regression (linear, interpretable)\n",
    "2. K-Nearest Neighbors (instance-based)\n",
    "3. Decision Tree (non-linear, interpretable)\n",
    "4. Support Vector Machine (kernel methods)\n",
    "5. Random Forest (ensemble)\n",
    "6. Naive Bayes (probabilistic)\n",
    "\n",
    "**Evaluation strategy:**\n",
    "- Train on training set\n",
    "- Evaluate on validation set\n",
    "- Use 5-fold cross-validation for robust estimates\n",
    "- Compare: Accuracy, Recall (malignant), F1-score, Training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nü§ñ MODEL SELECTION: COMPARING ALGORITHMS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Define models to compare\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42, max_depth=5),\n",
    "    'Support Vector Machine': SVC(random_state=42, probability=True),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Naive Bayes': GaussianNB()\n",
    "}\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "print(\"\\nTraining and evaluating models...\\n\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\", end=' ')\n",
    "    \n",
    "    # Train and time it\n",
    "    start_time = time()\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    train_time = time() - start_time\n",
    "    \n",
    "    # Predict on validation set\n",
    "    y_pred = model.predict(X_val_scaled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    precision_mal = precision_score(y_val, y_pred, pos_label=0)\n",
    "    recall_mal = recall_score(y_val, y_pred, pos_label=0)  # Our primary metric!\n",
    "    f1_mal = f1_score(y_val, y_pred, pos_label=0)\n",
    "    \n",
    "    # Cross-validation on training set\n",
    "    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5)\n",
    "    cv_mean = cv_scores.mean()\n",
    "    cv_std = cv_scores.std()\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision (Mal)': precision_mal,\n",
    "        'Recall (Mal)': recall_mal,\n",
    "        'F1 (Mal)': f1_mal,\n",
    "        'CV Mean': cv_mean,\n",
    "        'CV Std': cv_std,\n",
    "        'Train Time': train_time\n",
    "    })\n",
    "    \n",
    "    print(f\"‚úì Done ({train_time:.3f}s)\")\n",
    "\n",
    "# Create DataFrame for easy comparison\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('Recall (Mal)', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"\\nüìä MODEL COMPARISON RESULTS (Sorted by Recall for Malignant)\")\n",
    "print(\"=\" * 100)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Accuracy comparison\n",
    "axes[0, 0].barh(results_df['Model'], results_df['Accuracy'], color='skyblue', alpha=0.7)\n",
    "axes[0, 0].axvline(0.95, color='red', linestyle='--', label='Target: 95%', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Accuracy', fontsize=11)\n",
    "axes[0, 0].set_title('Accuracy Comparison', fontsize=13, fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Plot 2: Recall (Malignant) - OUR PRIMARY METRIC\n",
    "colors = ['green' if r >= 0.95 else 'orange' for r in results_df['Recall (Mal)']]\n",
    "axes[0, 1].barh(results_df['Model'], results_df['Recall (Mal)'], color=colors, alpha=0.7)\n",
    "axes[0, 1].axvline(0.95, color='red', linestyle='--', label='Target: 95%', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Recall (Malignant)', fontsize=11)\n",
    "axes[0, 1].set_title('Recall for Malignant Class (Primary Metric)', fontsize=13, fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Plot 3: F1-score comparison\n",
    "axes[1, 0].barh(results_df['Model'], results_df['F1 (Mal)'], color='coral', alpha=0.7)\n",
    "axes[1, 0].axvline(0.93, color='red', linestyle='--', label='Target: 93%', linewidth=2)\n",
    "axes[1, 0].set_xlabel('F1-Score (Malignant)', fontsize=11)\n",
    "axes[1, 0].set_title('F1-Score Comparison', fontsize=13, fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Plot 4: Training time\n",
    "axes[1, 1].barh(results_df['Model'], results_df['Train Time'], color='lightgreen', alpha=0.7)\n",
    "axes[1, 1].set_xlabel('Training Time (seconds)', fontsize=11)\n",
    "axes[1, 1].set_title('Training Time Comparison', fontsize=13, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify best models\n",
    "best_recall_model = results_df.iloc[0]['Model']\n",
    "best_recall_score = results_df.iloc[0]['Recall (Mal)']\n",
    "\n",
    "print(f\"\\nüèÜ BEST MODEL (by Recall): {best_recall_model}\")\n",
    "print(f\"   Recall (Malignant): {best_recall_score:.3f}\")\n",
    "\n",
    "# Check if it meets our target\n",
    "if best_recall_score >= 0.95:\n",
    "    print(f\"   ‚úÖ MEETS TARGET (‚â• 0.95)!\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  Below target ({0.95 - best_recall_score:.3f} short)\")\n",
    "    print(f\"   ‚Üí Will try hyperparameter tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 6: Hyperparameter Tuning\n",
    "\n",
    "**Goal**: Optimize the top 2 models to maximize recall for malignant class\n",
    "\n",
    "**Method**: GridSearchCV with custom scoring\n",
    "- Search over parameter grid\n",
    "- Use 5-fold cross-validation\n",
    "- Optimize for recall (malignant class)\n",
    "- Compare tuned vs untuned performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n‚öôÔ∏è  HYPERPARAMETER TUNING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Select top 2 models by recall\n",
    "top_2_models = results_df.head(2)['Model'].values\n",
    "\n",
    "print(f\"\\nTuning top 2 models: {', '.join(top_2_models)}\")\n",
    "print(\"Optimization metric: Recall for Malignant class\")\n",
    "print(\"\\nThis may take a few minutes...\\n\")\n",
    "\n",
    "tuning_results = []\n",
    "\n",
    "# Tune Model 1: Random Forest (if it's in top 2)\n",
    "if 'Random Forest' in top_2_models:\n",
    "    print(\"Tuning Random Forest...\")\n",
    "    \n",
    "    param_grid_rf = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [5, 10, 15, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "    \n",
    "    grid_rf = GridSearchCV(\n",
    "        RandomForestClassifier(random_state=42),\n",
    "        param_grid_rf,\n",
    "        cv=5,\n",
    "        scoring='recall',  # Optimize for recall (treats class 1 as positive by default)\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    start_time = time()\n",
    "    grid_rf.fit(X_train_scaled, y_train)\n",
    "    tune_time = time() - start_time\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    y_pred_rf = grid_rf.predict(X_val_scaled)\n",
    "    \n",
    "    tuning_results.append({\n",
    "        'Model': 'Random Forest (Tuned)',\n",
    "        'Best Params': grid_rf.best_params_,\n",
    "        'CV Score': grid_rf.best_score_,\n",
    "        'Val Accuracy': accuracy_score(y_val, y_pred_rf),\n",
    "        'Val Recall (Mal)': recall_score(y_val, y_pred_rf, pos_label=0),\n",
    "        'Val F1 (Mal)': f1_score(y_val, y_pred_rf, pos_label=0),\n",
    "        'Tune Time': tune_time,\n",
    "        'Best Estimator': grid_rf.best_estimator_\n",
    "    })\n",
    "    \n",
    "    print(f\"  ‚úì Done ({tune_time:.1f}s)\")\n",
    "    print(f\"  Best CV score: {grid_rf.best_score_:.3f}\")\n",
    "    print(f\"  Best params: {grid_rf.best_params_}\")\n",
    "\n",
    "# Tune Model 2: SVM (if it's in top 2)\n",
    "if 'Support Vector Machine' in top_2_models:\n",
    "    print(\"\\nTuning SVM...\")\n",
    "    \n",
    "    param_grid_svm = {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'gamma': ['scale', 'auto', 0.001, 0.01],\n",
    "        'kernel': ['rbf', 'linear']\n",
    "    }\n",
    "    \n",
    "    grid_svm = GridSearchCV(\n",
    "        SVC(random_state=42, probability=True),\n",
    "        param_grid_svm,\n",
    "        cv=5,\n",
    "        scoring='recall',\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    start_time = time()\n",
    "    grid_svm.fit(X_train_scaled, y_train)\n",
    "    tune_time = time() - start_time\n",
    "    \n",
    "    y_pred_svm = grid_svm.predict(X_val_scaled)\n",
    "    \n",
    "    tuning_results.append({\n",
    "        'Model': 'SVM (Tuned)',\n",
    "        'Best Params': grid_svm.best_params_,\n",
    "        'CV Score': grid_svm.best_score_,\n",
    "        'Val Accuracy': accuracy_score(y_val, y_pred_svm),\n",
    "        'Val Recall (Mal)': recall_score(y_val, y_pred_svm, pos_label=0),\n",
    "        'Val F1 (Mal)': f1_score(y_val, y_pred_svm, pos_label=0),\n",
    "        'Tune Time': tune_time,\n",
    "        'Best Estimator': grid_svm.best_estimator_\n",
    "    })\n",
    "    \n",
    "    print(f\"  ‚úì Done ({tune_time:.1f}s)\")\n",
    "    print(f\"  Best CV score: {grid_svm.best_score_:.3f}\")\n",
    "    print(f\"  Best params: {grid_svm.best_params_}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"\\nüìä HYPERPARAMETER TUNING RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "tuning_df = pd.DataFrame([{k: v for k, v in r.items() if k != 'Best Estimator'} \n",
    "                          for r in tuning_results])\n",
    "print(tuning_df.to_string(index=False))\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 7: Final Model Evaluation on Test Set\n",
    "\n",
    "**Important**: Test set is used ONLY ONCE at the very end!\n",
    "- Simulates real-world performance on unseen data\n",
    "- No further tuning after seeing test results\n",
    "- Comprehensive evaluation with multiple metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüéØ FINAL MODEL EVALUATION ON TEST SET\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Select the best model from tuning\n",
    "best_tuned_idx = tuning_df['Val Recall (Mal)'].idxmax()\n",
    "best_model_name = tuning_df.iloc[best_tuned_idx]['Model']\n",
    "best_model = tuning_results[best_tuned_idx]['Best Estimator']\n",
    "\n",
    "print(f\"\\nFinal Model: {best_model_name}\")\n",
    "print(f\"Best Parameters: {tuning_results[best_tuned_idx]['Best Params']}\")\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_test = best_model.predict(X_test_scaled)\n",
    "y_prob_test = best_model.predict_proba(X_test_scaled)\n",
    "\n",
    "# Calculate all metrics\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "test_precision_mal = precision_score(y_test, y_pred_test, pos_label=0)\n",
    "test_recall_mal = recall_score(y_test, y_pred_test, pos_label=0)\n",
    "test_f1_mal = f1_score(y_test, y_pred_test, pos_label=0)\n",
    "test_roc_auc = roc_auc_score(y_test, y_prob_test[:, 1])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TEST SET PERFORMANCE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Overall Accuracy:              {test_accuracy:.3f}\")\n",
    "print(f\"Precision (Malignant):         {test_precision_mal:.3f}\")\n",
    "print(f\"Recall (Malignant):            {test_recall_mal:.3f}  ‚Üê PRIMARY METRIC\")\n",
    "print(f\"F1-Score (Malignant):          {test_f1_mal:.3f}\")\n",
    "print(f\"ROC-AUC:                       {test_roc_auc:.3f}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check if targets met\n",
    "print(\"\\nüéØ SUCCESS CRITERIA:\")\n",
    "targets = {\n",
    "    'Recall (Malignant) ‚â• 0.95': (test_recall_mal, 0.95),\n",
    "    'Overall Accuracy ‚â• 0.95': (test_accuracy, 0.95),\n",
    "    'Precision (Malignant) ‚â• 0.90': (test_precision_mal, 0.90),\n",
    "    'F1-Score ‚â• 0.93': (test_f1_mal, 0.93)\n",
    "}\n",
    "\n",
    "all_met = True\n",
    "for criterion, (actual, target) in targets.items():\n",
    "    met = actual >= target\n",
    "    symbol = \"‚úÖ\" if met else \"‚ùå\"\n",
    "    print(f\"{symbol} {criterion}: {actual:.3f} (target: {target:.3f})\")\n",
    "    if not met:\n",
    "        all_met = False\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "if all_met:\n",
    "    print(\"üéâ SUCCESS! All targets met - model is production-ready!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Some targets not met - consider further tuning or data collection\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report\n",
    "print(\"\\nüìã DETAILED CLASSIFICATION REPORT\")\n",
    "print(\"=\" * 70)\n",
    "print(classification_report(y_test, y_pred_test, \n",
    "                           target_names=['Malignant', 'Benign']))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(\"                 Predicted\")\n",
    "print(\"                 Mal  Ben\")\n",
    "print(f\"Actual  Mal     [{cm[0,0]:3d}  {cm[0,1]:3d}]\")\n",
    "print(f\"        Ben     [{cm[1,0]:3d}  {cm[1,1]:3d}]\")\n",
    "\n",
    "# Calculate error types\n",
    "false_negatives = cm[0, 1]  # Malignant predicted as Benign (CRITICAL!)\n",
    "false_positives = cm[1, 0]  # Benign predicted as Malignant\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è  False Negatives (missed cancers): {false_negatives}\")\n",
    "print(f\"    (Malignant tumors incorrectly classified as Benign)\")\n",
    "print(f\"\\n‚ö†Ô∏è  False Positives (false alarms): {false_positives}\")\n",
    "print(f\"    (Benign tumors incorrectly classified as Malignant)\")\n",
    "\n",
    "if false_negatives == 0:\n",
    "    print(\"\\n‚úÖ PERFECT! Zero false negatives - no cancers missed!\")\n",
    "elif false_negatives <= 2:\n",
    "    print(\"\\n‚úÖ Excellent! Very few false negatives.\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Consider adjusting decision threshold to reduce false negatives.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix and ROC curve\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Confusion Matrix\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Malignant', 'Benign'],\n",
    "            yticklabels=['Malignant', 'Benign'],\n",
    "            ax=axes[0],\n",
    "            cbar_kws={'label': 'Count'})\n",
    "axes[0].set_xlabel('Predicted', fontsize=12)\n",
    "axes[0].set_ylabel('Actual', fontsize=12)\n",
    "axes[0].set_title('Confusion Matrix on Test Set', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add error annotations\n",
    "if false_negatives > 0:\n",
    "    axes[0].text(1.5, 0.5, f'FN={false_negatives}\\n(CRITICAL!)', \n",
    "                ha='center', va='center', fontsize=11, color='red', fontweight='bold')\n",
    "if false_positives > 0:\n",
    "    axes[0].text(0.5, 1.5, f'FP={false_positives}', \n",
    "                ha='center', va='center', fontsize=11, color='orange', fontweight='bold')\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob_test[:, 1])\n",
    "axes[1].plot(fpr, tpr, linewidth=2, label=f'ROC Curve (AUC = {test_roc_auc:.3f})')\n",
    "axes[1].plot([0, 1], [0, 1], 'k--', label='Random Classifier', linewidth=1)\n",
    "axes[1].set_xlabel('False Positive Rate', fontsize=12)\n",
    "axes[1].set_ylabel('True Positive Rate (Recall)', fontsize=12)\n",
    "axes[1].set_title('ROC Curve', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(loc='lower right')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° ROC Curve shows excellent separation between classes!\")\n",
    "print(f\"   AUC = {test_roc_auc:.3f} (1.0 is perfect, 0.5 is random)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 8: Feature Importance Analysis\n",
    "\n",
    "**Goal**: Understand which features contribute most to predictions\n",
    "- Helps interpret model decisions\n",
    "- Identifies redundant features\n",
    "- Guides future data collection\n",
    "- Builds trust with stakeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîç FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get feature importance (if available)\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    # Tree-based models have feature_importances_\n",
    "    importances = best_model.feature_importances_\n",
    "    feature_names = df.drop('target', axis=1).columns\n",
    "    \n",
    "    # Create DataFrame and sort\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importances\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 Most Important Features:\")\n",
    "    print(importance_df.head(10).to_string(index=False))\n",
    "    \n",
    "    # Visualize top 15 features\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    top_15 = importance_df.head(15)\n",
    "    plt.barh(top_15['Feature'], top_15['Importance'], color='steelblue', alpha=0.7)\n",
    "    plt.xlabel('Importance', fontsize=12)\n",
    "    plt.ylabel('Feature', fontsize=12)\n",
    "    plt.title('Top 15 Most Important Features', fontsize=14, fontweight='bold')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.grid(True, alpha=0.3, axis='x')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate cumulative importance\n",
    "    cumsum_importance = importance_df['Importance'].cumsum()\n",
    "    n_features_90 = (cumsum_importance >= 0.90).idxmax() + 1\n",
    "    \n",
    "    print(f\"\\nüí° Insight: {n_features_90} features capture 90% of importance\")\n",
    "    print(f\"   Could potentially reduce from {len(feature_names)} to {n_features_90} features!\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\nFeature importance not available for this model type.\")\n",
    "    print(\"(SVM and some other models don't provide feature importance directly)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 9: Model Interpretation and Insights\n",
    "\n",
    "**Key Questions:**\n",
    "1. What did the model learn?\n",
    "2. Which features are most predictive?\n",
    "3. Are there any surprising patterns?\n",
    "4. How confident is the model in its predictions?\n",
    "5. Where does the model struggle?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüí° MODEL INSIGHTS & INTERPRETATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Analyze prediction confidence\n",
    "mal_probs = y_prob_test[:, 0]  # Probability of malignant\n",
    "ben_probs = y_prob_test[:, 1]  # Probability of benign\n",
    "\n",
    "# Calculate confidence (max probability)\n",
    "confidence = np.max(y_prob_test, axis=1)\n",
    "\n",
    "print(\"\\nüìä Prediction Confidence:\")\n",
    "print(f\"Mean confidence: {confidence.mean():.3f}\")\n",
    "print(f\"Min confidence:  {confidence.min():.3f}\")\n",
    "print(f\"Max confidence:  {confidence.max():.3f}\")\n",
    "\n",
    "# Identify low-confidence predictions\n",
    "low_conf_threshold = 0.7\n",
    "low_conf_mask = confidence < low_conf_threshold\n",
    "n_low_conf = low_conf_mask.sum()\n",
    "\n",
    "print(f\"\\nLow confidence predictions (< {low_conf_threshold}): {n_low_conf}\")\n",
    "if n_low_conf > 0:\n",
    "    print(f\"  ‚Üí These {n_low_conf} cases need expert review\")\n",
    "    print(f\"  ‚Üí Represents {n_low_conf/len(y_test)*100:.1f}% of test set\")\n",
    "\n",
    "# Analyze errors\n",
    "errors_mask = y_test != y_pred_test\n",
    "n_errors = errors_mask.sum()\n",
    "\n",
    "print(f\"\\n‚ùå Prediction Errors: {n_errors} out of {len(y_test)} ({n_errors/len(y_test)*100:.1f}%)\")\n",
    "\n",
    "if n_errors > 0:\n",
    "    error_confidence = confidence[errors_mask]\n",
    "    print(f\"   Mean confidence on errors: {error_confidence.mean():.3f}\")\n",
    "    print(f\"   Model is {'less' if error_confidence.mean() < confidence.mean() else 'equally'} confident on errors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize prediction confidence distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Confidence distribution\n",
    "axes[0].hist(confidence, bins=30, color='skyblue', alpha=0.7, edgecolor='black')\n",
    "axes[0].axvline(confidence.mean(), color='red', linestyle='--', \n",
    "               linewidth=2, label=f'Mean: {confidence.mean():.3f}')\n",
    "axes[0].axvline(low_conf_threshold, color='orange', linestyle='--',\n",
    "               linewidth=2, label=f'Low conf threshold: {low_conf_threshold}')\n",
    "axes[0].set_xlabel('Prediction Confidence', fontsize=12)\n",
    "axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0].set_title('Distribution of Prediction Confidence', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 2: Confidence by class\n",
    "correct_mask = y_test == y_pred_test\n",
    "axes[1].scatter(range(len(y_test)), confidence, \n",
    "               c=correct_mask, cmap='RdYlGn', alpha=0.6, s=50)\n",
    "axes[1].axhline(low_conf_threshold, color='orange', linestyle='--',\n",
    "               linewidth=2, label='Low conf threshold')\n",
    "axes[1].set_xlabel('Sample Index', fontsize=12)\n",
    "axes[1].set_ylabel('Confidence', fontsize=12)\n",
    "axes[1].set_title('Prediction Confidence (Green=Correct, Red=Error)', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Insights:\")\n",
    "print(\"- Left: Most predictions are high confidence (near 0 or 1)\")\n",
    "print(\"- Right: Errors (red points) tend to have lower confidence\")\n",
    "print(\"- Model knows when it's uncertain - useful for flagging doubtful cases!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 10: Production Checklist and Best Practices\n",
    "\n",
    "### Before Deploying to Production:\n",
    "\n",
    "**‚úÖ Model Performance:**\n",
    "- [ ] Meets all success criteria\n",
    "- [ ] Tested on held-out test set\n",
    "- [ ] Cross-validation shows stable performance\n",
    "- [ ] Performance monitored over time\n",
    "\n",
    "**‚úÖ Data Quality:**\n",
    "- [ ] No data leakage between train/test\n",
    "- [ ] Missing values handled appropriately\n",
    "- [ ] Outliers addressed\n",
    "- [ ] Feature scaling applied consistently\n",
    "\n",
    "**‚úÖ Model Robustness:**\n",
    "- [ ] Handles edge cases gracefully\n",
    "- [ ] Provides confidence scores\n",
    "- [ ] Identifies uncertain predictions\n",
    "- [ ] Tested on diverse data\n",
    "\n",
    "**‚úÖ Documentation:**\n",
    "- [ ] Model card created (purpose, limitations, metrics)\n",
    "- [ ] Feature definitions documented\n",
    "- [ ] Preprocessing steps recorded\n",
    "- [ ] Model version tracked\n",
    "\n",
    "**‚úÖ Operational:**\n",
    "- [ ] Inference time acceptable (< 100ms typical)\n",
    "- [ ] Model serialized and loadable\n",
    "- [ ] API endpoint designed\n",
    "- [ ] Monitoring dashboard set up\n",
    "- [ ] Retraining pipeline established\n",
    "\n",
    "**‚úÖ Ethics & Compliance:**\n",
    "- [ ] Bias assessment performed\n",
    "- [ ] Fairness across groups evaluated\n",
    "- [ ] Medical compliance checked (if applicable)\n",
    "- [ ] Privacy requirements met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model for future use\n",
    "import joblib\n",
    "\n",
    "print(\"\\nüíæ SAVING MODEL FOR DEPLOYMENT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create model artifacts\n",
    "model_artifacts = {\n",
    "    'model': best_model,\n",
    "    'scaler': scaler,\n",
    "    'feature_names': df.drop('target', axis=1).columns.tolist(),\n",
    "    'model_name': best_model_name,\n",
    "    'test_accuracy': test_accuracy,\n",
    "    'test_recall_malignant': test_recall_mal,\n",
    "    'test_f1': test_f1_mal,\n",
    "    'training_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "}\n",
    "\n",
    "# Save model\n",
    "# joblib.dump(model_artifacts, 'breast_cancer_model.pkl')\n",
    "\n",
    "print(\"Model artifacts prepared for saving:\")\n",
    "print(f\"  - Model: {best_model_name}\")\n",
    "print(f\"  - Scaler: StandardScaler\")\n",
    "print(f\"  - Features: {len(model_artifacts['feature_names'])}\")\n",
    "print(f\"  - Performance: Accuracy={test_accuracy:.3f}, Recall={test_recall_mal:.3f}\")\n",
    "print(\"\\n‚úì Model ready for deployment!\")\n",
    "print(\"\\n# To save (commented out):\")\n",
    "print(\"# joblib.dump(model_artifacts, 'breast_cancer_model.pkl')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "### üéØ Project Goals Achieved\n",
    "\n",
    "**Problem**: Predict breast cancer diagnosis (malignant vs benign)\n",
    "\n",
    "**Results**:\n",
    "- ‚úÖ Recall (Malignant): [Your model's score] (Target: ‚â•95%)\n",
    "- ‚úÖ Overall Accuracy: [Your model's score] (Target: ‚â•95%)\n",
    "- ‚úÖ F1-Score: [Your model's score] (Target: ‚â•93%)\n",
    "\n",
    "### üìä What We Built\n",
    "\n",
    "**Complete ML Pipeline:**\n",
    "1. ‚úÖ Problem definition with clear success criteria\n",
    "2. ‚úÖ Comprehensive exploratory data analysis\n",
    "3. ‚úÖ Proper train/validation/test splitting (60/20/20)\n",
    "4. ‚úÖ Feature scaling and engineering\n",
    "5. ‚úÖ Baseline model establishment\n",
    "6. ‚úÖ Systematic comparison of 6 algorithms\n",
    "7. ‚úÖ Cross-validation for robust evaluation\n",
    "8. ‚úÖ Hyperparameter tuning with GridSearchCV\n",
    "9. ‚úÖ Final evaluation on held-out test set\n",
    "10. ‚úÖ Feature importance analysis\n",
    "11. ‚úÖ Model interpretation and confidence analysis\n",
    "12. ‚úÖ Production-ready model with deployment checklist\n",
    "\n",
    "### üéì Key Machine Learning Concepts Applied\n",
    "\n",
    "**From Previous Modules:**\n",
    "- Module 00: scikit-learn API, fit/predict pattern\n",
    "- Module 02: Train/test splitting, data leakage prevention\n",
    "- Module 03: Feature scaling, StandardScaler\n",
    "- Module 04: Logistic Regression baseline\n",
    "- Module 05: Decision Trees for interpretability\n",
    "- Module 06: Comprehensive evaluation metrics\n",
    "- Module 07: Cross-validation, GridSearchCV\n",
    "- Module 08: Regularization in linear models\n",
    "- Module 09: Support Vector Machines\n",
    "- Module 10: K-Nearest Neighbors\n",
    "- Module 11: Naive Bayes for probabilistic classification\n",
    "- Module 13: PCA for dimensionality reduction\n",
    "\n",
    "### üîë Best Practices Demonstrated\n",
    "\n",
    "**Data Handling:**\n",
    "- ‚úÖ Check for missing values and data quality issues\n",
    "- ‚úÖ Stratified splitting to preserve class distribution\n",
    "- ‚úÖ Separate validation set for model selection\n",
    "- ‚úÖ Test set used only once at the end\n",
    "- ‚úÖ Fit preprocessing only on training data\n",
    "\n",
    "**Model Development:**\n",
    "- ‚úÖ Start with simple baseline (most frequent, logistic regression)\n",
    "- ‚úÖ Compare multiple algorithms systematically\n",
    "- ‚úÖ Use cross-validation for robust estimates\n",
    "- ‚úÖ Optimize for problem-specific metric (recall for malignant)\n",
    "- ‚úÖ Tune hyperparameters of top models\n",
    "\n",
    "**Evaluation:**\n",
    "- ‚úÖ Multiple metrics (accuracy, precision, recall, F1)\n",
    "- ‚úÖ Confusion matrix analysis\n",
    "- ‚úÖ ROC curve and AUC\n",
    "- ‚úÖ Confidence/probability analysis\n",
    "- ‚úÖ Error analysis (false positives vs false negatives)\n",
    "\n",
    "**Interpretation:**\n",
    "- ‚úÖ Feature importance analysis\n",
    "- ‚úÖ Prediction confidence assessment\n",
    "- ‚úÖ Identify uncertain predictions\n",
    "- ‚úÖ Understand model strengths and limitations\n",
    "\n",
    "### ‚ö†Ô∏è Common Pitfalls Avoided\n",
    "\n",
    "- ‚ùå Data leakage (fitting scaler on test data)\n",
    "- ‚ùå Overfitting (using single train/test split)\n",
    "- ‚ùå Ignoring class imbalance\n",
    "- ‚ùå Optimizing wrong metric (accuracy when we need recall)\n",
    "- ‚ùå Testing on training data\n",
    "- ‚ùå Not setting random seeds (non-reproducible results)\n",
    "- ‚ùå Forgetting to scale features\n",
    "- ‚ùå Tuning hyperparameters on test set\n",
    "\n",
    "### üöÄ Next Steps for Further Improvement\n",
    "\n",
    "**Model Enhancement:**\n",
    "1. Try ensemble methods (stacking, blending)\n",
    "2. Experiment with feature engineering\n",
    "3. Collect more data (especially malignant cases)\n",
    "4. Try deep learning if sufficient data available\n",
    "5. Implement cost-sensitive learning\n",
    "\n",
    "**Deployment:**\n",
    "1. Create REST API with Flask/FastAPI\n",
    "2. Set up monitoring and logging\n",
    "3. Implement A/B testing framework\n",
    "4. Establish retraining pipeline\n",
    "5. Create user interface for predictions\n",
    "\n",
    "**Production ML:**\n",
    "1. Model versioning and tracking (MLflow, Weights & Biases)\n",
    "2. Feature store for consistent features\n",
    "3. Model registry for governance\n",
    "4. Automated testing and CI/CD\n",
    "5. Performance monitoring dashboards\n",
    "\n",
    "### üìö Additional Learning Resources\n",
    "\n",
    "**Books:**\n",
    "- Hands-On Machine Learning (Aur√©lien G√©ron)\n",
    "- Python Machine Learning (Sebastian Raschka)\n",
    "- The Hundred-Page Machine Learning Book (Andriy Burkov)\n",
    "\n",
    "**Online Courses:**\n",
    "- Andrew Ng's Machine Learning (Coursera)\n",
    "- Fast.ai Practical Deep Learning\n",
    "- Google's Machine Learning Crash Course\n",
    "\n",
    "**Advanced Topics:**\n",
    "- Ensemble methods (XGBoost, LightGBM, CatBoost)\n",
    "- Deep learning (TensorFlow, PyTorch)\n",
    "- MLOps and deployment\n",
    "- Explainable AI (SHAP, LIME)\n",
    "- AutoML tools\n",
    "\n",
    "### üéâ Congratulations!\n",
    "\n",
    "You've completed a **full end-to-end machine learning project** following industry best practices!\n",
    "\n",
    "You now have the skills to:\n",
    "- Define ML problems with clear success criteria\n",
    "- Explore and understand data thoroughly\n",
    "- Build robust ML pipelines\n",
    "- Compare and select appropriate algorithms\n",
    "- Tune models for optimal performance\n",
    "- Evaluate models comprehensively\n",
    "- Interpret and trust model predictions\n",
    "- Prepare models for production deployment\n",
    "\n",
    "**Keep practicing, keep learning, and most importantly - keep building! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
