{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 01: Supervised vs Unsupervised Learning\n",
    "\n",
    "**Difficulty**: ⭐ Beginner  \n",
    "**Estimated Time**: 75 minutes  \n",
    "**Prerequisites**: \n",
    "- [Module 00: Introduction to Machine Learning](00_introduction_to_machine_learning.ipynb)\n",
    "- Understanding of basic ML concepts\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Distinguish between supervised and unsupervised learning approaches\n",
    "2. Identify when to use classification vs regression\n",
    "3. Understand the difference between clustering and classification\n",
    "4. Apply both supervised and unsupervised algorithms to real datasets\n",
    "5. Recognize the appropriate ML approach for different problem types\n",
    "6. Implement examples of each learning paradigm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Scikit-learn for ML\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Supervised learning algorithms\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Unsupervised learning algorithms\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, mean_squared_error, r2_score,\n",
    "    silhouette_score, classification_report\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Supervised Learning Deep Dive\n",
    "\n",
    "### What is Supervised Learning?\n",
    "\n",
    "**Supervised learning** is learning from **labeled data**. Each training example consists of:\n",
    "- **Input features (X)**: The data we have\n",
    "- **Output labels (y)**: The correct answer we want to predict\n",
    "\n",
    "The algorithm learns to map inputs to outputs by finding patterns in the labeled examples.\n",
    "\n",
    "### The Two Types of Supervised Learning\n",
    "\n",
    "#### 1. Classification (Discrete Outputs)\n",
    "\n",
    "**Goal**: Predict which category/class an input belongs to\n",
    "\n",
    "**Examples**:\n",
    "- Email: Spam or Not Spam (binary classification)\n",
    "- Image: Cat, Dog, or Bird (multi-class classification)\n",
    "- Customer: Will churn or stay (binary classification)\n",
    "- Sentiment: Positive, Negative, or Neutral (multi-class)\n",
    "\n",
    "**Common Algorithms**:\n",
    "- Logistic Regression\n",
    "- Decision Trees\n",
    "- Random Forests\n",
    "- Support Vector Machines (SVM)\n",
    "- Neural Networks\n",
    "\n",
    "#### 2. Regression (Continuous Outputs)\n",
    "\n",
    "**Goal**: Predict a numerical value\n",
    "\n",
    "**Examples**:\n",
    "- House price based on features (price in dollars)\n",
    "- Temperature prediction (degrees)\n",
    "- Stock price (value)\n",
    "- Sales forecast (quantity)\n",
    "\n",
    "**Common Algorithms**:\n",
    "- Linear Regression\n",
    "- Polynomial Regression\n",
    "- Ridge/Lasso Regression\n",
    "- Random Forest Regressor\n",
    "- Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Example: Classification Problem\n",
    "\n",
    "Let's build a classification model to identify iris species based on flower measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_iris, y_iris, test_size=0.2, random_state=42, stratify=y_iris\n",
    ")\n",
    "\n",
    "# Train a Decision Tree classifier\n",
    "clf = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Classification Accuracy: {accuracy:.2%}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=iris.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize classification results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Actual vs Predicted (using two features for visualization)\n",
    "feature_idx = [2, 3]  # Petal length and width\n",
    "colors = ['red', 'green', 'blue']\n",
    "\n",
    "for i, species in enumerate(iris.target_names):\n",
    "    mask = y_test == i\n",
    "    axes[0].scatter(\n",
    "        X_test[mask, feature_idx[0]],\n",
    "        X_test[mask, feature_idx[1]],\n",
    "        c=colors[i],\n",
    "        label=f'Actual {species}',\n",
    "        marker='o',\n",
    "        s=100,\n",
    "        alpha=0.6\n",
    "    )\n",
    "\n",
    "# Mark misclassifications with X\n",
    "misclassified = y_test != y_pred\n",
    "if misclassified.any():\n",
    "    axes[0].scatter(\n",
    "        X_test[misclassified, feature_idx[0]],\n",
    "        X_test[misclassified, feature_idx[1]],\n",
    "        marker='x',\n",
    "        s=200,\n",
    "        c='black',\n",
    "        linewidths=3,\n",
    "        label='Misclassified'\n",
    "    )\n",
    "\n",
    "axes[0].set_xlabel(iris.feature_names[feature_idx[0]])\n",
    "axes[0].set_ylabel(iris.feature_names[feature_idx[1]])\n",
    "axes[0].set_title('Classification Results (Test Set)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=iris.target_names,\n",
    "            yticklabels=iris.target_names, ax=axes[1])\n",
    "axes[1].set_title('Confusion Matrix')\n",
    "axes[1].set_ylabel('Actual')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice: Classification assigns discrete labels (species names)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Example: Regression Problem\n",
    "\n",
    "Let's build a regression model to predict house prices based on various features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load California housing dataset\n",
    "housing = datasets.fetch_california_housing()\n",
    "X_housing = housing.data\n",
    "y_housing = housing.target  # Median house value (in $100,000s)\n",
    "\n",
    "# Use only first 1000 samples for faster computation\n",
    "X_housing = X_housing[:1000]\n",
    "y_housing = y_housing[:1000]\n",
    "\n",
    "print(\"Dataset shape:\", X_housing.shape)\n",
    "print(\"Features:\", housing.feature_names)\n",
    "print(\"Target: Median house value in $100,000s\")\n",
    "print(f\"Price range: ${y_housing.min()*100000:.0f} - ${y_housing.max()*100000:.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_housing, y_housing, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train a Linear Regression model\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_train = reg.predict(X_train)\n",
    "y_pred_test = reg.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "train_r2 = r2_score(y_train, y_pred_train)\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "print(f\"Training R² Score: {train_r2:.3f}\")\n",
    "print(f\"Test R² Score: {test_r2:.3f}\")\n",
    "print(f\"Test RMSE: ${test_rmse*100000:.2f}\")\n",
    "print(f\"\\nInterpretation: Model explains {test_r2:.1%} of variance in house prices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize regression results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Actual vs Predicted\n",
    "axes[0].scatter(y_test, y_pred_test, alpha=0.6, edgecolors='k')\n",
    "axes[0].plot(\n",
    "    [y_test.min(), y_test.max()],\n",
    "    [y_test.min(), y_test.max()],\n",
    "    'r--', lw=2, label='Perfect Prediction'\n",
    ")\n",
    "axes[0].set_xlabel('Actual Price ($100,000s)')\n",
    "axes[0].set_ylabel('Predicted Price ($100,000s)')\n",
    "axes[0].set_title('Regression: Actual vs Predicted Prices')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Residuals (prediction errors)\n",
    "residuals = y_test - y_pred_test\n",
    "axes[1].scatter(y_pred_test, residuals, alpha=0.6, edgecolors='k')\n",
    "axes[1].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "axes[1].set_xlabel('Predicted Price ($100,000s)')\n",
    "axes[1].set_ylabel('Residual (Actual - Predicted)')\n",
    "axes[1].set_title('Residual Plot')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice: Regression predicts continuous values (not discrete categories)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Unsupervised Learning Deep Dive\n",
    "\n",
    "### What is Unsupervised Learning?\n",
    "\n",
    "**Unsupervised learning** works with **unlabeled data**. We only have:\n",
    "- **Input features (X)**: The data\n",
    "- **No labels (y)**: We don't know the \"correct answer\"\n",
    "\n",
    "The algorithm tries to find hidden patterns, structures, or groupings in the data.\n",
    "\n",
    "### Main Types of Unsupervised Learning\n",
    "\n",
    "#### 1. Clustering\n",
    "\n",
    "**Goal**: Group similar data points together\n",
    "\n",
    "**Examples**:\n",
    "- Customer segmentation (group customers with similar behavior)\n",
    "- Document categorization (group similar articles)\n",
    "- Image compression (group similar colors)\n",
    "- Anomaly detection (find outliers that don't fit any group)\n",
    "\n",
    "**Common Algorithms**:\n",
    "- K-Means\n",
    "- DBSCAN\n",
    "- Hierarchical Clustering\n",
    "- Gaussian Mixture Models\n",
    "\n",
    "#### 2. Dimensionality Reduction\n",
    "\n",
    "**Goal**: Reduce number of features while preserving information\n",
    "\n",
    "**Examples**:\n",
    "- Visualizing high-dimensional data in 2D/3D\n",
    "- Feature extraction before training ML models\n",
    "- Data compression\n",
    "- Noise reduction\n",
    "\n",
    "**Common Algorithms**:\n",
    "- Principal Component Analysis (PCA)\n",
    "- t-SNE\n",
    "- UMAP\n",
    "- Autoencoders\n",
    "\n",
    "### Key Difference from Supervised Learning\n",
    "\n",
    "| Aspect | Supervised | Unsupervised |\n",
    "|--------|-----------|-------------|\n",
    "| **Data** | Labeled (X and y) | Unlabeled (only X) |\n",
    "| **Goal** | Predict labels | Find patterns |\n",
    "| **Evaluation** | Compare predictions to true labels | No ground truth; use metrics like silhouette score |\n",
    "| **Use Case** | When you know what to predict | When exploring data or don't have labels |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Example: Clustering (K-Means)\n",
    "\n",
    "Let's use K-Means to find groups in the Iris dataset, pretending we don't know the species labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use iris data but ignore the labels (unsupervised!)\n",
    "X_iris_unsupervised = iris.data\n",
    "\n",
    "# Apply K-Means clustering with 3 clusters\n",
    "# We choose 3 because we suspect there might be 3 groups\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "cluster_labels = kmeans.fit_predict(X_iris_unsupervised)\n",
    "\n",
    "# Evaluate clustering quality using silhouette score\n",
    "# Score ranges from -1 to 1; higher is better\n",
    "silhouette = silhouette_score(X_iris_unsupervised, cluster_labels)\n",
    "\n",
    "print(f\"Silhouette Score: {silhouette:.3f}\")\n",
    "print(f\"Cluster assignments (first 10 samples): {cluster_labels[:10]}\")\n",
    "print(f\"\\nCluster sizes:\")\n",
    "unique, counts = np.unique(cluster_labels, return_counts=True)\n",
    "for cluster, count in zip(unique, counts):\n",
    "    print(f\"  Cluster {cluster}: {count} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clusters\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Use petal measurements for visualization\n",
    "petal_length = X_iris_unsupervised[:, 2]\n",
    "petal_width = X_iris_unsupervised[:, 3]\n",
    "\n",
    "# Plot 1: K-Means clusters (what algorithm found)\n",
    "scatter1 = axes[0].scatter(\n",
    "    petal_length, petal_width,\n",
    "    c=cluster_labels, cmap='viridis',\n",
    "    s=100, alpha=0.6, edgecolors='k'\n",
    ")\n",
    "# Plot cluster centers\n",
    "centers = kmeans.cluster_centers_\n",
    "axes[0].scatter(\n",
    "    centers[:, 2], centers[:, 3],\n",
    "    c='red', marker='X', s=300,\n",
    "    edgecolors='black', linewidths=2,\n",
    "    label='Cluster Centers'\n",
    ")\n",
    "axes[0].set_xlabel('Petal Length (cm)')\n",
    "axes[0].set_ylabel('Petal Width (cm)')\n",
    "axes[0].set_title('Unsupervised: K-Means Clusters')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter1, ax=axes[0], label='Cluster')\n",
    "\n",
    "# Plot 2: True species labels (for comparison)\n",
    "scatter2 = axes[1].scatter(\n",
    "    petal_length, petal_width,\n",
    "    c=iris.target, cmap='viridis',\n",
    "    s=100, alpha=0.6, edgecolors='k'\n",
    ")\n",
    "axes[1].set_xlabel('Petal Length (cm)')\n",
    "axes[1].set_ylabel('Petal Width (cm)')\n",
    "axes[1].set_title('True Species Labels (for comparison)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter2, ax=axes[1], label='Species')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice: Clustering found groups WITHOUT being told the species labels!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Example: Dimensionality Reduction (PCA)\n",
    "\n",
    "Let's use PCA to reduce the Iris dataset from 4 features to 2 for easy visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA to reduce from 4D to 2D\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_iris_unsupervised)\n",
    "\n",
    "# Check how much variance is preserved\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print(f\"Variance explained by PC1: {explained_variance[0]:.1%}\")\n",
    "print(f\"Variance explained by PC2: {explained_variance[1]:.1%}\")\n",
    "print(f\"Total variance preserved: {explained_variance.sum():.1%}\")\n",
    "print(f\"\\nOriginal shape: {X_iris_unsupervised.shape}\")\n",
    "print(f\"Reduced shape: {X_pca.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize PCA results\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for i, species in enumerate(iris.target_names):\n",
    "    mask = iris.target == i\n",
    "    plt.scatter(\n",
    "        X_pca[mask, 0], X_pca[mask, 1],\n",
    "        label=species, s=100, alpha=0.7, edgecolors='k'\n",
    "    )\n",
    "\n",
    "plt.xlabel(f'First Principal Component ({explained_variance[0]:.1%} variance)')\n",
    "plt.ylabel(f'Second Principal Component ({explained_variance[1]:.1%} variance)')\n",
    "plt.title('PCA: 4D Iris Data Reduced to 2D')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"PCA reduced 4 features to 2 while keeping\", f\"{explained_variance.sum():.1%} of information!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Supervised vs Unsupervised: Side-by-Side Comparison\n",
    "\n",
    "Let's directly compare both approaches on the same dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare iris data\n",
    "X = iris.data\n",
    "y_true = iris.target\n",
    "\n",
    "# SUPERVISED: Train a classifier (knows the labels)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_true, test_size=0.3, random_state=42\n",
    ")\n",
    "supervised_model = LogisticRegression(max_iter=200)\n",
    "supervised_model.fit(X_train, y_train)\n",
    "y_pred_supervised = supervised_model.predict(X_test)\n",
    "supervised_accuracy = accuracy_score(y_test, y_pred_supervised)\n",
    "\n",
    "# UNSUPERVISED: Apply clustering (doesn't know the labels)\n",
    "unsupervised_model = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "y_pred_unsupervised = unsupervised_model.fit_predict(X)\n",
    "unsupervised_silhouette = silhouette_score(X, y_pred_unsupervised)\n",
    "\n",
    "# Compare results\n",
    "print(\"=\" * 60)\n",
    "print(\"SUPERVISED LEARNING (Logistic Regression)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Training data: {len(X_train)} labeled samples\")\n",
    "print(f\"Test accuracy: {supervised_accuracy:.2%}\")\n",
    "print(\"Evaluation: Compared predictions to true labels\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"UNSUPERVISED LEARNING (K-Means Clustering)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Training data: {len(X)} unlabeled samples\")\n",
    "print(f\"Silhouette score: {unsupervised_silhouette:.3f} (quality of clusters)\")\n",
    "print(\"Evaluation: No true labels; measured cluster cohesion\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"KEY DIFFERENCES\")\n",
    "print(\"=\" * 60)\n",
    "print(\"• Supervised NEEDS labels; Unsupervised works WITHOUT labels\")\n",
    "print(\"• Supervised PREDICTS labels; Unsupervised DISCOVERS patterns\")\n",
    "print(\"• Supervised has clear accuracy metric; Unsupervised uses internal metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. When to Use Each Approach\n",
    "\n",
    "### Choose Supervised Learning When:\n",
    "\n",
    "✅ You have labeled training data  \n",
    "✅ You know what you want to predict  \n",
    "✅ You can measure success objectively (accuracy, error, etc.)  \n",
    "✅ Examples: Spam detection, price prediction, medical diagnosis  \n",
    "\n",
    "### Choose Unsupervised Learning When:\n",
    "\n",
    "✅ You don't have labeled data (or it's expensive to obtain)  \n",
    "✅ You want to explore and discover patterns  \n",
    "✅ You need to reduce dimensionality or compress data  \n",
    "✅ Examples: Customer segmentation, anomaly detection, recommendation systems  \n",
    "\n",
    "### Can You Use Both?\n",
    "\n",
    "**Yes!** Common combinations:\n",
    "\n",
    "1. **Unsupervised → Supervised**:\n",
    "   - Use PCA to reduce features, then train classifier\n",
    "   - Use clustering to create new features\n",
    "\n",
    "2. **Semi-Supervised Learning**:\n",
    "   - Small amount of labeled data + large amount of unlabeled data\n",
    "   - Use unsupervised to learn structure, then supervised to predict\n",
    "\n",
    "3. **Unsupervised for Preprocessing**:\n",
    "   - Anomaly detection to clean data\n",
    "   - Feature extraction before supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Practice Exercises\n",
    "\n",
    "### Exercise 1: Classification vs Regression\n",
    "\n",
    "For each problem, identify whether it's classification or regression:\n",
    "\n",
    "1. Predicting tomorrow's temperature\n",
    "2. Detecting fraudulent credit card transactions\n",
    "3. Estimating the number of sales next month\n",
    "4. Classifying emails as important, promotional, or spam\n",
    "5. Predicting whether a patient has a disease\n",
    "\n",
    "Write your answers below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answers here (as comments)\n",
    "# 1. \n",
    "# 2. \n",
    "# 3. \n",
    "# 4. \n",
    "# 5. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Build a Regression Model\n",
    "\n",
    "Use the diabetes dataset from sklearn to build a regression model:\n",
    "1. Load the data using `datasets.load_diabetes()`\n",
    "2. Split into train/test sets\n",
    "3. Train a `RandomForestRegressor`\n",
    "4. Evaluate using R² score\n",
    "5. Compare to `LinearRegression` - which performs better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Clustering Different Numbers of Clusters\n",
    "\n",
    "Apply K-Means to the iris dataset with k=2, 3, 4, and 5 clusters. Calculate the silhouette score for each. Which value of k gives the best score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: Loop through k values, fit KMeans, calculate silhouette_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: PCA with Different Components\n",
    "\n",
    "Apply PCA to the iris dataset with 1, 2, and 3 components. For each, print the total variance explained. How many components do you need to explain at least 95% of variance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary\n",
    "\n",
    "### Key Concepts Learned\n",
    "\n",
    "1. **Supervised Learning**:\n",
    "   - Uses labeled data (X and y)\n",
    "   - Two types: Classification (discrete) and Regression (continuous)\n",
    "   - Goal: Learn to predict labels for new data\n",
    "   - Evaluated by comparing predictions to true labels\n",
    "\n",
    "2. **Unsupervised Learning**:\n",
    "   - Uses unlabeled data (only X)\n",
    "   - Two main types: Clustering and Dimensionality Reduction\n",
    "   - Goal: Discover hidden patterns or structure\n",
    "   - Evaluated using internal metrics (no ground truth)\n",
    "\n",
    "3. **Key Differences**:\n",
    "   - Supervised needs labels; unsupervised doesn't\n",
    "   - Supervised predicts; unsupervised explores\n",
    "   - Different use cases and evaluation methods\n",
    "\n",
    "4. **Practical Skills**:\n",
    "   - Built classification and regression models\n",
    "   - Applied clustering (K-Means)\n",
    "   - Used dimensionality reduction (PCA)\n",
    "   - Compared both approaches on same data\n",
    "\n",
    "### Decision Tree: Which Approach to Use?\n",
    "\n",
    "```\n",
    "Do you have labeled data?\n",
    "├── YES: Supervised Learning\n",
    "│   ├── Predicting categories? → Classification\n",
    "│   └── Predicting numbers? → Regression\n",
    "└── NO: Unsupervised Learning\n",
    "    ├── Finding groups? → Clustering\n",
    "    └── Reducing features? → Dimensionality Reduction\n",
    "```\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the next module, we'll dive deep into:\n",
    "- **Data preparation techniques**\n",
    "- **Train/test splitting strategies**\n",
    "- **Feature scaling and encoding**\n",
    "- **Handling missing data**\n",
    "- **Data leakage and how to avoid it**\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [Scikit-learn Choosing the Right Estimator](https://scikit-learn.org/stable/tutorial/machine_learning_map/)\n",
    "- [Supervised vs Unsupervised Learning](https://www.ibm.com/cloud/blog/supervised-vs-unsupervised-learning)\n",
    "- [Andrew Ng's ML Course - Week 1](https://www.coursera.org/learn/machine-learning)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
