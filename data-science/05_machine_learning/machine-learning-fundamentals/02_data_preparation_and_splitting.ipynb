{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 02: Data Preparation and Train/Test Split\n",
    "\n",
    "**Difficulty**: ⭐ Beginner  \n",
    "**Estimated Time**: 90 minutes  \n",
    "**Prerequisites**: \n",
    "- [Module 00: Introduction to Machine Learning](00_introduction_to_machine_learning.ipynb)\n",
    "- [Module 01: Supervised vs Unsupervised Learning](01_supervised_vs_unsupervised_learning.ipynb)\n",
    "- Pandas and NumPy proficiency\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Understand why data preparation is critical for ML success\n",
    "2. Handle missing data using various imputation strategies\n",
    "3. Encode categorical variables (one-hot, label encoding)\n",
    "4. Scale and normalize features properly\n",
    "5. Split data correctly to avoid data leakage\n",
    "6. Create training, validation, and test sets\n",
    "7. Recognize and prevent common data preparation mistakes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Scikit-learn preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler, MinMaxScaler, RobustScaler,\n",
    "    LabelEncoder, OneHotEncoder\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Example dataset\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Configuration\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Why Data Preparation Matters\n",
    "\n",
    "### The 80/20 Rule of Machine Learning\n",
    "\n",
    "**80% of ML work is data preparation**, only 20% is modeling!\n",
    "\n",
    "Real-world data is messy:\n",
    "- **Missing values**: Incomplete records\n",
    "- **Different scales**: Age (0-100) vs Income (0-1,000,000)\n",
    "- **Categorical data**: Text labels that need encoding\n",
    "- **Outliers**: Extreme values that can skew models\n",
    "- **Inconsistent formats**: Dates, currencies, text\n",
    "\n",
    "### Consequences of Poor Data Preparation\n",
    "\n",
    "❌ **Models fail to converge** (never finish training)  \n",
    "❌ **Poor performance** due to biased or noisy data  \n",
    "❌ **Data leakage** - accidentally using test information in training  \n",
    "❌ **Overfitting** on training quirks  \n",
    "\n",
    "### Our Goal\n",
    "\n",
    "Transform raw data into **clean, ML-ready format** where:\n",
    "- No missing values (or handled appropriately)\n",
    "- All features are numerical\n",
    "- Features are on similar scales\n",
    "- Training and test data are properly separated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating a Messy Dataset for Practice\n",
    "\n",
    "Let's create a realistic messy dataset to practice data preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a synthetic messy dataset\n",
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "\n",
    "# Create data with various issues\n",
    "data = {\n",
    "    'age': np.random.randint(18, 70, n_samples),\n",
    "    'income': np.random.normal(50000, 20000, n_samples),\n",
    "    'credit_score': np.random.randint(300, 850, n_samples),\n",
    "    'education': np.random.choice(['High School', 'Bachelor', 'Master', 'PhD'], n_samples),\n",
    "    'employment': np.random.choice(['Employed', 'Self-Employed', 'Unemployed'], n_samples),\n",
    "    'loan_approved': np.random.choice([0, 1], n_samples, p=[0.3, 0.7])\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Introduce missing values (realistic scenario)\n",
    "missing_indices_income = np.random.choice(df.index, size=20, replace=False)\n",
    "missing_indices_education = np.random.choice(df.index, size=15, replace=False)\n",
    "df.loc[missing_indices_income, 'income'] = np.nan\n",
    "df.loc[missing_indices_education, 'education'] = np.nan\n",
    "\n",
    "# Add some outliers\n",
    "df.loc[np.random.choice(df.index, 3), 'income'] = np.random.uniform(200000, 300000, 3)\n",
    "\n",
    "print(\"Created messy dataset:\")\n",
    "print(df.head(10))\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"\\nData types:\\n{df.dtypes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for data quality issues\n",
    "print(\"Data Quality Report:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())\n",
    "print(f\"\\nMissing Value Percentages:\")\n",
    "print((df.isnull().sum() / len(df) * 100).round(2))\n",
    "print(f\"\\nData Type Summary:\")\n",
    "print(df.dtypes.value_counts())\n",
    "print(f\"\\nBasic Statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Handling Missing Values\n",
    "\n",
    "### Strategies for Missing Data\n",
    "\n",
    "1. **Remove rows/columns**: If too much data is missing (>50%)\n",
    "2. **Imputation**: Fill missing values with:\n",
    "   - Mean/Median/Mode (for numerical)\n",
    "   - Most frequent category (for categorical)\n",
    "   - Forward/backward fill (for time series)\n",
    "   - Advanced: KNN or model-based imputation\n",
    "\n",
    "### Rule of Thumb\n",
    "\n",
    "- **< 5% missing**: Safe to impute\n",
    "- **5-25% missing**: Impute carefully, consider impact\n",
    "- **> 25% missing**: Consider dropping feature or collecting more data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing data patterns\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(df.isnull(), cbar=False, yticklabels=False, cmap='viridis')\n",
    "plt.title('Missing Data Pattern (Yellow = Missing)')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Samples')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Total missing values: {df.isnull().sum().sum()}\")\n",
    "print(f\"Complete rows: {df.dropna().shape[0]} out of {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "df_clean = df.copy()\n",
    "\n",
    "# For numerical: Impute with median (robust to outliers)\n",
    "income_imputer = SimpleImputer(strategy='median')\n",
    "df_clean['income'] = income_imputer.fit_transform(df_clean[['income']])\n",
    "\n",
    "# For categorical: Impute with most frequent\n",
    "education_imputer = SimpleImputer(strategy='most_frequent')\n",
    "df_clean['education'] = education_imputer.fit_transform(\n",
    "    df_clean[['education']]\n",
    ").ravel()\n",
    "\n",
    "print(\"After imputation:\")\n",
    "print(df_clean.isnull().sum())\n",
    "print(\"\\nNo more missing values!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Encoding Categorical Variables\n",
    "\n",
    "ML algorithms work with numbers, not text. We need to convert categorical data.\n",
    "\n",
    "### Two Main Approaches\n",
    "\n",
    "#### 1. Label Encoding (Ordinal)\n",
    "- Converts categories to integers: 0, 1, 2, ...\n",
    "- **Use when**: Categories have natural order (Low < Medium < High)\n",
    "- **Problem**: Implies order even when there isn't one\n",
    "\n",
    "#### 2. One-Hot Encoding (Nominal)\n",
    "- Creates binary column for each category\n",
    "- **Use when**: No natural order (Red, Blue, Green)\n",
    "- **Problem**: Can create many features (curse of dimensionality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Label Encoding (when order matters)\n",
    "# Let's say education has natural order\n",
    "education_order = {'High School': 0, 'Bachelor': 1, 'Master': 2, 'PhD': 3}\n",
    "df_clean['education_encoded'] = df_clean['education'].map(education_order)\n",
    "\n",
    "print(\"Label Encoding Example:\")\n",
    "print(df_clean[['education', 'education_encoded']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: One-Hot Encoding (when no natural order)\n",
    "# Employment status has no order\n",
    "employment_dummies = pd.get_dummies(\n",
    "    df_clean['employment'],\n",
    "    prefix='employment',\n",
    "    drop_first=True  # Avoid dummy variable trap\n",
    ")\n",
    "\n",
    "print(\"One-Hot Encoding Example:\")\n",
    "print(\"Original employment column:\")\n",
    "print(df_clean['employment'].head())\n",
    "print(\"\\nOne-hot encoded:\")\n",
    "print(employment_dummies.head())\n",
    "print(\"\\nNote: We dropped first category to avoid multicollinearity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all features\n",
    "df_encoded = df_clean.copy()\n",
    "df_encoded = pd.concat([df_encoded, employment_dummies], axis=1)\n",
    "\n",
    "# Drop original categorical columns\n",
    "df_encoded = df_encoded.drop(['employment', 'education'], axis=1)\n",
    "\n",
    "print(\"Fully encoded dataset:\")\n",
    "print(df_encoded.head())\n",
    "print(f\"\\nShape: {df_encoded.shape}\")\n",
    "print(f\"All numerical: {df_encoded.select_dtypes(include=[np.number]).shape[1] == df_encoded.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Scaling\n",
    "\n",
    "### Why Scale Features?\n",
    "\n",
    "Many ML algorithms are sensitive to feature scales:\n",
    "- Distance-based: KNN, SVM, K-Means\n",
    "- Gradient-based: Linear Regression, Neural Networks\n",
    "\n",
    "Example problem:\n",
    "- Age: 18-70 (range ~50)\n",
    "- Income: 20,000-200,000 (range ~180,000)\n",
    "\n",
    "Income will dominate distance calculations!\n",
    "\n",
    "### Scaling Methods\n",
    "\n",
    "#### 1. StandardScaler (Z-score normalization)\n",
    "- Formula: (x - mean) / std\n",
    "- Result: Mean=0, Std=1\n",
    "- **Use when**: Features are normally distributed\n",
    "\n",
    "#### 2. MinMaxScaler\n",
    "- Formula: (x - min) / (max - min)\n",
    "- Result: Range [0, 1]\n",
    "- **Use when**: Bounded range needed\n",
    "\n",
    "#### 3. RobustScaler\n",
    "- Uses median and IQR (interquartile range)\n",
    "- **Use when**: Data has outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different scaling methods\n",
    "feature_to_scale = df_encoded[['age', 'income', 'credit_score']].copy()\n",
    "\n",
    "# Original data\n",
    "print(\"Original Data Statistics:\")\n",
    "print(feature_to_scale.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StandardScaler\n",
    "standard_scaler = StandardScaler()\n",
    "scaled_standard = pd.DataFrame(\n",
    "    standard_scaler.fit_transform(feature_to_scale),\n",
    "    columns=feature_to_scale.columns\n",
    ")\n",
    "\n",
    "# MinMaxScaler\n",
    "minmax_scaler = MinMaxScaler()\n",
    "scaled_minmax = pd.DataFrame(\n",
    "    minmax_scaler.fit_transform(feature_to_scale),\n",
    "    columns=feature_to_scale.columns\n",
    ")\n",
    "\n",
    "# RobustScaler\n",
    "robust_scaler = RobustScaler()\n",
    "scaled_robust = pd.DataFrame(\n",
    "    robust_scaler.fit_transform(feature_to_scale),\n",
    "    columns=feature_to_scale.columns\n",
    ")\n",
    "\n",
    "print(\"\\nStandardScaler (mean≈0, std≈1):\")\n",
    "print(scaled_standard.describe().round(2))\n",
    "print(\"\\nMinMaxScaler (range [0,1]):\")\n",
    "print(scaled_minmax.describe().round(2))\n",
    "print(\"\\nRobustScaler (uses median & IQR):\")\n",
    "print(scaled_robust.describe().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the effect of scaling\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Original\n",
    "feature_to_scale.boxplot(ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Original Data')\n",
    "axes[0, 0].set_ylabel('Value')\n",
    "\n",
    "# StandardScaler\n",
    "scaled_standard.boxplot(ax=axes[0, 1])\n",
    "axes[0, 1].set_title('StandardScaler')\n",
    "axes[0, 1].set_ylabel('Scaled Value')\n",
    "\n",
    "# MinMaxScaler\n",
    "scaled_minmax.boxplot(ax=axes[1, 0])\n",
    "axes[1, 0].set_title('MinMaxScaler')\n",
    "axes[1, 0].set_ylabel('Scaled Value')\n",
    "\n",
    "# RobustScaler\n",
    "scaled_robust.boxplot(ax=axes[1, 1])\n",
    "axes[1, 1].set_title('RobustScaler')\n",
    "axes[1, 1].set_ylabel('Scaled Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice how all features are on similar scales after transformation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train/Test Split: The Right Way\n",
    "\n",
    "### The Golden Rule\n",
    "\n",
    "**ALWAYS split BEFORE any preprocessing!**\n",
    "\n",
    "### Why?\n",
    "\n",
    "To avoid **data leakage** - when information from test set influences training.\n",
    "\n",
    "### Wrong Way (Data Leakage)\n",
    "\n",
    "```python\n",
    "# WRONG! Don't do this\n",
    "scaler.fit(X)  # Fit on ALL data\n",
    "X_scaled = scaler.transform(X)\n",
    "X_train, X_test = train_test_split(X_scaled)  # Then split\n",
    "```\n",
    "\n",
    "Problem: Test data statistics influenced the scaling!\n",
    "\n",
    "### Right Way\n",
    "\n",
    "```python\n",
    "# RIGHT! Do this\n",
    "X_train, X_test = train_test_split(X)  # Split first\n",
    "scaler.fit(X_train)  # Fit only on training data\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)  # Apply same transformation\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X = df_encoded.drop('loan_approved', axis=1)\n",
    "y = df_encoded['loan_approved']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nFeature columns: {list(X.columns)}\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORRECT: Split first, then preprocess\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,  # 20% for testing\n",
    "    random_state=42,  # Reproducibility\n",
    "    stratify=y  # Maintain class proportions\n",
    ")\n",
    "\n",
    "print(\"Data Split:\")\n",
    "print(f\"Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X):.1%})\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(X):.1%})\")\n",
    "\n",
    "print(f\"\\nClass distribution in training set:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(f\"\\nClass distribution in test set:\")\n",
    "print(y_test.value_counts(normalize=True))\n",
    "print(\"\\nNote: Proportions are similar due to stratify=y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now scale features (AFTER splitting)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit scaler ONLY on training data\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Transform both sets using the same scaler\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Scaling completed correctly!\")\n",
    "print(f\"\\nTraining set mean: {X_train_scaled.mean():.6f}\")\n",
    "print(f\"Training set std: {X_train_scaled.std():.6f}\")\n",
    "print(f\"\\nTest set mean: {X_test_scaled.mean():.6f}\")\n",
    "print(f\"Test set std: {X_test_scaled.std():.6f}\")\n",
    "print(\"\\nNote: Test stats differ slightly - this is expected and correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train/Validation/Test Split\n",
    "\n",
    "For hyperparameter tuning, we need three sets:\n",
    "\n",
    "- **Training Set (60-70%)**: Train the model\n",
    "- **Validation Set (10-20%)**: Tune hyperparameters\n",
    "- **Test Set (10-20%)**: Final evaluation (never touch until the end!)\n",
    "\n",
    "### Why Three Sets?\n",
    "\n",
    "- Train on training set\n",
    "- Evaluate different hyperparameters on validation set\n",
    "- Once you pick best model, test ONCE on test set\n",
    "- Prevents overfitting to validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create three-way split\n",
    "# First split: Separate test set (20%)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Second split: Divide remaining into train (75%) and validation (25%)\n",
    "# This gives us 60% train, 20% val, 20% test overall\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(\"Three-way split:\")\n",
    "print(f\"Training: {len(X_train)} samples ({len(X_train)/len(X):.1%})\")\n",
    "print(f\"Validation: {len(X_val)} samples ({len(X_val)/len(X):.1%})\")\n",
    "print(f\"Test: {len(X_test)} samples ({len(X_test)/len(X):.1%})\")\n",
    "print(f\"Total: {len(X_train) + len(X_val) + len(X_test)} samples\")\n",
    "\n",
    "# Visualize the split\n",
    "sizes = [len(X_train), len(X_val), len(X_test)]\n",
    "labels = ['Training\\n60%', 'Validation\\n20%', 'Test\\n20%']\n",
    "colors = ['#66c2a5', '#fc8d62', '#8da0cb']\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.pie(sizes, labels=labels, colors=colors, autopct='%d', startangle=90)\n",
    "plt.title('Train/Validation/Test Split', fontsize=14)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Demonstrating Data Leakage Impact\n",
    "\n",
    "Let's see why data leakage matters by comparing correct vs incorrect approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRONG: Scale before split (data leakage)\n",
    "scaler_wrong = StandardScaler()\n",
    "X_scaled_wrong = scaler_wrong.fit_transform(X)  # Fit on ALL data\n",
    "X_train_wrong, X_test_wrong, y_train_wrong, y_test_wrong = train_test_split(\n",
    "    X_scaled_wrong, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train model with leaked data\n",
    "model_wrong = LogisticRegression(max_iter=1000)\n",
    "model_wrong.fit(X_train_wrong, y_train_wrong)\n",
    "acc_wrong = accuracy_score(y_test_wrong, model_wrong.predict(X_test_wrong))\n",
    "\n",
    "# RIGHT: Split first, then scale\n",
    "X_train_right, X_test_right, y_train_right, y_test_right = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "scaler_right = StandardScaler()\n",
    "X_train_right_scaled = scaler_right.fit_transform(X_train_right)\n",
    "X_test_right_scaled = scaler_right.transform(X_test_right)\n",
    "\n",
    "# Train model correctly\n",
    "model_right = LogisticRegression(max_iter=1000)\n",
    "model_right.fit(X_train_right_scaled, y_train_right)\n",
    "acc_right = accuracy_score(y_test_right, model_right.predict(X_test_right_scaled))\n",
    "\n",
    "print(\"Comparison: Data Leakage Impact\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"WRONG (scale before split): Accuracy = {acc_wrong:.2%}\")\n",
    "print(f\"RIGHT (split before scale): Accuracy = {acc_right:.2%}\")\n",
    "print(f\"\\nDifference: {abs(acc_wrong - acc_right):.2%}\")\n",
    "print(\"\\nNote: In this example, difference is small, but with smaller\")\n",
    "print(\"datasets or more complex preprocessing, it can be significant!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Practice Exercises\n",
    "\n",
    "### Exercise 1: Handle Missing Data\n",
    "\n",
    "Create a dataset with 30% missing values in one column. Compare imputation strategies:\n",
    "- Mean imputation\n",
    "- Median imputation\n",
    "- Mode imputation\n",
    "\n",
    "Which works best for your data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: One-Hot Encoding\n",
    "\n",
    "Create a DataFrame with a categorical column that has 5 unique values. Apply one-hot encoding with and without `drop_first=True`. How many columns do you get in each case? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Scaling Comparison\n",
    "\n",
    "Load the Boston housing dataset (or California housing). Apply StandardScaler, MinMaxScaler, and RobustScaler. Train a simple model with each. Which scaler gives the best performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Stratified Splitting\n",
    "\n",
    "Create an imbalanced dataset (90% class 0, 10% class 1). Split it with and without `stratify`. Compare the class distributions in test sets. What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Data Preparation is Critical**:\n",
    "   - 80% of ML work is data preparation\n",
    "   - Clean data = better models\n",
    "\n",
    "2. **Handling Missing Values**:\n",
    "   - Impute with mean/median for numerical\n",
    "   - Impute with mode for categorical\n",
    "   - Consider dropping if >25% missing\n",
    "\n",
    "3. **Encoding Categorical Variables**:\n",
    "   - Label encoding for ordinal data\n",
    "   - One-hot encoding for nominal data\n",
    "   - Always drop_first=True to avoid multicollinearity\n",
    "\n",
    "4. **Feature Scaling**:\n",
    "   - StandardScaler: For normally distributed data\n",
    "   - MinMaxScaler: For bounded ranges\n",
    "   - RobustScaler: For data with outliers\n",
    "\n",
    "5. **Train/Test Split**:\n",
    "   - ALWAYS split BEFORE preprocessing\n",
    "   - Fit transformers on training data only\n",
    "   - Use stratify for imbalanced datasets\n",
    "   - Consider 3-way split for hyperparameter tuning\n",
    "\n",
    "### The Correct Workflow\n",
    "\n",
    "```python\n",
    "# 1. Load data\n",
    "X, y = load_data()\n",
    "\n",
    "# 2. Split FIRST (prevent data leakage)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# 3. Fit preprocessing on training data\n",
    "imputer.fit(X_train)\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# 4. Transform both sets\n",
    "X_train_processed = scaler.transform(imputer.transform(X_train))\n",
    "X_test_processed = scaler.transform(imputer.transform(X_test))\n",
    "\n",
    "# 5. Train model\n",
    "model.fit(X_train_processed, y_train)\n",
    "\n",
    "# 6. Evaluate on test set\n",
    "score = model.score(X_test_processed, y_test)\n",
    "```\n",
    "\n",
    "### Common Pitfalls to Avoid\n",
    "\n",
    "❌ Preprocessing before splitting (data leakage)  \n",
    "❌ Using mean when data has outliers  \n",
    "❌ Forgetting to scale when using distance-based algorithms  \n",
    "❌ Not using stratify with imbalanced data  \n",
    "❌ Touching test set before final evaluation  \n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the next module, we'll dive into:\n",
    "- **Linear Regression** in depth\n",
    "- Mathematical foundations\n",
    "- Interpretation of coefficients\n",
    "- Evaluating regression models\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [Scikit-learn Preprocessing Guide](https://scikit-learn.org/stable/modules/preprocessing.html)\n",
    "- [Pandas Missing Data Handling](https://pandas.pydata.org/docs/user_guide/missing_data.html)\n",
    "- [Feature Engineering Book by Alice Zheng](https://www.oreilly.com/library/view/feature-engineering-for/9781491953235/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
