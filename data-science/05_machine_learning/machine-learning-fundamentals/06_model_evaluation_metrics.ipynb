{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 06: Model Evaluation Metrics\n",
    "\n",
    "**Difficulty**: \u2b50\u2b50 Intermediate  \n",
    "**Estimated Time**: 90 minutes  \n",
    "**Prerequisites**: \n",
    "- [Module 03: Linear Regression](03_linear_regression.ipynb)\n",
    "- [Module 04: Logistic Regression](04_logistic_regression.ipynb)\n",
    "- [Module 05: Decision Trees](05_decision_trees.ipynb)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Understand and calculate classification metrics (accuracy, precision, recall, F1)\n",
    "2. Interpret confusion matrices for multi-class problems\n",
    "3. Use ROC curves and AUC for model comparison\n",
    "4. Apply regression metrics (R\u00b2, MSE, RMSE, MAE, MAPE) appropriately\n",
    "5. Choose the right metric based on problem context\n",
    "6. Handle imbalanced datasets with appropriate metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report,\n",
    "    roc_curve, roc_auc_score, precision_recall_curve, auc,\n",
    "    mean_squared_error, mean_absolute_error, r2_score,\n",
    "    mean_absolute_percentage_error\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "np.random.seed(42)\n",
    "\n",
    "print('All libraries imported successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classification Metrics Overview\n",
    "\n",
    "### The Confusion Matrix\n",
    "\n",
    "Foundation of all classification metrics:\n",
    "\n",
    "```\n",
    "                 Predicted\n",
    "               Negative  Positive\n",
    "Actual Negative   TN        FP\n",
    "       Positive   FN        TP\n",
    "```\n",
    "\n",
    "- **TN** (True Negative): Correctly predicted negative\n",
    "- **TP** (True Positive): Correctly predicted positive\n",
    "- **FN** (False Negative): Actually positive, predicted negative (Type II error)\n",
    "- **FP** (False Positive): Actually negative, predicted positive (Type I error)\n",
    "\n",
    "### Key Metrics\n",
    "\n",
    "1. **Accuracy** = (TP + TN) / (TP + TN + FP + FN)\n",
    "   - Overall correctness\n",
    "   - Good when classes are balanced\n",
    "\n",
    "2. **Precision** = TP / (TP + FP)\n",
    "   - Of predicted positives, how many are actually positive?\n",
    "   - Important when false positives are costly\n",
    "\n",
    "3. **Recall (Sensitivity)** = TP / (TP + FN)\n",
    "   - Of actual positives, how many did we find?\n",
    "   - Important when false negatives are costly\n",
    "\n",
    "4. **F1-Score** = 2 \u00d7 (Precision \u00d7 Recall) / (Precision + Recall)\n",
    "   - Harmonic mean of precision and recall\n",
    "   - Good for imbalanced datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load breast cancer dataset\n",
    "cancer = datasets.load_breast_cancer()\n",
    "X, y = cancer.data, cancer.target\n",
    "\n",
    "# Split and train\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model = LogisticRegression(max_iter=10000, random_state=42)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "print('Breast Cancer Classification Model Trained')\n",
    "print(f'Classes: {cancer.target_names}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate all metrics\n",
    "print('Classification Metrics:')\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Recall:    {recall_score(y_test, y_pred):.4f}\")\n",
    "print(f\"F1-Score:  {f1_score(y_test, y_pred):.4f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "print(f\"\\nTN={cm[0,0]}, FP={cm[0,1]}\")\n",
    "print(f\"FN={cm[1,0]}, TP={cm[1,1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ROC Curve and AUC\n",
    "\n",
    "**ROC (Receiver Operating Characteristic)** curve plots:\n",
    "- X-axis: False Positive Rate (FPR) = FP / (FP + TN)\n",
    "- Y-axis: True Positive Rate (TPR) = TP / (TP + FN) = Recall\n",
    "\n",
    "**AUC (Area Under Curve)**:\n",
    "- Perfect classifier: AUC = 1.0\n",
    "- Random classifier: AUC = 0.5\n",
    "- Worse than random: AUC < 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get probability predictions\n",
    "y_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Calculate ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, 'b-', linewidth=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], 'r--', linewidth=2, label='Random (AUC = 0.5)')\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate (Recall)', fontsize=12)\n",
    "plt.title('ROC Curve', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'AUC Score: {roc_auc:.4f}')\n",
    "print('Interpretation: Higher AUC = Better model discrimination')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Practice Exercises\n",
    "\n",
    "### Exercise 1: Imbalanced Dataset\n",
    "\n",
    "Create an imbalanced dataset (90% class 0, 10% class 1). Calculate accuracy, precision, recall, and F1. Why might accuracy be misleading?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Threshold Tuning\n",
    "\n",
    "Using the breast cancer model, try different probability thresholds (0.3, 0.5, 0.7). How do precision and recall change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Multi-Class Metrics\n",
    "\n",
    "Train a classifier on the Iris dataset (3 classes). Calculate macro and micro-averaged precision, recall, and F1. What's the difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Classification Metrics**:\n",
    "   - Accuracy: Overall correctness (good for balanced data)\n",
    "   - Precision: Minimize false positives\n",
    "   - Recall: Minimize false negatives\n",
    "   - F1: Balance precision and recall\n",
    "\n",
    "2. **Choosing Metrics**:\n",
    "   - Medical diagnosis: High recall (don't miss diseases)\n",
    "   - Spam detection: High precision (don't block important emails)\n",
    "   - Balanced problem: F1-score or accuracy\n",
    "\n",
    "3. **ROC-AUC**:\n",
    "   - Threshold-independent metric\n",
    "   - Compares models at all thresholds\n",
    "   - Higher AUC = better discrimination\n",
    "\n",
    "4. **Regression Metrics**:\n",
    "   - R\u00b2: Variance explained (0-1)\n",
    "   - RMSE: Average error (same units as target)\n",
    "   - MAE: Robust to outliers\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In Module 07, we'll explore:\n",
    "- Cross-validation techniques\n",
    "- Hyperparameter tuning (GridSearchCV, RandomizedSearchCV)\n",
    "- Learning curves and model selection\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [Scikit-learn Metrics Guide](https://scikit-learn.org/stable/modules/model_evaluation.html)\n",
    "- [Understanding ROC Curves](https://www.youtube.com/watch?v=4jRBRDbJemM)\n",
    "- [Precision vs Recall](https://towardsdatascience.com/precision-vs-recall-386cf9f89488)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}