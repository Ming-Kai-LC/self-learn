{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 06: Model Evaluation Metrics\n",
    "\n",
    "**Difficulty**: ⭐⭐ Intermediate  \n",
    "**Estimated Time**: 70 minutes  \n",
    "**Prerequisites**: \n",
    "- [Module 03: Linear Regression](03_linear_regression.ipynb)\n",
    "- [Module 04: Logistic Regression](04_logistic_regression.ipynb)\n",
    "- [Module 05: Decision Trees](05_decision_trees.ipynb)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Understand and calculate classification metrics (accuracy, precision, recall, F1-score)\n",
    "2. Interpret confusion matrices for multiclass problems\n",
    "3. Use ROC curves and AUC for model comparison\n",
    "4. Apply regression metrics appropriately (MAE, MSE, RMSE, R²)\n",
    "5. Choose the right metric for your specific problem\n",
    "6. Understand the trade-offs between different metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Multiple Metrics?\n",
    "\n",
    "**Accuracy alone is not enough!**\n",
    "\n",
    "### Example: Cancer Detection\n",
    "Imagine a dataset with:\n",
    "- 990 healthy patients (99%)\n",
    "- 10 cancer patients (1%)\n",
    "\n",
    "A lazy model that predicts \"healthy\" for everyone:\n",
    "- **Accuracy**: 99% (looks great!)\n",
    "- **Problem**: Misses all 10 cancer cases (terrible!)\n",
    "\n",
    "### The Solution\n",
    "Use multiple metrics that capture different aspects of performance:\n",
    "- **Precision**: When model predicts positive, how often is it correct?\n",
    "- **Recall**: Of all actual positives, how many does model find?\n",
    "- **F1-Score**: Balance between precision and recall\n",
    "- **ROC/AUC**: Overall discriminative ability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Visualization settings\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"✓ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classification Metrics Deep Dive\n",
    "\n",
    "### Confusion Matrix Components\n",
    "\n",
    "```\n",
    "                 Predicted\n",
    "              Negative  Positive\n",
    "Actual Negative   TN       FP\n",
    "       Positive   FN       TP\n",
    "```\n",
    "\n",
    "- **TP (True Positive)**: Correctly predicted positive\n",
    "- **TN (True Negative)**: Correctly predicted negative\n",
    "- **FP (False Positive)**: Wrongly predicted positive (Type I error)\n",
    "- **FN (False Negative)**: Wrongly predicted negative (Type II error)\n",
    "\n",
    "### Key Metrics\n",
    "\n",
    "1. **Accuracy** = (TP + TN) / (TP + TN + FP + FN)\n",
    "   - Overall correctness\n",
    "   - **Problem**: Misleading with imbalanced classes\n",
    "\n",
    "2. **Precision** = TP / (TP + FP)\n",
    "   - Of predicted positives, how many are actually positive?\n",
    "   - **Use when**: False positives are costly\n",
    "   - **Example**: Spam detection (don't want to mark important emails as spam)\n",
    "\n",
    "3. **Recall (Sensitivity)** = TP / (TP + FN)\n",
    "   - Of actual positives, how many did we find?\n",
    "   - **Use when**: False negatives are costly\n",
    "   - **Example**: Cancer detection (don't want to miss cancer cases)\n",
    "\n",
    "4. **F1-Score** = 2 × (Precision × Recall) / (Precision + Recall)\n",
    "   - Harmonic mean of precision and recall\n",
    "   - **Use when**: Need balance between precision and recall\n",
    "\n",
    "5. **Specificity** = TN / (TN + FP)\n",
    "   - Of actual negatives, how many did we correctly identify?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load breast cancer dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "cancer_df = pd.read_csv('data/sample/breast_cancer.csv')\n",
    "\n",
    "# Prepare data\n",
    "X = cancer_df.drop(['target', 'diagnosis'], axis=1)\n",
    "y = cancer_df['target']\n",
    "\n",
    "# Split and scale\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train model\n",
    "model = LogisticRegression(random_state=42, max_iter=10000)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_proba = model.predict_proba(X_test_scaled)\n",
    "\n",
    "print(\"✓ Model trained and predictions made!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate all metrics\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                            f1_score, confusion_matrix, classification_report)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Classification Metrics:\")\n",
    "print(f\"Accuracy:  {accuracy:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall:    {recall:.3f}\")\n",
    "print(f\"F1-Score:  {f1:.3f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print(f\"\\nConfusion Matrix Breakdown:\")\n",
    "print(f\"True Negatives (TN):  {tn}\")\n",
    "print(f\"False Positives (FP): {fp}\")\n",
    "print(f\"False Negatives (FN): {fn}\")\n",
    "print(f\"True Positives (TP):  {tp}\")\n",
    "\n",
    "# Specificity\n",
    "specificity = tn / (tn + fp)\n",
    "print(f\"\\nSpecificity: {specificity:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Raw counts\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=axes[0],\n",
    "           xticklabels=['Malignant', 'Benign'],\n",
    "           yticklabels=['Malignant', 'Benign'])\n",
    "axes[0].set_xlabel('Predicted', fontsize=12)\n",
    "axes[0].set_ylabel('Actual', fontsize=12)\n",
    "axes[0].set_title('Confusion Matrix (Counts)', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Normalized (percentages)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.1%', cmap='Greens', cbar=False, ax=axes[1],\n",
    "           xticklabels=['Malignant', 'Benign'],\n",
    "           yticklabels=['Malignant', 'Benign'])\n",
    "axes[1].set_xlabel('Predicted', fontsize=12)\n",
    "axes[1].set_ylabel('Actual', fontsize=12)\n",
    "axes[1].set_title('Confusion Matrix (Percentages)', fontsize=13, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report\n",
    "print(\"Detailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Malignant', 'Benign']))\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(f\"- Support: Number of actual samples in each class\")\n",
    "print(f\"- Macro avg: Unweighted mean (treats all classes equally)\")\n",
    "print(f\"- Weighted avg: Weighted by number of samples in each class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ROC Curve and AUC\n",
    "\n",
    "### ROC (Receiver Operating Characteristic) Curve\n",
    "- Plots True Positive Rate (Recall) vs False Positive Rate\n",
    "- Shows performance across all classification thresholds\n",
    "- Closer to top-left corner = better\n",
    "\n",
    "### AUC (Area Under Curve)\n",
    "- Single number summarizing ROC curve\n",
    "- Range: 0 to 1\n",
    "- **AUC = 1.0**: Perfect classifier\n",
    "- **AUC = 0.5**: Random guessing (diagonal line)\n",
    "- **AUC < 0.5**: Worse than random\n",
    "\n",
    "### Interpretation\n",
    "- AUC = probability that model ranks a random positive higher than a random negative\n",
    "- Useful for comparing models\n",
    "- Threshold-independent metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROC curve\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Get probabilities for positive class\n",
    "y_proba_pos = y_proba[:, 1]\n",
    "\n",
    "# Calculate ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba_pos)\n",
    "auc_score = roc_auc_score(y_test, y_proba_pos)\n",
    "\n",
    "print(f\"AUC Score: {auc_score:.3f}\")\n",
    "print(f\"\\nInterpretation: The model has {auc_score:.1%} probability of ranking\")\n",
    "print(f\"a random benign sample higher than a random malignant sample.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curve\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(fpr, tpr, linewidth=3, label=f'Model (AUC = {auc_score:.3f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random Guessing (AUC = 0.5)')\n",
    "\n",
    "# Mark the point for default threshold (0.5)\n",
    "default_idx = np.argmin(np.abs(thresholds - 0.5))\n",
    "plt.plot(fpr[default_idx], tpr[default_idx], 'ro', markersize=10, \n",
    "        label=f'Threshold = 0.5')\n",
    "\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate (Recall)', fontsize=12)\n",
    "plt.title('ROC Curve\\nCloser to top-left = better', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11, loc='lower right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Points:\")\n",
    "print(\"- Top-left corner (0,1): Perfect classifier\")\n",
    "print(\"- Diagonal line: Random guessing\")\n",
    "print(\"- Our curve is well above diagonal: Good performance!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Precision-Recall Trade-off\n",
    "\n",
    "**Key Insight**: You can't maximize both precision and recall simultaneously!\n",
    "\n",
    "### The Trade-off\n",
    "- **High threshold** → High precision, Low recall (conservative)\n",
    "- **Low threshold** → Low precision, High recall (aggressive)\n",
    "\n",
    "### Which to Prioritize?\n",
    "\n",
    "**Prioritize Precision when** false positives are costly:\n",
    "- Spam detection (don't mark important emails as spam)\n",
    "- Video recommendations (don't show inappropriate content)\n",
    "\n",
    "**Prioritize Recall when** false negatives are costly:\n",
    "- Cancer detection (don't miss cancer cases)\n",
    "- Fraud detection (catch as many frauds as possible)\n",
    "- Security screening (better safe than sorry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate precision-recall trade-off\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precisions, recalls, pr_thresholds = precision_recall_curve(y_test, y_proba_pos)\n",
    "\n",
    "# Find threshold for different scenarios\n",
    "# Scenario 1: Maximize precision (conservative)\n",
    "high_precision_idx = np.argmax(precisions >= 0.95)\n",
    "high_prec_threshold = pr_thresholds[high_precision_idx]\n",
    "high_prec_recall = recalls[high_precision_idx]\n",
    "\n",
    "# Scenario 2: Maximize recall (aggressive)\n",
    "high_recall_idx = np.argmax(recalls >= 0.95)\n",
    "high_rec_threshold = pr_thresholds[high_recall_idx]\n",
    "high_rec_precision = precisions[high_recall_idx]\n",
    "\n",
    "print(\"Scenario 1: Conservative (High Precision)\")\n",
    "print(f\"  Threshold: {high_prec_threshold:.3f}\")\n",
    "print(f\"  Precision: {precisions[high_precision_idx]:.1%}\")\n",
    "print(f\"  Recall: {high_prec_recall:.1%}\")\n",
    "print(f\"  Trade-off: Very confident predictions, but miss some cases\\n\")\n",
    "\n",
    "print(\"Scenario 2: Aggressive (High Recall)\")\n",
    "print(f\"  Threshold: {high_rec_threshold:.3f}\")\n",
    "print(f\"  Precision: {high_rec_precision:.1%}\")\n",
    "print(f\"  Recall: {recalls[high_recall_idx]:.1%}\")\n",
    "print(f\"  Trade-off: Catch most cases, but more false alarms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot precision-recall curve\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(recalls, precisions, linewidth=3, label='Precision-Recall Curve')\n",
    "plt.scatter([high_prec_recall], [precisions[high_precision_idx]], \n",
    "           s=200, c='green', marker='*', \n",
    "           label='Conservative (High Precision)', zorder=5)\n",
    "plt.scatter([recalls[high_recall_idx]], [high_rec_precision], \n",
    "           s=200, c='red', marker='*', \n",
    "           label='Aggressive (High Recall)', zorder=5)\n",
    "\n",
    "plt.xlabel('Recall', fontsize=12)\n",
    "plt.ylabel('Precision', fontsize=12)\n",
    "plt.title('Precision-Recall Curve\\nShows trade-off between metrics', \n",
    "         fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multiclass Metrics\n",
    "\n",
    "For problems with more than 2 classes, metrics are calculated:\n",
    "\n",
    "### Averaging Strategies\n",
    "\n",
    "1. **Macro Average**: Unweighted mean\n",
    "   - Treats all classes equally\n",
    "   - Good for balanced datasets\n",
    "\n",
    "2. **Weighted Average**: Weighted by class frequency\n",
    "   - Accounts for class imbalance\n",
    "   - Usually reported as default\n",
    "\n",
    "3. **Micro Average**: Aggregate contributions of all classes\n",
    "   - Gives more weight to larger classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Iris dataset for multiclass classification\n",
    "iris_df = pd.read_csv('data/sample/iris.csv')\n",
    "\n",
    "feature_cols = ['sepal length (cm)', 'sepal width (cm)', \n",
    "                'petal length (cm)', 'petal width (cm)']\n",
    "X_iris = iris_df[feature_cols]\n",
    "y_iris = iris_df['species']\n",
    "\n",
    "# Split and scale\n",
    "X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(\n",
    "    X_iris, y_iris, test_size=0.3, random_state=42, stratify=y_iris\n",
    ")\n",
    "\n",
    "scaler_iris = StandardScaler()\n",
    "X_train_iris_scaled = scaler_iris.fit_transform(X_train_iris)\n",
    "X_test_iris_scaled = scaler_iris.transform(X_test_iris)\n",
    "\n",
    "# Train model\n",
    "model_iris = LogisticRegression(random_state=42, max_iter=10000)\n",
    "model_iris.fit(X_train_iris_scaled, y_train_iris)\n",
    "\n",
    "# Predictions\n",
    "y_pred_iris = model_iris.predict(X_test_iris_scaled)\n",
    "\n",
    "print(\"✓ Multiclass model trained!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics with different averaging\n",
    "print(\"Multiclass Metrics:\\n\")\n",
    "\n",
    "for average in ['macro', 'weighted', 'micro']:\n",
    "    prec = precision_score(y_test_iris, y_pred_iris, average=average)\n",
    "    rec = recall_score(y_test_iris, y_pred_iris, average=average)\n",
    "    f1 = f1_score(y_test_iris, y_pred_iris, average=average)\n",
    "    \n",
    "    print(f\"{average.capitalize()} Average:\")\n",
    "    print(f\"  Precision: {prec:.3f}\")\n",
    "    print(f\"  Recall:    {rec:.3f}\")\n",
    "    print(f\"  F1-Score:  {f1:.3f}\\n\")\n",
    "\n",
    "# Detailed report\n",
    "print(\"\\nPer-Class Report:\")\n",
    "print(classification_report(y_test_iris, y_pred_iris))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiclass confusion matrix\n",
    "cm_iris = confusion_matrix(y_test_iris, y_pred_iris)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_iris, annot=True, fmt='d', cmap='YlGnBu', cbar=True,\n",
    "           xticklabels=['Class 0', 'Class 1', 'Class 2'],\n",
    "           yticklabels=['Class 0', 'Class 1', 'Class 2'])\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.ylabel('Actual', fontsize=12)\n",
    "plt.title('Multiclass Confusion Matrix\\n(Iris Dataset)', \n",
    "         fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Diagonal values = correct predictions\")\n",
    "print(\"Off-diagonal values = misclassifications\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Regression Metrics\n",
    "\n",
    "### Common Regression Metrics\n",
    "\n",
    "1. **MAE (Mean Absolute Error)**\n",
    "   - Average absolute difference\n",
    "   - Same units as target\n",
    "   - Less sensitive to outliers\n",
    "\n",
    "2. **MSE (Mean Squared Error)**\n",
    "   - Average squared difference\n",
    "   - Penalizes large errors more\n",
    "   - Units are squared\n",
    "\n",
    "3. **RMSE (Root Mean Squared Error)**\n",
    "   - Square root of MSE\n",
    "   - Same units as target\n",
    "   - Popular and interpretable\n",
    "\n",
    "4. **R² (Coefficient of Determination)**\n",
    "   - Proportion of variance explained\n",
    "   - Range: -∞ to 1 (1 is perfect)\n",
    "   - 0 means model is as good as predicting mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load regression dataset\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "housing_df = pd.read_csv('data/sample/california_housing.csv')\n",
    "\n",
    "X_reg = housing_df.drop('median_house_value', axis=1)\n",
    "y_reg = housing_df['median_house_value']\n",
    "\n",
    "# Split data\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Train model\n",
    "model_reg = LinearRegression()\n",
    "model_reg.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "# Predictions\n",
    "y_pred_reg = model_reg.predict(X_test_reg)\n",
    "\n",
    "print(\"✓ Regression model trained!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate regression metrics\n",
    "mae = mean_absolute_error(y_test_reg, y_pred_reg)\n",
    "mse = mean_squared_error(y_test_reg, y_pred_reg)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test_reg, y_pred_reg)\n",
    "\n",
    "print(\"Regression Metrics:\")\n",
    "print(f\"MAE:  ${mae:,.2f}\")\n",
    "print(f\"MSE:  ${mse:,.2f}\")\n",
    "print(f\"RMSE: ${rmse:,.2f}\")\n",
    "print(f\"R²:   {r2:.3f}\")\n",
    "\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"- On average, predictions are off by ${mae:,.2f} (MAE)\")\n",
    "print(f\"- Typical prediction error is ${rmse:,.2f} (RMSE)\")\n",
    "print(f\"- Model explains {r2*100:.1f}% of variance (R²)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize residuals (errors)\n",
    "residuals = y_test_reg - y_pred_reg\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Residual plot\n",
    "axes[0].scatter(y_pred_reg, residuals, alpha=0.3, s=20)\n",
    "axes[0].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[0].set_xlabel('Predicted Values', fontsize=12)\n",
    "axes[0].set_ylabel('Residuals (Actual - Predicted)', fontsize=12)\n",
    "axes[0].set_title('Residual Plot\\n(Should be randomly scattered around 0)', \n",
    "                 fontsize=13, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residual distribution\n",
    "axes[1].hist(residuals, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(x=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[1].set_xlabel('Residuals', fontsize=12)\n",
    "axes[1].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1].set_title('Residual Distribution\\n(Should be normal around 0)', \n",
    "                 fontsize=13, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Residual Statistics:\")\n",
    "print(f\"Mean: ${residuals.mean():,.2f} (should be close to 0)\")\n",
    "print(f\"Std:  ${residuals.std():,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Choosing the Right Metric\n",
    "\n",
    "For each scenario, identify which metric to prioritize and why:\n",
    "\n",
    "1. Email spam detection\n",
    "2. Credit card fraud detection\n",
    "3. Medical diagnosis for rare disease\n",
    "4. Product recommendation system\n",
    "5. House price prediction\n",
    "\n",
    "Write your answers as comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answers:\n",
    "# 1. \n",
    "# 2. \n",
    "# 3. \n",
    "# 4. \n",
    "# 5. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: ROC Curve Comparison\n",
    "\n",
    "Compare two models using ROC curves:\n",
    "\n",
    "1. Train both LogisticRegression and DecisionTreeClassifier on breast cancer data\n",
    "2. Plot both ROC curves on the same graph\n",
    "3. Calculate AUC for both\n",
    "4. Which model performs better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Finding Optimal Threshold\n",
    "\n",
    "For the breast cancer model:\n",
    "1. Try different thresholds from 0.1 to 0.9\n",
    "2. Calculate precision, recall, and F1-score for each\n",
    "3. Plot all three metrics vs threshold\n",
    "4. Find the threshold that maximizes F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Regression Metrics Comparison\n",
    "\n",
    "On the diabetes dataset:\n",
    "1. Train LinearRegression and DecisionTreeRegressor (max_depth=5)\n",
    "2. Calculate MAE, RMSE, and R² for both\n",
    "3. Create a comparison table\n",
    "4. Which model is better according to different metrics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Classification Metrics**:\n",
    "   - **Accuracy**: Overall correctness (misleading with imbalanced data)\n",
    "   - **Precision**: Of predicted positive, how many correct? (minimize FP)\n",
    "   - **Recall**: Of actual positive, how many found? (minimize FN)\n",
    "   - **F1-Score**: Balance of precision and recall\n",
    "   - **ROC-AUC**: Overall discriminative ability (threshold-independent)\n",
    "\n",
    "2. **Metric Selection**:\n",
    "   - **High FP cost** → Prioritize precision (spam detection)\n",
    "   - **High FN cost** → Prioritize recall (cancer detection)\n",
    "   - **Balance both** → Use F1-score\n",
    "   - **Compare models** → Use ROC-AUC\n",
    "\n",
    "3. **Regression Metrics**:\n",
    "   - **MAE**: Average error (robust to outliers)\n",
    "   - **RMSE**: Typical error (penalizes large errors)\n",
    "   - **R²**: Variance explained (0-1 scale)\n",
    "\n",
    "4. **Best Practices**:\n",
    "   - Never rely on accuracy alone\n",
    "   - Consider class imbalance\n",
    "   - Understand business costs of errors\n",
    "   - Use multiple metrics for comprehensive evaluation\n",
    "   - Visualize confusion matrices and ROC curves\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In **Module 07: Cross-Validation and Hyperparameter Tuning**, you'll learn:\n",
    "- K-fold cross-validation for robust evaluation\n",
    "- Grid search and random search\n",
    "- Hyperparameter optimization\n",
    "- Avoiding overfitting in model selection\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [Precision and Recall - StatQuest](https://www.youtube.com/watch?v=Kdsp6soqA7o)\n",
    "- [ROC and AUC - StatQuest](https://www.youtube.com/watch?v=4jRBRDbJemM)\n",
    "- [scikit-learn Metrics Guide](https://scikit-learn.org/stable/modules/model_evaluation.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
