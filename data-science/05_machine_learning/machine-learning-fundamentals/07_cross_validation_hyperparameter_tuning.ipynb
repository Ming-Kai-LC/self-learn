{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 07: Cross-Validation and Hyperparameter Tuning\n",
    "\n",
    "**Difficulty**: ⭐⭐ Intermediate  \n",
    "**Estimated Time**: 70 minutes  \n",
    "**Prerequisites**: \n",
    "- [Module 02: Data Preparation and Train-Test Split](02_data_preparation_train_test_split.ipynb)\n",
    "- [Module 06: Model Evaluation Metrics](06_model_evaluation_metrics.ipynb)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Understand why cross-validation is needed and how it works\n",
    "2. Implement K-fold and Stratified K-fold cross-validation\n",
    "3. Use GridSearchCV for exhaustive hyperparameter search\n",
    "4. Use RandomizedSearchCV for faster hyperparameter tuning\n",
    "5. Avoid common pitfalls in model selection\n",
    "6. Choose optimal hyperparameters for your models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Cross-Validation?\n",
    "\n",
    "### The Problem with Single Train-Test Split\n",
    "\n",
    "**Scenario**: You split data 70-30 and get 85% accuracy.\n",
    "\n",
    "**Questions**:\n",
    "- Was it luck? What if we chose a different split?\n",
    "- Did we happen to get an \"easy\" test set?\n",
    "- How confident can we be in this 85%?\n",
    "\n",
    "### The Solution: Cross-Validation\n",
    "\n",
    "**Key Idea**: Test model on multiple different splits!\n",
    "\n",
    "**Benefits**:\n",
    "1. **More reliable estimate** of model performance\n",
    "2. **Uses all data** for both training and validation\n",
    "3. **Reduces variance** in performance estimates\n",
    "4. **Detects overfitting** more reliably\n",
    "\n",
    "### Real-World Analogy\n",
    "Instead of taking one practice exam, take five different practice exams. Your average score is a better indicator of your true ability!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Visualization settings\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"✓ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. K-Fold Cross-Validation\n",
    "\n",
    "### How It Works\n",
    "\n",
    "**K-Fold Process** (typically K=5 or K=10):\n",
    "\n",
    "```\n",
    "Fold 1: [TEST][TRAIN][TRAIN][TRAIN][TRAIN]\n",
    "Fold 2: [TRAIN][TEST][TRAIN][TRAIN][TRAIN]\n",
    "Fold 3: [TRAIN][TRAIN][TEST][TRAIN][TRAIN]\n",
    "Fold 4: [TRAIN][TRAIN][TRAIN][TEST][TRAIN]\n",
    "Fold 5: [TRAIN][TRAIN][TRAIN][TRAIN][TEST]\n",
    "```\n",
    "\n",
    "**Steps**:\n",
    "1. Split data into K equal parts (\"folds\")\n",
    "2. For each fold:\n",
    "   - Use that fold as test set\n",
    "   - Use other K-1 folds as training set\n",
    "   - Train model and evaluate\n",
    "3. Average the K scores\n",
    "\n",
    "**Result**: K different scores → Mean ± Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load iris dataset\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "iris_df = pd.read_csv('data/sample/iris.csv')\n",
    "\n",
    "# Prepare data\n",
    "feature_cols = ['sepal length (cm)', 'sepal width (cm)', \n",
    "                'petal length (cm)', 'petal width (cm)']\n",
    "X = iris_df[feature_cols]\n",
    "y = iris_df['species']\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Classes: {y.unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare single split vs cross-validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Method 1: Single train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "model = LogisticRegression(random_state=42, max_iter=10000)\n",
    "model.fit(X_train, y_train)\n",
    "single_score = model.score(X_test, y_test)\n",
    "\n",
    "print(\"Method 1: Single Train-Test Split\")\n",
    "print(f\"Accuracy: {single_score:.3f}\")\n",
    "print(\"Problem: Only one score - could be lucky or unlucky!\\n\")\n",
    "\n",
    "# Method 2: 5-Fold Cross-Validation\n",
    "model_cv = LogisticRegression(random_state=42, max_iter=10000)\n",
    "cv_scores = cross_val_score(model_cv, X_scaled, y, cv=5)\n",
    "\n",
    "print(\"Method 2: 5-Fold Cross-Validation\")\n",
    "print(f\"Fold scores: {cv_scores}\")\n",
    "print(f\"Mean accuracy: {cv_scores.mean():.3f} ± {cv_scores.std():.3f}\")\n",
    "print(\"Benefit: More reliable estimate with confidence interval!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cross-validation scores\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar plot of fold scores\n",
    "fold_numbers = [f'Fold {i+1}' for i in range(len(cv_scores))]\n",
    "axes[0].bar(fold_numbers, cv_scores, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "axes[0].axhline(y=cv_scores.mean(), color='red', linestyle='--', \n",
    "               linewidth=2, label=f'Mean: {cv_scores.mean():.3f}')\n",
    "axes[0].set_xlabel('Fold', fontsize=12)\n",
    "axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[0].set_title('Cross-Validation Scores by Fold', fontsize=13, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Box plot showing distribution\n",
    "axes[1].boxplot(cv_scores, vert=True, widths=0.5)\n",
    "axes[1].scatter([1]*len(cv_scores), cv_scores, color='steelblue', \n",
    "               s=100, alpha=0.6, zorder=3)\n",
    "axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1].set_title('Cross-Validation Score Distribution', fontsize=13, fontweight='bold')\n",
    "axes[1].set_xticks([1])\n",
    "axes[1].set_xticklabels(['5-Fold CV'])\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Consistency check: Low std ({cv_scores.std():.3f}) = stable model!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Stratified K-Fold for Classification\n",
    "\n",
    "### The Problem with Regular K-Fold\n",
    "\n",
    "**Scenario**: Dataset with 90% Class A, 10% Class B\n",
    "\n",
    "**Risk**: One fold might have:\n",
    "- All Class B samples (imbalanced training)\n",
    "- No Class B samples (can't evaluate properly)\n",
    "\n",
    "### The Solution: Stratified K-Fold\n",
    "\n",
    "**Key Feature**: Preserves class proportions in each fold\n",
    "\n",
    "- If original data: 90% A, 10% B\n",
    "- Each fold will have: ~90% A, ~10% B\n",
    "\n",
    "**Rule of Thumb**: Always use Stratified K-Fold for classification!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load wine dataset (more imbalanced)\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "wine_df = pd.read_csv('data/sample/wine.csv')\n",
    "\n",
    "X_wine = wine_df.drop('target', axis=1)\n",
    "y_wine = wine_df['target']\n",
    "\n",
    "# Check class distribution\n",
    "print(\"Original class distribution:\")\n",
    "print(y_wine.value_counts(normalize=True))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare regular vs stratified K-Fold\n",
    "print(\"=\" * 60)\n",
    "print(\"REGULAR K-FOLD (may have imbalanced folds)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(kfold.split(X_wine), 1):\n",
    "    y_fold = y_wine.iloc[test_idx]\n",
    "    distribution = y_fold.value_counts(normalize=True).sort_index()\n",
    "    print(f\"Fold {fold_idx}: {dict(distribution)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STRATIFIED K-FOLD (maintains class proportions)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(stratified_kfold.split(X_wine, y_wine), 1):\n",
    "    y_fold = y_wine.iloc[test_idx]\n",
    "    distribution = y_fold.value_counts(normalize=True).sort_index()\n",
    "    print(f\"Fold {fold_idx}: {dict(distribution)}\")\n",
    "\n",
    "print(\"\\nNotice: Stratified K-Fold keeps similar proportions across all folds!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare performance\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model_wine = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Regular K-Fold\n",
    "regular_scores = cross_val_score(model_wine, X_wine, y_wine, cv=5)\n",
    "\n",
    "# Stratified K-Fold (default for classification)\n",
    "stratified_scores = cross_val_score(model_wine, X_wine, y_wine, cv=5)\n",
    "\n",
    "print(\"Regular K-Fold:\")\n",
    "print(f\"  Mean: {regular_scores.mean():.3f} ± {regular_scores.std():.3f}\")\n",
    "print(f\"  Scores: {regular_scores}\\n\")\n",
    "\n",
    "print(\"Stratified K-Fold:\")\n",
    "print(f\"  Mean: {stratified_scores.mean():.3f} ± {stratified_scores.std():.3f}\")\n",
    "print(f\"  Scores: {stratified_scores}\")\n",
    "\n",
    "print(\"\\nNote: cross_val_score uses Stratified K-Fold by default for classification!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hyperparameter Tuning Basics\n",
    "\n",
    "### What are Hyperparameters?\n",
    "\n",
    "**Parameters**: Learned from data (e.g., weights in linear regression)\n",
    "**Hyperparameters**: Set before training (e.g., tree depth, learning rate)\n",
    "\n",
    "### Common Hyperparameters\n",
    "\n",
    "**Decision Tree**:\n",
    "- `max_depth`: Maximum tree depth\n",
    "- `min_samples_split`: Minimum samples to split a node\n",
    "- `min_samples_leaf`: Minimum samples in leaf node\n",
    "\n",
    "**Random Forest**:\n",
    "- `n_estimators`: Number of trees\n",
    "- `max_features`: Features to consider for split\n",
    "- Plus all decision tree hyperparameters\n",
    "\n",
    "### Why Tune Hyperparameters?\n",
    "\n",
    "**Default values** may not be optimal for your specific dataset!\n",
    "\n",
    "- Too simple → Underfitting (high bias)\n",
    "- Too complex → Overfitting (high variance)\n",
    "- Just right → Best generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate impact of hyperparameters\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Test different max_depth values\n",
    "depths = [1, 2, 3, 5, 10, 20, None]\n",
    "train_scores = []\n",
    "cv_scores_list = []\n",
    "\n",
    "for depth in depths:\n",
    "    model = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
    "    \n",
    "    # Training score (on full data)\n",
    "    model.fit(X_scaled, y)\n",
    "    train_score = model.score(X_scaled, y)\n",
    "    train_scores.append(train_score)\n",
    "    \n",
    "    # Cross-validation score\n",
    "    cv_scores = cross_val_score(model, X_scaled, y, cv=5)\n",
    "    cv_scores_list.append(cv_scores.mean())\n",
    "\n",
    "# Create results dataframe\n",
    "depth_labels = [str(d) if d is not None else 'None' for d in depths]\n",
    "results_df = pd.DataFrame({\n",
    "    'max_depth': depth_labels,\n",
    "    'Training Score': train_scores,\n",
    "    'CV Score': cv_scores_list,\n",
    "    'Gap (Overfit)': [train - cv for train, cv in zip(train_scores, cv_scores_list)]\n",
    "})\n",
    "\n",
    "print(\"Impact of max_depth hyperparameter:\")\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"\\nObservations:\")\n",
    "print(\"- Too shallow (depth=1,2): Underfitting (low scores)\")\n",
    "print(\"- Too deep (depth=None): Overfitting (large gap)\")\n",
    "print(\"- Just right (depth=3-5): Best CV performance!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training vs CV scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "x_pos = range(len(depth_labels))\n",
    "\n",
    "plt.plot(x_pos, train_scores, marker='o', linewidth=2, markersize=8,\n",
    "        label='Training Score', color='blue')\n",
    "plt.plot(x_pos, cv_scores_list, marker='s', linewidth=2, markersize=8,\n",
    "        label='CV Score', color='red')\n",
    "\n",
    "# Highlight best CV score\n",
    "best_idx = np.argmax(cv_scores_list)\n",
    "plt.scatter([best_idx], [cv_scores_list[best_idx]], \n",
    "           s=300, c='green', marker='*', \n",
    "           label=f'Best: depth={depth_labels[best_idx]}', zorder=5)\n",
    "\n",
    "plt.xticks(x_pos, depth_labels)\n",
    "plt.xlabel('max_depth', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('Training vs Cross-Validation Scores\\n(Large gap = overfitting)', \n",
    "         fontsize=13, fontweight='bold')\n",
    "plt.legend(fontsize=11, loc='lower right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. GridSearchCV - Exhaustive Search\n",
    "\n",
    "### How It Works\n",
    "\n",
    "**GridSearchCV** tries every combination of hyperparameters:\n",
    "\n",
    "```python\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "# Tests: 3 × 3 = 9 combinations\n",
    "# With 5-fold CV: 9 × 5 = 45 model fits!\n",
    "```\n",
    "\n",
    "**Process**:\n",
    "1. Define parameter grid\n",
    "2. For each combination:\n",
    "   - Perform K-fold cross-validation\n",
    "   - Record mean score\n",
    "3. Select best combination\n",
    "4. Retrain on full training data\n",
    "\n",
    "**Pros**: Guaranteed to find best combination in grid  \n",
    "**Cons**: Slow for large grids (exponential growth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV example\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 7, None],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "print(\"Parameter grid:\")\n",
    "for param, values in param_grid.items():\n",
    "    print(f\"  {param}: {values}\")\n",
    "\n",
    "total_combinations = np.prod([len(v) for v in param_grid.values()])\n",
    "print(f\"\\nTotal combinations: {total_combinations}\")\n",
    "print(f\"With 5-fold CV: {total_combinations * 5} model fits!\")\n",
    "print(\"\\nSearching for best hyperparameters...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform grid search\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf_model,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,  # Use all CPU cores\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Fit on iris data\n",
    "grid_search.fit(X_scaled, y)\n",
    "\n",
    "print(\"Grid Search Complete!\\n\")\n",
    "print(\"Best hyperparameters:\")\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "print(f\"\\nBest cross-validation score: {grid_search.best_score_:.3f}\")\n",
    "print(f\"\\nThe best model is already fitted and ready to use!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze grid search results\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Select relevant columns\n",
    "relevant_cols = ['param_n_estimators', 'param_max_depth', 'param_min_samples_split',\n",
    "                'mean_test_score', 'std_test_score', 'rank_test_score']\n",
    "results_summary = results_df[relevant_cols].sort_values('rank_test_score')\n",
    "\n",
    "print(\"Top 10 hyperparameter combinations:\\n\")\n",
    "print(results_summary.head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\nKey Insights:\")\n",
    "print(f\"- Best score: {results_summary['mean_test_score'].max():.3f}\")\n",
    "print(f\"- Worst score: {results_summary['mean_test_score'].min():.3f}\")\n",
    "print(f\"- Score range: {results_summary['mean_test_score'].max() - results_summary['mean_test_score'].min():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize hyperparameter impact\n",
    "# Focus on two hyperparameters for visualization\n",
    "pivot_data = results_df.pivot_table(\n",
    "    values='mean_test_score',\n",
    "    index='param_max_depth',\n",
    "    columns='param_n_estimators',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(pivot_data, annot=True, fmt='.3f', cmap='YlGnBu', \n",
    "           cbar_kws={'label': 'Mean CV Score'})\n",
    "plt.xlabel('n_estimators', fontsize=12)\n",
    "plt.ylabel('max_depth', fontsize=12)\n",
    "plt.title('GridSearchCV: Hyperparameter Impact on Performance', \n",
    "         fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Interpretation: Darker colors = better performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. RandomizedSearchCV - Faster Alternative\n",
    "\n",
    "### The Problem with Grid Search\n",
    "\n",
    "**Large grids are slow!**\n",
    "\n",
    "Example:\n",
    "- 5 parameters\n",
    "- 10 values each\n",
    "- Total combinations: 10^5 = 100,000\n",
    "- With 5-fold CV: 500,000 fits!\n",
    "\n",
    "### The Solution: Randomized Search\n",
    "\n",
    "**Key Idea**: Don't test everything - sample randomly!\n",
    "\n",
    "**Process**:\n",
    "1. Define parameter distributions (not fixed grids)\n",
    "2. Randomly sample N combinations\n",
    "3. Evaluate each with cross-validation\n",
    "4. Select best combination\n",
    "\n",
    "**Pros**: Much faster, often finds good solutions  \n",
    "**Cons**: Not guaranteed to find absolute best\n",
    "\n",
    "**Rule of Thumb**: Use RandomizedSearchCV first to narrow down range, then GridSearchCV for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomizedSearchCV example\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Define parameter distributions\n",
    "param_distributions = {\n",
    "    'n_estimators': randint(50, 300),  # Random integers from 50 to 300\n",
    "    'max_depth': [3, 5, 7, 10, None],  # Can still use lists\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'min_samples_leaf': randint(1, 10),\n",
    "    'max_features': uniform(0.3, 0.7)  # Random float from 0.3 to 1.0\n",
    "}\n",
    "\n",
    "print(\"Parameter distributions:\")\n",
    "for param, dist in param_distributions.items():\n",
    "    print(f\"  {param}: {dist}\")\n",
    "\n",
    "n_iterations = 50\n",
    "print(f\"\\nWill try {n_iterations} random combinations\")\n",
    "print(f\"With 5-fold CV: {n_iterations * 5} model fits (much faster than GridSearch!)\")\n",
    "print(\"\\nSearching...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform randomized search\n",
    "rf_random = RandomForestClassifier(random_state=42)\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf_random,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=50,  # Number of random combinations to try\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "random_search.fit(X_scaled, y)\n",
    "\n",
    "print(\"Randomized Search Complete!\\n\")\n",
    "print(\"Best hyperparameters:\")\n",
    "for param, value in random_search.best_params_.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {param}: {value:.3f}\")\n",
    "    else:\n",
    "        print(f\"  {param}: {value}\")\n",
    "\n",
    "print(f\"\\nBest cross-validation score: {random_search.best_score_:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare GridSearchCV vs RandomizedSearchCV\n",
    "comparison = pd.DataFrame({\n",
    "    'Method': ['GridSearchCV', 'RandomizedSearchCV'],\n",
    "    'Best Score': [grid_search.best_score_, random_search.best_score_],\n",
    "    'Combinations Tried': [len(grid_search.cv_results_['params']), \n",
    "                          len(random_search.cv_results_['params'])]\n",
    "})\n",
    "\n",
    "print(\"Comparison of Search Methods:\\n\")\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "print(\"\\nConclusion:\")\n",
    "print(f\"- RandomizedSearch tried {comparison.iloc[1, 2]} combinations\")\n",
    "print(f\"- GridSearch tried {comparison.iloc[0, 2]} combinations\")\n",
    "print(f\"- RandomizedSearch was {comparison.iloc[0, 2] / comparison.iloc[1, 2]:.1f}x faster\")\n",
    "print(f\"- Performance difference: {abs(comparison.iloc[0, 1] - comparison.iloc[1, 1]):.4f}\")\n",
    "print(\"\\n→ RandomizedSearch found nearly optimal solution much faster!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Best Practices and Common Pitfalls\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Always use cross-validation** for model evaluation\n",
    "   - Never tune on test set!\n",
    "   - Use cross-validation on training data only\n",
    "\n",
    "2. **Use Stratified K-Fold for classification**\n",
    "   - Maintains class proportions\n",
    "   - More reliable estimates\n",
    "\n",
    "3. **Choose appropriate K**\n",
    "   - K=5: Good balance (common default)\n",
    "   - K=10: More reliable but slower\n",
    "   - Small datasets: Use larger K\n",
    "\n",
    "4. **Hyperparameter tuning strategy**:\n",
    "   - Start with RandomizedSearchCV (broad search)\n",
    "   - Narrow down promising ranges\n",
    "   - Use GridSearchCV for fine-tuning\n",
    "\n",
    "5. **Scale features before CV**\n",
    "   - Fit scaler on training folds only\n",
    "   - Use Pipeline to avoid data leakage\n",
    "\n",
    "### Common Pitfalls\n",
    "\n",
    "❌ **Data Leakage**: Fitting preprocessor on entire dataset\n",
    "```python\n",
    "# WRONG - leakage!\n",
    "X_scaled = scaler.fit_transform(X)  # Uses test data!\n",
    "cross_val_score(model, X_scaled, y, cv=5)\n",
    "\n",
    "# CORRECT - use Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipe = Pipeline([('scaler', StandardScaler()), ('model', model)])\n",
    "cross_val_score(pipe, X, y, cv=5)\n",
    "```\n",
    "\n",
    "❌ **Overfitting on CV**: Trying too many configurations\n",
    "- More combinations = higher chance of luck\n",
    "- Always evaluate best model on held-out test set\n",
    "\n",
    "❌ **Wrong metric**: Using accuracy on imbalanced data\n",
    "- Specify scoring parameter: `scoring='f1'`, `scoring='roc_auc'`\n",
    "\n",
    "❌ **Not using random_state**: Results not reproducible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate correct way: Pipeline to prevent data leakage\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Scaling step\n",
    "    ('classifier', SVC(random_state=42))  # Model step\n",
    "])\n",
    "\n",
    "# Define parameter grid for pipeline\n",
    "param_grid_pipeline = {\n",
    "    'classifier__C': [0.1, 1, 10],\n",
    "    'classifier__kernel': ['linear', 'rbf'],\n",
    "    'classifier__gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Grid search on pipeline\n",
    "grid_pipeline = GridSearchCV(\n",
    "    pipeline, \n",
    "    param_grid_pipeline, \n",
    "    cv=5, \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit on RAW data (pipeline handles scaling internally for each fold)\n",
    "grid_pipeline.fit(X, y)  # Note: X not X_scaled!\n",
    "\n",
    "print(\"✓ Pipeline approach prevents data leakage!\")\n",
    "print(f\"\\nBest parameters: {grid_pipeline.best_params_}\")\n",
    "print(f\"Best CV score: {grid_pipeline.best_score_:.3f}\")\n",
    "print(\"\\nBenefit: Scaler is fit on training folds only (no test data leakage)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Manual K-Fold Implementation\n",
    "\n",
    "Implement 5-fold cross-validation manually:\n",
    "1. Use `KFold` to generate train/test indices\n",
    "2. For each fold, train a LogisticRegression model\n",
    "3. Calculate and store accuracy for each fold\n",
    "4. Compute mean and standard deviation\n",
    "5. Compare with `cross_val_score` result\n",
    "\n",
    "Use wine dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Optimal K Selection\n",
    "\n",
    "Determine the optimal value of K for cross-validation:\n",
    "1. Try K values from 3 to 10\n",
    "2. For each K, perform cross-validation with LogisticRegression\n",
    "3. Record mean score and standard deviation\n",
    "4. Plot mean score and std vs K\n",
    "5. Which K gives the best balance of performance and stability?\n",
    "\n",
    "Use iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Custom Grid Search\n",
    "\n",
    "Perform grid search for DecisionTreeClassifier on wine dataset:\n",
    "1. Define parameter grid:\n",
    "   - `max_depth`: [2, 4, 6, 8, 10]\n",
    "   - `min_samples_split`: [2, 5, 10, 20]\n",
    "   - `criterion`: ['gini', 'entropy']\n",
    "2. Use GridSearchCV with 5-fold CV\n",
    "3. Print best parameters and score\n",
    "4. Create a visualization comparing top 5 combinations\n",
    "5. Test best model on held-out test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: RandomizedSearchCV Exploration\n",
    "\n",
    "Compare different numbers of iterations in RandomizedSearchCV:\n",
    "1. Use RandomForestClassifier on iris dataset\n",
    "2. Try n_iter values: [10, 25, 50, 100]\n",
    "3. For each n_iter:\n",
    "   - Run RandomizedSearchCV\n",
    "   - Record best score and time taken\n",
    "4. Plot best score vs n_iter\n",
    "5. What's the point of diminishing returns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Cross-Validation**:\n",
    "   - More reliable than single train-test split\n",
    "   - Tests model on multiple different data splits\n",
    "   - Provides mean ± std for confidence interval\n",
    "   - **K-Fold**: Split data into K equal parts\n",
    "   - **Stratified K-Fold**: Maintains class proportions (use for classification!)\n",
    "\n",
    "2. **Hyperparameter Tuning**:\n",
    "   - Hyperparameters are set before training (not learned)\n",
    "   - Default values are rarely optimal\n",
    "   - Must tune on training data only (never test data)\n",
    "\n",
    "3. **GridSearchCV**:\n",
    "   - Exhaustive search over parameter grid\n",
    "   - Tests every combination\n",
    "   - Guaranteed to find best in grid\n",
    "   - Can be slow for large grids\n",
    "\n",
    "4. **RandomizedSearchCV**:\n",
    "   - Samples random combinations\n",
    "   - Much faster than grid search\n",
    "   - Often finds near-optimal solutions\n",
    "   - Good for initial broad search\n",
    "\n",
    "5. **Best Practices**:\n",
    "   - Use Stratified K-Fold for classification\n",
    "   - Use Pipeline to prevent data leakage\n",
    "   - Start with RandomizedSearchCV, then GridSearchCV\n",
    "   - Always validate final model on held-out test set\n",
    "   - Set random_state for reproducibility\n",
    "\n",
    "6. **Common Pitfalls to Avoid**:\n",
    "   - ❌ Fitting preprocessors on full dataset (data leakage)\n",
    "   - ❌ Tuning on test set\n",
    "   - ❌ Using regular K-Fold for classification (use Stratified)\n",
    "   - ❌ Not using appropriate scoring metric\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In **Module 08: Regularization (L1, L2, Elastic Net)**, you'll learn:\n",
    "- Understanding overfitting vs underfitting\n",
    "- Ridge regression (L2 regularization)\n",
    "- Lasso regression (L1 regularization and feature selection)\n",
    "- Elastic Net (combining L1 and L2)\n",
    "- Bias-variance tradeoff\n",
    "- Choosing regularization strength\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [Cross-Validation Explained - StatQuest](https://www.youtube.com/watch?v=fSytzGwwBVw)\n",
    "- [Hyperparameter Tuning - Andrew Ng](https://www.coursera.org/lecture/machine-learning/model-selection-and-train-validation-test-sets-QGKbr)\n",
    "- [scikit-learn Cross-Validation Guide](https://scikit-learn.org/stable/modules/cross_validation.html)\n",
    "- [Grid Search vs Random Search](https://scikit-learn.org/stable/modules/grid_search.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
