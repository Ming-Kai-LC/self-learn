{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Feature Matching\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "- Match features between two images\n",
    "- Use Brute-Force matcher for feature matching\n",
    "- Use FLANN-based matcher for fast matching\n",
    "- Apply Lowe's ratio test to filter good matches\n",
    "- Find objects in scenes using homography\n",
    "- Understand when to use different distance metrics\n",
    "- Build practical object recognition systems\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configure matplotlib\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (14, 8)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"OpenCV version: {cv.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Introduction to Feature Matching\n",
    "\n",
    "### What is Feature Matching?\n",
    "\n",
    "**Feature matching** is the process of finding corresponding features (keypoints) between two or more images of the same scene or object.\n",
    "\n",
    "### Why Match Features?\n",
    "\n",
    "Feature matching enables:\n",
    "- **Object recognition** - Find specific objects in images\n",
    "- **Image stitching** - Create panoramas from multiple photos\n",
    "- **3D reconstruction** - Build 3D models from 2D images\n",
    "- **Motion tracking** - Track object movement across frames\n",
    "- **Image registration** - Align images taken at different times\n",
    "- **Visual odometry** - Estimate camera movement\n",
    "\n",
    "### Matching Process:\n",
    "\n",
    "1. **Detect features** in both images (SIFT, ORB, etc.)\n",
    "2. **Compute descriptors** for each feature\n",
    "3. **Match descriptors** between images\n",
    "4. **Filter matches** to keep only good ones\n",
    "5. **Use matches** for alignment, recognition, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Create Test Images\n",
    "\n",
    "Let's create two images with a common object to demonstrate matching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create image 1 with an object\n",
    "img1 = np.ones((400, 500), dtype=np.uint8) * 200\n",
    "\n",
    "# Add a distinctive pattern (our \"object\")\n",
    "cv.rectangle(img1, (150, 100), (350, 300), 80, -1)\n",
    "cv.rectangle(img1, (180, 130), (220, 270), 150, -1)\n",
    "cv.rectangle(img1, (280, 130), (320, 270), 150, -1)\n",
    "cv.circle(img1, (250, 200), 30, 120, -1)\n",
    "cv.rectangle(img1, (200, 240), (300, 260), 140, -1)\n",
    "\n",
    "# Create image 2 - same object but scaled and rotated\n",
    "# First create the object\n",
    "obj = np.ones((400, 500), dtype=np.uint8) * 200\n",
    "cv.rectangle(obj, (150, 100), (350, 300), 80, -1)\n",
    "cv.rectangle(obj, (180, 130), (220, 270), 150, -1)\n",
    "cv.rectangle(obj, (280, 130), (320, 270), 150, -1)\n",
    "cv.circle(obj, (250, 200), 30, 120, -1)\n",
    "cv.rectangle(obj, (200, 240), (300, 260), 140, -1)\n",
    "\n",
    "# Scale and rotate\n",
    "scaled = cv.resize(obj, None, fx=0.7, fy=0.7)\n",
    "center = (scaled.shape[1] // 2, scaled.shape[0] // 2)\n",
    "rotation_matrix = cv.getRotationMatrix2D(center, 15, 1.0)\n",
    "img2 = cv.warpAffine(scaled, rotation_matrix, (scaled.shape[1], scaled.shape[0]))\n",
    "\n",
    "# Add it to a different scene\n",
    "img2_scene = np.ones((400, 500), dtype=np.uint8) * 180\n",
    "img2_scene[50 : 50 + img2.shape[0], 100 : 100 + img2.shape[1]] = img2\n",
    "\n",
    "# Add some noise and other objects to scene\n",
    "cv.circle(img2_scene, (80, 350), 25, 100, -1)\n",
    "cv.rectangle(img2_scene, (400, 50), (480, 120), 90, -1)\n",
    "\n",
    "# Display\n",
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(img1, cmap=\"gray\")\n",
    "plt.title(\"Image 1: Query Object\", fontsize=14)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(img2_scene, cmap=\"gray\")\n",
    "plt.title(\"Image 2: Scene (object scaled, rotated)\", fontsize=14)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Goal: Find and match the object from Image 1 in Image 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Brute-Force Matcher\n",
    "\n",
    "### What is Brute-Force Matching?\n",
    "\n",
    "**Brute-Force (BF) matcher** compares each descriptor in the first set with all descriptors in the second set and returns the closest match.\n",
    "\n",
    "### Distance Metrics:\n",
    "\n",
    "- **NORM_L1**: Manhattan distance (sum of absolute differences)\n",
    "- **NORM_L2**: Euclidean distance (used for SIFT, SURF)\n",
    "- **NORM_HAMMING**: Hamming distance (used for ORB, BRIEF, BRISK)\n",
    "- **NORM_HAMMING2**: For ORB when WTA_K == 3 or 4\n",
    "\n",
    "### When to Use:\n",
    "- **Small datasets** - Fast enough for < 1000 features\n",
    "- **Accuracy critical** - Guarantees finding best match\n",
    "- **Binary descriptors** - Very fast with Hamming distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect features using ORB\n",
    "orb = cv.ORB_create(nfeatures=500)\n",
    "\n",
    "# Find keypoints and descriptors\n",
    "kp1, des1 = orb.detectAndCompute(img1, None)\n",
    "kp2, des2 = orb.detectAndCompute(img2_scene, None)\n",
    "\n",
    "print(f\"Image 1: {len(kp1)} keypoints detected\")\n",
    "print(f\"Image 2: {len(kp2)} keypoints detected\")\n",
    "\n",
    "# Create BF Matcher\n",
    "# Use NORM_HAMMING for ORB (binary descriptors)\n",
    "bf = cv.BFMatcher(cv.NORM_HAMMING, crossCheck=True)\n",
    "\n",
    "# Match descriptors\n",
    "matches = bf.match(des1, des2)\n",
    "\n",
    "# Sort matches by distance (best matches first)\n",
    "matches = sorted(matches, key=lambda x: x.distance)\n",
    "\n",
    "print(f\"\\nFound {len(matches)} matches\")\n",
    "print(f\"Best match distance: {matches[0].distance}\")\n",
    "print(f\"Worst match distance: {matches[-1].distance}\")\n",
    "\n",
    "# Draw top 50 matches\n",
    "img_matches = cv.drawMatches(\n",
    "    img1, kp1, img2_scene, kp2, matches[:50], None, flags=cv.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.imshow(img_matches)\n",
    "plt.title(f\"Brute-Force Matching: Top 50 matches (Total: {len(matches)})\", fontsize=14)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: FLANN-based Matcher\n",
    "\n",
    "### What is FLANN?\n",
    "\n",
    "**FLANN** (Fast Library for Approximate Nearest Neighbors) is much faster than Brute-Force for large datasets.\n",
    "\n",
    "### Trade-off:\n",
    "- **Speed**: Much faster (especially for 1000+ features)\n",
    "- **Accuracy**: Approximate (may miss perfect match)\n",
    "\n",
    "### Index Types:\n",
    "\n",
    "1. **KDTreeIndex** - For SIFT, SURF (float descriptors)\n",
    "2. **LSHIndex** - For ORB, BRIEF (binary descriptors)\n",
    "3. **AutotunedIndex** - Automatically choose best algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Detect with SIFT (for FLANN example)\n",
    "sift = cv.SIFT_create(nfeatures=500)\n",
    "kp1_sift, des1_sift = sift.detectAndCompute(img1, None)\n",
    "kp2_sift, des2_sift = sift.detectAndCompute(img2_scene, None)\n",
    "\n",
    "# FLANN parameters for SIFT\n",
    "FLANN_INDEX_KDTREE = 1\n",
    "index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "search_params = dict(checks=50)  # Higher = more accurate, slower\n",
    "\n",
    "# Create FLANN matcher\n",
    "flann = cv.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "# Match\n",
    "start = time.time()\n",
    "matches_flann = flann.knnMatch(des1_sift, des2_sift, k=2)\n",
    "flann_time = time.time() - start\n",
    "\n",
    "# Compare with BF matcher\n",
    "bf_sift = cv.BFMatcher(cv.NORM_L2)\n",
    "start = time.time()\n",
    "matches_bf = bf_sift.knnMatch(des1_sift, des2_sift, k=2)\n",
    "bf_time = time.time() - start\n",
    "\n",
    "print(f\"FLANN matching time: {flann_time*1000:.2f}ms\")\n",
    "print(f\"BF matching time: {bf_time*1000:.2f}ms\")\n",
    "print(f\"FLANN is {bf_time/flann_time:.1f}x faster!\")\n",
    "print(f\"\\nFLANN found {len(matches_flann)} match pairs\")\n",
    "print(f\"BF found {len(matches_bf)} match pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Lowe's Ratio Test\n",
    "\n",
    "### Problem: False Matches\n",
    "\n",
    "Not all matches are good. Some are false positives caused by:\n",
    "- Similar-looking features\n",
    "- Repetitive patterns\n",
    "- Noise\n",
    "\n",
    "### Lowe's Ratio Test Solution:\n",
    "\n",
    "For each feature, find the **2 best matches**:\n",
    "- If the best match is **much better** than the second best, it's likely correct\n",
    "- Ratio = distance1 / distance2\n",
    "- Keep match if ratio < 0.7 (typical threshold)\n",
    "\n",
    "### Why it Works:\n",
    "\n",
    "- **Good match**: Best is much better than second (low ratio)\n",
    "- **Bad match**: Best and second are similar (high ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Lowe's ratio test\n",
    "good_matches = []\n",
    "for m, n in matches_flann:\n",
    "    if m.distance < 0.7 * n.distance:\n",
    "        good_matches.append(m)\n",
    "\n",
    "print(f\"Total matches: {len(matches_flann)}\")\n",
    "print(f\"Good matches (ratio test): {len(good_matches)}\")\n",
    "print(\n",
    "    f\"Filtered out: {len(matches_flann) - len(good_matches)} ({(1 - len(good_matches)/len(matches_flann))*100:.1f}%)\"\n",
    ")\n",
    "\n",
    "# Draw matches\n",
    "img_all_matches = cv.drawMatches(\n",
    "    img1,\n",
    "    kp1_sift,\n",
    "    img2_scene,\n",
    "    kp2_sift,\n",
    "    [m for m, n in matches_flann[:100]],\n",
    "    None,\n",
    "    flags=cv.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS,\n",
    ")\n",
    "\n",
    "img_good_matches = cv.drawMatches(\n",
    "    img1,\n",
    "    kp1_sift,\n",
    "    img2_scene,\n",
    "    kp2_sift,\n",
    "    good_matches[:100],\n",
    "    None,\n",
    "    flags=cv.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS,\n",
    ")\n",
    "\n",
    "# Display comparison\n",
    "plt.figure(figsize=(20, 16))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.imshow(img_all_matches)\n",
    "plt.title(f\"All Matches (no filtering): {len(matches_flann)} total\", fontsize=14)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.imshow(img_good_matches)\n",
    "plt.title(f\"Good Matches (after ratio test): {len(good_matches)} total\", fontsize=14)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nRatio test effectively removes false matches!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### ðŸ’¡ Parameter Tuning Tips for Feature Matching\n",
    "\n",
    "**Lowe's Ratio Test**:\n",
    "- `ratio_threshold` (0.5-0.9): Lower = stricter, fewer but better matches\n",
    "  - 0.5-0.6: Very strict, best matches only\n",
    "  - 0.7: Recommended by Lowe (good balance)\n",
    "  - 0.8-0.9: More matches, but may include false positives\n",
    "\n",
    "**FLANN Matcher Parameters**:\n",
    "- `trees` (1-16): More trees = better accuracy but slower\n",
    "- `checks` (10-100): More checks = better accuracy but slower\n",
    "- Typical: trees=5, checks=50 for balanced performance\n",
    "\n",
    "**RANSAC for Homography**:\n",
    "- `ransacReprojThreshold` (1.0-10.0): Pixel threshold for inliers\n",
    "  - 1.0-3.0: Strict, very accurate homography\n",
    "  - 4.0-5.0: Balanced (recommended)\n",
    "  - 6.0-10.0: Loose, more inliers but less accurate\n",
    "\n",
    "**Minimum Matches**:\n",
    "- At least 4 points needed for homography\n",
    "- Recommended: 10-20 minimum for robust results\n",
    "- More matches = more reliable homography\n",
    "\n",
    "**When to adjust**:\n",
    "- Too many false matches: Decrease ratio threshold\n",
    "- Too few matches: Increase ratio threshold, decrease RANSAC threshold\n",
    "- Homography fails: Increase minimum matches requirement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Finding Objects with Homography\n",
    "\n",
    "### What is Homography?\n",
    "\n",
    "A **homography** is a transformation matrix that maps points from one plane to another. It handles:\n",
    "- Rotation\n",
    "- Translation\n",
    "- Scale\n",
    "- Perspective distortion\n",
    "\n",
    "### Using Homography for Object Detection:\n",
    "\n",
    "1. Match features between query and scene\n",
    "2. Find homography matrix from matches\n",
    "3. Use homography to transform query corners to scene\n",
    "4. Draw bounding box around detected object\n",
    "\n",
    "### RANSAC:\n",
    "\n",
    "**RANSAC** (RANdom SAmple Consensus) is used to find homography robustly:\n",
    "- Handles outliers (false matches)\n",
    "- Finds best transformation that fits most matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum number of matches required\n",
    "MIN_MATCH_COUNT = 10\n",
    "\n",
    "if len(good_matches) > MIN_MATCH_COUNT:\n",
    "    # Extract location of good matches\n",
    "    src_pts = np.float32([kp1_sift[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "    dst_pts = np.float32([kp2_sift[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "\n",
    "    # Find homography\n",
    "    M, mask = cv.findHomography(src_pts, dst_pts, cv.RANSAC, 5.0)\n",
    "    matchesMask = mask.ravel().tolist()\n",
    "\n",
    "    # Get dimensions of query image\n",
    "    h, w = img1.shape\n",
    "\n",
    "    # Define corners of query image\n",
    "    pts = np.float32([[0, 0], [0, h - 1], [w - 1, h - 1], [w - 1, 0]]).reshape(-1, 1, 2)\n",
    "\n",
    "    # Transform corners to scene\n",
    "    dst = cv.perspectiveTransform(pts, M)\n",
    "\n",
    "    # Draw bounding box in scene image\n",
    "    img2_detected = cv.cvtColor(img2_scene, cv.COLOR_GRAY2BGR)\n",
    "    img2_detected = cv.polylines(img2_detected, [np.int32(dst)], True, (0, 255, 0), 3, cv.LINE_AA)\n",
    "\n",
    "    # Count inliers\n",
    "    inliers = np.sum(matchesMask)\n",
    "\n",
    "    print(f\"Object detected!\")\n",
    "    print(f\"Inliers: {inliers}/{len(good_matches)} ({inliers/len(good_matches)*100:.1f}%)\")\n",
    "    print(f\"Homography matrix found:\")\n",
    "    print(M)\n",
    "\n",
    "else:\n",
    "    print(f\"Not enough matches found - {len(good_matches)}/{MIN_MATCH_COUNT}\")\n",
    "    matchesMask = None\n",
    "    img2_detected = cv.cvtColor(img2_scene, cv.COLOR_GRAY2BGR)\n",
    "\n",
    "# Draw matches with inliers highlighted\n",
    "draw_params = dict(\n",
    "    matchColor=(0, 255, 0),  # Green for inliers\n",
    "    singlePointColor=None,\n",
    "    matchesMask=matchesMask,  # Only draw inliers\n",
    "    flags=2,\n",
    ")\n",
    "\n",
    "img_homography = cv.drawMatches(\n",
    "    img1, kp1_sift, img2_scene, kp2_sift, good_matches, None, **draw_params\n",
    ")\n",
    "\n",
    "# Display results\n",
    "plt.figure(figsize=(20, 16))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.imshow(img_homography)\n",
    "plt.title(f\"Feature Matches (green = inliers used for homography)\", fontsize=14)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.imshow(cv.cvtColor(img2_detected, cv.COLOR_BGR2RGB))\n",
    "plt.title(\"Object Detection Result (green box)\", fontsize=14)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Complete Object Recognition Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_object(query_img, scene_img, min_matches=10, ratio_thresh=0.7):\n",
    "    \"\"\"\n",
    "    Complete pipeline to find query object in scene image.\n",
    "\n",
    "    Args:\n",
    "        query_img: Query/template image (grayscale)\n",
    "        scene_img: Scene image to search in (grayscale)\n",
    "        min_matches: Minimum number of good matches required\n",
    "        ratio_thresh: Ratio test threshold (default 0.7)\n",
    "\n",
    "    Returns:\n",
    "        detected_img: Scene image with bounding box\n",
    "        matches_img: Visualization of matches\n",
    "        found: Boolean indicating if object was found\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Detect features\n",
    "    sift = cv.SIFT_create()\n",
    "    kp1, des1 = sift.detectAndCompute(query_img, None)\n",
    "    kp2, des2 = sift.detectAndCompute(scene_img, None)\n",
    "\n",
    "    print(f\"Step 1: Feature Detection\")\n",
    "    print(f\"  Query: {len(kp1)} keypoints\")\n",
    "    print(f\"  Scene: {len(kp2)} keypoints\")\n",
    "\n",
    "    # Step 2: Match features\n",
    "    FLANN_INDEX_KDTREE = 1\n",
    "    index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "    search_params = dict(checks=50)\n",
    "    flann = cv.FlannBasedMatcher(index_params, search_params)\n",
    "    matches = flann.knnMatch(des1, des2, k=2)\n",
    "\n",
    "    print(f\"\\nStep 2: Feature Matching\")\n",
    "    print(f\"  Total matches: {len(matches)}\")\n",
    "\n",
    "    # Step 3: Apply ratio test\n",
    "    good_matches = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < ratio_thresh * n.distance:\n",
    "            good_matches.append(m)\n",
    "\n",
    "    print(f\"\\nStep 3: Ratio Test (threshold={ratio_thresh})\")\n",
    "    print(f\"  Good matches: {len(good_matches)}\")\n",
    "    print(f\"  Filtered: {len(matches) - len(good_matches)}\")\n",
    "\n",
    "    # Step 4: Find homography\n",
    "    if len(good_matches) > min_matches:\n",
    "        src_pts = np.float32([kp1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "        dst_pts = np.float32([kp2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "\n",
    "        M, mask = cv.findHomography(src_pts, dst_pts, cv.RANSAC, 5.0)\n",
    "        matchesMask = mask.ravel().tolist()\n",
    "        inliers = np.sum(matchesMask)\n",
    "\n",
    "        print(f\"\\nStep 4: Homography (RANSAC)\")\n",
    "        print(f\"  Inliers: {inliers}/{len(good_matches)} ({inliers/len(good_matches)*100:.1f}%)\")\n",
    "\n",
    "        # Transform corners\n",
    "        h, w = query_img.shape\n",
    "        pts = np.float32([[0, 0], [0, h - 1], [w - 1, h - 1], [w - 1, 0]]).reshape(-1, 1, 2)\n",
    "        dst = cv.perspectiveTransform(pts, M)\n",
    "\n",
    "        # Draw detection\n",
    "        detected_img = cv.cvtColor(scene_img, cv.COLOR_GRAY2BGR)\n",
    "        detected_img = cv.polylines(detected_img, [np.int32(dst)], True, (0, 255, 0), 3)\n",
    "        cv.putText(detected_img, \"DETECTED\", (10, 30), cv.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "        # Draw matches\n",
    "        draw_params = dict(\n",
    "            matchColor=(0, 255, 0), singlePointColor=None, matchesMask=matchesMask, flags=2\n",
    "        )\n",
    "        matches_img = cv.drawMatches(\n",
    "            query_img, kp1, scene_img, kp2, good_matches, None, **draw_params\n",
    "        )\n",
    "\n",
    "        print(f\"\\nâœ“ Object FOUND!\")\n",
    "        return detected_img, matches_img, True\n",
    "\n",
    "    else:\n",
    "        print(f\"\\nâœ— Object NOT FOUND\")\n",
    "        print(f\"  Reason: Insufficient matches ({len(good_matches)}/{min_matches})\")\n",
    "\n",
    "        detected_img = cv.cvtColor(scene_img, cv.COLOR_GRAY2BGR)\n",
    "        cv.putText(detected_img, \"NOT FOUND\", (10, 30), cv.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "        matches_img = cv.drawMatches(\n",
    "            query_img,\n",
    "            kp1,\n",
    "            scene_img,\n",
    "            kp2,\n",
    "            good_matches,\n",
    "            None,\n",
    "            flags=cv.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS,\n",
    "        )\n",
    "\n",
    "        return detected_img, matches_img, False\n",
    "\n",
    "\n",
    "# Test the pipeline\n",
    "print(\"=\" * 60)\n",
    "print(\"OBJECT RECOGNITION PIPELINE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "detected, matches_viz, found = find_object(img1, img2_scene)\n",
    "\n",
    "# Display\n",
    "plt.figure(figsize=(20, 16))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.imshow(img1, cmap=\"gray\")\n",
    "plt.title(\"Query Object\", fontsize=14)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.imshow(img2_scene, cmap=\"gray\")\n",
    "plt.title(\"Scene Image\", fontsize=14)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.imshow(matches_viz)\n",
    "plt.title(\"Feature Matches\", fontsize=14)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.imshow(cv.cvtColor(detected, cv.COLOR_BGR2RGB))\n",
    "plt.title(\"Detection Result\", fontsize=14, fontweight=\"bold\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Matcher Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create more complex scene\n",
    "complex_query = np.ones((300, 300), dtype=np.uint8) * 200\n",
    "cv.rectangle(complex_query, (80, 80), (220, 220), 60, -1)\n",
    "cv.rectangle(complex_query, (100, 100), (120, 200), 150, -1)\n",
    "cv.rectangle(complex_query, (180, 100), (200, 200), 150, -1)\n",
    "cv.circle(complex_query, (150, 160), 25, 100, -1)\n",
    "\n",
    "complex_scene = np.ones((500, 600), dtype=np.uint8) * 180\n",
    "\n",
    "# Add rotated query\n",
    "M_rot = cv.getRotationMatrix2D((150, 150), -30, 0.8)\n",
    "rotated = cv.warpAffine(complex_query, M_rot, (300, 300))\n",
    "complex_scene[100:400, 150:450] = rotated\n",
    "\n",
    "# Add clutter\n",
    "cv.circle(complex_scene, (80, 80), 30, 100, -1)\n",
    "cv.rectangle(complex_scene, (500, 350), (580, 450), 90, -1)\n",
    "cv.circle(complex_scene, (500, 100), 40, 110, -1)\n",
    "\n",
    "# Test with different detectors and matchers\n",
    "results = {}\n",
    "\n",
    "# 1. SIFT + BF\n",
    "print(\"Testing SIFT + BF Matcher...\")\n",
    "sift = cv.SIFT_create()\n",
    "kp_q, des_q = sift.detectAndCompute(complex_query, None)\n",
    "kp_s, des_s = sift.detectAndCompute(complex_scene, None)\n",
    "bf = cv.BFMatcher(cv.NORM_L2)\n",
    "matches = bf.knnMatch(des_q, des_s, k=2)\n",
    "good = [m for m, n in matches if m.distance < 0.7 * n.distance]\n",
    "results[\"SIFT + BF\"] = len(good)\n",
    "img_sift_bf = cv.drawMatches(complex_query, kp_q, complex_scene, kp_s, good[:30], None, flags=2)\n",
    "\n",
    "# 2. SIFT + FLANN\n",
    "print(\"Testing SIFT + FLANN Matcher...\")\n",
    "flann = cv.FlannBasedMatcher(dict(algorithm=1, trees=5), dict(checks=50))\n",
    "matches = flann.knnMatch(des_q, des_s, k=2)\n",
    "good = [m for m, n in matches if m.distance < 0.7 * n.distance]\n",
    "results[\"SIFT + FLANN\"] = len(good)\n",
    "img_sift_flann = cv.drawMatches(complex_query, kp_q, complex_scene, kp_s, good[:30], None, flags=2)\n",
    "\n",
    "# 3. ORB + BF\n",
    "print(\"Testing ORB + BF Matcher...\")\n",
    "orb = cv.ORB_create(nfeatures=500)\n",
    "kp_q_orb, des_q_orb = orb.detectAndCompute(complex_query, None)\n",
    "kp_s_orb, des_s_orb = orb.detectAndCompute(complex_scene, None)\n",
    "bf_orb = cv.BFMatcher(cv.NORM_HAMMING)\n",
    "matches = bf_orb.knnMatch(des_q_orb, des_s_orb, k=2)\n",
    "good = [m for m, n in matches if m.distance < 0.7 * n.distance]\n",
    "results[\"ORB + BF\"] = len(good)\n",
    "img_orb_bf = cv.drawMatches(\n",
    "    complex_query, kp_q_orb, complex_scene, kp_s_orb, good[:30], None, flags=2\n",
    ")\n",
    "\n",
    "# Display comparison\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "plt.subplot(3, 2, 1)\n",
    "plt.imshow(complex_query, cmap=\"gray\")\n",
    "plt.title(\"Query Object\", fontsize=14, fontweight=\"bold\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(3, 2, 2)\n",
    "plt.imshow(complex_scene, cmap=\"gray\")\n",
    "plt.title(\"Scene (rotated, scaled)\", fontsize=14, fontweight=\"bold\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(3, 2, 3)\n",
    "plt.imshow(img_sift_bf)\n",
    "plt.title(f'SIFT + BF: {results[\"SIFT + BF\"]} good matches', fontsize=12)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(3, 2, 4)\n",
    "plt.imshow(img_sift_flann)\n",
    "plt.title(f'SIFT + FLANN: {results[\"SIFT + FLANN\"]} good matches', fontsize=12)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(3, 2, 5)\n",
    "plt.imshow(img_orb_bf)\n",
    "plt.title(f'ORB + BF: {results[\"ORB + BF\"]} good matches', fontsize=12)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Summary\n",
    "plt.subplot(3, 2, 6)\n",
    "plt.text(\n",
    "    0.5,\n",
    "    0.5,\n",
    "    f\"Match Quality Comparison\\n\\n\"\n",
    "    + f'SIFT + BF: {results[\"SIFT + BF\"]} matches\\n'\n",
    "    + f'SIFT + FLANN: {results[\"SIFT + FLANN\"]} matches\\n'\n",
    "    + f'ORB + BF: {results[\"ORB + BF\"]} matches\\n\\n'\n",
    "    + f\"Recommendations:\\n\"\n",
    "    + f\"â€¢ SIFT: Best quality\\n\"\n",
    "    + f\"â€¢ FLANN: Best speed\\n\"\n",
    "    + f\"â€¢ ORB: Best free option\",\n",
    "    ha=\"center\",\n",
    "    va=\"center\",\n",
    "    fontsize=12,\n",
    "    family=\"monospace\",\n",
    ")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MATCHER COMPARISON RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "for method, count in results.items():\n",
    "    print(f\"{method:<20} {count} good matches\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Practical Exercises\n",
    "\n",
    "### Exercise 1: Tune Matching Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Experiment with ratio test threshold:\n",
    "# - Try values: 0.5, 0.6, 0.7, 0.8, 0.9\n",
    "# - How does it affect number of matches?\n",
    "# - How does it affect match quality?\n",
    "# - What's the best threshold for your use case?\n",
    "\n",
    "print(\"Experiment with different ratio thresholds!\")\n",
    "print(\"Lower threshold = fewer but better matches\")\n",
    "print(\"Higher threshold = more matches but more false positives\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### Exercise 2: Build a Multi-Object Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a scene with multiple objects\n",
    "# Try to detect each object separately\n",
    "# Draw bounding boxes with different colors\n",
    "# Count how many of each object is in the scene\n",
    "\n",
    "print(\"Build a system that can find multiple different objects!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### Exercise 3: Image Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Take two images of the same scene from different angles\n",
    "# Use feature matching and homography to align them\n",
    "# Warp one image to match the other's perspective\n",
    "# This is the basis for panorama stitching!\n",
    "\n",
    "print(\"Use feature matching for image alignment!\")\n",
    "print(\"Hint: Use cv.warpPerspective with the homography matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "Congratulations! You've completed Feature Matching. You now know:\n",
    "\n",
    "âœ“ Feature matching concepts and applications  \n",
    "âœ“ Brute-Force matcher - accurate but slower  \n",
    "âœ“ FLANN matcher - fast approximate matching  \n",
    "âœ“ Distance metrics (L2, Hamming) for different descriptors  \n",
    "âœ“ Lowe's ratio test for filtering false matches  \n",
    "âœ“ Homography for object detection and localization  \n",
    "âœ“ RANSAC for robust estimation  \n",
    "âœ“ Complete object recognition pipeline  \n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Feature matching finds correspondences** between images\n",
    "2. **BF matcher is accurate** - checks all possibilities\n",
    "3. **FLANN is fast** - approximate but good enough\n",
    "4. **Distance metric matters** - L2 for SIFT, Hamming for ORB\n",
    "5. **Ratio test filters false matches** - ratio < 0.7 typical\n",
    "6. **Homography finds object location** - transforms query to scene\n",
    "7. **RANSAC handles outliers** - robust to false matches\n",
    "8. **Need sufficient matches** - typically 10+ for homography\n",
    "\n",
    "---\n",
    "\n",
    "## Matching Strategy Guide\n",
    "\n",
    "| Scenario | Detector | Matcher | Distance | Why |\n",
    "|----------|----------|---------|----------|-----|\n",
    "| **High accuracy** | SIFT | BF | NORM_L2 | Best quality |\n",
    "| **Large dataset** | SIFT | FLANN | NORM_L2 | Fast matching |\n",
    "| **Real-time** | ORB | BF | NORM_HAMMING | Fast everything |\n",
    "| **Free/commercial** | ORB | FLANN | NORM_HAMMING | No patents |\n",
    "| **Rotation invariant** | SIFT/ORB | Either | Appropriate | Handle rotation |\n",
    "| **Scale invariant** | SIFT | Either | NORM_L2 | Handle scaling |\n",
    "\n",
    "---\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "In the next notebook (**09_hand_gesture_recognition.ipynb**), you'll learn:\n",
    "- MediaPipe hand tracking\n",
    "- Hand landmark detection\n",
    "- Gesture recognition\n",
    "- Building gesture-controlled applications\n",
    "\n",
    "---\n",
    "\n",
    "## Real-World Applications\n",
    "\n",
    "- **Augmented Reality**: Overlay graphics on real objects\n",
    "- **Panorama Stitching**: Combine multiple photos\n",
    "- **3D Reconstruction**: Build 3D models from 2D photos\n",
    "- **Visual SLAM**: Robot/drone navigation\n",
    "- **Product Recognition**: Identify products from photos\n",
    "- **Document Alignment**: Scan and align documents\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Coding!** ðŸŽ¯ðŸ”—"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
