{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Feature Detection and Description\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "- Understand what features are and why they're important\n",
    "- Detect corners using Harris and Shi-Tomasi algorithms\n",
    "- Use SIFT for scale-invariant feature detection\n",
    "- Apply FAST for real-time corner detection\n",
    "- Use ORB as a free alternative to SIFT\n",
    "- Understand feature descriptors and keypoints\n",
    "- Compare different feature detection methods\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u6rogsyab3",
   "source": "---\n\n**â±ï¸ Estimated Time**: 90-120 minutes  \n**ðŸ“š Level**: Advanced  \n**ðŸ“‹ Prerequisites**: Completed notebooks 00-06\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configure matplotlib\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (14, 8)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"OpenCV version: {cv.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Introduction to Features\n",
    "\n",
    "### What Are Features?\n",
    "\n",
    "**Features** are interesting or distinctive parts of an image that can be easily recognized and located. They are essential for:\n",
    "- **Object recognition** - Identifying objects in images\n",
    "- **Image matching** - Finding similar images\n",
    "- **3D reconstruction** - Building 3D models from 2D images\n",
    "- **Motion tracking** - Following objects across frames\n",
    "- **Image stitching** - Creating panoramas\n",
    "\n",
    "### Types of Features:\n",
    "\n",
    "1. **Edges** - Boundaries where intensity changes rapidly\n",
    "2. **Corners** - Points where two edges meet (high curvature)\n",
    "3. **Blobs** - Regions that differ in properties (color, brightness)\n",
    "4. **Ridges** - Line-like features\n",
    "\n",
    "### Good Features Should Be:\n",
    "\n",
    "- **Repeatable** - Found in different images of the same scene\n",
    "- **Distinctive** - Unique, easy to distinguish from other features\n",
    "- **Efficient** - Fast to compute\n",
    "- **Local** - Not affected by occlusion or clutter\n",
    "- **Invariant** - Resistant to changes in:\n",
    "  - Scale (zoom in/out)\n",
    "  - Rotation (turned images)\n",
    "  - Illumination (lighting changes)\n",
    "  - Viewpoint (different angles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test image to demonstrate features\n",
    "test_img = np.ones((400, 500, 3), dtype=np.uint8) * 200\n",
    "\n",
    "# Add various features\n",
    "# 1. Corners (L-shape)\n",
    "cv.rectangle(test_img, (50, 50), (150, 80), (100, 100, 100), -1)\n",
    "cv.rectangle(test_img, (50, 80), (80, 180), (100, 100, 100), -1)\n",
    "\n",
    "# 2. More corners (squares)\n",
    "cv.rectangle(test_img, (200, 50), (300, 150), (80, 80, 80), -1)\n",
    "cv.rectangle(test_img, (350, 50), (450, 150), (90, 90, 90), -1)\n",
    "\n",
    "# 3. Circular features (blobs)\n",
    "cv.circle(test_img, (100, 280), 40, (70, 70, 70), -1)\n",
    "cv.circle(test_img, (250, 300), 35, (75, 75, 75), -1)\n",
    "cv.circle(test_img, (400, 290), 38, (85, 85, 85), -1)\n",
    "\n",
    "# 4. Edge (vertical line)\n",
    "cv.rectangle(test_img, (175, 200), (185, 350), (60, 60, 60), -1)\n",
    "\n",
    "# Convert to grayscale for feature detection\n",
    "test_gray = cv.cvtColor(test_img, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "# Annotate feature types\n",
    "annotated = test_img.copy()\n",
    "cv.circle(annotated, (150, 80), 8, (0, 0, 255), -1)  # Corner\n",
    "cv.putText(annotated, \"Corner\", (160, 85), cv.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "\n",
    "cv.circle(annotated, (300, 150), 8, (0, 255, 0), -1)  # Corner\n",
    "cv.putText(annotated, \"Corner\", (310, 155), cv.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "cv.circle(annotated, (100, 280), 8, (255, 0, 0), -1)  # Blob\n",
    "cv.putText(annotated, \"Blob\", (50, 330), cv.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "\n",
    "cv.putText(annotated, \"Edge\", (190, 280), cv.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 2)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(test_gray, cmap=\"gray\")\n",
    "plt.title(\"Test Image for Feature Detection\", fontsize=14)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(cv.cvtColor(annotated, cv.COLOR_BGR2RGB))\n",
    "plt.title(\"Types of Features\", fontsize=14)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Features are interesting points that can be reliably detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Harris Corner Detector\n",
    "\n",
    "### What is a Corner?\n",
    "\n",
    "A **corner** is a point where two edges meet. It has large intensity variation in multiple directions.\n",
    "\n",
    "- **Flat region**: No change in any direction\n",
    "- **Edge**: Change in one direction\n",
    "- **Corner**: Change in all directions âœ“\n",
    "\n",
    "### Harris Corner Detection:\n",
    "\n",
    "The **Harris corner detector** analyzes local gradient changes to find corners.\n",
    "\n",
    "**Parameters**:\n",
    "- `blockSize`: Size of neighborhood for corner detection\n",
    "- `ksize`: Aperture parameter for Sobel derivative\n",
    "- `k`: Harris detector free parameter (typically 0.04-0.06)\n",
    "\n",
    "**Syntax**: `cv.cornerHarris(image, blockSize, ksize, k)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Harris corner detection\n",
    "harris_corners = cv.cornerHarris(test_gray, blockSize=2, ksize=3, k=0.04)\n",
    "\n",
    "# Dilate to mark corners\n",
    "harris_corners = cv.dilate(harris_corners, None)\n",
    "\n",
    "# Threshold: mark corners in red\n",
    "threshold = 0.01 * harris_corners.max()\n",
    "result_harris = test_img.copy()\n",
    "result_harris[harris_corners > threshold] = [0, 0, 255]\n",
    "\n",
    "# Display\n",
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(test_gray, cmap=\"gray\")\n",
    "plt.title(\"Original Image\", fontsize=14)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(harris_corners, cmap=\"hot\")\n",
    "plt.title(\"Harris Corner Response\\n(Brighter = stronger corner)\", fontsize=12)\n",
    "plt.colorbar()\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(cv.cvtColor(result_harris, cv.COLOR_BGR2RGB))\n",
    "plt.title(\"Harris Corners Detected (Red)\", fontsize=14)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Count corners\n",
    "num_corners = np.sum(harris_corners > threshold)\n",
    "print(f\"Harris detected {num_corners} corner points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Shi-Tomasi Corner Detector\n",
    "\n",
    "### Improved Harris Detector\n",
    "\n",
    "The **Shi-Tomasi** method is an improved version of Harris corner detector. It provides better corner detection by using a different scoring function.\n",
    "\n",
    "**Advantages**:\n",
    "- More accurate corner selection\n",
    "- Better for tracking applications\n",
    "- Can specify maximum number of corners\n",
    "- Ensures minimum distance between corners\n",
    "\n",
    "**Parameters**:\n",
    "- `maxCorners`: Maximum number of corners to return\n",
    "- `qualityLevel`: Minimum quality of corners (0-1)\n",
    "- `minDistance`: Minimum Euclidean distance between returned corners\n",
    "\n",
    "**Syntax**: `cv.goodFeaturesToTrack(image, maxCorners, qualityLevel, minDistance)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Shi-Tomasi corner detection\n",
    "corners = cv.goodFeaturesToTrack(test_gray, maxCorners=50, qualityLevel=0.01, minDistance=10)\n",
    "\n",
    "# Draw corners\n",
    "result_shi_tomasi = test_img.copy()\n",
    "if corners is not None:\n",
    "    corners = corners.astype(int)\n",
    "    for corner in corners:\n",
    "        x, y = corner.ravel()\n",
    "        cv.circle(result_shi_tomasi, (x, y), 5, (0, 255, 0), -1)\n",
    "\n",
    "# Compare with different parameters\n",
    "corners_few = cv.goodFeaturesToTrack(test_gray, maxCorners=10, qualityLevel=0.05, minDistance=30)\n",
    "result_few = test_img.copy()\n",
    "if corners_few is not None:\n",
    "    corners_few = corners_few.astype(int)\n",
    "    for i, corner in enumerate(corners_few):\n",
    "        x, y = corner.ravel()\n",
    "        cv.circle(result_few, (x, y), 7, (255, 0, 0), -1)\n",
    "        cv.putText(\n",
    "            result_few, str(i + 1), (x + 10, y), cv.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2\n",
    "        )\n",
    "\n",
    "# Display\n",
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(test_gray, cmap=\"gray\")\n",
    "plt.title(\"Original Image\", fontsize=14)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(cv.cvtColor(result_shi_tomasi, cv.COLOR_BGR2RGB))\n",
    "plt.title(f\"Shi-Tomasi: {len(corners)} corners\\n(maxCorners=50, quality=0.01)\", fontsize=12)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(cv.cvtColor(result_few, cv.COLOR_BGR2RGB))\n",
    "plt.title(\n",
    "    f\"Shi-Tomasi: {len(corners_few)} best corners\\n(maxCorners=10, quality=0.05)\", fontsize=12\n",
    ")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Shi-Tomasi is better at selecting the strongest corners\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: SIFT - Scale-Invariant Feature Transform\n",
    "\n",
    "### What is SIFT?\n",
    "\n",
    "**SIFT** (Scale-Invariant Feature Transform) is an advanced feature detection algorithm that is invariant to:\n",
    "- **Scale** (zoom in/out)\n",
    "- **Rotation** (image turned)\n",
    "- **Illumination** (lighting changes)\n",
    "- Partial **viewpoint** changes\n",
    "\n",
    "### How SIFT Works:\n",
    "\n",
    "1. **Scale-space extrema detection** - Find potential keypoints across different scales\n",
    "2. **Keypoint localization** - Refine keypoint locations\n",
    "3. **Orientation assignment** - Assign orientation to each keypoint\n",
    "4. **Descriptor generation** - Create unique fingerprint (128-dim vector)\n",
    "\n",
    "### SIFT Keypoint Properties:\n",
    "\n",
    "- `pt` - (x, y) coordinates\n",
    "- `size` - Diameter of meaningful keypoint neighborhood\n",
    "- `angle` - Orientation in degrees\n",
    "- `response` - Strength of the keypoint\n",
    "- `octave` - Pyramid layer where keypoint was detected\n",
    "\n",
    "**Note**: SIFT is patented. Free for research, license required for commercial use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SIFT detector\n",
    "sift = cv.SIFT_create()\n",
    "\n",
    "# Detect keypoints and compute descriptors\n",
    "keypoints_sift, descriptors_sift = sift.detectAndCompute(test_gray, None)\n",
    "\n",
    "# Draw keypoints\n",
    "result_sift = cv.drawKeypoints(\n",
    "    test_img, keypoints_sift, None, flags=cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS\n",
    ")\n",
    "\n",
    "# Display\n",
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(test_gray, cmap=\"gray\")\n",
    "plt.title(\"Original Image\", fontsize=14)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(cv.cvtColor(result_sift, cv.COLOR_BGR2RGB))\n",
    "plt.title(\n",
    "    f\"SIFT Keypoints: {len(keypoints_sift)} detected\\n(Circle = size, Line = orientation)\",\n",
    "    fontsize=12,\n",
    ")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"SIFT detected {len(keypoints_sift)} keypoints\")\n",
    "print(f\"Each descriptor is a {descriptors_sift.shape[1]}-dimensional vector\")\n",
    "print(f\"\\nSample keypoint properties:\")\n",
    "if len(keypoints_sift) > 0:\n",
    "    kp = keypoints_sift[0]\n",
    "    print(f\"  Position: ({kp.pt[0]:.1f}, {kp.pt[1]:.1f})\")\n",
    "    print(f\"  Size: {kp.size:.1f}\")\n",
    "    print(f\"  Angle: {kp.angle:.1f}Â°\")\n",
    "    print(f\"  Response: {kp.response:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "### SIFT Scale Invariance Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create same image at different scales\n",
    "small_img = cv.resize(test_gray, None, fx=0.5, fy=0.5)\n",
    "large_img = cv.resize(test_gray, None, fx=1.5, fy=1.5)\n",
    "\n",
    "# Detect SIFT features at different scales\n",
    "kp_original, _ = sift.detectAndCompute(test_gray, None)\n",
    "kp_small, _ = sift.detectAndCompute(small_img, None)\n",
    "kp_large, _ = sift.detectAndCompute(large_img, None)\n",
    "\n",
    "# Draw keypoints\n",
    "result_original = cv.drawKeypoints(test_gray, kp_original, None, (0, 255, 0))\n",
    "result_small = cv.drawKeypoints(small_img, kp_small, None, (0, 255, 0))\n",
    "result_large = cv.drawKeypoints(large_img, kp_large, None, (0, 255, 0))\n",
    "\n",
    "# Display\n",
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(result_small, cmap=\"gray\")\n",
    "plt.title(f\"Small (50%)\\n{len(kp_small)} keypoints\", fontsize=12)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(result_original, cmap=\"gray\")\n",
    "plt.title(f\"Original (100%)\\n{len(kp_original)} keypoints\", fontsize=12)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(result_large, cmap=\"gray\")\n",
    "plt.title(f\"Large (150%)\\n{len(kp_large)} keypoints\", fontsize=12)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"SIFT detects similar features across different scales!\")\n",
    "print(\"This makes it excellent for object recognition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: FAST - Features from Accelerated Segment Test\n",
    "\n",
    "### What is FAST?\n",
    "\n",
    "**FAST** is a corner detection algorithm designed for **real-time** applications. It's much faster than Harris and SIFT.\n",
    "\n",
    "### How FAST Works:\n",
    "\n",
    "1. Select a pixel `p`\n",
    "2. Consider a circle of 16 pixels around `p`\n",
    "3. If N contiguous pixels are all brighter or darker than `p` (+ threshold), then `p` is a corner\n",
    "4. Use N = 12 for high-speed detection (FAST-12)\n",
    "\n",
    "### Advantages:\n",
    "- **Very fast** - suitable for real-time applications\n",
    "- **Simple** - easy to implement\n",
    "- **Effective** - good corner detection\n",
    "\n",
    "### Disadvantages:\n",
    "- Not scale-invariant\n",
    "- Not rotation-invariant\n",
    "- Sensitive to noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FAST detector\n",
    "fast = cv.FastFeatureDetector_create()\n",
    "\n",
    "# Detect keypoints\n",
    "keypoints_fast = fast.detect(test_gray, None)\n",
    "\n",
    "# Draw keypoints\n",
    "result_fast = cv.drawKeypoints(test_img, keypoints_fast, None, color=(0, 255, 0))\n",
    "\n",
    "# Try with different thresholds\n",
    "fast_low = cv.FastFeatureDetector_create(threshold=10)\n",
    "fast_high = cv.FastFeatureDetector_create(threshold=50)\n",
    "\n",
    "kp_low = fast_low.detect(test_gray, None)\n",
    "kp_high = fast_high.detect(test_gray, None)\n",
    "\n",
    "result_low = cv.drawKeypoints(test_img, kp_low, None, color=(255, 0, 0))\n",
    "result_high = cv.drawKeypoints(test_img, kp_high, None, color=(0, 0, 255))\n",
    "\n",
    "# Display\n",
    "plt.figure(figsize=(18, 10))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.imshow(test_gray, cmap=\"gray\")\n",
    "plt.title(\"Original Image\", fontsize=14)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.imshow(cv.cvtColor(result_fast, cv.COLOR_BGR2RGB))\n",
    "plt.title(f\"FAST (default threshold)\\n{len(keypoints_fast)} keypoints\", fontsize=12)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.imshow(cv.cvtColor(result_low, cv.COLOR_BGR2RGB))\n",
    "plt.title(f\"FAST (threshold=10, sensitive)\\n{len(kp_low)} keypoints\", fontsize=12)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.imshow(cv.cvtColor(result_high, cv.COLOR_BGR2RGB))\n",
    "plt.title(f\"FAST (threshold=50, strict)\\n{len(kp_high)} keypoints\", fontsize=12)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"FAST is very fast but detects many keypoints\")\n",
    "print(f\"Lower threshold = more keypoints (may include noise)\")\n",
    "print(f\"Higher threshold = fewer keypoints (only strong corners)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: ORB - Oriented FAST and Rotated BRIEF\n",
    "\n",
    "### What is ORB?\n",
    "\n",
    "**ORB** (Oriented FAST and Rotated BRIEF) is a **free** alternative to SIFT. It combines:\n",
    "- **FAST** keypoint detector\n",
    "- **BRIEF** descriptor (modified for rotation invariance)\n",
    "\n",
    "### Advantages:\n",
    "\n",
    "- **Fast** - 2 orders of magnitude faster than SIFT\n",
    "- **Free** - No patent restrictions\n",
    "- **Rotation invariant** - Handles rotated images\n",
    "- **Robust** - Good performance in many scenarios\n",
    "- **Binary descriptors** - Efficient matching using Hamming distance\n",
    "\n",
    "### ORB vs SIFT:\n",
    "\n",
    "| Feature | SIFT | ORB |\n",
    "|---------|------|-----|\n",
    "| Speed | Slow | Fast |\n",
    "| Patent | Yes | No |\n",
    "| Descriptor | 128-dim float | 256-bit binary |\n",
    "| Scale invariant | Yes | Limited |\n",
    "| Rotation invariant | Yes | Yes |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ORB detector\n",
    "orb = cv.ORB_create()\n",
    "\n",
    "# Detect keypoints and compute descriptors\n",
    "keypoints_orb, descriptors_orb = orb.detectAndCompute(test_gray, None)\n",
    "\n",
    "# Draw keypoints with orientation\n",
    "result_orb = cv.drawKeypoints(\n",
    "    test_img,\n",
    "    keypoints_orb,\n",
    "    None,\n",
    "    flags=cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS,\n",
    "    color=(255, 0, 255),\n",
    ")\n",
    "\n",
    "# Compare ORB with different number of features\n",
    "orb_few = cv.ORB_create(nfeatures=20)\n",
    "orb_many = cv.ORB_create(nfeatures=200)\n",
    "\n",
    "kp_few, _ = orb_few.detectAndCompute(test_gray, None)\n",
    "kp_many, _ = orb_many.detectAndCompute(test_gray, None)\n",
    "\n",
    "result_few = cv.drawKeypoints(test_img, kp_few, None, color=(0, 255, 255))\n",
    "result_many = cv.drawKeypoints(test_img, kp_many, None, color=(255, 255, 0))\n",
    "\n",
    "# Display\n",
    "plt.figure(figsize=(18, 10))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.imshow(test_gray, cmap=\"gray\")\n",
    "plt.title(\"Original Image\", fontsize=14)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.imshow(cv.cvtColor(result_orb, cv.COLOR_BGR2RGB))\n",
    "plt.title(f\"ORB (default)\\n{len(keypoints_orb)} keypoints\", fontsize=12)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.imshow(cv.cvtColor(result_few, cv.COLOR_BGR2RGB))\n",
    "plt.title(f\"ORB (nfeatures=20)\\n{len(kp_few)} keypoints\", fontsize=12)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.imshow(cv.cvtColor(result_many, cv.COLOR_BGR2RGB))\n",
    "plt.title(f\"ORB (nfeatures=200)\\n{len(kp_many)} keypoints\", fontsize=12)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"ORB detected {len(keypoints_orb)} keypoints\")\n",
    "print(f\"Descriptor type: {descriptors_orb.dtype} (binary)\")\n",
    "print(f\"Descriptor shape: {descriptors_orb.shape} (256 bits = 32 bytes)\")\n",
    "print(f\"\\nORB is fast and free - great for real-time applications!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "### ORB Rotation Invariance Demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": "### ðŸ’¡ Parameter Tuning Tips for Feature Detectors\n\n**Harris Corner Detector**:\n- `blockSize` (3-7): Size of neighborhood, larger = less sensitive\n- `ksize` (3-31, odd): Sobel kernel size, larger = smoother gradients\n- `k` (0.04-0.06): Harris parameter, 0.04-0.06 works well for most cases\n\n**Shi-Tomasi (Good Features to Track)**:\n- `maxCorners` (10-1000): Number of corners to detect\n- `qualityLevel` (0.01-0.1): Only corners above quality*max_quality are returned\n- `minDistance` (5-50): Minimum distance between detected corners\n\n**SIFT**:\n- `nfeatures` (0-5000): Max number of features (0 = unlimited)\n- `contrastThreshold` (0.03-0.1): Higher = fewer features, more robust\n- `edgeThreshold` (5-20): Higher = more features including edges\n\n**FAST**:\n- `threshold` (1-50): Pixel intensity difference threshold, lower = more features\n- `nonmaxSuppression`: Always use True for better corner localization\n\n**ORB**:\n- `nfeatures` (100-10000): Number of features to detect\n- `scaleFactor` (1.1-2.0): Pyramid decimation ratio, higher = faster but less scales\n- `nlevels` (3-12): Number of pyramid levels, more = better scale invariance\n\n**General Tips**:\n- Start with defaults and adjust based on your specific image type\n- More features â‰  better results; quality matters more than quantity\n- Balance between detection speed and accuracy"
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": "### ðŸ’¡ Parameter Tuning Tips for Feature Detectors\n\n**Harris Corner Detector**:\n- `blockSize` (3-7): Size of neighborhood, larger = less sensitive\n- `ksize` (3-31, odd): Sobel kernel size, larger = smoother gradients\n- `k` (0.04-0.06): Harris parameter, 0.04-0.06 works well for most cases\n\n**Shi-Tomasi (Good Features to Track)**:\n- `maxCorners` (10-1000): Number of corners to detect\n- `qualityLevel` (0.01-0.1): Only corners above quality*max_quality are returned\n- `minDistance` (5-50): Minimum distance between detected corners\n\n**SIFT**:\n- `nfeatures` (0-5000): Max number of features (0 = unlimited)\n- `contrastThreshold` (0.03-0.1): Higher = fewer features, more robust\n- `edgeThreshold` (5-20): Higher = more features including edges\n\n**FAST**:\n- `threshold` (1-50): Pixel intensity difference threshold, lower = more features\n- `nonmaxSuppression`: Always use True for better corner localization\n\n**ORB**:\n- `nfeatures` (100-10000): Number of features to detect\n- `scaleFactor` (1.1-2.0): Pyramid decimation ratio, higher = faster but less scales\n- `nlevels` (3-12): Number of pyramid levels, more = better scale invariance\n\n**General Tips**:\n- Start with defaults and adjust based on your specific image type\n- More features â‰  better results; quality matters more than quantity\n- Balance between detection speed and accuracy"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Create a more complex test image\n",
    "complex_img = np.ones((500, 600), dtype=np.uint8) * 150\n",
    "\n",
    "# Add various features\n",
    "cv.rectangle(complex_img, (50, 50), (200, 150), 80, -1)\n",
    "cv.rectangle(complex_img, (250, 50), (350, 200), 70, -1)\n",
    "cv.circle(complex_img, (450, 100), 60, 90, -1)\n",
    "cv.rectangle(complex_img, (100, 250), (500, 300), 100, -1)\n",
    "for i in range(5):\n",
    "    cv.circle(complex_img, (100 + i * 100, 400), 30, 60, -1)\n",
    "\n",
    "# Apply all detectors and measure time\n",
    "results = {}\n",
    "\n",
    "# Harris\n",
    "start = time.time()\n",
    "harris = cv.cornerHarris(complex_img, 2, 3, 0.04)\n",
    "harris = cv.dilate(harris, None)\n",
    "harris_time = time.time() - start\n",
    "harris_count = np.sum(harris > 0.01 * harris.max())\n",
    "results[\"Harris\"] = (harris_count, harris_time)\n",
    "\n",
    "# Shi-Tomasi\n",
    "start = time.time()\n",
    "shi_tomasi = cv.goodFeaturesToTrack(complex_img, 100, 0.01, 10)\n",
    "shi_tomasi_time = time.time() - start\n",
    "shi_tomasi_count = len(shi_tomasi) if shi_tomasi is not None else 0\n",
    "results[\"Shi-Tomasi\"] = (shi_tomasi_count, shi_tomasi_time)\n",
    "\n",
    "# SIFT\n",
    "start = time.time()\n",
    "sift = cv.SIFT_create()\n",
    "kp_sift, _ = sift.detectAndCompute(complex_img, None)\n",
    "sift_time = time.time() - start\n",
    "results[\"SIFT\"] = (len(kp_sift), sift_time)\n",
    "\n",
    "# FAST\n",
    "start = time.time()\n",
    "fast = cv.FastFeatureDetector_create()\n",
    "kp_fast = fast.detect(complex_img, None)\n",
    "fast_time = time.time() - start\n",
    "results[\"FAST\"] = (len(kp_fast), fast_time)\n",
    "\n",
    "# ORB\n",
    "start = time.time()\n",
    "orb = cv.ORB_create()\n",
    "kp_orb, _ = orb.detectAndCompute(complex_img, None)\n",
    "orb_time = time.time() - start\n",
    "results[\"ORB\"] = (len(kp_orb), orb_time)\n",
    "\n",
    "# Create comparison visualization\n",
    "result_harris_img = cv.cvtColor(complex_img, cv.COLOR_GRAY2BGR)\n",
    "result_harris_img[harris > 0.01 * harris.max()] = [0, 0, 255]\n",
    "\n",
    "result_shi_img = cv.cvtColor(complex_img, cv.COLOR_GRAY2BGR)\n",
    "if shi_tomasi is not None:\n",
    "    for corner in shi_tomasi.astype(int):\n",
    "        x, y = corner.ravel()\n",
    "        cv.circle(result_shi_img, (x, y), 3, (0, 255, 0), -1)\n",
    "\n",
    "result_sift_img = cv.drawKeypoints(\n",
    "    cv.cvtColor(complex_img, cv.COLOR_GRAY2BGR), kp_sift, None, color=(255, 0, 255)\n",
    ")\n",
    "result_fast_img = cv.drawKeypoints(\n",
    "    cv.cvtColor(complex_img, cv.COLOR_GRAY2BGR), kp_fast, None, color=(0, 255, 255)\n",
    ")\n",
    "result_orb_img = cv.drawKeypoints(\n",
    "    cv.cvtColor(complex_img, cv.COLOR_GRAY2BGR), kp_orb, None, color=(255, 255, 0)\n",
    ")\n",
    "\n",
    "# Display all\n",
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.imshow(complex_img, cmap=\"gray\")\n",
    "plt.title(\"Original Image\", fontsize=14, fontweight=\"bold\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.imshow(cv.cvtColor(result_harris_img, cv.COLOR_BGR2RGB))\n",
    "plt.title(f\"Harris Corner Detector\\n{harris_count} corners | {harris_time*1000:.2f}ms\", fontsize=11)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.imshow(cv.cvtColor(result_shi_img, cv.COLOR_BGR2RGB))\n",
    "plt.title(f\"Shi-Tomasi\\n{shi_tomasi_count} corners | {shi_tomasi_time*1000:.2f}ms\", fontsize=11)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.imshow(cv.cvtColor(result_sift_img, cv.COLOR_BGR2RGB))\n",
    "plt.title(f\"SIFT\\n{len(kp_sift)} keypoints | {sift_time*1000:.2f}ms\", fontsize=11)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.imshow(cv.cvtColor(result_fast_img, cv.COLOR_BGR2RGB))\n",
    "plt.title(f\"FAST\\n{len(kp_fast)} keypoints | {fast_time*1000:.2f}ms âš¡\", fontsize=11)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.imshow(cv.cvtColor(result_orb_img, cv.COLOR_BGR2RGB))\n",
    "plt.title(f\"ORB\\n{len(kp_orb)} keypoints | {orb_time*1000:.2f}ms âš¡\", fontsize=11)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"FEATURE DETECTION COMPARISON\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"{'Method':<15} {'Keypoints':<15} {'Time (ms)':<15} {'Speed'}\")\n",
    "print(f\"{'-'*70}\")\n",
    "for method, (count, exec_time) in results.items():\n",
    "    speed = \"âš¡âš¡âš¡\" if exec_time < 0.01 else (\"âš¡âš¡\" if exec_time < 0.05 else \"âš¡\")\n",
    "    print(f\"{method:<15} {count:<15} {exec_time*1000:<15.2f} {speed}\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\nKey Insights:\")\n",
    "print(f\"- FAST is the fastest (hence the name!)\")\n",
    "print(f\"- SIFT provides scale-invariant features but is slower\")\n",
    "print(f\"- ORB is fast and free - best overall choice for many applications\")\n",
    "print(f\"- Harris/Shi-Tomasi are good for simple corner detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Practical Exercises\n",
    "\n",
    "### Exercise 1: Tune Feature Detector Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create your own image and experiment with:\n",
    "# - Harris k parameter (0.04, 0.06, 0.08)\n",
    "# - Shi-Tomasi quality level (0.01, 0.05, 0.1)\n",
    "# - FAST threshold (10, 30, 50)\n",
    "# - ORB nfeatures (50, 100, 500)\n",
    "# See how parameters affect detection\n",
    "\n",
    "print(\"Experiment with different parameters!\")\n",
    "print(\"Understanding parameters helps you optimize for your specific use case\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "### Exercise 2: Feature Detection on Real Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a more realistic image (building, object, pattern)\n",
    "# Compare how different detectors perform:\n",
    "# - Which finds the most useful features?\n",
    "# - Which is most stable across transformations?\n",
    "# - Which is fastest?\n",
    "\n",
    "print(\"Test detectors on realistic images!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "### Exercise 3: Build a Feature Detector Comparison Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a function that:\n",
    "# 1. Takes an image and a list of detectors\n",
    "# 2. Runs all detectors\n",
    "# 3. Displays results side-by-side\n",
    "# 4. Shows statistics (count, time, coverage)\n",
    "\n",
    "\n",
    "def compare_detectors(image, detectors=[\"harris\", \"sift\", \"fast\", \"orb\"]):\n",
    "    \"\"\"\n",
    "    Compare multiple feature detectors on the same image.\n",
    "    \"\"\"\n",
    "    # Your implementation here!\n",
    "    pass\n",
    "\n",
    "\n",
    "print(\"Build a comprehensive comparison tool!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "Congratulations! You've completed Feature Detection. You now know:\n",
    "\n",
    "âœ“ What features are and why they're important  \n",
    "âœ“ Harris corner detector - classic corner detection  \n",
    "âœ“ Shi-Tomasi - improved Harris for better corner selection  \n",
    "âœ“ SIFT - scale-invariant feature detection and description  \n",
    "âœ“ FAST - ultra-fast corner detection for real-time use  \n",
    "âœ“ ORB - free, fast, rotation-invariant alternative to SIFT  \n",
    "âœ“ Keypoint properties and descriptors  \n",
    "âœ“ Trade-offs between speed, accuracy, and invariance  \n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Features are distinctive points** - corners, blobs, edges\n",
    "2. **Good features are repeatable and distinctive** - found consistently\n",
    "3. **Harris and Shi-Tomasi find corners** - fast, simple\n",
    "4. **SIFT is scale and rotation invariant** - best quality, slower, patented\n",
    "5. **FAST is extremely fast** - real-time applications\n",
    "6. **ORB is the best free alternative** - fast, rotation-invariant, no patent\n",
    "7. **Binary vs float descriptors** - ORB uses binary (Hamming), SIFT uses float (L2)\n",
    "8. **Trade-off between speed and invariance** - choose based on your needs\n",
    "\n",
    "---\n",
    "\n",
    "## Feature Detector Quick Reference\n",
    "\n",
    "| Method | Speed | Scale Inv. | Rotation Inv. | Free | Best For |\n",
    "|--------|-------|------------|---------------|------|----------|\n",
    "| **Harris** | Fast | âŒ | âŒ | âœ“ | Simple corner detection |\n",
    "| **Shi-Tomasi** | Fast | âŒ | âŒ | âœ“ | Corner tracking |\n",
    "| **SIFT** | Slow | âœ“ | âœ“ | âŒ | High-quality matching |\n",
    "| **FAST** | Very Fast | âŒ | âŒ | âœ“ | Real-time applications |\n",
    "| **ORB** | Fast | Limited | âœ“ | âœ“ | General purpose |\n",
    "\n",
    "---\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "In the next notebook (**08_feature_matching.ipynb**), you'll learn:\n",
    "- Matching features between images\n",
    "- Brute-force vs FLANN matchers\n",
    "- Ratio test for good matches\n",
    "- Finding objects using homography\n",
    "- Image alignment and panorama stitching\n",
    "\n",
    "---\n",
    "\n",
    "## Real-World Applications\n",
    "\n",
    "- **Object Recognition**: Identifying objects in images\n",
    "- **Image Stitching**: Creating panoramas\n",
    "- **3D Reconstruction**: Building 3D models from photos\n",
    "- **Motion Tracking**: Following objects in video\n",
    "- **Augmented Reality**: Overlaying digital content on real world\n",
    "- **Robot Navigation**: Visual SLAM and localization\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Coding!** ðŸ”ðŸ“·"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
