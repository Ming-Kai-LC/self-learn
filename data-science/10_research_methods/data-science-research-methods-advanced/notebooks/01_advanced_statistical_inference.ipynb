{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 01: Advanced Statistical Inference\n",
    "\n",
    "**Estimated Time:** 45 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this module, you will be able to:\n",
    "\n",
    "1. ‚úÖ Conduct and interpret hypothesis tests (t-tests, ANOVA, chi-square)\n",
    "2. ‚úÖ Calculate and interpret effect sizes (Cohen's d, eta-squared, Cram√©r's V)\n",
    "3. ‚úÖ Understand and calculate statistical power\n",
    "4. ‚úÖ Distinguish between statistical and practical significance\n",
    "5. ‚úÖ Address multiple comparison problems with corrections\n",
    "6. ‚úÖ Report statistical results professionally\n",
    "\n",
    "## Why Advanced Statistical Inference Matters\n",
    "\n",
    "**Beginner Question:** \"Is there a difference?\"\n",
    "\n",
    "**Intermediate Question:** \"How big is the difference? Is it meaningful? Am I confident in this result?\"\n",
    "\n",
    "### The Problem with p-values Alone\n",
    "\n",
    "A common mistake:\n",
    "- ‚ùå \"p < 0.05, therefore it's important!\"\n",
    "- ‚úÖ \"p < 0.05, effect size is large, power is adequate, therefore it's important!\"\n",
    "\n",
    "**Statistical significance ‚â† Practical significance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Try to import pingouin for advanced stats\n",
    "try:\n",
    "    import pingouin as pg\n",
    "\n",
    "    PINGOUIN_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PINGOUIN_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è  Pingouin not installed. Install with: pip install pingouin\")\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "\n",
    "# Random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create output directory\n",
    "import os\n",
    "\n",
    "output_dir = \"outputs/module01\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Module 01: Advanced Statistical Inference - Setup Complete!\")\n",
    "print(f\"üìÅ Outputs will be saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Hypothesis Testing Fundamentals\n",
    "\n",
    "### The Null Hypothesis Significance Testing (NHST) Framework\n",
    "\n",
    "#### Core Concepts:\n",
    "\n",
    "1. **Null Hypothesis (H‚ÇÄ)**: No effect/difference exists\n",
    "2. **Alternative Hypothesis (H‚ÇÅ)**: An effect/difference exists\n",
    "3. **Test Statistic**: Quantifies how extreme your data is\n",
    "4. **p-value**: Probability of observing this data (or more extreme) if H‚ÇÄ is true\n",
    "5. **Significance Level (Œ±)**: Threshold for rejecting H‚ÇÄ (typically 0.05)\n",
    "\n",
    "### Types of Errors\n",
    "\n",
    "| Reality | Decision: Accept H‚ÇÄ | Decision: Reject H‚ÇÄ |\n",
    "|---------|---------------------|----------------------|\n",
    "| **H‚ÇÄ is TRUE** | ‚úÖ Correct | ‚ùå Type I Error (False Positive) |\n",
    "| **H‚ÇÄ is FALSE** | ‚ùå Type II Error (False Negative) | ‚úÖ Correct |\n",
    "\n",
    "**Type I Error (Œ±)**: False alarm - saying there's an effect when there isn't\n",
    "\n",
    "**Type II Error (Œ≤)**: Missing a real effect\n",
    "\n",
    "**Power (1-Œ≤)**: Probability of correctly detecting a real effect\n",
    "\n",
    "### Common Hypothesis Tests\n",
    "\n",
    "| Test | Use Case | Variables |\n",
    "|------|----------|----------|\n",
    "| **t-test** | Compare 2 group means | 1 categorical (2 levels) + 1 continuous |\n",
    "| **ANOVA** | Compare 3+ group means | 1 categorical (3+ levels) + 1 continuous |\n",
    "| **Chi-square** | Test independence | 2 categorical variables |\n",
    "| **Correlation** | Test association | 2 continuous variables |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate Type I and Type II errors through simulation\n",
    "\n",
    "\n",
    "def simulate_hypothesis_tests(true_effect_size=0, n_simulations=1000, sample_size=50, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Simulate hypothesis tests to demonstrate error rates.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    true_effect_size : float\n",
    "        True Cohen's d (0 = no effect)\n",
    "    n_simulations : int\n",
    "        Number of simulations to run\n",
    "    sample_size : int\n",
    "        Sample size per group\n",
    "    alpha : float\n",
    "        Significance level\n",
    "    \"\"\"\n",
    "    significant_results = 0\n",
    "\n",
    "    for _ in range(n_simulations):\n",
    "        # Generate data\n",
    "        group1 = np.random.normal(0, 1, sample_size)\n",
    "        group2 = np.random.normal(true_effect_size, 1, sample_size)\n",
    "\n",
    "        # Perform t-test\n",
    "        t_stat, p_value = stats.ttest_ind(group1, group2)\n",
    "\n",
    "        if p_value < alpha:\n",
    "            significant_results += 1\n",
    "\n",
    "    proportion_significant = significant_results / n_simulations\n",
    "\n",
    "    return proportion_significant\n",
    "\n",
    "\n",
    "# Simulate Type I error (no true effect)\n",
    "print(\"SIMULATION: Type I Error Rate\")\n",
    "print(\"=\" * 80)\n",
    "type1_rate = simulate_hypothesis_tests(true_effect_size=0, n_simulations=1000)\n",
    "print(f\"True effect size: 0 (no effect)\")\n",
    "print(f\"Proportion of significant results: {type1_rate:.3f}\")\n",
    "print(f\"Expected Type I error rate (Œ±): 0.05\")\n",
    "print(f\"Actual rate: {type1_rate:.3f}\")\n",
    "print(\"\\nüí° When there's no effect, we should reject H‚ÇÄ about 5% of the time (Type I error)\")\n",
    "\n",
    "# Simulate Power (with true effect)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SIMULATION: Statistical Power\")\n",
    "print(\"=\" * 80)\n",
    "power = simulate_hypothesis_tests(true_effect_size=0.5, n_simulations=1000)\n",
    "print(f\"True effect size: 0.5 (medium effect)\")\n",
    "print(f\"Proportion of significant results (Power): {power:.3f}\")\n",
    "print(f\"Type II error rate (Œ≤): {1-power:.3f}\")\n",
    "print(\"\\nüí° Power tells us how likely we are to detect a real effect when it exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: T-Tests in Detail\n",
    "\n",
    "### Types of T-Tests\n",
    "\n",
    "#### 1. Independent Samples T-Test\n",
    "- Compare two **independent** groups\n",
    "- Example: Treatment vs Control (different participants)\n",
    "\n",
    "**Assumptions:**\n",
    "- Independence of observations\n",
    "- Normality (for each group)\n",
    "- Homogeneity of variance (equal variances)\n",
    "\n",
    "#### 2. Paired Samples T-Test\n",
    "- Compare two **related** measurements\n",
    "- Example: Before vs After (same participants)\n",
    "\n",
    "#### 3. One-Sample T-Test\n",
    "- Compare sample mean to a known value\n",
    "- Example: Is average score different from 50?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independent samples t-test example\n",
    "\n",
    "# Generate realistic data: Study time effect on test scores\n",
    "np.random.seed(42)\n",
    "n_per_group = 40\n",
    "\n",
    "# Control group: no extra study (mean=70, sd=10)\n",
    "control_scores = np.random.normal(70, 10, n_per_group)\n",
    "\n",
    "# Treatment group: extra study (mean=76, sd=10)\n",
    "treatment_scores = np.random.normal(76, 10, n_per_group)\n",
    "\n",
    "# Perform t-test\n",
    "t_stat, p_value = stats.ttest_ind(control_scores, treatment_scores)\n",
    "\n",
    "print(\"INDEPENDENT SAMPLES T-TEST\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nResearch Question: Does extra study time improve test scores?\")\n",
    "print(f\"\\nControl group (n={n_per_group}):\")\n",
    "print(f\"  Mean = {control_scores.mean():.2f}\")\n",
    "print(f\"  SD = {control_scores.std(ddof=1):.2f}\")\n",
    "\n",
    "print(f\"\\nTreatment group (n={n_per_group}):\")\n",
    "print(f\"  Mean = {treatment_scores.mean():.2f}\")\n",
    "print(f\"  SD = {treatment_scores.std(ddof=1):.2f}\")\n",
    "\n",
    "print(f\"\\nMean difference: {treatment_scores.mean() - control_scores.mean():.2f} points\")\n",
    "\n",
    "print(f\"\\nTest Results:\")\n",
    "print(f\"  t-statistic = {t_stat:.3f}\")\n",
    "print(f\"  p-value = {p_value:.4f}\")\n",
    "print(f\"  df = {n_per_group + n_per_group - 2}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(f\"\\n‚úÖ Result: SIGNIFICANT (p < 0.05)\")\n",
    "    print(f\"   We reject H‚ÇÄ: The treatment group scored significantly higher\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå Result: NOT SIGNIFICANT (p ‚â• 0.05)\")\n",
    "    print(f\"   We fail to reject H‚ÇÄ: No significant difference detected\")\n",
    "\n",
    "# But wait - is this MEANINGFUL? We'll calculate effect size next!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the t-test\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Box plot\n",
    "data_dict = {\n",
    "    \"Score\": np.concatenate([control_scores, treatment_scores]),\n",
    "    \"Group\": [\"Control\"] * n_per_group + [\"Treatment\"] * n_per_group,\n",
    "}\n",
    "df_ttest = pd.DataFrame(data_dict)\n",
    "\n",
    "sns.boxplot(data=df_ttest, x=\"Group\", y=\"Score\", ax=ax1)\n",
    "sns.swarmplot(data=df_ttest, x=\"Group\", y=\"Score\", color=\"black\", alpha=0.3, ax=ax1)\n",
    "ax1.set_title(\"Test Scores by Group\", fontsize=14, fontweight=\"bold\")\n",
    "ax1.set_ylabel(\"Score\", fontsize=12)\n",
    "ax1.set_xlabel(\"Group\", fontsize=12)\n",
    "\n",
    "# Distribution plot\n",
    "ax2.hist(control_scores, bins=15, alpha=0.6, label=\"Control\", color=\"lightblue\", edgecolor=\"black\")\n",
    "ax2.hist(treatment_scores, bins=15, alpha=0.6, label=\"Treatment\", color=\"orange\", edgecolor=\"black\")\n",
    "ax2.axvline(control_scores.mean(), color=\"blue\", linestyle=\"--\", linewidth=2, label=\"Control Mean\")\n",
    "ax2.axvline(\n",
    "    treatment_scores.mean(), color=\"red\", linestyle=\"--\", linewidth=2, label=\"Treatment Mean\"\n",
    ")\n",
    "ax2.set_xlabel(\"Score\", fontsize=12)\n",
    "ax2.set_ylabel(\"Frequency\", fontsize=12)\n",
    "ax2.set_title(\"Distribution of Scores\", fontsize=14, fontweight=\"bold\")\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, \"ttest_visualization.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Visual inspection helps understand the magnitude of difference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Effect Sizes - The Missing Piece\n",
    "\n",
    "### Why Effect Sizes Matter\n",
    "\n",
    "**Problem with p-values:**\n",
    "- With large samples, tiny differences become \"significant\"\n",
    "- With small samples, large differences may be \"non-significant\"\n",
    "- p-value doesn't tell you HOW BIG the effect is\n",
    "\n",
    "**Solution: Report Effect Sizes!**\n",
    "\n",
    "### Common Effect Sizes\n",
    "\n",
    "#### For T-Tests: Cohen's d\n",
    "\n",
    "$$d = \\frac{\\bar{x}_1 - \\bar{x}_2}{s_{pooled}}$$\n",
    "\n",
    "**Interpretation (Cohen, 1988):**\n",
    "- Small: d = 0.2\n",
    "- Medium: d = 0.5\n",
    "- Large: d = 0.8\n",
    "\n",
    "#### For ANOVA: Eta-Squared (Œ∑¬≤)\n",
    "\n",
    "$$\\eta^2 = \\frac{SS_{between}}{SS_{total}}$$\n",
    "\n",
    "**Interpretation:**\n",
    "- Small: Œ∑¬≤ = 0.01 (1% of variance explained)\n",
    "- Medium: Œ∑¬≤ = 0.06 (6% of variance explained)\n",
    "- Large: Œ∑¬≤ = 0.14 (14% of variance explained)\n",
    "\n",
    "#### For Chi-Square: Cram√©r's V\n",
    "\n",
    "$$V = \\sqrt{\\frac{\\chi^2}{n \\cdot (k-1)}}$$\n",
    "\n",
    "where k = min(rows, columns)\n",
    "\n",
    "### Statistical vs Practical Significance\n",
    "\n",
    "| Scenario | p-value | Effect Size | Interpretation |\n",
    "|----------|---------|-------------|----------------|\n",
    "| A | 0.001 | d = 0.15 | Statistically significant, but TINY effect |\n",
    "| B | 0.08 | d = 0.9 | Not significant, but LARGE effect (underpowered?) |\n",
    "| C | 0.01 | d = 0.8 | Significant AND large effect! |\n",
    "\n",
    "**Best practice: Always report BOTH p-value AND effect size!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Cohen's d\n",
    "\n",
    "\n",
    "def cohens_d(group1, group2):\n",
    "    \"\"\"\n",
    "    Calculate Cohen's d for independent samples.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    group1, group2 : array-like\n",
    "        Data for each group\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        Cohen's d effect size\n",
    "    \"\"\"\n",
    "    n1, n2 = len(group1), len(group2)\n",
    "    var1, var2 = np.var(group1, ddof=1), np.var(group2, ddof=1)\n",
    "\n",
    "    # Pooled standard deviation\n",
    "    pooled_std = np.sqrt(((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2))\n",
    "\n",
    "    # Cohen's d\n",
    "    d = (np.mean(group1) - np.mean(group2)) / pooled_std\n",
    "\n",
    "    return d\n",
    "\n",
    "\n",
    "# Calculate for our earlier example\n",
    "d = cohens_d(treatment_scores, control_scores)\n",
    "\n",
    "print(\"EFFECT SIZE CALCULATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nCohen's d = {d:.3f}\")\n",
    "\n",
    "# Interpret\n",
    "if abs(d) < 0.2:\n",
    "    interpretation = \"Very Small\"\n",
    "elif abs(d) < 0.5:\n",
    "    interpretation = \"Small\"\n",
    "elif abs(d) < 0.8:\n",
    "    interpretation = \"Medium\"\n",
    "else:\n",
    "    interpretation = \"Large\"\n",
    "\n",
    "print(f\"Interpretation: {interpretation} effect\")\n",
    "\n",
    "# What does this mean in practical terms?\n",
    "print(f\"\\nPractical Meaning:\")\n",
    "print(f\"  The treatment improved scores by {d:.2f} standard deviations\")\n",
    "print(\n",
    "    f\"  In our example: {abs(treatment_scores.mean() - control_scores.mean()):.1f} points difference\"\n",
    ")\n",
    "\n",
    "# Combine with p-value for complete reporting\n",
    "print(f\"\\nüìä Complete Reporting:\")\n",
    "print(\n",
    "    f\"  The treatment group (M = {treatment_scores.mean():.2f}, SD = {treatment_scores.std(ddof=1):.2f})\"\n",
    ")\n",
    "print(f\"  scored significantly higher than the control group\")\n",
    "print(f\"  (M = {control_scores.mean():.2f}, SD = {control_scores.std(ddof=1):.2f}),\")\n",
    "print(f\"  t({n_per_group*2-2}) = {t_stat:.2f}, p = {p_value:.3f}, d = {d:.2f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the relationship between sample size, effect size, and significance\n",
    "\n",
    "sample_sizes = [10, 30, 50, 100, 200, 500]\n",
    "true_effect = 0.3  # Small-to-medium effect\n",
    "\n",
    "results = []\n",
    "\n",
    "for n in sample_sizes:\n",
    "    # Generate data\n",
    "    np.random.seed(42)\n",
    "    group1 = np.random.normal(0, 1, n)\n",
    "    group2 = np.random.normal(true_effect, 1, n)\n",
    "\n",
    "    # Test\n",
    "    t, p = stats.ttest_ind(group1, group2)\n",
    "    d = cohens_d(group1, group2)\n",
    "\n",
    "    results.append(\n",
    "        {\"N per group\": n, \"p-value\": p, \"Cohen's d\": d, \"Significant\": \"Yes\" if p < 0.05 else \"No\"}\n",
    "    )\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"EFFECT OF SAMPLE SIZE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"True effect size (Cohen's d): {true_effect}\")\n",
    "print(\"\\n\" + results_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nüí° Key Insights:\")\n",
    "print(\"  ‚Ä¢ Effect size stays relatively constant (it's the 'true' effect)\")\n",
    "print(\"  ‚Ä¢ p-value decreases as sample size increases\")\n",
    "print(\"  ‚Ä¢ With large samples, even small effects become 'significant'\")\n",
    "print(\"  ‚Ä¢ Effect size tells you if the result is MEANINGFUL!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Statistical Power Analysis\n",
    "\n",
    "### What is Statistical Power?\n",
    "\n",
    "**Power = Probability of detecting an effect when it truly exists**\n",
    "\n",
    "**Power = 1 - Œ≤ (Type II error rate)**\n",
    "\n",
    "### Factors Affecting Power\n",
    "\n",
    "1. **Effect Size** ‚Üë ‚Üí Power ‚Üë\n",
    "   - Larger effects are easier to detect\n",
    "\n",
    "2. **Sample Size** ‚Üë ‚Üí Power ‚Üë\n",
    "   - More data = better ability to detect effects\n",
    "\n",
    "3. **Significance Level (Œ±)** ‚Üë ‚Üí Power ‚Üë\n",
    "   - More lenient threshold = easier to reject H‚ÇÄ\n",
    "   - But also increases Type I error!\n",
    "\n",
    "4. **Variability** ‚Üì ‚Üí Power ‚Üë\n",
    "   - Less noise = clearer signal\n",
    "\n",
    "### Standard Power Levels\n",
    "\n",
    "- **0.80**: Minimum acceptable (80% chance of detecting effect)\n",
    "- **0.90**: Better (90% chance)\n",
    "- **0.95**: Excellent (95% chance)\n",
    "\n",
    "### A Priori vs Post-Hoc Power Analysis\n",
    "\n",
    "**A Priori (Before Study):**\n",
    "- \"How many participants do I need to detect this effect?\"\n",
    "- **USE THIS!** It's the gold standard\n",
    "\n",
    "**Post-Hoc (After Study):**\n",
    "- \"What was my power given my sample?\"\n",
    "- Controversial - many statisticians discourage this\n",
    "- If result is significant, post-hoc power is not informative\n",
    "- If result is non-significant, better to report confidence intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A priori power analysis for t-test\n",
    "from scipy.stats import t as t_dist, norm\n",
    "\n",
    "\n",
    "def calculate_sample_size_ttest(effect_size, power=0.8, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Calculate required sample size for independent t-test.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    effect_size : float\n",
    "        Expected Cohen's d\n",
    "    power : float\n",
    "        Desired power (typically 0.80)\n",
    "    alpha : float\n",
    "        Significance level (typically 0.05)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    int\n",
    "        Required sample size per group\n",
    "    \"\"\"\n",
    "    # Critical values\n",
    "    z_alpha = norm.ppf(1 - alpha / 2)  # Two-tailed\n",
    "    z_beta = norm.ppf(power)\n",
    "\n",
    "    # Sample size calculation\n",
    "    n = 2 * ((z_alpha + z_beta) / effect_size) ** 2\n",
    "\n",
    "    return int(np.ceil(n))\n",
    "\n",
    "\n",
    "print(\"A PRIORI POWER ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nResearch Question: How many participants do I need?\")\n",
    "print(\"\\nScenario 1: Detecting a SMALL effect (d = 0.2)\")\n",
    "n_small = calculate_sample_size_ttest(effect_size=0.2, power=0.8)\n",
    "print(f\"  Required n per group: {n_small}\")\n",
    "print(f\"  Total participants: {n_small * 2}\")\n",
    "\n",
    "print(\"\\nScenario 2: Detecting a MEDIUM effect (d = 0.5)\")\n",
    "n_medium = calculate_sample_size_ttest(effect_size=0.5, power=0.8)\n",
    "print(f\"  Required n per group: {n_medium}\")\n",
    "print(f\"  Total participants: {n_medium * 2}\")\n",
    "\n",
    "print(\"\\nScenario 3: Detecting a LARGE effect (d = 0.8)\")\n",
    "n_large = calculate_sample_size_ttest(effect_size=0.8, power=0.8)\n",
    "print(f\"  Required n per group: {n_large}\")\n",
    "print(f\"  Total participants: {n_large * 2}\")\n",
    "\n",
    "print(\"\\nüí° Key Insight: Smaller effects require MUCH larger samples!\")\n",
    "print(f\"   To detect d=0.2 requires {n_small/n_large:.1f}x more participants than d=0.8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize power as a function of sample size\n",
    "\n",
    "\n",
    "def calculate_power_ttest(n_per_group, effect_size, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Calculate statistical power for t-test.\n",
    "    \"\"\"\n",
    "    # Non-centrality parameter\n",
    "    ncp = effect_size * np.sqrt(n_per_group / 2)\n",
    "\n",
    "    # Degrees of freedom\n",
    "    df = 2 * n_per_group - 2\n",
    "\n",
    "    # Critical t-value\n",
    "    t_crit = t_dist.ppf(1 - alpha / 2, df)\n",
    "\n",
    "    # Power\n",
    "    power = 1 - t_dist.cdf(t_crit, df, ncp) + t_dist.cdf(-t_crit, df, ncp)\n",
    "\n",
    "    return power\n",
    "\n",
    "\n",
    "# Generate power curves\n",
    "sample_sizes = np.arange(10, 201, 5)\n",
    "effect_sizes = [0.2, 0.5, 0.8]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for d in effect_sizes:\n",
    "    powers = [calculate_power_ttest(n, d) for n in sample_sizes]\n",
    "    ax.plot(sample_sizes, powers, label=f\"d = {d}\", linewidth=2)\n",
    "\n",
    "ax.axhline(y=0.8, color=\"red\", linestyle=\"--\", label=\"Desired Power (0.80)\", alpha=0.7)\n",
    "ax.set_xlabel(\"Sample Size per Group\", fontsize=12, fontweight=\"bold\")\n",
    "ax.set_ylabel(\"Statistical Power\", fontsize=12, fontweight=\"bold\")\n",
    "ax.set_title(\"Power Analysis for Different Effect Sizes\", fontsize=14, fontweight=\"bold\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, \"power_curve.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Power Curve Insights:\")\n",
    "print(\"  ‚Ä¢ Larger effects reach 80% power with smaller samples\")\n",
    "print(\"  ‚Ä¢ Power increases rapidly at first, then plateaus\")\n",
    "print(\"  ‚Ä¢ Always plan for adequate power BEFORE collecting data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Multiple Comparisons Problem\n",
    "\n",
    "### The Problem\n",
    "\n",
    "If you do 20 independent tests at Œ± = 0.05:\n",
    "- Expected number of false positives: 20 √ó 0.05 = **1 false positive**\n",
    "\n",
    "**The more tests you run, the more likely you'll find a \"significant\" result by chance!**\n",
    "\n",
    "### Family-Wise Error Rate (FWER)\n",
    "\n",
    "Probability of making at least ONE Type I error across all tests:\n",
    "\n",
    "$$FWER = 1 - (1 - \\alpha)^m$$\n",
    "\n",
    "where m = number of tests\n",
    "\n",
    "### Correction Methods\n",
    "\n",
    "#### 1. Bonferroni Correction\n",
    "**Most conservative**\n",
    "\n",
    "$$\\alpha_{corrected} = \\frac{\\alpha}{m}$$\n",
    "\n",
    "**Pros:** Simple, controls FWER strongly\n",
    "**Cons:** Very conservative, reduces power\n",
    "\n",
    "#### 2. Holm-Bonferroni (Step-Down)\n",
    "**Less conservative than Bonferroni**\n",
    "\n",
    "- Sort p-values from smallest to largest\n",
    "- Compare each to Œ±/(m-i+1)\n",
    "\n",
    "**Pros:** More powerful than Bonferroni\n",
    "**Cons:** Still conservative\n",
    "\n",
    "#### 3. False Discovery Rate (FDR) - Benjamini-Hochberg\n",
    "**Controls proportion of false discoveries**\n",
    "\n",
    "**Pros:** More powerful, appropriate for exploratory research\n",
    "**Cons:** Less stringent than FWER control\n",
    "\n",
    "### When to Correct?\n",
    "\n",
    "**YES, correct when:**\n",
    "- Testing multiple hypotheses in same analysis\n",
    "- Looking at multiple outcomes\n",
    "- Doing post-hoc comparisons after ANOVA\n",
    "\n",
    "**NO correction needed when:**\n",
    "- You have ONE pre-specified hypothesis\n",
    "- Tests are independent research questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate multiple comparison problem\n",
    "\n",
    "\n",
    "def simulate_multiple_testing(n_tests=20, alpha=0.05, n_simulations=1000):\n",
    "    \"\"\"\n",
    "    Simulate multiple testing to show inflation of Type I error.\n",
    "    \"\"\"\n",
    "    any_significant = 0\n",
    "\n",
    "    for _ in range(n_simulations):\n",
    "        # Generate null data (no real effects)\n",
    "        p_values = []\n",
    "        for _ in range(n_tests):\n",
    "            group1 = np.random.normal(0, 1, 30)\n",
    "            group2 = np.random.normal(0, 1, 30)  # Same distribution!\n",
    "            _, p = stats.ttest_ind(group1, group2)\n",
    "            p_values.append(p)\n",
    "\n",
    "        # Check if ANY test was significant\n",
    "        if any(p < alpha for p in p_values):\n",
    "            any_significant += 1\n",
    "\n",
    "    return any_significant / n_simulations\n",
    "\n",
    "\n",
    "print(\"MULTIPLE COMPARISONS PROBLEM\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nSimulation: 20 tests, all NULL (no real effects)\")\n",
    "\n",
    "fwer = simulate_multiple_testing(n_tests=20)\n",
    "\n",
    "print(f\"\\nTheoretical FWER: {1 - (1-0.05)**20:.3f}\")\n",
    "print(f\"Observed FWER (simulation): {fwer:.3f}\")\n",
    "print(f\"\\n‚ö†Ô∏è  With 20 tests, you have a {fwer*100:.1f}% chance of at least one false positive!\")\n",
    "print(f\"   Even though NONE of the effects are real!\")\n",
    "\n",
    "# Show correction impact\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CORRECTION METHODS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "original_alpha = 0.05\n",
    "n_tests = 10\n",
    "p_values = [0.001, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.08, 0.1, 0.15]\n",
    "\n",
    "# Bonferroni\n",
    "bonferroni_alpha = original_alpha / n_tests\n",
    "bonf_sig = sum(1 for p in p_values if p < bonferroni_alpha)\n",
    "\n",
    "print(f\"\\nOriginal Œ± = {original_alpha}\")\n",
    "print(f\"Number of tests = {n_tests}\")\n",
    "print(f\"\\nP-values: {p_values}\")\n",
    "\n",
    "print(f\"\\n1. No Correction:\")\n",
    "print(f\"   Significant results: {sum(1 for p in p_values if p < original_alpha)}/{n_tests}\")\n",
    "\n",
    "print(f\"\\n2. Bonferroni Correction:\")\n",
    "print(f\"   Corrected Œ± = {bonferroni_alpha:.4f}\")\n",
    "print(f\"   Significant results: {bonf_sig}/{n_tests}\")\n",
    "\n",
    "print(\"\\nüí° Bonferroni is conservative - reduces false positives but also power\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: ANOVA - Comparing Multiple Groups\n",
    "\n",
    "### When to Use ANOVA\n",
    "\n",
    "**Research Question:** Are there differences among 3+ groups?\n",
    "\n",
    "**Example:** Compare test scores across 4 teaching methods\n",
    "\n",
    "### Why Not Multiple T-Tests?\n",
    "\n",
    "With 4 groups, you'd need 6 t-tests (all pairs):\n",
    "- A vs B, A vs C, A vs D, B vs C, B vs D, C vs D\n",
    "\n",
    "**Problem:** Multiple comparison problem! FWER inflates.\n",
    "\n",
    "**Solution:** Use ANOVA first (omnibus test), then post-hoc comparisons if significant.\n",
    "\n",
    "### ANOVA Logic\n",
    "\n",
    "**H‚ÇÄ:** All group means are equal (Œº‚ÇÅ = Œº‚ÇÇ = Œº‚ÇÉ = ...)\n",
    "\n",
    "**H‚ÇÅ:** At least one mean is different\n",
    "\n",
    "**F-statistic:**\n",
    "\n",
    "$$F = \\frac{\\text{Variance between groups}}{\\text{Variance within groups}}$$\n",
    "\n",
    "If F is large ‚Üí groups differ more than expected by chance\n",
    "\n",
    "### Post-Hoc Tests\n",
    "\n",
    "If ANOVA is significant, which groups differ?\n",
    "\n",
    "**Common Post-Hoc Tests:**\n",
    "- **Tukey HSD**: Controls FWER, all pairwise comparisons\n",
    "- **Bonferroni**: Conservative, simple\n",
    "- **Dunnett**: Compare all groups to one control group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANOVA Example: Teaching methods\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate data for 4 teaching methods\n",
    "method_A = np.random.normal(70, 10, 30)  # Traditional\n",
    "method_B = np.random.normal(75, 10, 30)  # Flipped classroom\n",
    "method_C = np.random.normal(72, 10, 30)  # Online\n",
    "method_D = np.random.normal(78, 10, 30)  # Hybrid\n",
    "\n",
    "# Perform one-way ANOVA\n",
    "f_stat, p_value = stats.f_oneway(method_A, method_B, method_C, method_D)\n",
    "\n",
    "print(\"ONE-WAY ANOVA\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nResearch Question: Do teaching methods affect test scores?\")\n",
    "\n",
    "print(\"\\nDescriptive Statistics:\")\n",
    "methods_data = {\n",
    "    \"Method\": [\"Traditional\", \"Flipped\", \"Online\", \"Hybrid\"],\n",
    "    \"Mean\": [method_A.mean(), method_B.mean(), method_C.mean(), method_D.mean()],\n",
    "    \"SD\": [method_A.std(ddof=1), method_B.std(ddof=1), method_C.std(ddof=1), method_D.std(ddof=1)],\n",
    "    \"n\": [len(method_A), len(method_B), len(method_C), len(method_D)],\n",
    "}\n",
    "methods_df = pd.DataFrame(methods_data)\n",
    "print(methods_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nANOVA Results:\")\n",
    "print(f\"  F({3}, {len(method_A)*4-4}) = {f_stat:.3f}\")\n",
    "print(f\"  p-value = {p_value:.4f}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(f\"\\n‚úÖ Significant: At least one method differs from the others\")\n",
    "    print(f\"   ‚Üí Need post-hoc tests to determine which groups differ\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå Not significant: No evidence of differences among methods\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate eta-squared (effect size for ANOVA)\n",
    "\n",
    "# Combine all data\n",
    "all_scores = np.concatenate([method_A, method_B, method_C, method_D])\n",
    "all_groups = (\n",
    "    [\"A\"] * len(method_A) + [\"B\"] * len(method_B) + [\"C\"] * len(method_C) + [\"D\"] * len(method_D)\n",
    ")\n",
    "\n",
    "# Calculate sums of squares\n",
    "grand_mean = all_scores.mean()\n",
    "ss_total = np.sum((all_scores - grand_mean) ** 2)\n",
    "\n",
    "group_means = [method_A.mean(), method_B.mean(), method_C.mean(), method_D.mean()]\n",
    "group_sizes = [len(method_A), len(method_B), len(method_C), len(method_D)]\n",
    "ss_between = sum(n * (mean - grand_mean) ** 2 for mean, n in zip(group_means, group_sizes))\n",
    "\n",
    "# Eta-squared\n",
    "eta_squared = ss_between / ss_total\n",
    "\n",
    "print(\"\\nEFFECT SIZE (Eta-Squared)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Œ∑¬≤ = {eta_squared:.4f}\")\n",
    "print(\n",
    "    f\"\\nInterpretation: {eta_squared*100:.2f}% of variance in scores is explained by teaching method\"\n",
    ")\n",
    "\n",
    "if eta_squared < 0.01:\n",
    "    print(\"Effect size: Very Small\")\n",
    "elif eta_squared < 0.06:\n",
    "    print(\"Effect size: Small\")\n",
    "elif eta_squared < 0.14:\n",
    "    print(\"Effect size: Medium\")\n",
    "else:\n",
    "    print(\"Effect size: Large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Practice Exercises\n",
    "\n",
    "### Exercise 1: Conduct a Complete T-Test Analysis\n",
    "\n",
    "**Scenario:** A company tests two website designs (A vs B) for conversion rate.\n",
    "\n",
    "**Data:** Design A: n=50, Design B: n=50 (generate synthetic data)\n",
    "\n",
    "**Tasks:**\n",
    "1. Conduct independent t-test\n",
    "2. Calculate Cohen's d\n",
    "3. Report results professionally\n",
    "4. Visualize the distributions\n",
    "\n",
    "### Exercise 2: Power Analysis\n",
    "\n",
    "**Scenario:** You're planning a study to test if a new drug reduces blood pressure.\n",
    "\n",
    "**Tasks:**\n",
    "1. Calculate required sample size for d=0.5, power=0.80\n",
    "2. Create a power curve showing n from 10 to 100\n",
    "3. What happens to power if you can only get n=30 per group?\n",
    "\n",
    "### Exercise 3: Multiple Comparisons\n",
    "\n",
    "**Scenario:** You test 15 different nutrients for their effect on plant growth.\n",
    "\n",
    "**Tasks:**\n",
    "1. If Œ±=0.05, what's the FWER?\n",
    "2. What's the Bonferroni-corrected Œ±?\n",
    "3. If p-values are [0.001, 0.01, 0.03, 0.05, 0.07], which remain significant after correction?\n",
    "\n",
    "### Exercise 4: ANOVA Practice\n",
    "\n",
    "**Scenario:** Compare 5 different diet plans on weight loss.\n",
    "\n",
    "**Tasks:**\n",
    "1. Generate synthetic data for 5 groups\n",
    "2. Conduct one-way ANOVA\n",
    "3. Calculate eta-squared\n",
    "4. Create box plots for visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary and Key Takeaways\n",
    "\n",
    "### üéØ What We Learned\n",
    "\n",
    "1. **Hypothesis Testing Framework**\n",
    "   - Null vs alternative hypotheses\n",
    "   - Type I and Type II errors\n",
    "   - p-values and significance levels\n",
    "\n",
    "2. **Effect Sizes**\n",
    "   - Cohen's d for t-tests\n",
    "   - Eta-squared for ANOVA\n",
    "   - Statistical vs practical significance\n",
    "\n",
    "3. **Statistical Power**\n",
    "   - Definition and importance\n",
    "   - Factors affecting power\n",
    "   - A priori sample size calculation\n",
    "\n",
    "4. **Multiple Comparisons**\n",
    "   - FWER inflation problem\n",
    "   - Bonferroni and other corrections\n",
    "   - When to correct\n",
    "\n",
    "5. **ANOVA**\n",
    "   - Comparing 3+ groups\n",
    "   - Post-hoc tests\n",
    "   - Effect sizes\n",
    "\n",
    "### üìö Best Practices for Reporting\n",
    "\n",
    "**Always include:**\n",
    "1. ‚úÖ Descriptive statistics (M, SD, n)\n",
    "2. ‚úÖ Test statistic and degrees of freedom\n",
    "3. ‚úÖ Exact p-value (not just p<0.05)\n",
    "4. ‚úÖ Effect size with interpretation\n",
    "5. ‚úÖ Confidence intervals when possible\n",
    "\n",
    "**Example of good reporting:**\n",
    "> \"The treatment group (M = 76.2, SD = 9.8) scored significantly higher than the control group (M = 70.1, SD = 10.3), t(78) = 2.87, p = .005, d = 0.64, 95% CI [1.9, 10.3]. This represents a medium-to-large effect.\"\n",
    "\n",
    "### üöÄ Next Steps\n",
    "\n",
    "1. **Practice**: Complete the exercises above\n",
    "2. **Apply**: Use these techniques in your own data analysis\n",
    "3. **Read**: Study statistical reporting in your field's journals\n",
    "4. **Prepare** for Module 02: Causal Inference (understanding WHY, not just IF)\n",
    "\n",
    "### üí° Remember\n",
    "\n",
    "> \"The goal is not just to find significance, but to understand the magnitude and meaning of effects.\"\n",
    "\n",
    "Statistical significance + Large effect size + Adequate power = Convincing evidence!\n",
    "\n",
    "---\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "### Software\n",
    "- **G*Power**: Free power analysis software\n",
    "- **Pingouin**: Python statistical package\n",
    "- **statsmodels**: Comprehensive Python stats\n",
    "\n",
    "### Reading\n",
    "- \"Statistical Power Analysis\" by Cohen (1988)\n",
    "- \"Statistics Done Wrong\" by Reinhart\n",
    "- \"The Essential Guide to Effect Sizes\" by Ellis (2010)\n",
    "\n",
    "### Online Resources\n",
    "- [Statistics Hell](https://statisticshell.com/)\n",
    "- [Cross Validated](https://stats.stackexchange.com/)\n",
    "- [Stat 545](https://stat545.com/)\n",
    "\n",
    "---\n",
    "\n",
    "**Next Module:** [02_causal_inference_fundamentals.ipynb](02_causal_inference_fundamentals.ipynb) - Learn to distinguish correlation from causation and understand confounding!\n",
    "\n",
    "---\n",
    "\n",
    "*Last updated: 2024*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
