{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 04: Survey Design & Measurement\n",
    "\n",
    "**Estimated Time**: 50 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this module, you will be able to:\n",
    "\n",
    "1. **Design** effective survey questions that minimize bias and maximize clarity\n",
    "2. **Select** appropriate response scales for different question types\n",
    "3. **Evaluate** psychometric properties: reliability (consistency) and validity (accuracy)\n",
    "4. **Calculate** Cronbach's alpha and other reliability metrics\n",
    "5. **Identify** and mitigate common survey biases (acquiescence, social desirability, etc.)\n",
    "6. **Distinguish** between measurement scales (nominal, ordinal, interval, ratio)\n",
    "7. **Conduct** item analysis to refine survey instruments\n",
    "8. **Implement** survey pretesting and cognitive interviewing\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "**Bad measurement = bad data = bad science**\n",
    "\n",
    "Even the most sophisticated analysis cannot overcome poor measurement:\n",
    "- Ambiguous questions lead to meaningless responses\n",
    "- Biased questions distort reality\n",
    "- Unreliable measures add noise and reduce power\n",
    "- Invalid measures answer the wrong question\n",
    "\n",
    "This module teaches you to create measurement instruments that are:\n",
    "- **Reliable**: Consistent across time and contexts\n",
    "- **Valid**: Actually measure what they claim to measure\n",
    "- **Unbiased**: Minimize systematic distortions\n",
    "- **Actionable**: Produce data you can trust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set style\n",
    "plt.style.use(\"seaborn-v0_8-darkgrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create output directory\n",
    "import os\n",
    "\n",
    "os.makedirs(\"outputs/module_04\", exist_ok=True)\n",
    "\n",
    "print(\"âœ“ Libraries imported successfully\")\n",
    "print(\"âœ“ Output directory created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Question Design Principles\n",
    "\n",
    "### The Seven Deadly Sins of Survey Questions\n",
    "\n",
    "#### 1. Double-Barreled Questions\n",
    "**Bad**: \"Do you think the government should increase taxes and improve healthcare?\"\n",
    "- Problem: Two questions in one. What if someone supports one but not the other?\n",
    "\n",
    "**Good**: Split into two questions:\n",
    "- \"Should the government increase taxes?\"\n",
    "- \"Should the government improve healthcare?\"\n",
    "\n",
    "#### 2. Leading Questions\n",
    "**Bad**: \"Don't you agree that climate change is the most important issue facing humanity?\"\n",
    "- Problem: Guides respondent toward a particular answer\n",
    "\n",
    "**Good**: \"How important is climate change relative to other issues?\"\n",
    "\n",
    "#### 3. Loaded Questions\n",
    "**Bad**: \"Should we allow dangerous criminals to walk free?\"\n",
    "- Problem: Emotional language biases response\n",
    "\n",
    "**Good**: \"Should non-violent offenders be eligible for early release?\"\n",
    "\n",
    "#### 4. Ambiguous Questions\n",
    "**Bad**: \"How often do you exercise?\"\n",
    "- Problem: What counts as exercise? Walking? Housework? Sports?\n",
    "\n",
    "**Good**: \"In a typical week, on how many days do you engage in at least 30 minutes of moderate-to-vigorous physical activity (e.g., brisk walking, jogging, cycling, swimming)?\"\n",
    "\n",
    "#### 5. Negatively Worded Questions\n",
    "**Bad**: \"To what extent do you disagree that you are not dissatisfied with our service?\"\n",
    "- Problem: Double/triple negatives confuse respondents\n",
    "\n",
    "**Good**: \"How satisfied are you with our service?\"\n",
    "\n",
    "#### 6. Questions Assuming Knowledge\n",
    "**Bad**: \"What is your opinion on the proposed amendment to Section 1031 of the Internal Revenue Code?\"\n",
    "- Problem: Most respondents won't know what this is\n",
    "\n",
    "**Good**: Include brief explanation or add \"Don't know\" option\n",
    "\n",
    "#### 7. Questions Beyond Recall Ability\n",
    "**Bad**: \"How many times did you sneeze in the past year?\"\n",
    "- Problem: No one can accurately remember this\n",
    "\n",
    "**Good**: Shorten timeframe: \"How many times did you sneeze today?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate impact of question wording on responses\n",
    "\n",
    "# Simulate responses to differently worded questions on same topic\n",
    "np.random.seed(123)\n",
    "n_respondents = 200\n",
    "\n",
    "# Scenario: Support for environmental policy\n",
    "# True underlying support: 60% (what we'd get with neutral wording)\n",
    "true_support_rate = 0.60\n",
    "\n",
    "# Neutral wording: \"Should the government regulate carbon emissions?\"\n",
    "neutral_responses = np.random.binomial(1, true_support_rate, n_respondents)\n",
    "\n",
    "# Leading/positive wording: \"Should the government protect our environment by regulating carbon emissions?\"\n",
    "leading_positive = np.random.binomial(\n",
    "    1, true_support_rate + 0.20, n_respondents\n",
    ")  # Inflates support\n",
    "\n",
    "# Leading/negative wording: \"Should the government impose costly regulations on carbon emissions?\"\n",
    "leading_negative = np.random.binomial(\n",
    "    1, true_support_rate - 0.25, n_respondents\n",
    ")  # Deflates support\n",
    "\n",
    "# Ambiguous wording: \"Should the government do something about pollution?\"\n",
    "ambiguous = np.random.binomial(\n",
    "    1, true_support_rate + 0.15, n_respondents\n",
    ")  # Vague â†’ higher agreement\n",
    "\n",
    "# Create summary\n",
    "wording_comparison = pd.DataFrame(\n",
    "    {\n",
    "        \"Question Wording\": [\"Neutral\", \"Leading (Positive)\", \"Leading (Negative)\", \"Ambiguous\"],\n",
    "        \"Support Rate (%)\": [\n",
    "            neutral_responses.mean() * 100,\n",
    "            leading_positive.mean() * 100,\n",
    "            leading_negative.mean() * 100,\n",
    "            ambiguous.mean() * 100,\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Impact of Question Wording on Response Rates:\\n\")\n",
    "print(wording_comparison.to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "colors = [\"#06A77D\", \"#F4A261\", \"#E63946\", \"#457B9D\"]\n",
    "bars = ax.barh(\n",
    "    wording_comparison[\"Question Wording\"],\n",
    "    wording_comparison[\"Support Rate (%)\"],\n",
    "    color=colors,\n",
    "    edgecolor=\"black\",\n",
    "    linewidth=1.5,\n",
    ")\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, val) in enumerate(zip(bars, wording_comparison[\"Support Rate (%)\"])):\n",
    "    ax.text(val + 1, i, f\"{val:.1f}%\", va=\"center\", fontweight=\"bold\")\n",
    "\n",
    "# Mark true rate\n",
    "ax.axvline(\n",
    "    x=true_support_rate * 100,\n",
    "    color=\"black\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=f\"True Support ({true_support_rate*100:.0f}%)\",\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"Support Rate (%)\", fontsize=12, fontweight=\"bold\")\n",
    "ax.set_title(\"How Question Wording Affects Survey Responses\", fontsize=14, fontweight=\"bold\")\n",
    "ax.set_xlim([0, 100])\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis=\"x\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"outputs/module_04/question_wording_bias.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ The same underlying attitude produces wildly different results\")\n",
    "print(\"   depending on how the question is worded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Response Scales and Formats\n",
    "\n",
    "### Common Scale Types\n",
    "\n",
    "#### Likert Scales\n",
    "**Usage**: Measuring agreement, frequency, importance, satisfaction\n",
    "\n",
    "**Example (5-point agreement)**:\n",
    "1. Strongly Disagree\n",
    "2. Disagree\n",
    "3. Neither Agree nor Disagree\n",
    "4. Agree\n",
    "5. Strongly Agree\n",
    "\n",
    "**Design Choices**:\n",
    "- **Number of points**: 5-7 is standard (more points â‰  more precision)\n",
    "- **Middle option**: Include neutral option? (Yes: allows true neutrality; No: forces choice)\n",
    "- **Labels**: Label all points vs. only endpoints\n",
    "\n",
    "#### Semantic Differential Scales\n",
    "**Usage**: Measuring attitudes along bipolar dimensions\n",
    "\n",
    "**Example**:\n",
    "```\n",
    "Please rate this product:\n",
    "Inexpensive  1  2  3  4  5  6  7  Expensive\n",
    "Low Quality  1  2  3  4  5  6  7  High Quality\n",
    "Unattractive 1  2  3  4  5  6  7  Attractive\n",
    "```\n",
    "\n",
    "#### Visual Analog Scales (VAS)\n",
    "**Usage**: Continuous measurement (pain, satisfaction, emotion)\n",
    "\n",
    "**Example**:\n",
    "```\n",
    "How much pain are you experiencing?\n",
    "|------------------------------------------------|\n",
    "No pain                                  Worst pain imaginable\n",
    "```\n",
    "\n",
    "Respondent marks on the line; researcher measures distance.\n",
    "\n",
    "#### Rating Scales\n",
    "**Usage**: Evaluating specific attributes\n",
    "\n",
    "**Example (0-10 scale)**:\n",
    "\"On a scale from 0 to 10, where 0 is 'not at all likely' and 10 is 'extremely likely', how likely are you to recommend our product?\"\n",
    "\n",
    "### Scale Selection Guidelines\n",
    "\n",
    "| Construct | Recommended Scale | Rationale |\n",
    "|-----------|------------------|----------|\n",
    "| Agreement | 5-point Likert | Standard, well-understood |\n",
    "| Frequency | 5-point Likert | Ordinal categories (Never to Always) |\n",
    "| Satisfaction | 5 or 7-point Likert | Allows nuance |\n",
    "| Pain/Discomfort | VAS or 0-10 | Continuous, sensitive |\n",
    "| Binary choice | Yes/No | When middle ground doesn't exist |\n",
    "| Ranking | Drag-and-drop or numbered | When priorities matter |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different scale formats on same construct\n",
    "\n",
    "np.random.seed(456)\n",
    "n = 150\n",
    "\n",
    "# True underlying satisfaction (0-100 scale)\n",
    "true_satisfaction = np.random.beta(2, 2, n) * 100  # Beta distribution for realistic spread\n",
    "\n",
    "# Convert to different scale formats\n",
    "\n",
    "# 5-point Likert (1-5)\n",
    "likert_5 = np.digitize(true_satisfaction, bins=[0, 20, 40, 60, 80, 100])\n",
    "\n",
    "# 7-point Likert (1-7)\n",
    "likert_7 = np.digitize(true_satisfaction, bins=[0, 14.3, 28.6, 42.9, 57.2, 71.5, 85.8, 100])\n",
    "\n",
    "# Binary (0-1)\n",
    "binary = (true_satisfaction >= 50).astype(int)\n",
    "\n",
    "# 0-10 rating\n",
    "rating_10 = np.round(true_satisfaction / 10).astype(int)\n",
    "rating_10 = np.clip(rating_10, 0, 10)\n",
    "\n",
    "# Create dataframe\n",
    "df_scales = pd.DataFrame(\n",
    "    {\n",
    "        \"True_Satisfaction\": true_satisfaction,\n",
    "        \"Likert_5\": likert_5,\n",
    "        \"Likert_7\": likert_7,\n",
    "        \"Binary\": binary,\n",
    "        \"Rating_10\": rating_10,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Calculate information loss (correlation with true score)\n",
    "correlations = {\n",
    "    \"5-Point Likert\": pearsonr(df_scales[\"True_Satisfaction\"], df_scales[\"Likert_5\"])[0],\n",
    "    \"7-Point Likert\": pearsonr(df_scales[\"True_Satisfaction\"], df_scales[\"Likert_7\"])[0],\n",
    "    \"Binary (Yes/No)\": pearsonr(df_scales[\"True_Satisfaction\"], df_scales[\"Binary\"])[0],\n",
    "    \"0-10 Rating\": pearsonr(df_scales[\"True_Satisfaction\"], df_scales[\"Rating_10\"])[0],\n",
    "}\n",
    "\n",
    "print(\"Correlation with True Satisfaction (higher = less information loss):\\n\")\n",
    "for scale, corr in sorted(correlations.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{scale:20s}: r = {corr:.3f}\")\n",
    "\n",
    "# Visualize distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "scale_names = [\"Likert_5\", \"Likert_7\", \"Binary\", \"Rating_10\"]\n",
    "scale_labels = [\"5-Point Likert\", \"7-Point Likert\", \"Binary (Yes/No)\", \"0-10 Rating\"]\n",
    "colors_palette = [\"#E63946\", \"#F4A261\", \"#06A77D\", \"#457B9D\"]\n",
    "\n",
    "for i, (scale_name, label, color) in enumerate(zip(scale_names, scale_labels, colors_palette)):\n",
    "    # Count frequencies\n",
    "    value_counts = df_scales[scale_name].value_counts().sort_index()\n",
    "\n",
    "    axes[i].bar(\n",
    "        value_counts.index,\n",
    "        value_counts.values,\n",
    "        color=color,\n",
    "        alpha=0.7,\n",
    "        edgecolor=\"black\",\n",
    "        linewidth=1.5,\n",
    "    )\n",
    "\n",
    "    axes[i].set_xlabel(\"Response Value\", fontsize=11, fontweight=\"bold\")\n",
    "    axes[i].set_ylabel(\"Frequency\", fontsize=11, fontweight=\"bold\")\n",
    "    axes[i].set_title(\n",
    "        f\"{label}\\n(r = {correlations[label]:.3f} with true score)\", fontsize=12, fontweight=\"bold\"\n",
    "    )\n",
    "    axes[i].grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "plt.suptitle(\"Response Distributions Across Scale Formats\", fontsize=15, fontweight=\"bold\", y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"outputs/module_04/scale_comparison.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ More response options generally preserve more information,\")\n",
    "print(\"   but gains diminish beyond 7-10 points.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Psychometric Properties: Reliability\n",
    "\n",
    "**Reliability** = Consistency of measurement\n",
    "\n",
    "A reliable measure produces similar results when:\n",
    "- The same person completes it multiple times (test-retest reliability)\n",
    "- Different items measure the same construct (internal consistency)\n",
    "- Different raters evaluate the same thing (inter-rater reliability)\n",
    "\n",
    "### Types of Reliability\n",
    "\n",
    "#### 1. Test-Retest Reliability\n",
    "**Method**: Administer same survey to same people at two time points\n",
    "**Metric**: Correlation between Time 1 and Time 2 scores\n",
    "**Interpretation**: r > 0.70 is acceptable\n",
    "\n",
    "#### 2. Internal Consistency Reliability\n",
    "**Method**: Examine how well items measuring the same construct correlate\n",
    "**Metrics**:\n",
    "- **Cronbach's Alpha (Î±)**: Most common\n",
    "- **Split-half reliability**: Correlation between two halves of scale\n",
    "\n",
    "**Cronbach's Alpha formula**:\n",
    "\n",
    "$$\\alpha = \\frac{k}{k-1} \\left(1 - \\frac{\\sum_{i=1}^{k} \\sigma_{i}^{2}}{\\sigma_{\\text{total}}^{2}}\\right)$$\n",
    "\n",
    "Where:\n",
    "- $k$ = number of items\n",
    "- $\\sigma_{i}^{2}$ = variance of item $i$\n",
    "- $\\sigma_{\\text{total}}^{2}$ = variance of total scale scores\n",
    "\n",
    "**Interpretation**:\n",
    "- Î± < 0.60: Unacceptable\n",
    "- Î± = 0.60-0.70: Questionable\n",
    "- Î± = 0.70-0.80: Acceptable\n",
    "- Î± = 0.80-0.90: Good\n",
    "- Î± > 0.90: Excellent (but check for redundancy)\n",
    "\n",
    "#### 3. Inter-Rater Reliability\n",
    "**Method**: Multiple raters evaluate same targets\n",
    "**Metrics**:\n",
    "- **Cohen's Kappa (Îº)**: For categorical ratings\n",
    "- **Intraclass Correlation (ICC)**: For continuous ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Cronbach's Alpha for a multi-item scale\n",
    "\n",
    "\n",
    "def cronbach_alpha(data):\n",
    "    \"\"\"\n",
    "    Calculate Cronbach's Alpha for internal consistency.\n",
    "\n",
    "    Parameters:\n",
    "    - data: DataFrame or 2D array where rows = respondents, columns = items\n",
    "\n",
    "    Returns:\n",
    "    - alpha: Cronbach's alpha coefficient\n",
    "    \"\"\"\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        data = data.values\n",
    "\n",
    "    # Number of items\n",
    "    k = data.shape[1]\n",
    "\n",
    "    # Variance of each item\n",
    "    item_variances = np.var(data, axis=0, ddof=1)\n",
    "\n",
    "    # Variance of total scores (sum across items)\n",
    "    total_scores = np.sum(data, axis=1)\n",
    "    total_variance = np.var(total_scores, ddof=1)\n",
    "\n",
    "    # Cronbach's alpha\n",
    "    alpha = (k / (k - 1)) * (1 - np.sum(item_variances) / total_variance)\n",
    "\n",
    "    return alpha\n",
    "\n",
    "\n",
    "# Simulate survey data: Depression scale with 8 items\n",
    "np.random.seed(789)\n",
    "n_respondents = 250\n",
    "n_items = 8\n",
    "\n",
    "# Each person has true depression level (latent variable)\n",
    "true_depression = np.random.normal(50, 15, n_respondents)\n",
    "\n",
    "# Items are noisy measurements of true depression\n",
    "# Good scale: Items highly correlated with true score\n",
    "item_responses = np.zeros((n_respondents, n_items))\n",
    "\n",
    "for i in range(n_items):\n",
    "    # Each item = true score + item-specific noise\n",
    "    item_responses[:, i] = true_depression + np.random.normal(0, 8, n_respondents)\n",
    "    # Convert to 1-5 Likert scale\n",
    "    item_responses[:, i] = np.digitize(item_responses[:, i], bins=[0, 35, 45, 55, 65, 100])\n",
    "\n",
    "# Create dataframe\n",
    "item_names = [f\"Item_{i+1}\" for i in range(n_items)]\n",
    "df_depression = pd.DataFrame(item_responses, columns=item_names)\n",
    "\n",
    "print(\"Depression Scale Data (first 10 respondents):\\n\")\n",
    "print(df_depression.head(10))\n",
    "\n",
    "# Calculate Cronbach's alpha\n",
    "alpha = cronbach_alpha(df_depression)\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"RELIABILITY ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nCronbach's Alpha: Î± = {alpha:.3f}\")\n",
    "\n",
    "if alpha >= 0.90:\n",
    "    interpretation = \"Excellent (consider removing redundant items)\"\n",
    "elif alpha >= 0.80:\n",
    "    interpretation = \"Good\"\n",
    "elif alpha >= 0.70:\n",
    "    interpretation = \"Acceptable\"\n",
    "elif alpha >= 0.60:\n",
    "    interpretation = \"Questionable\"\n",
    "else:\n",
    "    interpretation = \"Unacceptable\"\n",
    "\n",
    "print(f\"Interpretation: {interpretation}\")\n",
    "print(f\"\\nNumber of items: {n_items}\")\n",
    "print(f\"Number of respondents: {n_respondents}\")\n",
    "\n",
    "# Item-total correlations (how well each item correlates with total score)\n",
    "total_score = df_depression.sum(axis=1)\n",
    "item_total_corr = [pearsonr(df_depression[item], total_score)[0] for item in item_names]\n",
    "\n",
    "print(f\"\\nItem-Total Correlations:\")\n",
    "for item, corr in zip(item_names, item_total_corr):\n",
    "    print(f\"  {item}: r = {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alpha if item deleted analysis\n",
    "# Shows how alpha changes if each item is removed\n",
    "\n",
    "alpha_if_deleted = []\n",
    "\n",
    "for i in range(n_items):\n",
    "    # Create dataset without item i\n",
    "    items_subset = df_depression.drop(columns=[item_names[i]])\n",
    "    alpha_deleted = cronbach_alpha(items_subset)\n",
    "    alpha_if_deleted.append(alpha_deleted)\n",
    "\n",
    "# Create summary table\n",
    "item_analysis = pd.DataFrame(\n",
    "    {\n",
    "        \"Item\": item_names,\n",
    "        \"Item-Total Correlation\": item_total_corr,\n",
    "        \"Alpha if Deleted\": alpha_if_deleted,\n",
    "        \"Keep?\": [\n",
    "            \"âœ“\" if corr > 0.30 and alpha_del <= alpha else \"âœ— Consider removing\"\n",
    "            for corr, alpha_del in zip(item_total_corr, alpha_if_deleted)\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ITEM ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nCurrent Alpha: {alpha:.3f}\")\n",
    "print(\"\\nItem Performance:\")\n",
    "print(item_analysis.to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Panel 1: Item-total correlations\n",
    "colors = [\"#06A77D\" if corr > 0.30 else \"#E63946\" for corr in item_total_corr]\n",
    "axes[0].barh(item_names, item_total_corr, color=colors, edgecolor=\"black\", linewidth=1.5, alpha=0.7)\n",
    "axes[0].axvline(\n",
    "    x=0.30, color=\"black\", linestyle=\"--\", linewidth=2, label=\"Minimum threshold (0.30)\"\n",
    ")\n",
    "axes[0].set_xlabel(\"Item-Total Correlation\", fontsize=12, fontweight=\"bold\")\n",
    "axes[0].set_title(\"Item-Total Correlations\", fontsize=13, fontweight=\"bold\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3, axis=\"x\")\n",
    "\n",
    "# Panel 2: Alpha if deleted\n",
    "colors2 = [\"#E63946\" if a > alpha else \"#06A77D\" for a in alpha_if_deleted]\n",
    "axes[1].barh(\n",
    "    item_names, alpha_if_deleted, color=colors2, edgecolor=\"black\", linewidth=1.5, alpha=0.7\n",
    ")\n",
    "axes[1].axvline(\n",
    "    x=alpha, color=\"black\", linestyle=\"--\", linewidth=2, label=f\"Current Alpha ({alpha:.3f})\"\n",
    ")\n",
    "axes[1].set_xlabel(\"Cronbach's Alpha if Item Deleted\", fontsize=12, fontweight=\"bold\")\n",
    "axes[1].set_title(\"Effect of Removing Each Item\", fontsize=13, fontweight=\"bold\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3, axis=\"x\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"outputs/module_04/reliability_analysis.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Items with low item-total correlations (< 0.30) should be revised or removed.\")\n",
    "print(\"   If removing an item increases alpha, that item may be measuring something different.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Psychometric Properties: Validity\n",
    "\n",
    "**Validity** = Accuracy of measurement\n",
    "\n",
    "A valid measure actually measures what it claims to measure.\n",
    "\n",
    "**Important**: Reliability is necessary but not sufficient for validity.\n",
    "- A bathroom scale could consistently show you weigh 150 lbs (reliable)\n",
    "- But if you actually weigh 180 lbs, it's not valid\n",
    "\n",
    "### Types of Validity\n",
    "\n",
    "#### 1. Face Validity\n",
    "**Definition**: Does it appear to measure what it claims?\n",
    "**Assessment**: Subjective expert judgment\n",
    "**Example**: \"I feel sad\" has face validity for depression\n",
    "\n",
    "#### 2. Content Validity\n",
    "**Definition**: Does it cover the full domain of the construct?\n",
    "**Assessment**: Expert review\n",
    "**Example**: A math test should cover all relevant topics, not just geometry\n",
    "\n",
    "#### 3. Criterion Validity\n",
    "**Definition**: Does it correlate with an external criterion?\n",
    "\n",
    "**Subtypes**:\n",
    "- **Concurrent validity**: Correlates with criterion measured at same time\n",
    "  - Example: New depression scale correlates with established BDI\n",
    "- **Predictive validity**: Predicts future outcomes\n",
    "  - Example: SAT scores predict college GPA\n",
    "\n",
    "#### 4. Construct Validity\n",
    "**Definition**: Does it behave as theory predicts?\n",
    "\n",
    "**Subtypes**:\n",
    "- **Convergent validity**: Correlates with measures of related constructs\n",
    "  - Example: Depression scale correlates with anxiety scale\n",
    "- **Discriminant validity**: Does NOT correlate with unrelated constructs\n",
    "  - Example: Depression scale does NOT correlate with height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate construct validity: convergent and discriminant\n",
    "\n",
    "np.random.seed(999)\n",
    "n = 200\n",
    "\n",
    "# Latent true constructs\n",
    "true_depression = np.random.normal(50, 15, n)\n",
    "true_anxiety = 0.6 * true_depression + np.random.normal(20, 10, n)  # Correlated with depression\n",
    "true_height = np.random.normal(170, 10, n)  # Unrelated to depression\n",
    "\n",
    "# Measured variables (with noise)\n",
    "depression_scale_new = true_depression + np.random.normal(0, 8, n)  # Our new scale\n",
    "depression_scale_old = true_depression + np.random.normal(\n",
    "    0, 10, n\n",
    ")  # Established scale (more noise)\n",
    "anxiety_scale = true_anxiety + np.random.normal(0, 8, n)\n",
    "height_measured = true_height + np.random.normal(0, 2, n)\n",
    "\n",
    "# Create dataframe\n",
    "df_validity = pd.DataFrame(\n",
    "    {\n",
    "        \"Depression_New\": depression_scale_new,\n",
    "        \"Depression_Established\": depression_scale_old,\n",
    "        \"Anxiety\": anxiety_scale,\n",
    "        \"Height_cm\": height_measured,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Construct Validity Analysis\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Criterion validity (concurrent): Correlate with established measure\n",
    "criterion_corr, criterion_p = pearsonr(\n",
    "    df_validity[\"Depression_New\"], df_validity[\"Depression_Established\"]\n",
    ")\n",
    "print(f\"\\n1. CRITERION VALIDITY (Concurrent)\")\n",
    "print(f\"   Correlation with established depression scale:\")\n",
    "print(f\"   r = {criterion_corr:.3f}, p = {criterion_p:.4f}\")\n",
    "if criterion_corr > 0.70:\n",
    "    print(f\"   âœ“ Good criterion validity (r > 0.70)\")\n",
    "else:\n",
    "    print(f\"   âœ— Questionable criterion validity (r < 0.70)\")\n",
    "\n",
    "# Convergent validity: Should correlate with related construct\n",
    "convergent_corr, convergent_p = pearsonr(df_validity[\"Depression_New\"], df_validity[\"Anxiety\"])\n",
    "print(f\"\\n2. CONVERGENT VALIDITY\")\n",
    "print(f\"   Correlation with anxiety (related construct):\")\n",
    "print(f\"   r = {convergent_corr:.3f}, p = {convergent_p:.4f}\")\n",
    "if convergent_corr > 0.30 and convergent_p < 0.05:\n",
    "    print(f\"   âœ“ Good convergent validity (moderate-strong correlation)\")\n",
    "else:\n",
    "    print(f\"   âœ— Poor convergent validity\")\n",
    "\n",
    "# Discriminant validity: Should NOT correlate with unrelated construct\n",
    "discriminant_corr, discriminant_p = pearsonr(\n",
    "    df_validity[\"Depression_New\"], df_validity[\"Height_cm\"]\n",
    ")\n",
    "print(f\"\\n3. DISCRIMINANT VALIDITY\")\n",
    "print(f\"   Correlation with height (unrelated construct):\")\n",
    "print(f\"   r = {discriminant_corr:.3f}, p = {discriminant_p:.4f}\")\n",
    "if abs(discriminant_corr) < 0.20 or discriminant_p > 0.05:\n",
    "    print(f\"   âœ“ Good discriminant validity (weak/no correlation)\")\n",
    "else:\n",
    "    print(f\"   âœ— Poor discriminant validity (unexpected correlation)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize validity evidence\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Panel 1: Criterion validity\n",
    "axes[0].scatter(\n",
    "    df_validity[\"Depression_Established\"],\n",
    "    df_validity[\"Depression_New\"],\n",
    "    alpha=0.5,\n",
    "    s=50,\n",
    "    color=\"#06A77D\",\n",
    "    edgecolors=\"black\",\n",
    "    linewidths=0.5,\n",
    ")\n",
    "# Add regression line\n",
    "z = np.polyfit(df_validity[\"Depression_Established\"], df_validity[\"Depression_New\"], 1)\n",
    "p = np.poly1d(z)\n",
    "axes[0].plot(\n",
    "    df_validity[\"Depression_Established\"],\n",
    "    p(df_validity[\"Depression_Established\"]),\n",
    "    \"r-\",\n",
    "    linewidth=2,\n",
    "    label=f\"r = {criterion_corr:.3f}\",\n",
    ")\n",
    "axes[0].set_xlabel(\"Established Depression Scale\", fontsize=11, fontweight=\"bold\")\n",
    "axes[0].set_ylabel(\"New Depression Scale\", fontsize=11, fontweight=\"bold\")\n",
    "axes[0].set_title(\"Criterion Validity\\n(Should be high)\", fontsize=12, fontweight=\"bold\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Panel 2: Convergent validity\n",
    "axes[1].scatter(\n",
    "    df_validity[\"Anxiety\"],\n",
    "    df_validity[\"Depression_New\"],\n",
    "    alpha=0.5,\n",
    "    s=50,\n",
    "    color=\"#F4A261\",\n",
    "    edgecolors=\"black\",\n",
    "    linewidths=0.5,\n",
    ")\n",
    "z2 = np.polyfit(df_validity[\"Anxiety\"], df_validity[\"Depression_New\"], 1)\n",
    "p2 = np.poly1d(z2)\n",
    "axes[1].plot(\n",
    "    df_validity[\"Anxiety\"],\n",
    "    p2(df_validity[\"Anxiety\"]),\n",
    "    \"r-\",\n",
    "    linewidth=2,\n",
    "    label=f\"r = {convergent_corr:.3f}\",\n",
    ")\n",
    "axes[1].set_xlabel(\"Anxiety Scale\", fontsize=11, fontweight=\"bold\")\n",
    "axes[1].set_ylabel(\"New Depression Scale\", fontsize=11, fontweight=\"bold\")\n",
    "axes[1].set_title(\"Convergent Validity\\n(Should be moderate)\", fontsize=12, fontweight=\"bold\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Panel 3: Discriminant validity\n",
    "axes[2].scatter(\n",
    "    df_validity[\"Height_cm\"],\n",
    "    df_validity[\"Depression_New\"],\n",
    "    alpha=0.5,\n",
    "    s=50,\n",
    "    color=\"#E63946\",\n",
    "    edgecolors=\"black\",\n",
    "    linewidths=0.5,\n",
    ")\n",
    "z3 = np.polyfit(df_validity[\"Height_cm\"], df_validity[\"Depression_New\"], 1)\n",
    "p3 = np.poly1d(z3)\n",
    "axes[2].plot(\n",
    "    df_validity[\"Height_cm\"],\n",
    "    p3(df_validity[\"Height_cm\"]),\n",
    "    \"r-\",\n",
    "    linewidth=2,\n",
    "    label=f\"r = {discriminant_corr:.3f}\",\n",
    ")\n",
    "axes[2].set_xlabel(\"Height (cm)\", fontsize=11, fontweight=\"bold\")\n",
    "axes[2].set_ylabel(\"New Depression Scale\", fontsize=11, fontweight=\"bold\")\n",
    "axes[2].set_title(\"Discriminant Validity\\n(Should be near zero)\", fontsize=12, fontweight=\"bold\")\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"outputs/module_04/validity_analysis.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ A valid measure shows:\")\n",
    "print(\"   âœ“ High correlation with established measures (criterion validity)\")\n",
    "print(\"   âœ“ Moderate correlation with related constructs (convergent validity)\")\n",
    "print(\"   âœ“ Low correlation with unrelated constructs (discriminant validity)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Common Survey Biases\n",
    "\n",
    "### 1. Acquiescence Bias (Yea-Saying)\n",
    "**Problem**: Tendency to agree with statements regardless of content\n",
    "**Solution**: Include reverse-coded items\n",
    "\n",
    "**Example**:\n",
    "- Regular item: \"I enjoy social gatherings\" (Agree = extraverted)\n",
    "- Reverse item: \"I prefer to avoid social gatherings\" (Agree = introverted)\n",
    "\n",
    "### 2. Social Desirability Bias\n",
    "**Problem**: Answering in socially acceptable ways rather than truthfully\n",
    "**Solutions**:\n",
    "- Guarantee anonymity\n",
    "- Use indirect questioning\n",
    "- Normalize sensitive behaviors\n",
    "\n",
    "**Example**:\n",
    "- Bad: \"Do you ever cheat on your taxes?\"\n",
    "- Better: \"Studies show that many people occasionally report less income than they earned. Have you ever done this?\"\n",
    "\n",
    "### 3. Recency/Primacy Effects\n",
    "**Problem**: Respondents favor first or last options in a list\n",
    "**Solution**: Randomize order of response options\n",
    "\n",
    "### 4. Central Tendency Bias\n",
    "**Problem**: Avoiding extreme responses; clustering around middle\n",
    "**Solution**: Use forced-choice formats or remove neutral option\n",
    "\n",
    "### 5. Demand Characteristics\n",
    "**Problem**: Respondents guess study hypothesis and respond accordingly\n",
    "**Solution**: Disguise true purpose; use filler items\n",
    "\n",
    "### 6. Response Set\n",
    "**Problem**: Answering all items same way without reading (speeders, straightliners)\n",
    "**Solutions**:\n",
    "- Include attention checks\n",
    "- Vary item direction\n",
    "- Flag suspicious patterns in data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect response biases in survey data\n",
    "\n",
    "\n",
    "def detect_response_biases(df):\n",
    "    \"\"\"\n",
    "    Detect common response biases in survey data.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame with survey items (rows = respondents, columns = items)\n",
    "\n",
    "    Returns:\n",
    "    - Dictionary with bias indicators\n",
    "    \"\"\"\n",
    "    n_respondents = len(df)\n",
    "\n",
    "    biases = {\n",
    "        \"straightliners\": [],  # Always same response\n",
    "        \"speeders\": [],  # Low variance (not thinking)\n",
    "        \"acquiescent\": [],  # Always high scores\n",
    "        \"nay_sayers\": [],  # Always low scores\n",
    "    }\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        responses = row.values\n",
    "\n",
    "        # Straightlining: All responses identical\n",
    "        if len(np.unique(responses)) == 1:\n",
    "            biases[\"straightliners\"].append(idx)\n",
    "\n",
    "        # Speeding: Very low variance\n",
    "        if np.var(responses) < 0.5:\n",
    "            biases[\"speeders\"].append(idx)\n",
    "\n",
    "        # Acquiescence: Mean response > 4 (on 1-5 scale)\n",
    "        if np.mean(responses) > 4:\n",
    "            biases[\"acquiescent\"].append(idx)\n",
    "\n",
    "        # Nay-saying: Mean response < 2\n",
    "        if np.mean(responses) < 2:\n",
    "            biases[\"nay_sayers\"].append(idx)\n",
    "\n",
    "    return biases\n",
    "\n",
    "\n",
    "# Simulate survey data with some biased respondents\n",
    "np.random.seed(111)\n",
    "n_good = 180\n",
    "n_bad = 20\n",
    "n_items = 10\n",
    "\n",
    "# Good respondents: Thoughtful, varied responses\n",
    "good_responses = np.random.choice(\n",
    "    [1, 2, 3, 4, 5], size=(n_good, n_items), p=[0.10, 0.20, 0.40, 0.20, 0.10]\n",
    ")\n",
    "\n",
    "# Bad respondents\n",
    "straightliners = np.full((5, n_items), 3)  # All 3s\n",
    "acquiescent = np.random.choice([4, 5], size=(5, n_items))  # All high\n",
    "nay_sayers = np.random.choice([1, 2], size=(5, n_items))  # All low\n",
    "speeders = np.random.choice([2, 3, 4], size=(5, n_items))  # Low variance\n",
    "\n",
    "# Combine\n",
    "all_responses = np.vstack([good_responses, straightliners, acquiescent, nay_sayers, speeders])\n",
    "\n",
    "df_survey = pd.DataFrame(all_responses, columns=[f\"Q{i+1}\" for i in range(n_items)])\n",
    "\n",
    "# Detect biases\n",
    "biases_detected = detect_response_biases(df_survey)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"RESPONSE BIAS DETECTION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTotal respondents: {len(df_survey)}\")\n",
    "print(f\"\\nBiased response patterns detected:\")\n",
    "print(\n",
    "    f\"\\n1. Straightliners (all same response): {len(biases_detected['straightliners'])} respondents\"\n",
    ")\n",
    "print(f\"   Example IDs: {biases_detected['straightliners'][:5]}\")\n",
    "\n",
    "print(f\"\\n2. Speeders (very low variance): {len(biases_detected['speeders'])} respondents\")\n",
    "print(f\"   Example IDs: {biases_detected['speeders'][:5]}\")\n",
    "\n",
    "print(f\"\\n3. Acquiescent (always agree): {len(biases_detected['acquiescent'])} respondents\")\n",
    "print(f\"   Example IDs: {biases_detected['acquiescent'][:5]}\")\n",
    "\n",
    "print(f\"\\n4. Nay-sayers (always disagree): {len(biases_detected['nay_sayers'])} respondents\")\n",
    "print(f\"   Example IDs: {biases_detected['nay_sayers'][:5]}\")\n",
    "\n",
    "# Calculate percentage flagged\n",
    "all_flagged = set(\n",
    "    biases_detected[\"straightliners\"]\n",
    "    + biases_detected[\"speeders\"]\n",
    "    + biases_detected[\"acquiescent\"]\n",
    "    + biases_detected[\"nay_sayers\"]\n",
    ")\n",
    "print(\n",
    "    f\"\\nTotal unique respondents flagged: {len(all_flagged)} ({len(all_flagged)/len(df_survey)*100:.1f}%)\"\n",
    ")\n",
    "print(f\"\\nðŸ’¡ These respondents should be carefully reviewed and possibly excluded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Measurement Scales\n",
    "\n",
    "Understanding scale types determines appropriate analyses.\n",
    "\n",
    "### Scale Types (Stevens, 1946)\n",
    "\n",
    "| Scale | Properties | Examples | Allowed Operations | Appropriate Statistics |\n",
    "|-------|-----------|----------|-------------------|------------------------|\n",
    "| **Nominal** | Categories, no order | Gender, ethnicity, country | =, â‰  | Mode, chi-square |\n",
    "| **Ordinal** | Categories, ordered, unequal intervals | Education level, Likert scales | =, â‰ , <, > | Median, percentiles, Spearman correlation |\n",
    "| **Interval** | Ordered, equal intervals, no true zero | Temperature (Â°C), IQ scores | =, â‰ , <, >, +, âˆ’ | Mean, SD, Pearson correlation, t-test |\n",
    "| **Ratio** | Ordered, equal intervals, true zero | Height, weight, income, age | =, â‰ , <, >, +, âˆ’, Ã—, Ã· | All statistics, including ratios |\n",
    "\n",
    "### Examples\n",
    "\n",
    "**Nominal**: Eye color  \n",
    "- Blue, Brown, Green, Hazel\n",
    "- Cannot say \"Brown > Blue\" (no inherent order)\n",
    "\n",
    "**Ordinal**: Education  \n",
    "- Less than HS < HS < Some College < Bachelor's < Graduate\n",
    "- Order exists, but intervals unequal (HSâ†’Some College â‰  Bachelor'sâ†’Graduate)\n",
    "\n",
    "**Interval**: Temperature  \n",
    "- 20Â°C to 30Â°C = 30Â°C to 40Â°C (equal intervals)\n",
    "- But 40Â°C is NOT \"twice as hot\" as 20Â°C (no true zero)\n",
    "\n",
    "**Ratio**: Income  \n",
    "- $0 = absolute absence of money (true zero)\n",
    "- $100K is twice $50K (ratios meaningful)\n",
    "\n",
    "### The Likert Debate\n",
    "\n",
    "**Question**: Are Likert scales ordinal or interval?\n",
    "\n",
    "**Technically**: Ordinal  \n",
    "- Distance between \"Agree\" and \"Strongly Agree\" is not necessarily equal to distance between \"Disagree\" and \"Neutral\"\n",
    "\n",
    "**In practice**: Often treated as interval  \n",
    "- When multiple items are summed (scale scores), tends toward interval\n",
    "- Robust statistics (t-tests, ANOVA) perform well even with ordinal Likert data\n",
    "\n",
    "**Recommendation**:\n",
    "- Single Likert items: Use non-parametric tests (Mann-Whitney U, Kruskal-Wallis)\n",
    "- Likert scale scores (summed items): Parametric tests usually acceptable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate appropriate analyses for different scale types\n",
    "\n",
    "np.random.seed(222)\n",
    "n = 100\n",
    "\n",
    "# Generate data for different scale types\n",
    "data_demo = pd.DataFrame(\n",
    "    {\n",
    "        # Nominal: Gender\n",
    "        \"Gender\": np.random.choice([\"Male\", \"Female\", \"Non-binary\"], n, p=[0.48, 0.48, 0.04]),\n",
    "        # Ordinal: Education\n",
    "        \"Education\": np.random.choice(\n",
    "            [\"High School\", \"Some College\", \"Bachelor\", \"Graduate\"], n, p=[0.25, 0.30, 0.30, 0.15]\n",
    "        ),\n",
    "        # Interval: IQ (no true zero)\n",
    "        \"IQ\": np.random.normal(100, 15, n).astype(int),\n",
    "        # Ratio: Income (true zero exists)\n",
    "        \"Income\": np.random.lognormal(10.5, 0.5, n).astype(int),\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Sample Data with Different Scale Types:\\n\")\n",
    "print(data_demo.head(10))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"APPROPRIATE STATISTICS BY SCALE TYPE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Nominal: Mode and frequency\n",
    "print(\"\\n1. NOMINAL (Gender)\")\n",
    "print(\"   Appropriate statistics: Mode, frequencies\")\n",
    "print(\"\\n   Frequency distribution:\")\n",
    "print(data_demo[\"Gender\"].value_counts())\n",
    "print(f\"\\n   Mode: {data_demo['Gender'].mode()[0]}\")\n",
    "\n",
    "# Ordinal: Median and percentiles\n",
    "print(\"\\n2. ORDINAL (Education)\")\n",
    "print(\"   Appropriate statistics: Median, percentiles\")\n",
    "print(\"\\n   Frequency distribution:\")\n",
    "edu_order = [\"High School\", \"Some College\", \"Bachelor\", \"Graduate\"]\n",
    "edu_counts = data_demo[\"Education\"].value_counts()[edu_order]\n",
    "print(edu_counts)\n",
    "\n",
    "# Interval: Mean, SD\n",
    "print(\"\\n3. INTERVAL (IQ)\")\n",
    "print(\"   Appropriate statistics: Mean, SD, correlation, t-tests\")\n",
    "print(f\"\\n   Mean: {data_demo['IQ'].mean():.1f}\")\n",
    "print(f\"   SD: {data_demo['IQ'].std():.1f}\")\n",
    "print(f\"   Range: {data_demo['IQ'].min()} - {data_demo['IQ'].max()}\")\n",
    "\n",
    "# Ratio: All statistics including ratios\n",
    "print(\"\\n4. RATIO (Income)\")\n",
    "print(\"   Appropriate statistics: All statistics + ratios\")\n",
    "print(f\"\\n   Mean: ${data_demo['Income'].mean():,.0f}\")\n",
    "print(f\"   Median: ${data_demo['Income'].median():,.0f}\")\n",
    "print(f\"   SD: ${data_demo['Income'].std():,.0f}\")\n",
    "print(f\"\\n   Ratio example: Person with income ${data_demo['Income'].max():,}\")\n",
    "print(f\"   earns {data_demo['Income'].max() / data_demo['Income'].median():.1f}x the median\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Survey Pretesting\n",
    "\n",
    "**Never deploy a survey without pretesting!**\n",
    "\n",
    "### Pretesting Methods\n",
    "\n",
    "#### 1. Cognitive Interviewing\n",
    "**Process**: Ask participants to \"think aloud\" while completing survey\n",
    "\n",
    "**Goals**:\n",
    "- Identify confusing questions\n",
    "- Understand how respondents interpret questions\n",
    "- Detect missing response options\n",
    "\n",
    "**Example questions to ask**:\n",
    "- \"What does this question mean to you?\"\n",
    "- \"How did you arrive at your answer?\"\n",
    "- \"Was anything confusing or unclear?\"\n",
    "\n",
    "**Sample size**: 5-15 participants per round\n",
    "\n",
    "#### 2. Pilot Testing\n",
    "**Process**: Administer survey to small sample under realistic conditions\n",
    "\n",
    "**Goals**:\n",
    "- Test survey flow and timing\n",
    "- Check skip logic and branching\n",
    "- Examine response distributions\n",
    "- Calculate preliminary reliability\n",
    "\n",
    "**Sample size**: 30-50 participants (from target population)\n",
    "\n",
    "#### 3. Expert Review\n",
    "**Process**: Subject matter experts review questions\n",
    "\n",
    "**Goals**:\n",
    "- Assess content validity\n",
    "- Identify biased or leading questions\n",
    "- Ensure comprehensiveness\n",
    "\n",
    "**Reviewers**: 3-5 experts in the domain\n",
    "\n",
    "### Red Flags in Pilot Data\n",
    "\n",
    "1. **High non-response**: Question may be sensitive or confusing\n",
    "2. **All choose same option**: Question not discriminating\n",
    "3. **High \"Other\" selection**: Missing important response categories\n",
    "4. **Low reliability (Î± < 0.70)**: Items don't cohere\n",
    "5. **Unexpected patterns**: May indicate misunderstanding\n",
    "\n",
    "### Pretesting Checklist\n",
    "\n",
    "```\n",
    "â–¡ Conduct cognitive interviews (n â‰¥ 5)\n",
    "â–¡ Revise based on feedback\n",
    "â–¡ Get expert review\n",
    "â–¡ Pilot test (n â‰¥ 30)\n",
    "â–¡ Check item distributions\n",
    "â–¡ Calculate reliability (Cronbach's Î±)\n",
    "â–¡ Analyze completion time\n",
    "â–¡ Test on multiple devices (if online)\n",
    "â–¡ Check skip logic and branching\n",
    "â–¡ Final revisions\n",
    "â–¡ Document all changes\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a survey development checklist\n",
    "\n",
    "survey_checklist = pd.DataFrame(\n",
    "    {\n",
    "        \"Stage\": [\n",
    "            \"Planning\",\n",
    "            \"Planning\",\n",
    "            \"Planning\",\n",
    "            \"Design\",\n",
    "            \"Design\",\n",
    "            \"Design\",\n",
    "            \"Design\",\n",
    "            \"Design\",\n",
    "            \"Pretesting\",\n",
    "            \"Pretesting\",\n",
    "            \"Pretesting\",\n",
    "            \"Pretesting\",\n",
    "            \"Revision\",\n",
    "            \"Revision\",\n",
    "            \"Deployment\",\n",
    "            \"Deployment\",\n",
    "            \"Deployment\",\n",
    "        ],\n",
    "        \"Task\": [\n",
    "            \"Define research questions and constructs\",\n",
    "            \"Review existing validated measures\",\n",
    "            \"Determine target population and sample size\",\n",
    "            \"Write clear, unbiased questions\",\n",
    "            \"Select appropriate response scales\",\n",
    "            \"Include reverse-coded items (if applicable)\",\n",
    "            \"Add attention checks and validity items\",\n",
    "            \"Organize logical flow and grouping\",\n",
    "            \"Cognitive interviews (n=5-15)\",\n",
    "            \"Expert review (n=3-5)\",\n",
    "            \"Pilot test (n=30-50)\",\n",
    "            \"Calculate reliability (Cronbach's Î±)\",\n",
    "            \"Revise based on pretest feedback\",\n",
    "            \"Retest if major changes made\",\n",
    "            \"Finalize survey platform\",\n",
    "            \"Test on multiple devices\",\n",
    "            \"Deploy and monitor initial responses\",\n",
    "        ],\n",
    "        \"Priority\": [\n",
    "            \"Critical\",\n",
    "            \"High\",\n",
    "            \"Critical\",\n",
    "            \"Critical\",\n",
    "            \"Critical\",\n",
    "            \"High\",\n",
    "            \"Medium\",\n",
    "            \"High\",\n",
    "            \"Critical\",\n",
    "            \"High\",\n",
    "            \"Critical\",\n",
    "            \"Critical\",\n",
    "            \"Critical\",\n",
    "            \"High\",\n",
    "            \"Critical\",\n",
    "            \"High\",\n",
    "            \"High\",\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"SURVEY DEVELOPMENT CHECKLIST\")\n",
    "print(\"=\" * 80)\n",
    "print(survey_checklist.to_string(index=False))\n",
    "\n",
    "# Save checklist\n",
    "survey_checklist.to_csv(\"outputs/module_04/survey_development_checklist.csv\", index=False)\n",
    "print(\"\\nâœ“ Checklist saved to outputs/module_04/survey_development_checklist.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Practice Exercises\n",
    "\n",
    "### Exercise 1: Identify Question Problems\n",
    "\n",
    "For each question, identify the flaw(s) and suggest improvement:\n",
    "\n",
    "1. **\"Don't you think that irresponsible people shouldn't be allowed to vote?\"**  \n",
    "   Flaw: ___________  \n",
    "   Improved: ___________\n",
    "\n",
    "2. **\"How often do you exercise and eat healthy?\"**  \n",
    "   Flaw: ___________  \n",
    "   Improved: ___________\n",
    "\n",
    "3. **\"On a scale of 1-5, how satisfied are you with your extremely comprehensive and helpful health insurance plan?\"**  \n",
    "   Flaw: ___________  \n",
    "   Improved: ___________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Calculate Cronbach's Alpha for your own data\n",
    "# Create a 5-item scale measuring \"Academic Motivation\"\n",
    "\n",
    "# Simulate responses from 100 students\n",
    "np.random.seed(555)\n",
    "n_students = 100\n",
    "\n",
    "# Each student has true motivation level\n",
    "true_motivation = np.random.normal(3, 1, n_students)\n",
    "\n",
    "# Create 5 items (with noise)\n",
    "items = {}\n",
    "for i in range(1, 6):\n",
    "    items[f\"Item_{i}\"] = true_motivation + np.random.normal(0, 0.5, n_students)\n",
    "    # Convert to 1-5 Likert\n",
    "    items[f\"Item_{i}\"] = np.clip(np.round(items[f\"Item_{i}\"]), 1, 5)\n",
    "\n",
    "df_motivation = pd.DataFrame(items)\n",
    "\n",
    "# YOUR TASK:\n",
    "# 1. Calculate Cronbach's alpha\n",
    "# 2. Calculate item-total correlations\n",
    "# 3. Determine if any items should be removed\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# alpha = cronbach_alpha(df_motivation)\n",
    "# print(f\"Cronbach's Alpha: {alpha:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Detect biased respondents\n",
    "# Use the detect_response_biases function on new data\n",
    "\n",
    "# Generate survey data\n",
    "np.random.seed(777)\n",
    "test_data = np.random.choice([1, 2, 3, 4, 5], size=(150, 8))\n",
    "\n",
    "# Add some biased respondents (you decide how)\n",
    "# Hint: Create straightliners, acquiescent respondents, etc.\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Key Takeaways\n",
    "\n",
    "### The Golden Rules of Survey Design\n",
    "\n",
    "1. **Keep it simple**: One idea per question\n",
    "2. **Avoid bias**: Neutral wording, balanced options\n",
    "3. **Be specific**: Define terms, specify timeframes\n",
    "4. **Match scales to constructs**: Choose appropriate response formats\n",
    "5. **Test psychometric properties**: Reliability and validity are non-negotiable\n",
    "6. **Pretest extensively**: Cognitive interviews â†’ Pilot â†’ Revise â†’ Repeat\n",
    "7. **Monitor data quality**: Detect and handle biased responses\n",
    "8. **Document everything**: Question development, changes, decisions\n",
    "\n",
    "### Reliability vs. Validity Decision Tree\n",
    "\n",
    "```\n",
    "Is your measure RELIABLE (consistent)?\n",
    "    â”‚\n",
    "    NO â†’ Fix it! (Review items, increase length, train raters)\n",
    "    â”‚\n",
    "    YES â†’ Is it VALID (accurate)?\n",
    "           â”‚\n",
    "           NO â†’ It's consistently measuring the WRONG thing\n",
    "           â”‚     â””â”€> Assess construct validity, revise items\n",
    "           â”‚\n",
    "           YES â†’ Good to go! (But keep monitoring)\n",
    "```\n",
    "\n",
    "### Common Mistakes to Avoid\n",
    "\n",
    "âŒ Skipping pretesting  \n",
    "âŒ Using double-barreled questions  \n",
    "âŒ Leading or loaded language  \n",
    "âŒ Assuming all Likert scales are reliable  \n",
    "âŒ Ignoring scale type when analyzing  \n",
    "âŒ Not checking for response biases  \n",
    "âŒ Deploying without pilot testing  \n",
    "\n",
    "### Best Practices\n",
    "\n",
    "âœ“ Use validated measures when available  \n",
    "âœ“ Include reverse-coded items  \n",
    "âœ“ Add attention checks  \n",
    "âœ“ Calculate Cronbach's Î± (target: â‰¥ 0.70)  \n",
    "âœ“ Assess validity (convergent, discriminant, criterion)  \n",
    "âœ“ Screen for biased response patterns  \n",
    "âœ“ Report psychometric properties in publications  \n",
    "\n",
    "### Moving Forward\n",
    "\n",
    "Now you can create measurement instruments that produce trustworthy data. The next module covers **sampling strategies**, teaching you how to select representative samples for your surveys and studies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Additional Resources\n",
    "\n",
    "### Essential Readings\n",
    "\n",
    "1. **Dillman, Smyth, & Christian (2014)**. *Internet, Phone, Mail, and Mixed-Mode Surveys: The Tailored Design Method*\n",
    "   - Comprehensive guide to survey methodology\n",
    "\n",
    "2. **Fowler (2014)**. *Survey Research Methods* (5th ed.)\n",
    "   - Classic textbook on survey design\n",
    "\n",
    "3. **DeVellis & Thorpe (2021)**. *Scale Development: Theory and Applications* (5th ed.)\n",
    "   - The definitive guide to creating measurement scales\n",
    "\n",
    "4. **Tourangeau, Rips, & Rasinski (2000)**. *The Psychology of Survey Response*\n",
    "   - Understanding cognitive processes in survey taking\n",
    "\n",
    "### Online Resources\n",
    "\n",
    "- **American Association for Public Opinion Research (AAPOR)**: Best practices and ethics\n",
    "- **Questionnaire Design Tips** (Pew Research): Practical guidelines\n",
    "- **Cognitive Interviewing Guide** (Centers for Disease Control): Free manual\n",
    "\n",
    "### Tools and Software\n",
    "\n",
    "- **Survey Platforms**: Qualtrics, SurveyMonkey, Google Forms, LimeSurvey (open-source)\n",
    "- **Reliability Calculators**: SPSS, R (psych package), Python (pingouin)\n",
    "- **Validated Scales**: APA PsycTests, National Cancer Institute Grid-Enabled Measures Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Congratulations!\n",
    "\n",
    "You've completed **Module 04: Survey Design & Measurement**. You can now:\n",
    "\n",
    "âœ“ Design effective, unbiased survey questions  \n",
    "âœ“ Select appropriate response scales  \n",
    "âœ“ Evaluate reliability using Cronbach's alpha  \n",
    "âœ“ Assess validity (criterion, convergent, discriminant)  \n",
    "âœ“ Identify and mitigate survey biases  \n",
    "âœ“ Understand measurement scale types  \n",
    "âœ“ Conduct comprehensive survey pretesting  \n",
    "âœ“ Detect problematic response patterns  \n",
    "\n",
    "**Next Module**: Sampling Strategies  \n",
    "**File**: `05_sampling_strategies.ipynb`\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
