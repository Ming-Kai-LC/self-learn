{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 03: Advanced Experimental Designs\n",
    "\n",
    "**Estimated Time**: 45 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this module, you will be able to:\n",
    "\n",
    "1. **Distinguish** between true experimental, quasi-experimental, and observational designs\n",
    "2. **Select** appropriate experimental designs for different research contexts\n",
    "3. **Implement** within-subjects and between-subjects designs with proper analysis\n",
    "4. **Apply** crossover designs with appropriate washout periods\n",
    "5. **Analyze** interrupted time series data to detect intervention effects\n",
    "6. **Understand** regression discontinuity designs for causal inference\n",
    "7. **Identify** and leverage natural experiments in observational data\n",
    "8. **Conduct** sensitivity analyses to test robustness of findings\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "While **Randomized Controlled Trials (RCTs)** are the gold standard for causal inference, they're often:\n",
    "- **Impractical** (can't randomize people to smoke cigarettes)\n",
    "- **Unethical** (can't withhold beneficial treatments)\n",
    "- **Expensive** (require large samples and long follow-ups)\n",
    "- **Limited** (may not generalize to real-world settings)\n",
    "\n",
    "Advanced experimental designs allow researchers to:\n",
    "- Make **causal claims** in observational settings\n",
    "- **Reduce bias** through clever design choices\n",
    "- **Increase statistical power** with repeated measures\n",
    "- **Answer important questions** that RCTs cannot address\n",
    "\n",
    "This module equips you with a sophisticated toolkit for designing rigorous studies when perfect experiments aren't possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import ttest_rel, ttest_ind, f_oneway\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set style\n",
    "plt.style.use(\"seaborn-v0_8-darkgrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create output directory\n",
    "import os\n",
    "\n",
    "os.makedirs(\"outputs/module_03\", exist_ok=True)\n",
    "\n",
    "print(\"‚úì Libraries imported successfully\")\n",
    "print(\"‚úì Output directory created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Experimental Design Landscape\n",
    "\n",
    "### The Hierarchy of Evidence\n",
    "\n",
    "Research designs vary in their ability to support causal claims:\n",
    "\n",
    "```\n",
    "STRONGEST EVIDENCE ‚Üì\n",
    "\n",
    "1. Systematic Reviews & Meta-Analyses\n",
    "   ‚îî‚îÄ Synthesize multiple RCTs\n",
    "\n",
    "2. Randomized Controlled Trials (RCTs)\n",
    "   ‚îî‚îÄ Random assignment eliminates confounding\n",
    "\n",
    "3. Quasi-Experimental Designs\n",
    "   ‚îú‚îÄ Regression Discontinuity\n",
    "   ‚îú‚îÄ Interrupted Time Series\n",
    "   ‚îú‚îÄ Difference-in-Differences\n",
    "   ‚îî‚îÄ Natural Experiments\n",
    "\n",
    "4. Cohort Studies\n",
    "   ‚îî‚îÄ Follow groups over time\n",
    "\n",
    "5. Case-Control Studies\n",
    "   ‚îî‚îÄ Compare cases to controls retrospectively\n",
    "\n",
    "6. Cross-Sectional Studies\n",
    "   ‚îî‚îÄ Snapshot at one time point\n",
    "\n",
    "7. Case Reports & Expert Opinion\n",
    "   ‚îî‚îÄ Individual observations\n",
    "\n",
    "WEAKEST EVIDENCE ‚Üë\n",
    "```\n",
    "\n",
    "### Key Dimensions of Experimental Design\n",
    "\n",
    "| Dimension | Options | Trade-offs |\n",
    "|-----------|---------|------------|\n",
    "| **Assignment** | Random vs. Non-random | Eliminates confounding vs. Practical |\n",
    "| **Comparison** | Between-subjects vs. Within-subjects | Independent vs. Powerful |\n",
    "| **Timing** | Cross-sectional vs. Longitudinal | Quick vs. Causal |\n",
    "| **Control** | Active vs. Placebo vs. None | Specific vs. General |\n",
    "| **Blinding** | Single vs. Double vs. Triple | Reduces bias vs. Practical |\n",
    "\n",
    "Let's explore advanced designs that maximize causal inference when RCTs aren't feasible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the design decision tree\n",
    "design_data = {\n",
    "    \"Design Type\": [\n",
    "        \"RCT\",\n",
    "        \"RCT\",\n",
    "        \"Quasi-Exp\",\n",
    "        \"Quasi-Exp\",\n",
    "        \"Quasi-Exp\",\n",
    "        \"Observational\",\n",
    "        \"Observational\",\n",
    "    ],\n",
    "    \"Specific Design\": [\n",
    "        \"Between-Subjects\",\n",
    "        \"Within-Subjects\",\n",
    "        \"Regression Discontinuity\",\n",
    "        \"Interrupted Time Series\",\n",
    "        \"Natural Experiment\",\n",
    "        \"Cohort\",\n",
    "        \"Cross-Sectional\",\n",
    "    ],\n",
    "    \"Causal Strength\": [95, 90, 80, 75, 70, 50, 30],\n",
    "    \"Feasibility\": [30, 40, 70, 75, 80, 85, 95],\n",
    "}\n",
    "\n",
    "df_designs = pd.DataFrame(design_data)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "colors = {\"RCT\": \"#2E86AB\", \"Quasi-Exp\": \"#A23B72\", \"Observational\": \"#F18F01\"}\n",
    "for design_type in df_designs[\"Design Type\"].unique():\n",
    "    subset = df_designs[df_designs[\"Design Type\"] == design_type]\n",
    "    ax.scatter(\n",
    "        subset[\"Feasibility\"],\n",
    "        subset[\"Causal Strength\"],\n",
    "        s=200,\n",
    "        alpha=0.6,\n",
    "        label=design_type,\n",
    "        color=colors[design_type],\n",
    "    )\n",
    "\n",
    "    for idx, row in subset.iterrows():\n",
    "        ax.annotate(\n",
    "            row[\"Specific Design\"],\n",
    "            (row[\"Feasibility\"], row[\"Causal Strength\"]),\n",
    "            xytext=(5, 5),\n",
    "            textcoords=\"offset points\",\n",
    "            fontsize=8,\n",
    "        )\n",
    "\n",
    "ax.set_xlabel(\"Feasibility (Ease of Implementation)\", fontsize=12, fontweight=\"bold\")\n",
    "ax.set_ylabel(\"Causal Inference Strength\", fontsize=12, fontweight=\"bold\")\n",
    "ax.set_title(\"The Trade-off: Causal Strength vs. Feasibility\", fontsize=14, fontweight=\"bold\")\n",
    "ax.legend(title=\"Design Category\", loc=\"lower left\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"outputs/module_03/design_tradeoff.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä The ideal design balances causal strength with practical feasibility\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Between-Subjects vs. Within-Subjects Designs\n",
    "\n",
    "### Between-Subjects (Independent Groups)\n",
    "\n",
    "**Design**: Different participants in each condition\n",
    "\n",
    "**Advantages**:\n",
    "- No carryover effects\n",
    "- No practice effects\n",
    "- No demand characteristics from repeated exposure\n",
    "\n",
    "**Disadvantages**:\n",
    "- Requires larger sample size\n",
    "- Individual differences add noise\n",
    "- Lower statistical power\n",
    "\n",
    "### Within-Subjects (Repeated Measures)\n",
    "\n",
    "**Design**: Same participants in all conditions\n",
    "\n",
    "**Advantages**:\n",
    "- Controls for individual differences\n",
    "- Requires fewer participants\n",
    "- Higher statistical power\n",
    "\n",
    "**Disadvantages**:\n",
    "- Carryover effects possible\n",
    "- Practice/fatigue effects\n",
    "- Attrition concerns\n",
    "\n",
    "### Power Comparison\n",
    "\n",
    "Let's demonstrate why within-subjects designs are more powerful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate the power advantage of within-subjects designs\n",
    "\n",
    "\n",
    "def simulate_experiment(n_participants, effect_size, design=\"between\", n_simulations=1000):\n",
    "    \"\"\"\n",
    "    Simulate experiments to compare power of different designs.\n",
    "\n",
    "    Parameters:\n",
    "    - n_participants: Number of participants\n",
    "    - effect_size: Cohen's d\n",
    "    - design: 'between' or 'within'\n",
    "    - n_simulations: Number of simulations to run\n",
    "\n",
    "    Returns:\n",
    "    - proportion of significant results (empirical power)\n",
    "    \"\"\"\n",
    "    significant_count = 0\n",
    "\n",
    "    for _ in range(n_simulations):\n",
    "        if design == \"between\":\n",
    "            # Between-subjects: independent groups\n",
    "            group1 = np.random.normal(0, 1, n_participants)\n",
    "            group2 = np.random.normal(effect_size, 1, n_participants)\n",
    "            _, p_value = ttest_ind(group1, group2)\n",
    "\n",
    "        else:  # within-subjects\n",
    "            # Within-subjects: same participants, correlated measures\n",
    "            # Individual differences (baseline ability)\n",
    "            baseline = np.random.normal(0, 1, n_participants)\n",
    "\n",
    "            # Condition 1: baseline + random noise\n",
    "            condition1 = baseline + np.random.normal(0, 0.5, n_participants)\n",
    "\n",
    "            # Condition 2: baseline + effect + random noise\n",
    "            condition2 = baseline + effect_size + np.random.normal(0, 0.5, n_participants)\n",
    "\n",
    "            _, p_value = ttest_rel(condition1, condition2)\n",
    "\n",
    "        if p_value < 0.05:\n",
    "            significant_count += 1\n",
    "\n",
    "    return significant_count / n_simulations\n",
    "\n",
    "\n",
    "# Compare power across sample sizes\n",
    "sample_sizes = range(10, 101, 10)\n",
    "effect_size = 0.5  # Medium effect\n",
    "\n",
    "power_between = [simulate_experiment(n, effect_size, \"between\", 500) for n in sample_sizes]\n",
    "power_within = [simulate_experiment(n, effect_size, \"within\", 500) for n in sample_sizes]\n",
    "\n",
    "# Visualize power comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.plot(\n",
    "    sample_sizes,\n",
    "    power_between,\n",
    "    \"o-\",\n",
    "    linewidth=2,\n",
    "    markersize=8,\n",
    "    label=\"Between-Subjects\",\n",
    "    color=\"#E63946\",\n",
    ")\n",
    "ax.plot(\n",
    "    sample_sizes,\n",
    "    power_within,\n",
    "    \"s-\",\n",
    "    linewidth=2,\n",
    "    markersize=8,\n",
    "    label=\"Within-Subjects\",\n",
    "    color=\"#06A77D\",\n",
    ")\n",
    "ax.axhline(y=0.80, color=\"gray\", linestyle=\"--\", linewidth=1.5, label=\"Target Power (80%)\")\n",
    "\n",
    "ax.set_xlabel(\"Sample Size (N per group)\", fontsize=12, fontweight=\"bold\")\n",
    "ax.set_ylabel(\"Statistical Power\", fontsize=12, fontweight=\"bold\")\n",
    "ax.set_title(\n",
    "    f\"Power Comparison: Within vs. Between (Effect Size d = {effect_size})\",\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "ax.legend(loc=\"lower right\", fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"outputs/module_03/power_comparison.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Find required sample sizes for 80% power\n",
    "n_between_80 = next((n for n, p in zip(sample_sizes, power_between) if p >= 0.80), None)\n",
    "n_within_80 = next((n for n, p in zip(sample_sizes, power_within) if p >= 0.80), None)\n",
    "\n",
    "print(f\"\\nüìä Power Analysis Results (d = {effect_size}):\")\n",
    "print(f\"\\nBetween-subjects requires N ‚âà {n_between_80} per group for 80% power\")\n",
    "print(f\"Within-subjects requires N ‚âà {n_within_80} participants for 80% power\")\n",
    "print(f\"\\nüí° Within-subjects design is ~{n_between_80/n_within_80:.1f}x more efficient!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use Each Design\n",
    "\n",
    "**Use Between-Subjects when**:\n",
    "- Exposure to one condition affects responses to others (e.g., learning)\n",
    "- The intervention permanently changes participants (e.g., training)\n",
    "- You're studying stable traits (e.g., personality)\n",
    "- Repeated testing is impractical or expensive\n",
    "\n",
    "**Use Within-Subjects when**:\n",
    "- Individual differences are large (increases power)\n",
    "- Sample size is limited (clinical populations)\n",
    "- Conditions are clearly distinguishable (no confusion)\n",
    "- Order effects can be controlled (counterbalancing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Crossover Designs\n",
    "\n",
    "**Crossover designs** are a special type of within-subjects design where participants receive treatments in different sequences.\n",
    "\n",
    "### Classic 2√ó2 Crossover Design\n",
    "\n",
    "```\n",
    "Group 1: A ‚Üí [washout] ‚Üí B\n",
    "Group 2: B ‚Üí [washout] ‚Üí A\n",
    "```\n",
    "\n",
    "**Key Feature**: The **washout period** allows the first treatment's effects to dissipate before the second treatment.\n",
    "\n",
    "### Advantages\n",
    "1. Controls for individual differences (within-subjects)\n",
    "2. Controls for order effects (counterbalancing)\n",
    "3. High statistical power\n",
    "4. Each participant serves as their own control\n",
    "\n",
    "### Challenges\n",
    "1. **Carryover effects**: Treatment A affects response to Treatment B\n",
    "2. **Washout determination**: How long is sufficient?\n",
    "3. **Attrition**: Longer studies = more dropout\n",
    "4. **Period effects**: Changes over time confound treatment effects\n",
    "\n",
    "### Example: Medication Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a crossover trial for two pain medications\n",
    "\n",
    "np.random.seed(123)\n",
    "n_participants = 40\n",
    "\n",
    "# Individual baseline pain levels (person-specific)\n",
    "baseline_pain = np.random.normal(50, 10, n_participants)\n",
    "\n",
    "# True treatment effects\n",
    "effect_drug_A = -15  # Reduces pain by 15 points\n",
    "effect_drug_B = -10  # Reduces pain by 10 points\n",
    "\n",
    "# Assign to sequences\n",
    "sequence = np.random.choice([\"A-B\", \"B-A\"], size=n_participants)\n",
    "\n",
    "# Simulate outcomes\n",
    "data_crossover = []\n",
    "\n",
    "for i in range(n_participants):\n",
    "    person_baseline = baseline_pain[i]\n",
    "\n",
    "    if sequence[i] == \"A-B\":\n",
    "        # Period 1: Drug A\n",
    "        period1_pain = person_baseline + effect_drug_A + np.random.normal(0, 5)\n",
    "        # Period 2: Drug B (after washout)\n",
    "        period2_pain = person_baseline + effect_drug_B + np.random.normal(0, 5)\n",
    "\n",
    "        data_crossover.append(\n",
    "            {\n",
    "                \"Participant\": i + 1,\n",
    "                \"Sequence\": \"A‚ÜíB\",\n",
    "                \"Period_1_Treatment\": \"Drug A\",\n",
    "                \"Period_1_Pain\": period1_pain,\n",
    "                \"Period_2_Treatment\": \"Drug B\",\n",
    "                \"Period_2_Pain\": period2_pain,\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        # Period 1: Drug B\n",
    "        period1_pain = person_baseline + effect_drug_B + np.random.normal(0, 5)\n",
    "        # Period 2: Drug A (after washout)\n",
    "        period2_pain = person_baseline + effect_drug_A + np.random.normal(0, 5)\n",
    "\n",
    "        data_crossover.append(\n",
    "            {\n",
    "                \"Participant\": i + 1,\n",
    "                \"Sequence\": \"B‚ÜíA\",\n",
    "                \"Period_1_Treatment\": \"Drug B\",\n",
    "                \"Period_1_Pain\": period1_pain,\n",
    "                \"Period_2_Treatment\": \"Drug A\",\n",
    "                \"Period_2_Pain\": period2_pain,\n",
    "            }\n",
    "        )\n",
    "\n",
    "df_crossover = pd.DataFrame(data_crossover)\n",
    "\n",
    "print(\"Crossover Trial Data (first 10 participants):\")\n",
    "print(df_crossover.head(10))\n",
    "\n",
    "# Reshape for analysis\n",
    "df_long = pd.concat(\n",
    "    [\n",
    "        df_crossover[[\"Participant\", \"Sequence\", \"Period_1_Treatment\", \"Period_1_Pain\"]]\n",
    "        .rename(columns={\"Period_1_Treatment\": \"Treatment\", \"Period_1_Pain\": \"Pain_Score\"})\n",
    "        .assign(Period=1),\n",
    "        df_crossover[[\"Participant\", \"Sequence\", \"Period_2_Treatment\", \"Period_2_Pain\"]]\n",
    "        .rename(columns={\"Period_2_Treatment\": \"Treatment\", \"Period_2_Pain\": \"Pain_Score\"})\n",
    "        .assign(Period=2),\n",
    "    ],\n",
    "    ignore_index=True,\n",
    ")\n",
    "\n",
    "# Analyze\n",
    "pain_A = df_long[df_long[\"Treatment\"] == \"Drug A\"][\"Pain_Score\"]\n",
    "pain_B = df_long[df_long[\"Treatment\"] == \"Drug B\"][\"Pain_Score\"]\n",
    "\n",
    "t_stat, p_value = ttest_ind(pain_A, pain_B)\n",
    "\n",
    "print(f\"\\nüìä Crossover Trial Results:\")\n",
    "print(f\"Mean pain with Drug A: {pain_A.mean():.1f} (SD = {pain_A.std():.1f})\")\n",
    "print(f\"Mean pain with Drug B: {pain_B.mean():.1f} (SD = {pain_B.std():.1f})\")\n",
    "print(f\"Difference: {pain_B.mean() - pain_A.mean():.1f} points\")\n",
    "print(f\"t-test: t = {t_stat:.3f}, p = {p_value:.4f}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(f\"\\n‚úì Drug A is significantly more effective than Drug B (p < .05)\")\n",
    "else:\n",
    "    print(f\"\\n‚úó No significant difference between drugs (p ‚â• .05)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize crossover results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Panel 1: Spaghetti plot (individual trajectories)\n",
    "for seq in df_crossover[\"Sequence\"].unique():\n",
    "    subset = df_crossover[df_crossover[\"Sequence\"] == seq]\n",
    "    color = \"#E63946\" if seq == \"A‚ÜíB\" else \"#06A77D\"\n",
    "\n",
    "    for _, row in subset.iterrows():\n",
    "        axes[0].plot(\n",
    "            [1, 2],\n",
    "            [row[\"Period_1_Pain\"], row[\"Period_2_Pain\"]],\n",
    "            color=color,\n",
    "            alpha=0.3,\n",
    "            linewidth=1,\n",
    "        )\n",
    "\n",
    "# Add sequence means\n",
    "for seq, color, label in [(\"A‚ÜíB\", \"#E63946\", \"Sequence: A‚ÜíB\"), (\"B‚ÜíA\", \"#06A77D\", \"Sequence: B‚ÜíA\")]:\n",
    "    subset = df_crossover[df_crossover[\"Sequence\"] == seq]\n",
    "    means = [subset[\"Period_1_Pain\"].mean(), subset[\"Period_2_Pain\"].mean()]\n",
    "    axes[0].plot(\n",
    "        [1, 2], means, \"o-\", color=color, linewidth=3, markersize=10, label=label, alpha=0.8\n",
    "    )\n",
    "\n",
    "axes[0].set_xlabel(\"Period\", fontsize=12, fontweight=\"bold\")\n",
    "axes[0].set_ylabel(\"Pain Score\", fontsize=12, fontweight=\"bold\")\n",
    "axes[0].set_title(\"Individual Trajectories by Sequence\", fontsize=13, fontweight=\"bold\")\n",
    "axes[0].set_xticks([1, 2])\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Panel 2: Treatment comparison\n",
    "treatment_summary = df_long.groupby(\"Treatment\")[\"Pain_Score\"].agg([\"mean\", \"std\", \"count\"])\n",
    "treatment_summary[\"se\"] = treatment_summary[\"std\"] / np.sqrt(treatment_summary[\"count\"])\n",
    "\n",
    "x_pos = [0, 1]\n",
    "bars = axes[1].bar(\n",
    "    x_pos,\n",
    "    treatment_summary[\"mean\"],\n",
    "    yerr=treatment_summary[\"se\"] * 1.96,  # 95% CI\n",
    "    capsize=10,\n",
    "    color=[\"#E63946\", \"#06A77D\"],\n",
    "    alpha=0.7,\n",
    "    edgecolor=\"black\",\n",
    "    linewidth=1.5,\n",
    ")\n",
    "\n",
    "axes[1].set_ylabel(\"Pain Score (Lower = Better)\", fontsize=12, fontweight=\"bold\")\n",
    "axes[1].set_title(\"Treatment Comparison\\n(Mean ¬± 95% CI)\", fontsize=13, fontweight=\"bold\")\n",
    "axes[1].set_xticks(x_pos)\n",
    "axes[1].set_xticklabels([\"Drug A\", \"Drug B\"])\n",
    "axes[1].grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "# Add significance indicator\n",
    "if p_value < 0.05:\n",
    "    y_max = max(treatment_summary[\"mean\"]) + 5\n",
    "    axes[1].plot([0, 1], [y_max, y_max], \"k-\", linewidth=1.5)\n",
    "    axes[1].text(0.5, y_max + 1, f\"p = {p_value:.3f}\", ha=\"center\", fontsize=10, fontweight=\"bold\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"outputs/module_03/crossover_results.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° The crossover design allowed each participant to try both drugs,\")\n",
    "print(\"   controlling for individual differences in pain sensitivity.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Interrupted Time Series (ITS) Analysis\n",
    "\n",
    "**Interrupted Time Series** designs evaluate interventions by comparing trends before and after an event.\n",
    "\n",
    "### Structure\n",
    "```\n",
    "Observations: ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ|INTERVENTION|‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "              Before (baseline)              After (treatment)\n",
    "```\n",
    "\n",
    "### What ITS Detects\n",
    "1. **Level change**: Immediate jump after intervention\n",
    "2. **Slope change**: Change in trend after intervention\n",
    "\n",
    "### Statistical Model\n",
    "\n",
    "$$Y_t = \\beta_0 + \\beta_1 \\cdot \\text{Time}_t + \\beta_2 \\cdot \\text{Intervention}_t + \\beta_3 \\cdot \\text{Time After}_t + \\varepsilon_t$$\n",
    "\n",
    "Where:\n",
    "- $\\beta_1$ = baseline trend\n",
    "- $\\beta_2$ = level change (immediate effect)\n",
    "- $\\beta_3$ = slope change (sustained effect)\n",
    "\n",
    "### Example: Policy Intervention on Hospital Infections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate hospital infection rates before and after hand hygiene policy\n",
    "\n",
    "np.random.seed(456)\n",
    "\n",
    "# Time points\n",
    "n_before = 24  # 24 months before\n",
    "n_after = 24  # 24 months after\n",
    "n_total = n_before + n_after\n",
    "\n",
    "time = np.arange(1, n_total + 1)\n",
    "intervention = (time > n_before).astype(int)\n",
    "time_after = np.maximum(0, time - n_before)\n",
    "\n",
    "# True parameters\n",
    "beta_0 = 15.0  # Baseline intercept (15 infections per 1000 patient-days)\n",
    "beta_1 = 0.2  # Baseline trend (slowly increasing)\n",
    "beta_2 = -5.0  # Immediate drop after intervention\n",
    "beta_3 = -0.3  # Improved trend after intervention (decreasing)\n",
    "\n",
    "# Generate infection rates\n",
    "infection_rate = (\n",
    "    beta_0\n",
    "    + beta_1 * time\n",
    "    + beta_2 * intervention\n",
    "    + beta_3 * time_after\n",
    "    + np.random.normal(0, 1.5, n_total)\n",
    ")\n",
    "\n",
    "# Create dataframe\n",
    "df_its = pd.DataFrame(\n",
    "    {\n",
    "        \"Month\": time,\n",
    "        \"Intervention\": intervention,\n",
    "        \"Time_After\": time_after,\n",
    "        \"Infection_Rate\": infection_rate,\n",
    "        \"Phase\": [\"Before\" if i == 0 else \"After\" for i in intervention],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Interrupted Time Series Data:\")\n",
    "print(df_its.head(10))\n",
    "print(\"\\n...\\n\")\n",
    "print(df_its.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit ITS regression model\n",
    "from scipy.stats import linregress\n",
    "\n",
    "# Using statsmodels for proper regression\n",
    "try:\n",
    "    import statsmodels.api as sm\n",
    "\n",
    "    # Prepare design matrix\n",
    "    X = df_its[[\"Month\", \"Intervention\", \"Time_After\"]]\n",
    "    X = sm.add_constant(X)\n",
    "    y = df_its[\"Infection_Rate\"]\n",
    "\n",
    "    # Fit model\n",
    "    model = sm.OLS(y, X).fit()\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"INTERRUPTED TIME SERIES REGRESSION RESULTS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(model.summary())\n",
    "\n",
    "    # Extract coefficients\n",
    "    coefs = model.params\n",
    "    pvals = model.pvalues\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"INTERPRETATION\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    print(f\"\\n1. Baseline Trend (Œ≤‚ÇÅ = {coefs['Month']:.3f}, p = {pvals['Month']:.4f}):\")\n",
    "    if pvals[\"Month\"] < 0.05:\n",
    "        direction = \"increasing\" if coefs[\"Month\"] > 0 else \"decreasing\"\n",
    "        print(f\"   Infection rates were significantly {direction} before intervention\")\n",
    "    else:\n",
    "        print(f\"   No significant trend before intervention\")\n",
    "\n",
    "    print(f\"\\n2. Level Change (Œ≤‚ÇÇ = {coefs['Intervention']:.3f}, p = {pvals['Intervention']:.4f}):\")\n",
    "    if pvals[\"Intervention\"] < 0.05:\n",
    "        direction = \"drop\" if coefs[\"Intervention\"] < 0 else \"jump\"\n",
    "        print(\n",
    "            f\"   Immediate {direction} of {abs(coefs['Intervention']):.2f} infections after policy\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"   No immediate change after intervention\")\n",
    "\n",
    "    print(f\"\\n3. Slope Change (Œ≤‚ÇÉ = {coefs['Time_After']:.3f}, p = {pvals['Time_After']:.4f}):\")\n",
    "    if pvals[\"Time_After\"] < 0.05:\n",
    "        direction = \"improved\" if coefs[\"Time_After\"] < 0 else \"worsened\"\n",
    "        print(\n",
    "            f\"   Trend {direction} by {abs(coefs['Time_After']):.3f} infections per month after policy\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"   No change in trend after intervention\")\n",
    "\n",
    "    # Generate predictions\n",
    "    df_its[\"Predicted\"] = model.predict(X)\n",
    "\n",
    "    # Counterfactual: what would have happened without intervention?\n",
    "    X_counterfactual = df_its[[\"Month\", \"Intervention\", \"Time_After\"]].copy()\n",
    "    X_counterfactual[\"Intervention\"] = 0\n",
    "    X_counterfactual[\"Time_After\"] = 0\n",
    "    X_counterfactual = sm.add_constant(X_counterfactual)\n",
    "    df_its[\"Counterfactual\"] = model.predict(X_counterfactual)\n",
    "\n",
    "    statsmodels_available = True\n",
    "\n",
    "except ImportError:\n",
    "    print(\"\\n‚ö† statsmodels not available. Using simplified linear regression.\")\n",
    "    statsmodels_available = False\n",
    "\n",
    "    # Simplified approach using scipy\n",
    "    X_simple = np.column_stack([df_its[\"Month\"], df_its[\"Intervention\"], df_its[\"Time_After\"]])\n",
    "    # Note: This is a simplified version and won't give proper statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ITS results\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "# Plot observed data\n",
    "before_data = df_its[df_its[\"Phase\"] == \"Before\"]\n",
    "after_data = df_its[df_its[\"Phase\"] == \"After\"]\n",
    "\n",
    "ax.scatter(\n",
    "    before_data[\"Month\"],\n",
    "    before_data[\"Infection_Rate\"],\n",
    "    color=\"#E63946\",\n",
    "    s=80,\n",
    "    alpha=0.6,\n",
    "    label=\"Before Intervention\",\n",
    "    zorder=3,\n",
    ")\n",
    "ax.scatter(\n",
    "    after_data[\"Month\"],\n",
    "    after_data[\"Infection_Rate\"],\n",
    "    color=\"#06A77D\",\n",
    "    s=80,\n",
    "    alpha=0.6,\n",
    "    label=\"After Intervention\",\n",
    "    zorder=3,\n",
    ")\n",
    "\n",
    "if statsmodels_available:\n",
    "    # Plot fitted line\n",
    "    ax.plot(\n",
    "        df_its[\"Month\"],\n",
    "        df_its[\"Predicted\"],\n",
    "        color=\"#1D3557\",\n",
    "        linewidth=3,\n",
    "        label=\"Fitted Model\",\n",
    "        zorder=4,\n",
    "    )\n",
    "\n",
    "    # Plot counterfactual (what would have happened without intervention)\n",
    "    ax.plot(\n",
    "        after_data[\"Month\"],\n",
    "        after_data[\"Counterfactual\"],\n",
    "        color=\"gray\",\n",
    "        linewidth=2,\n",
    "        linestyle=\"--\",\n",
    "        label=\"Counterfactual (no intervention)\",\n",
    "        zorder=2,\n",
    "    )\n",
    "\n",
    "# Mark intervention point\n",
    "ax.axvline(\n",
    "    x=n_before, color=\"black\", linestyle=\"-\", linewidth=2, label=\"Intervention Start\", zorder=1\n",
    ")\n",
    "\n",
    "# Shade regions\n",
    "ax.axvspan(0, n_before, alpha=0.1, color=\"red\", zorder=0)\n",
    "ax.axvspan(n_before, n_total, alpha=0.1, color=\"green\", zorder=0)\n",
    "\n",
    "ax.set_xlabel(\"Month\", fontsize=13, fontweight=\"bold\")\n",
    "ax.set_ylabel(\"Hospital Infection Rate\\n(per 1000 patient-days)\", fontsize=13, fontweight=\"bold\")\n",
    "ax.set_title(\n",
    "    \"Interrupted Time Series: Effect of Hand Hygiene Policy on Infection Rates\",\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "ax.legend(loc=\"upper right\", fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"outputs/module_03/its_analysis.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "if statsmodels_available:\n",
    "    # Calculate effect size\n",
    "    effect_at_end = (\n",
    "        df_its[df_its[\"Month\"] == n_total][\"Counterfactual\"].values[0]\n",
    "        - df_its[df_its[\"Month\"] == n_total][\"Predicted\"].values[0]\n",
    "    )\n",
    "\n",
    "    print(f\"\\nüìä Policy Impact Summary:\")\n",
    "    print(f\"\\nImmediate effect: {-coefs['Intervention']:.2f} fewer infections\")\n",
    "    print(f\"Total effect by month {n_total}: {effect_at_end:.2f} fewer infections\")\n",
    "    print(f\"\\nüí° The ITS design provides strong evidence of policy effectiveness\")\n",
    "    print(f\"   by comparing observed trends to counterfactual projections.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strengths and Limitations of ITS\n",
    "\n",
    "**Strengths**:\n",
    "‚úì No control group needed  \n",
    "‚úì Can evaluate population-level interventions  \n",
    "‚úì Controls for pre-existing trends  \n",
    "‚úì Transparent visual analysis  \n",
    "\n",
    "**Limitations**:\n",
    "‚úó Assumes no other changes occurred at intervention time  \n",
    "‚úó Requires sufficient pre- and post-intervention observations (‚â•8 each)  \n",
    "‚úó Vulnerable to autocorrelation (consecutive observations correlated)  \n",
    "‚úó Cannot rule out confounding events  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Regression Discontinuity Design (RDD)\n",
    "\n",
    "**Regression Discontinuity** exploits assignment rules based on a threshold to identify causal effects.\n",
    "\n",
    "### Key Idea\n",
    "When treatment is assigned based on a cutoff:\n",
    "- People just above the threshold receive treatment\n",
    "- People just below the threshold do not\n",
    "- These groups are nearly identical except for treatment\n",
    "\n",
    "### Example: Scholarship on Academic Performance\n",
    "\n",
    "Students with entrance exam scores ‚â•70 receive a scholarship. Does the scholarship improve GPA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate regression discontinuity design\n",
    "\n",
    "np.random.seed(789)\n",
    "n_students = 500\n",
    "\n",
    "# Entrance exam scores (running variable)\n",
    "exam_scores = np.random.normal(70, 10, n_students)\n",
    "\n",
    "# Cutoff for scholarship\n",
    "cutoff = 70\n",
    "scholarship = (exam_scores >= cutoff).astype(int)\n",
    "\n",
    "# True scholarship effect on GPA\n",
    "scholarship_effect = 0.4  # Scholarship increases GPA by 0.4 points\n",
    "\n",
    "# GPA depends on exam score (continuous relationship) + scholarship effect\n",
    "# Baseline: GPA increases with exam performance\n",
    "gpa = (\n",
    "    2.0\n",
    "    + 0.02 * exam_scores\n",
    "    + scholarship_effect * scholarship\n",
    "    + np.random.normal(0, 0.3, n_students)\n",
    ")\n",
    "\n",
    "# Ensure GPA is in valid range [0, 4]\n",
    "gpa = np.clip(gpa, 0, 4)\n",
    "\n",
    "# Create dataframe\n",
    "df_rdd = pd.DataFrame(\n",
    "    {\n",
    "        \"Exam_Score\": exam_scores,\n",
    "        \"Scholarship\": scholarship,\n",
    "        \"GPA\": gpa,\n",
    "        \"Distance_from_Cutoff\": exam_scores - cutoff,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Regression Discontinuity Data:\")\n",
    "print(df_rdd.head(10))\n",
    "\n",
    "# Analyze effect near the cutoff\n",
    "bandwidth = 5  # Look at students within ¬±5 points of cutoff\n",
    "df_near_cutoff = df_rdd[np.abs(df_rdd[\"Distance_from_Cutoff\"]) <= bandwidth]\n",
    "\n",
    "gpa_just_above = df_near_cutoff[df_near_cutoff[\"Scholarship\"] == 1][\"GPA\"]\n",
    "gpa_just_below = df_near_cutoff[df_near_cutoff[\"Scholarship\"] == 0][\"GPA\"]\n",
    "\n",
    "t_stat, p_value = ttest_ind(gpa_just_above, gpa_just_below)\n",
    "\n",
    "print(f\"\\nüìä RDD Analysis (within {bandwidth} points of cutoff):\")\n",
    "print(f\"\\nStudents just above cutoff (scholarship): Mean GPA = {gpa_just_above.mean():.3f}\")\n",
    "print(f\"Students just below cutoff (no scholarship): Mean GPA = {gpa_just_below.mean():.3f}\")\n",
    "print(\n",
    "    f\"Estimated scholarship effect: {gpa_just_above.mean() - gpa_just_below.mean():.3f} GPA points\"\n",
    ")\n",
    "print(f\"\\nt-test: t = {t_stat:.3f}, p = {p_value:.4f}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(f\"\\n‚úì Scholarship has a significant positive effect on GPA\")\n",
    "else:\n",
    "    print(f\"\\n‚úó No significant scholarship effect detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize regression discontinuity\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Panel 1: Full scatter plot\n",
    "colors = [\"#E63946\" if s == 0 else \"#06A77D\" for s in df_rdd[\"Scholarship\"]]\n",
    "axes[0].scatter(df_rdd[\"Exam_Score\"], df_rdd[\"GPA\"], c=colors, alpha=0.4, s=50, edgecolors=\"none\")\n",
    "\n",
    "# Fit separate regression lines on each side\n",
    "below_cutoff = df_rdd[df_rdd[\"Exam_Score\"] < cutoff]\n",
    "above_cutoff = df_rdd[df_rdd[\"Exam_Score\"] >= cutoff]\n",
    "\n",
    "if len(below_cutoff) > 0:\n",
    "    slope_below, intercept_below, _, _, _ = linregress(\n",
    "        below_cutoff[\"Exam_Score\"], below_cutoff[\"GPA\"]\n",
    "    )\n",
    "    x_below = np.linspace(below_cutoff[\"Exam_Score\"].min(), cutoff, 100)\n",
    "    y_below = slope_below * x_below + intercept_below\n",
    "    axes[0].plot(x_below, y_below, color=\"#E63946\", linewidth=3, label=\"No Scholarship\")\n",
    "\n",
    "if len(above_cutoff) > 0:\n",
    "    slope_above, intercept_above, _, _, _ = linregress(\n",
    "        above_cutoff[\"Exam_Score\"], above_cutoff[\"GPA\"]\n",
    "    )\n",
    "    x_above = np.linspace(cutoff, above_cutoff[\"Exam_Score\"].max(), 100)\n",
    "    y_above = slope_above * x_above + intercept_above\n",
    "    axes[0].plot(x_above, y_above, color=\"#06A77D\", linewidth=3, label=\"Scholarship\")\n",
    "\n",
    "# Mark discontinuity\n",
    "axes[0].axvline(x=cutoff, color=\"black\", linestyle=\"--\", linewidth=2, label=\"Cutoff (70)\")\n",
    "\n",
    "axes[0].set_xlabel(\"Entrance Exam Score\", fontsize=12, fontweight=\"bold\")\n",
    "axes[0].set_ylabel(\"College GPA\", fontsize=12, fontweight=\"bold\")\n",
    "axes[0].set_title(\"Regression Discontinuity Design\", fontsize=13, fontweight=\"bold\")\n",
    "axes[0].legend(loc=\"lower right\")\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Panel 2: Zoomed in near cutoff\n",
    "zoom_range = 10\n",
    "df_zoom = df_rdd[\n",
    "    (df_rdd[\"Exam_Score\"] >= cutoff - zoom_range) & (df_rdd[\"Exam_Score\"] <= cutoff + zoom_range)\n",
    "]\n",
    "\n",
    "colors_zoom = [\"#E63946\" if s == 0 else \"#06A77D\" for s in df_zoom[\"Scholarship\"]]\n",
    "axes[1].scatter(\n",
    "    df_zoom[\"Exam_Score\"],\n",
    "    df_zoom[\"GPA\"],\n",
    "    c=colors_zoom,\n",
    "    alpha=0.6,\n",
    "    s=80,\n",
    "    edgecolors=\"black\",\n",
    "    linewidths=0.5,\n",
    ")\n",
    "\n",
    "# Fit lines in zoom window\n",
    "below_zoom = df_zoom[df_zoom[\"Exam_Score\"] < cutoff]\n",
    "above_zoom = df_zoom[df_zoom[\"Exam_Score\"] >= cutoff]\n",
    "\n",
    "if len(below_zoom) > 0:\n",
    "    slope_bz, intercept_bz, _, _, _ = linregress(below_zoom[\"Exam_Score\"], below_zoom[\"GPA\"])\n",
    "    x_bz = np.linspace(cutoff - zoom_range, cutoff, 50)\n",
    "    y_bz = slope_bz * x_bz + intercept_bz\n",
    "    axes[1].plot(x_bz, y_bz, color=\"#E63946\", linewidth=3)\n",
    "\n",
    "if len(above_zoom) > 0:\n",
    "    slope_az, intercept_az, _, _, _ = linregress(above_zoom[\"Exam_Score\"], above_zoom[\"GPA\"])\n",
    "    x_az = np.linspace(cutoff, cutoff + zoom_range, 50)\n",
    "    y_az = slope_az * x_az + intercept_az\n",
    "    axes[1].plot(x_az, y_az, color=\"#06A77D\", linewidth=3)\n",
    "\n",
    "# Mark discontinuity and effect size\n",
    "axes[1].axvline(x=cutoff, color=\"black\", linestyle=\"--\", linewidth=2)\n",
    "\n",
    "# Calculate discontinuity size at cutoff\n",
    "if len(below_zoom) > 0 and len(above_zoom) > 0:\n",
    "    y_below_at_cutoff = slope_bz * cutoff + intercept_bz\n",
    "    y_above_at_cutoff = slope_az * cutoff + intercept_az\n",
    "    discontinuity = y_above_at_cutoff - y_below_at_cutoff\n",
    "\n",
    "    # Draw arrow showing discontinuity\n",
    "    axes[1].annotate(\n",
    "        \"\",\n",
    "        xy=(cutoff + 0.5, y_above_at_cutoff),\n",
    "        xytext=(cutoff + 0.5, y_below_at_cutoff),\n",
    "        arrowprops=dict(arrowstyle=\"<->\", color=\"red\", lw=2),\n",
    "    )\n",
    "    axes[1].text(\n",
    "        cutoff + 1,\n",
    "        (y_above_at_cutoff + y_below_at_cutoff) / 2,\n",
    "        f\"Effect:\\n{discontinuity:.3f}\",\n",
    "        fontsize=10,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "axes[1].set_xlabel(\"Entrance Exam Score\", fontsize=12, fontweight=\"bold\")\n",
    "axes[1].set_ylabel(\"College GPA\", fontsize=12, fontweight=\"bold\")\n",
    "axes[1].set_title(f\"Zoomed: ¬±{zoom_range} Points from Cutoff\", fontsize=13, fontweight=\"bold\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"outputs/module_03/rdd_analysis.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° The discontinuity at the cutoff reveals the causal effect of the scholarship.\")\n",
    "print(\"   Students just above and below the cutoff are nearly identical, except for treatment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD Assumptions and Validity\n",
    "\n",
    "**Key Assumption**: Units cannot precisely manipulate their running variable to get treatment.\n",
    "\n",
    "**Validity Checks**:\n",
    "1. **McCrary density test**: Check for suspicious bunching at cutoff\n",
    "2. **Covariate balance**: Pre-treatment variables should be continuous at cutoff\n",
    "3. **Placebo cutoffs**: No discontinuity at arbitrary thresholds\n",
    "4. **Bandwidth sensitivity**: Results should be stable across bandwidths\n",
    "\n",
    "**Strengths**:\n",
    "‚úì Provides credible causal estimates  \n",
    "‚úì Transparent and intuitive  \n",
    "‚úì No need for randomization  \n",
    "\n",
    "**Limitations**:\n",
    "‚úó Only applies at the cutoff (limited external validity)  \n",
    "‚úó Requires large samples near cutoff  \n",
    "‚úó Vulnerable to manipulation  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Natural Experiments\n",
    "\n",
    "**Natural experiments** occur when external events create quasi-random treatment assignment.\n",
    "\n",
    "### Classic Examples\n",
    "\n",
    "1. **Vietnam Draft Lottery** (Angrist, 1990)\n",
    "   - Random assignment: Birthdate lottery ‚Üí Draft status\n",
    "   - Outcome: Lifetime earnings\n",
    "   - Finding: Military service reduced earnings\n",
    "\n",
    "2. **London Cholera Outbreak** (Snow, 1854)\n",
    "   - Natural variation: Two water companies served overlapping areas\n",
    "   - One company drew water upstream (clean), other downstream (contaminated)\n",
    "   - Outcome: Cholera deaths\n",
    "   - Finding: Contaminated water caused cholera\n",
    "\n",
    "3. **Minimum Wage Increases** (Card & Krueger, 1994)\n",
    "   - Policy change: New Jersey raised minimum wage, Pennsylvania did not\n",
    "   - Outcome: Fast food employment\n",
    "   - Finding: No decrease in employment\n",
    "\n",
    "### When to Use Natural Experiments\n",
    "- When randomization is impossible or unethical\n",
    "- When policy changes create comparison groups\n",
    "- When geographic or temporal variation exists\n",
    "\n",
    "### Example: School Closing Policy (Simulated)\n",
    "\n",
    "Two neighboring districts: One closes schools during flu season, the other doesn't.  \n",
    "**Question**: Does school closure reduce flu transmission?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate natural experiment: school closure effect on flu rates\n",
    "\n",
    "np.random.seed(101)\n",
    "n_weeks = 20\n",
    "\n",
    "# Week numbers\n",
    "weeks = np.arange(1, n_weeks + 1)\n",
    "\n",
    "# Closure happens in week 11\n",
    "closure_week = 11\n",
    "\n",
    "# District A: Implements school closure\n",
    "# Flu rates rise naturally, then drop after closure\n",
    "district_A_before = 50 + 5 * np.arange(1, closure_week) + np.random.normal(0, 5, closure_week - 1)\n",
    "district_A_after = (\n",
    "    70\n",
    "    - 3 * np.arange(1, n_weeks - closure_week + 2)\n",
    "    + np.random.normal(0, 5, n_weeks - closure_week + 1)\n",
    ")\n",
    "district_A_flu = np.concatenate([district_A_before, district_A_after])\n",
    "\n",
    "# District B: No school closure (control)\n",
    "# Flu rates continue rising\n",
    "district_B_flu = 50 + 5 * np.arange(1, n_weeks + 1) + np.random.normal(0, 5, n_weeks)\n",
    "\n",
    "# Create dataframe\n",
    "df_natural = pd.DataFrame(\n",
    "    {\n",
    "        \"Week\": np.tile(weeks, 2),\n",
    "        \"District\": np.repeat([\"A (Closure)\", \"B (Control)\"], n_weeks),\n",
    "        \"Flu_Rate\": np.concatenate([district_A_flu, district_B_flu]),\n",
    "        \"Intervention\": np.tile(weeks >= closure_week, 2),\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Natural Experiment Data:\")\n",
    "print(df_natural.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize natural experiment\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "for district, color, marker in [(\"A (Closure)\", \"#06A77D\", \"o\"), (\"B (Control)\", \"#E63946\", \"s\")]:\n",
    "    subset = df_natural[df_natural[\"District\"] == district]\n",
    "    ax.plot(\n",
    "        subset[\"Week\"],\n",
    "        subset[\"Flu_Rate\"],\n",
    "        marker=marker,\n",
    "        linestyle=\"-\",\n",
    "        linewidth=2,\n",
    "        markersize=8,\n",
    "        color=color,\n",
    "        label=district,\n",
    "        alpha=0.8,\n",
    "    )\n",
    "\n",
    "# Mark intervention\n",
    "ax.axvline(\n",
    "    x=closure_week,\n",
    "    color=\"black\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=\"School Closure (District A only)\",\n",
    ")\n",
    "ax.axvspan(0, closure_week, alpha=0.05, color=\"gray\")\n",
    "ax.axvspan(closure_week, n_weeks, alpha=0.1, color=\"green\")\n",
    "\n",
    "ax.set_xlabel(\"Week\", fontsize=13, fontweight=\"bold\")\n",
    "ax.set_ylabel(\"Flu Rate (cases per 10,000)\", fontsize=13, fontweight=\"bold\")\n",
    "ax.set_title(\n",
    "    \"Natural Experiment: School Closure Effect on Flu Transmission\", fontsize=14, fontweight=\"bold\"\n",
    ")\n",
    "ax.legend(loc=\"upper left\", fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"outputs/module_03/natural_experiment.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Difference-in-differences analysis\n",
    "A_before = df_natural[\n",
    "    (df_natural[\"District\"] == \"A (Closure)\") & (df_natural[\"Week\"] < closure_week)\n",
    "][\"Flu_Rate\"].mean()\n",
    "A_after = df_natural[\n",
    "    (df_natural[\"District\"] == \"A (Closure)\") & (df_natural[\"Week\"] >= closure_week)\n",
    "][\"Flu_Rate\"].mean()\n",
    "B_before = df_natural[\n",
    "    (df_natural[\"District\"] == \"B (Control)\") & (df_natural[\"Week\"] < closure_week)\n",
    "][\"Flu_Rate\"].mean()\n",
    "B_after = df_natural[\n",
    "    (df_natural[\"District\"] == \"B (Control)\") & (df_natural[\"Week\"] >= closure_week)\n",
    "][\"Flu_Rate\"].mean()\n",
    "\n",
    "# Difference-in-differences estimate\n",
    "did_estimate = (A_after - A_before) - (B_after - B_before)\n",
    "\n",
    "print(f\"\\nüìä Difference-in-Differences Analysis:\")\n",
    "print(f\"\\nDistrict A (Closure):\")\n",
    "print(f\"  Before: {A_before:.2f} cases\")\n",
    "print(f\"  After:  {A_after:.2f} cases\")\n",
    "print(f\"  Change: {A_after - A_before:.2f} cases\")\n",
    "\n",
    "print(f\"\\nDistrict B (Control):\")\n",
    "print(f\"  Before: {B_before:.2f} cases\")\n",
    "print(f\"  After:  {B_after:.2f} cases\")\n",
    "print(f\"  Change: {B_after - B_before:.2f} cases\")\n",
    "\n",
    "print(f\"\\nüìê Difference-in-Differences Estimate: {did_estimate:.2f} cases\")\n",
    "print(f\"\\nüí° School closure reduced flu transmission by {abs(did_estimate):.2f} cases per 10,000,\")\n",
    "print(f\"   after accounting for the natural trend (control district).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Sensitivity Analysis\n",
    "\n",
    "**Sensitivity analysis** tests how robust your findings are to:\n",
    "1. Different analytical choices\n",
    "2. Violations of assumptions\n",
    "3. Unmeasured confounding\n",
    "\n",
    "### Types of Sensitivity Analysis\n",
    "\n",
    "#### 1. Model Specification\n",
    "- Try different functional forms (linear, quadratic, log)\n",
    "- Include/exclude covariates\n",
    "- Use different estimation methods\n",
    "\n",
    "#### 2. Sample Restrictions\n",
    "- Drop outliers\n",
    "- Restrict to subgroups\n",
    "- Vary bandwidth (in RDD)\n",
    "\n",
    "#### 3. Unmeasured Confounding\n",
    "- Calculate how strong a confounder would need to be to eliminate effect\n",
    "- E-value: Minimum strength of confounding to explain away result\n",
    "\n",
    "### Example: Testing Robustness of Treatment Effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensitivity analysis: How robust is our finding?\n",
    "\n",
    "# Simulate treatment effect study\n",
    "np.random.seed(202)\n",
    "n = 200\n",
    "\n",
    "# Treatment assignment (e.g., training program)\n",
    "treatment = np.random.binomial(1, 0.5, n)\n",
    "\n",
    "# Outcome (e.g., test score)\n",
    "true_effect = 10  # Training increases scores by 10 points\n",
    "outcome = 50 + true_effect * treatment + np.random.normal(0, 15, n)\n",
    "\n",
    "# Observed effect\n",
    "treated_scores = outcome[treatment == 1]\n",
    "control_scores = outcome[treatment == 0]\n",
    "observed_effect = treated_scores.mean() - control_scores.mean()\n",
    "\n",
    "print(f\"Observed treatment effect: {observed_effect:.2f} points\")\n",
    "\n",
    "# Sensitivity Analysis 1: Effect of unmeasured confounder\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SENSITIVITY TO UNMEASURED CONFOUNDING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nHow strong would a confounder need to be to eliminate the effect?\\n\")\n",
    "\n",
    "# Simulate different confounding scenarios\n",
    "confounder_strengths = np.arange(0, 0.6, 0.1)\n",
    "adjusted_effects = []\n",
    "\n",
    "for strength in confounder_strengths:\n",
    "    # Confounder affects both treatment and outcome\n",
    "    confounder = np.random.normal(0, 1, n)\n",
    "\n",
    "    # Treatment probability depends on confounder\n",
    "    treatment_conf = np.random.binomial(1, 0.5 + strength * (confounder / confounder.std()), n)\n",
    "\n",
    "    # Outcome depends on treatment and confounder\n",
    "    outcome_conf = (\n",
    "        50 + true_effect * treatment + 20 * strength * confounder + np.random.normal(0, 15, n)\n",
    "    )\n",
    "\n",
    "    # Naive estimate (ignoring confounder)\n",
    "    treated_conf = outcome_conf[treatment_conf == 1]\n",
    "    control_conf = outcome_conf[treatment_conf == 0]\n",
    "\n",
    "    if len(treated_conf) > 0 and len(control_conf) > 0:\n",
    "        biased_effect = treated_conf.mean() - control_conf.mean()\n",
    "        adjusted_effects.append(biased_effect)\n",
    "    else:\n",
    "        adjusted_effects.append(np.nan)\n",
    "\n",
    "# Create sensitivity table\n",
    "sensitivity_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Confounder_Strength\": confounder_strengths,\n",
    "        \"Biased_Effect_Estimate\": adjusted_effects,\n",
    "        \"Percent_Bias\": [\n",
    "            (e - observed_effect) / observed_effect * 100 if not np.isnan(e) else np.nan\n",
    "            for e in adjusted_effects\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(sensitivity_df.to_string(index=False))\n",
    "print(\"\\nüí° This shows how effect estimates change under different confounding scenarios.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sensitivity analysis\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.plot(\n",
    "    confounder_strengths,\n",
    "    adjusted_effects,\n",
    "    \"o-\",\n",
    "    linewidth=3,\n",
    "    markersize=10,\n",
    "    color=\"#E63946\",\n",
    "    label=\"Biased Estimate\",\n",
    ")\n",
    "ax.axhline(\n",
    "    y=observed_effect,\n",
    "    color=\"#06A77D\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=f\"Original Estimate ({observed_effect:.2f})\",\n",
    ")\n",
    "ax.axhline(y=0, color=\"gray\", linestyle=\":\", linewidth=1.5, label=\"No Effect\")\n",
    "\n",
    "ax.fill_between(confounder_strengths, 0, adjusted_effects, alpha=0.2, color=\"#E63946\")\n",
    "\n",
    "ax.set_xlabel(\"Strength of Unmeasured Confounder\", fontsize=13, fontweight=\"bold\")\n",
    "ax.set_ylabel(\"Estimated Treatment Effect\", fontsize=13, fontweight=\"bold\")\n",
    "ax.set_title(\n",
    "    \"Sensitivity Analysis: Effect of Unmeasured Confounding\", fontsize=14, fontweight=\"bold\"\n",
    ")\n",
    "ax.legend(loc=\"upper right\", fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"outputs/module_03/sensitivity_analysis.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Interpretation:\")\n",
    "print(\"   As unmeasured confounding increases, the treatment effect estimate shrinks.\")\n",
    "print(\"   A moderately strong confounder (‚â•0.3) could eliminate the observed effect.\")\n",
    "print(\"\\nüí° This analysis helps researchers understand the robustness of their findings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Practice Exercises\n",
    "\n",
    "Apply what you've learned to solidify your understanding.\n",
    "\n",
    "### Exercise 1: Design Selection\n",
    "\n",
    "For each scenario, select the most appropriate design:\n",
    "\n",
    "1. **Scenario**: You want to test if a new teaching method improves student performance, but you can only recruit 30 students.\n",
    "   - **Answer**: ____________\n",
    "\n",
    "2. **Scenario**: A state implements a seatbelt law in 2020. You have monthly traffic fatality data from 2015-2023.\n",
    "   - **Answer**: ____________\n",
    "\n",
    "3. **Scenario**: College admission is based on entrance exam scores. Students scoring ‚â•80 are admitted.\n",
    "   - **Answer**: ____________\n",
    "\n",
    "4. **Scenario**: Two medications for hypertension, each taken for 4 weeks. You want to compare them.\n",
    "   - **Answer**: ____________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Power Calculation\n",
    "# Calculate required sample size for a within-subjects design\n",
    "# Target: 85% power, alpha = 0.05, expected effect size d = 0.4\n",
    "\n",
    "# YOUR CODE HERE\n",
    "target_power = 0.85\n",
    "alpha = 0.05\n",
    "effect_size = 0.4\n",
    "\n",
    "# Hint: Use the simulation function from earlier\n",
    "# Test different sample sizes until you reach target power\n",
    "\n",
    "# Example starter:\n",
    "# for n in range(20, 100, 5):\n",
    "#     power = simulate_experiment(n, effect_size, 'within', 500)\n",
    "#     if power >= target_power:\n",
    "#         print(f\"Required sample size: {n}\")\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: ITS Analysis\n",
    "# You have monthly crime data before and after a policing intervention\n",
    "# Analyze whether the intervention reduced crime\n",
    "\n",
    "# Generate data\n",
    "np.random.seed(999)\n",
    "months = np.arange(1, 37)  # 36 months\n",
    "intervention_month = 19\n",
    "\n",
    "# Crime rate decreases after intervention (month 19)\n",
    "crime_before = (\n",
    "    100 + 2 * np.arange(1, intervention_month) + np.random.normal(0, 5, intervention_month - 1)\n",
    ")\n",
    "crime_after = (\n",
    "    130\n",
    "    - 3 * np.arange(1, len(months) - intervention_month + 2)\n",
    "    + np.random.normal(0, 5, len(months) - intervention_month + 1)\n",
    ")\n",
    "crime_rate = np.concatenate([crime_before, crime_after])\n",
    "\n",
    "# YOUR TASK:\n",
    "# 1. Create appropriate variables (time, intervention dummy, time_after)\n",
    "# 2. Fit regression model\n",
    "# 3. Interpret coefficients\n",
    "# 4. Visualize results\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Key Takeaways\n",
    "\n",
    "### Design Comparison Matrix\n",
    "\n",
    "| Design | Causal Strength | Required Sample | Key Assumption | Best Use Case |\n",
    "|--------|----------------|-----------------|----------------|---------------|\n",
    "| **RCT (Between)** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Large | Random assignment | Gold standard when feasible |\n",
    "| **RCT (Within)** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Small-Medium | No carryover | Limited samples, reversible treatments |\n",
    "| **Crossover** | ‚≠ê‚≠ê‚≠ê‚≠ê | Small | Adequate washout | Chronic conditions, medications |\n",
    "| **ITS** | ‚≠ê‚≠ê‚≠ê | Time series data | No concurrent events | Policy evaluation |\n",
    "| **RDD** | ‚≠ê‚≠ê‚≠ê‚≠ê | Large (near cutoff) | No manipulation | Threshold-based assignment |\n",
    "| **Natural Experiment** | ‚≠ê‚≠ê‚≠ê | Varies | Exogenous variation | When randomization impossible |\n",
    "\n",
    "### Decision Framework\n",
    "\n",
    "```\n",
    "Can you randomize? ‚îÄ‚îÄYES‚îÄ‚îÄ> RCT\n",
    "        ‚îÇ                     ‚îÇ\n",
    "       NO                     ‚îú‚îÄ Large sample? ‚îÄ‚îÄ> Between-subjects\n",
    "        ‚îÇ                     ‚îî‚îÄ Small sample? ‚îÄ‚îÄ> Within-subjects/Crossover\n",
    "        ‚îÇ\n",
    "        ‚îú‚îÄ Is there a cutoff/threshold? ‚îÄ‚îÄYES‚îÄ‚îÄ> Regression Discontinuity\n",
    "        ‚îÇ\n",
    "        ‚îú‚îÄ Is there time series data? ‚îÄ‚îÄYES‚îÄ‚îÄ> Interrupted Time Series\n",
    "        ‚îÇ\n",
    "        ‚îú‚îÄ Is there natural variation? ‚îÄ‚îÄYES‚îÄ‚îÄ> Natural Experiment\n",
    "        ‚îÇ\n",
    "        ‚îî‚îÄ Otherwise ‚îÄ‚îÄ> Observational + Causal Inference Methods\n",
    "```\n",
    "\n",
    "### Critical Reminders\n",
    "\n",
    "1. **No design is perfect**: Every design has trade-offs between internal validity, external validity, and feasibility.\n",
    "\n",
    "2. **Transparency is key**: Clearly document your design choices, assumptions, and limitations.\n",
    "\n",
    "3. **Always check assumptions**: Violation of key assumptions can invalidate causal claims.\n",
    "\n",
    "4. **Conduct sensitivity analyses**: Test how robust your findings are to different specifications.\n",
    "\n",
    "5. **Match design to question**: Let your research question guide design selection, not convenience.\n",
    "\n",
    "6. **Power matters**: Underpowered studies waste resources and produce unreliable results.\n",
    "\n",
    "### Moving Forward\n",
    "\n",
    "You now have a sophisticated toolkit for causal inference when perfect experiments aren't possible. The next module will cover **survey design and measurement**, essential skills for collecting high-quality data in any research design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Additional Resources\n",
    "\n",
    "### Essential Readings\n",
    "\n",
    "1. **Shadish, Cook, & Campbell (2002)**. *Experimental and Quasi-Experimental Designs for Generalized Causal Inference*\n",
    "   - The definitive guide to experimental design\n",
    "\n",
    "2. **Angrist & Pischke (2009)**. *Mostly Harmless Econometrics*\n",
    "   - Practical guide to causal inference in observational studies\n",
    "\n",
    "3. **Bernal, Cummins, & Gasparrini (2017)**. \"Interrupted time series regression for the evaluation of public health interventions\"\n",
    "   - Modern ITS methods\n",
    "\n",
    "4. **Lee & Lemieux (2010)**. \"Regression Discontinuity Designs in Economics\"\n",
    "   - Comprehensive RDD tutorial\n",
    "\n",
    "### Online Tools\n",
    "\n",
    "- **Power calculators**: G*Power (free software)\n",
    "- **RDD visualization**: rdrobust package (R/Stata)\n",
    "- **ITS analysis**: itsa package (Stata), nlme (R)\n",
    "\n",
    "### Practice Datasets\n",
    "\n",
    "- **NHANES**: National Health and Nutrition Examination Survey\n",
    "- **IPUMS**: Census and survey data\n",
    "- **Dataverse**: Research data repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a design selection checklist\n",
    "checklist = pd.DataFrame(\n",
    "    {\n",
    "        \"Question\": [\n",
    "            \"1. Can you randomize participants to conditions?\",\n",
    "            \"2. Are there carryover or practice effects?\",\n",
    "            \"3. Is the treatment reversible?\",\n",
    "            \"4. Is there a clear assignment cutoff/threshold?\",\n",
    "            \"5. Do you have time series data pre/post intervention?\",\n",
    "            \"6. Are there natural sources of variation?\",\n",
    "            \"7. What is your sample size?\",\n",
    "            \"8. What are ethical constraints?\",\n",
    "            \"9. What is your budget?\",\n",
    "            \"10. What is the expected effect size?\",\n",
    "        ],\n",
    "        \"Implication\": [\n",
    "            \"YES ‚Üí RCT possible | NO ‚Üí Quasi-experimental\",\n",
    "            \"YES ‚Üí Between-subjects | NO ‚Üí Within-subjects ok\",\n",
    "            \"YES ‚Üí Crossover possible | NO ‚Üí Between-subjects\",\n",
    "            \"YES ‚Üí Consider RDD | NO ‚Üí Other designs\",\n",
    "            \"YES ‚Üí Consider ITS | NO ‚Üí Other designs\",\n",
    "            \"YES ‚Üí Consider natural experiment | NO ‚Üí RCT needed\",\n",
    "            \"Small ‚Üí Within-subjects | Large ‚Üí Between-subjects\",\n",
    "            \"Restricts randomization options\",\n",
    "            \"Low ‚Üí Favor within-subjects or quasi-experimental\",\n",
    "            \"Small ‚Üí Need larger N for adequate power\",\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "checklist.to_csv(\"outputs/module_03/design_selection_checklist.csv\", index=False)\n",
    "print(\"‚úì Design selection checklist saved to outputs/module_03/\")\n",
    "print(\"\\n\" + checklist.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Congratulations!\n",
    "\n",
    "You've completed **Module 03: Advanced Experimental Designs**. You can now:\n",
    "\n",
    "‚úì Select appropriate experimental designs for different research contexts  \n",
    "‚úì Understand power advantages of within-subjects designs  \n",
    "‚úì Implement and analyze crossover trials  \n",
    "‚úì Conduct interrupted time series analyses  \n",
    "‚úì Apply regression discontinuity designs  \n",
    "‚úì Identify and leverage natural experiments  \n",
    "‚úì Perform sensitivity analyses to test robustness  \n",
    "\n",
    "**Next Module**: Survey Design & Measurement  \n",
    "**File**: `04_survey_design_measurement.ipynb`\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
