{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 10: Advanced Reproducibility & Containerization\n",
    "\n",
    "**Estimated Time:** 50 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this module, you will be able to:\n",
    "\n",
    "1. Explain computational reproducibility and its importance\n",
    "2. Create Docker containers for reproducible research environments\n",
    "3. Use version control for data and models (DVC, Git-LFS)\n",
    "4. Manage dependencies with conda and pip\n",
    "5. Automate workflows with Make and Snakemake\n",
    "6. Implement reproducible random number generation\n",
    "7. Document computational environments thoroughly\n",
    "8. Create a fully reproducible research project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import platform\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "plt.rcParams[\"font.size\"] = 11\n",
    "\n",
    "# Create output directory\n",
    "import os\n",
    "\n",
    "os.makedirs(\"../notebooks/outputs/module_10\", exist_ok=True)\n",
    "\n",
    "print(\"‚úì Libraries imported successfully\")\n",
    "print(\"‚úì Output directory created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is Computational Reproducibility?\n",
    "\n",
    "**Computational Reproducibility**: The ability for independent researchers to recreate the same results using the same code and data.\n",
    "\n",
    "### The Reproducibility Spectrum\n",
    "\n",
    "```\n",
    "Low                                                                High\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                                  ‚îÇ\n",
    "No code        Code          Code +         Code +        Containerized\n",
    "shared         shared        dependencies   automation    + automated\n",
    "                             documented     (workflows)   testing\n",
    "‚îÇ              ‚îÇ             ‚îÇ              ‚îÇ             ‚îÇ\n",
    "Not            Minimally     Reproducible   Highly        Fully\n",
    "reproducible   reproducible  (in principle) reproducible  reproducible\n",
    "```\n",
    "\n",
    "### Common Barriers to Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Survey data on reproducibility barriers (simulated from literature)\n",
    "barriers_data = {\n",
    "    \"Barrier\": [\n",
    "        \"Different software\\nversions\",\n",
    "        \"Missing\\ndependencies\",\n",
    "        \"Undocumented\\nsteps\",\n",
    "        \"Hardware\\ndifferences\",\n",
    "        \"Random seed\\nnot set\",\n",
    "        \"Data not\\navailable\",\n",
    "        \"Code not\\nshared\",\n",
    "        \"OS differences\",\n",
    "    ],\n",
    "    \"Frequency\": [78, 72, 68, 45, 42, 38, 35, 32],  # % of failures\n",
    "    \"Solution\": [\n",
    "        \"Lockfiles\",\n",
    "        \"Conda/Docker\",\n",
    "        \"Workflow tools\",\n",
    "        \"Containers\",\n",
    "        \"Set seeds\",\n",
    "        \"Share data\",\n",
    "        \"Share code\",\n",
    "        \"Containers\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "barriers_df = pd.DataFrame(barriers_data)\n",
    "barriers_df = barriers_df.sort_values(\"Frequency\", ascending=True)\n",
    "\n",
    "# Create visualization\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Color by solution type\n",
    "solution_colors = {\n",
    "    \"Lockfiles\": \"#3498db\",\n",
    "    \"Conda/Docker\": \"#e74c3c\",\n",
    "    \"Workflow tools\": \"#2ecc71\",\n",
    "    \"Containers\": \"#e74c3c\",\n",
    "    \"Set seeds\": \"#f39c12\",\n",
    "    \"Share data\": \"#9b59b6\",\n",
    "    \"Share code\": \"#9b59b6\",\n",
    "}\n",
    "\n",
    "colors = [solution_colors[sol] for sol in barriers_df[\"Solution\"]]\n",
    "\n",
    "bars = ax.barh(\n",
    "    barriers_df[\"Barrier\"],\n",
    "    barriers_df[\"Frequency\"],\n",
    "    color=colors,\n",
    "    edgecolor=\"black\",\n",
    "    linewidth=1.5,\n",
    "    alpha=0.8,\n",
    ")\n",
    "\n",
    "# Add value labels\n",
    "for i, (barrier, freq) in enumerate(zip(barriers_df[\"Barrier\"], barriers_df[\"Frequency\"])):\n",
    "    ax.text(freq + 2, i, f\"{freq}%\", va=\"center\", fontsize=11, fontweight=\"bold\")\n",
    "\n",
    "ax.set_xlabel(\"Percentage of Reproducibility Failures\", fontsize=12, fontweight=\"bold\")\n",
    "ax.set_title(\n",
    "    \"Common Barriers to Computational Reproducibility\", fontsize=14, fontweight=\"bold\", pad=20\n",
    ")\n",
    "ax.set_xlim([0, 90])\n",
    "ax.grid(axis=\"x\", alpha=0.3, linestyle=\"--\")\n",
    "\n",
    "# Add legend\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "legend_elements = [\n",
    "    Patch(facecolor=\"#e74c3c\", edgecolor=\"black\", label=\"Containerization\"),\n",
    "    Patch(facecolor=\"#3498db\", edgecolor=\"black\", label=\"Dependency locking\"),\n",
    "    Patch(facecolor=\"#2ecc71\", edgecolor=\"black\", label=\"Workflow automation\"),\n",
    "    Patch(facecolor=\"#f39c12\", edgecolor=\"black\", label=\"Random seeds\"),\n",
    "    Patch(facecolor=\"#9b59b6\", edgecolor=\"black\", label=\"Open science\"),\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc=\"lower right\", fontsize=10, title=\"Solution Type\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    \"../notebooks/outputs/module_10/reproducibility_barriers.png\", dpi=300, bbox_inches=\"tight\"\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Barriers visualization saved\")\n",
    "print(\"\\nüìä Top 3 Barriers:\")\n",
    "for i, row in barriers_df.tail(3).iterrows():\n",
    "    print(\n",
    "        f\"   {row['Barrier'].replace(chr(10), ' ')}: {row['Frequency']}% (solution: {row['Solution']})\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Docker and Containerization\n",
    "\n",
    "### What are Containers?\n",
    "\n",
    "**Containers** package your code, dependencies, and operating system into a single, portable unit.\n",
    "\n",
    "**Benefits:**\n",
    "- ‚úì \"Works on my machine\" ‚Üí \"Works on ANY machine\"\n",
    "- ‚úì Isolates dependencies (no conflicts)\n",
    "- ‚úì Lightweight (compared to virtual machines)\n",
    "- ‚úì Versioned and shareable\n",
    "\n",
    "### Docker Basics\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Image**: Blueprint for container (like a class)\n",
    "- **Container**: Running instance of image (like an object)\n",
    "- **Dockerfile**: Recipe for building an image\n",
    "- **Registry**: Repository for images (Docker Hub)\n",
    "\n",
    "### Example Dockerfile for Research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create example Dockerfile for data science research\n",
    "dockerfile_content = \"\"\"# Research Project Dockerfile\n",
    "# This creates a reproducible environment for data analysis\n",
    "\n",
    "# Start from official Python image\n",
    "FROM python:3.10-slim\n",
    "\n",
    "# Set metadata\n",
    "LABEL maintainer=\"your.email@university.edu\"\n",
    "LABEL description=\"Reproducible environment for Sleep & Memory Study\"\n",
    "LABEL version=\"1.0\"\n",
    "\n",
    "# Set working directory\n",
    "WORKDIR /research\n",
    "\n",
    "# Install system dependencies\n",
    "RUN apt-get update && apt-get install -y \\\\\n",
    "    build-essential \\\\\n",
    "    git \\\\\n",
    "    curl \\\\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Copy requirements file\n",
    "COPY requirements.txt .\n",
    "\n",
    "# Install Python dependencies\n",
    "# Pin exact versions for reproducibility\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copy project files\n",
    "COPY . .\n",
    "\n",
    "# Set environment variables\n",
    "ENV PYTHONUNBUFFERED=1\n",
    "ENV MPLBACKEND=Agg\n",
    "\n",
    "# Default command: run analysis\n",
    "CMD [\"python\", \"analysis/main_analysis.py\"]\n",
    "\n",
    "# To build: docker build -t my-research:v1.0 .\n",
    "# To run: docker run -v $(pwd)/data:/research/data my-research:v1.0\n",
    "\"\"\"\n",
    "\n",
    "print(\"üì¶ EXAMPLE DOCKERFILE\")\n",
    "print(\"=\" * 70)\n",
    "print(dockerfile_content)\n",
    "\n",
    "# Save Dockerfile\n",
    "with open(\"../notebooks/outputs/module_10/Dockerfile.example\", \"w\") as f:\n",
    "    f.write(dockerfile_content)\n",
    "\n",
    "print(\"\\n‚úì Dockerfile saved to outputs/module_10/Dockerfile.example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create docker-compose for more complex setups\n",
    "docker_compose = \"\"\"# docker-compose.yml\n",
    "# For multi-container research setups\n",
    "\n",
    "version: '3.8'\n",
    "\n",
    "services:\n",
    "  # Jupyter notebook server\n",
    "  jupyter:\n",
    "    build: .\n",
    "    ports:\n",
    "      - \"8888:8888\"\n",
    "    volumes:\n",
    "      - ./data:/research/data\n",
    "      - ./notebooks:/research/notebooks\n",
    "      - ./results:/research/results\n",
    "    environment:\n",
    "      - JUPYTER_ENABLE_LAB=yes\n",
    "    command: jupyter lab --ip=0.0.0.0 --no-browser --allow-root\n",
    "  \n",
    "  # Database (if needed)\n",
    "  postgres:\n",
    "    image: postgres:14\n",
    "    environment:\n",
    "      - POSTGRES_DB=research_db\n",
    "      - POSTGRES_USER=researcher\n",
    "      - POSTGRES_PASSWORD=secure_password\n",
    "    volumes:\n",
    "      - postgres_data:/var/lib/postgresql/data\n",
    "\n",
    "volumes:\n",
    "  postgres_data:\n",
    "\n",
    "# Usage:\n",
    "# docker-compose up -d        # Start all services\n",
    "# docker-compose down         # Stop all services\n",
    "# docker-compose logs jupyter # View logs\n",
    "\"\"\"\n",
    "\n",
    "print(\"üê≥ DOCKER-COMPOSE EXAMPLE\")\n",
    "print(\"=\" * 70)\n",
    "print(docker_compose)\n",
    "\n",
    "with open(\"../notebooks/outputs/module_10/docker-compose.yml.example\", \"w\") as f:\n",
    "    f.write(docker_compose)\n",
    "\n",
    "print(\"\\n‚úì Docker-compose saved to outputs/module_10/docker-compose.yml.example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Version Control for Data and Models\n",
    "\n",
    "### The Problem with Large Files in Git\n",
    "\n",
    "Git is designed for code (small text files), not:\n",
    "- Large datasets (>100 MB)\n",
    "- Binary files (models, images)\n",
    "- Files that change frequently\n",
    "\n",
    "### Solutions\n",
    "\n",
    "#### 1. DVC (Data Version Control)\n",
    "\n",
    "**DVC** tracks large files separately from Git, storing metadata in Git and data in cloud storage.\n",
    "\n",
    "```bash\n",
    "# Initialize DVC\n",
    "dvc init\n",
    "\n",
    "# Track large file\n",
    "dvc add data/raw/large_dataset.csv\n",
    "git add data/raw/large_dataset.csv.dvc .gitignore\n",
    "git commit -m \"Add dataset (tracked with DVC)\"\n",
    "\n",
    "# Configure remote storage (S3, Google Drive, etc.)\n",
    "dvc remote add -d myremote s3://mybucket/dvcstore\n",
    "\n",
    "# Push data to remote\n",
    "dvc push\n",
    "\n",
    "# Others can pull data\n",
    "dvc pull\n",
    "```\n",
    "\n",
    "#### 2. Git-LFS (Large File Storage)\n",
    "\n",
    "**Git-LFS** replaces large files with pointers in Git.\n",
    "\n",
    "```bash\n",
    "# Install Git-LFS\n",
    "git lfs install\n",
    "\n",
    "# Track file types\n",
    "git lfs track \"*.psd\"\n",
    "git lfs track \"*.pkl\"\n",
    "git lfs track \"*.h5\"\n",
    "\n",
    "# Add and commit as normal\n",
    "git add model.pkl\n",
    "git commit -m \"Add trained model\"\n",
    "git push\n",
    "```\n",
    "\n",
    "### DVC vs Git-LFS Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "comparison_data = {\n",
    "    \"Feature\": [\n",
    "        \"Storage\",\n",
    "        \"Max File Size\",\n",
    "        \"Cost\",\n",
    "        \"Pipeline Support\",\n",
    "        \"Cloud Options\",\n",
    "        \"Learning Curve\",\n",
    "        \"Best For\",\n",
    "    ],\n",
    "    \"DVC\": [\n",
    "        \"Separate from Git\",\n",
    "        \"Unlimited\",\n",
    "        \"Free (use own storage)\",\n",
    "        \"Excellent (dvc.yaml)\",\n",
    "        \"S3, GCS, Azure, SSH, etc.\",\n",
    "        \"Moderate\",\n",
    "        \"ML pipelines, large datasets\",\n",
    "    ],\n",
    "    \"Git-LFS\": [\n",
    "        \"GitHub LFS\",\n",
    "        \"2 GB per file\",\n",
    "        \"Free tier: 1 GB storage, 1 GB bandwidth\",\n",
    "        \"Limited\",\n",
    "        \"GitHub, GitLab, Bitbucket\",\n",
    "        \"Easy\",\n",
    "        \"Binary assets, small models\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"üìä DVC vs GIT-LFS COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\n",
    "    \"\\nüí° Recommendation: Use DVC for data science workflows, Git-LFS for occasional large files.\"\n",
    ")\n",
    "\n",
    "# Save comparison\n",
    "comparison_df.to_csv(\"../notebooks/outputs/module_10/dvc_vs_gitlfs.csv\", index=False)\n",
    "print(\"\\n‚úì Comparison saved to outputs/module_10/dvc_vs_gitlfs.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dependency Management\n",
    "\n",
    "### The Dependency Hell Problem\n",
    "\n",
    "**Scenario:** Your code works today, but in 6 months:\n",
    "- Package versions have changed\n",
    "- Dependencies conflict\n",
    "- Code breaks\n",
    "\n",
    "**Solution:** Lock exact versions of ALL dependencies.\n",
    "\n",
    "### Python: pip and conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate requirements.txt with exact versions\n",
    "requirements_locked = \"\"\"# requirements.txt (LOCKED VERSIONS)\n",
    "# Generated: 2025-01-20\n",
    "# Python: 3.10.8\n",
    "\n",
    "# Core scientific computing\n",
    "numpy==1.24.2\n",
    "pandas==2.0.1\n",
    "scipy==1.10.1\n",
    "\n",
    "# Statistics\n",
    "statsmodels==0.14.0\n",
    "pingouin==0.5.3\n",
    "scikit-learn==1.3.0\n",
    "\n",
    "# Visualization\n",
    "matplotlib==3.7.1\n",
    "seaborn==0.12.2\n",
    "plotly==5.17.0\n",
    "\n",
    "# Jupyter\n",
    "jupyter==1.0.0\n",
    "ipykernel==6.23.1\n",
    "nbconvert==7.6.0\n",
    "\n",
    "# To generate this file:\n",
    "# pip freeze > requirements.txt\n",
    "\n",
    "# To install exact versions:\n",
    "# pip install -r requirements.txt\n",
    "\"\"\"\n",
    "\n",
    "print(\"üì¶ LOCKED REQUIREMENTS.TXT\")\n",
    "print(\"=\" * 70)\n",
    "print(requirements_locked)\n",
    "\n",
    "with open(\"../notebooks/outputs/module_10/requirements_locked.txt\", \"w\") as f:\n",
    "    f.write(requirements_locked)\n",
    "\n",
    "print(\"\\n‚úì Requirements file saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate conda environment.yml\n",
    "conda_env = \"\"\"# environment.yml (CONDA ENVIRONMENT)\n",
    "name: research-env\n",
    "channels:\n",
    "  - conda-forge\n",
    "  - defaults\n",
    "\n",
    "dependencies:\n",
    "  # Python version\n",
    "  - python=3.10.8\n",
    "  \n",
    "  # Core packages\n",
    "  - numpy=1.24.2\n",
    "  - pandas=2.0.1\n",
    "  - scipy=1.10.1\n",
    "  - matplotlib=3.7.1\n",
    "  - seaborn=0.12.2\n",
    "  - scikit-learn=1.3.0\n",
    "  - statsmodels=0.14.0\n",
    "  \n",
    "  # Jupyter\n",
    "  - jupyter=1.0.0\n",
    "  - ipykernel=6.23.1\n",
    "  \n",
    "  # pip packages (if not in conda)\n",
    "  - pip:\n",
    "    - pingouin==0.5.3\n",
    "    - plotly==5.17.0\n",
    "\n",
    "# To create environment:\n",
    "# conda env create -f environment.yml\n",
    "\n",
    "# To activate:\n",
    "# conda activate research-env\n",
    "\n",
    "# To export current environment:\n",
    "# conda env export > environment.yml\n",
    "\"\"\"\n",
    "\n",
    "print(\"üêç CONDA ENVIRONMENT.YML\")\n",
    "print(\"=\" * 70)\n",
    "print(conda_env)\n",
    "\n",
    "with open(\"../notebooks/outputs/module_10/environment.yml\", \"w\") as f:\n",
    "    f.write(conda_env)\n",
    "\n",
    "print(\"\\n‚úì Conda environment file saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Practices\n",
    "\n",
    "1. **Lock versions early:** Generate requirements.txt or environment.yml at project start\n",
    "2. **Update deliberately:** Don't automatically update packages\n",
    "3. **Test after updates:** Run tests before committing version changes\n",
    "4. **Document Python version:** Include in requirements\n",
    "5. **Use virtual environments:** Never install packages globally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Workflow Automation\n",
    "\n",
    "### Why Automate Workflows?\n",
    "\n",
    "Manual workflows lead to:\n",
    "- ‚ùå Forgotten steps\n",
    "- ‚ùå Inconsistent execution order\n",
    "- ‚ùå Wasted time re-running unchanged steps\n",
    "\n",
    "Automated workflows provide:\n",
    "- ‚úì Reproducibility (same steps, same order)\n",
    "- ‚úì Efficiency (only re-run what changed)\n",
    "- ‚úì Documentation (workflow IS the documentation)\n",
    "\n",
    "### Make and Makefiles\n",
    "\n",
    "**Make** is a classic build automation tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create example Makefile for research project\n",
    "makefile_content = \"\"\"# Makefile for Research Project\n",
    "# Automates the entire analysis pipeline\n",
    "\n",
    ".PHONY: all clean data analysis report\n",
    "\n",
    "# Default target: run entire pipeline\n",
    "all: report\n",
    "\n",
    "# Download and preprocess data\n",
    "data: data/processed/clean_data.csv\n",
    "\n",
    "data/processed/clean_data.csv: scripts/01_download_data.py scripts/02_clean_data.py\n",
    "\t@echo \"Downloading and cleaning data...\"\n",
    "\tpython scripts/01_download_data.py\n",
    "\tpython scripts/02_clean_data.py\n",
    "\t@echo \"‚úì Data ready\"\n",
    "\n",
    "# Run statistical analysis\n",
    "analysis: results/analysis_output.csv\n",
    "\n",
    "results/analysis_output.csv: data/processed/clean_data.csv scripts/03_analyze.py\n",
    "\t@echo \"Running analysis...\"\n",
    "\tpython scripts/03_analyze.py\n",
    "\t@echo \"‚úì Analysis complete\"\n",
    "\n",
    "# Generate figures\n",
    "figures: results/figures/figure1.png\n",
    "\n",
    "results/figures/figure1.png: results/analysis_output.csv scripts/04_visualize.py\n",
    "\t@echo \"Creating figures...\"\n",
    "\tpython scripts/04_visualize.py\n",
    "\t@echo \"‚úì Figures created\"\n",
    "\n",
    "# Compile final report\n",
    "report: results/final_report.pdf\n",
    "\n",
    "results/final_report.pdf: results/analysis_output.csv results/figures/figure1.png manuscript/main.Rmd\n",
    "\t@echo \"Compiling report...\"\n",
    "\tRscript -e \"rmarkdown::render('manuscript/main.Rmd', output_dir='results')\"\n",
    "\t@echo \"‚úì Report complete: results/final_report.pdf\"\n",
    "\n",
    "# Clean all generated files\n",
    "clean:\n",
    "\t@echo \"Cleaning generated files...\"\n",
    "\trm -rf data/processed/*\n",
    "\trm -rf results/*\n",
    "\t@echo \"‚úì Clean complete\"\n",
    "\n",
    "# Usage:\n",
    "# make          # Run entire pipeline\n",
    "# make data     # Only download/clean data\n",
    "# make analysis # Only run analysis\n",
    "# make clean    # Remove all generated files\n",
    "\"\"\"\n",
    "\n",
    "print(\"üî® EXAMPLE MAKEFILE\")\n",
    "print(\"=\" * 70)\n",
    "print(makefile_content)\n",
    "\n",
    "with open(\"../notebooks/outputs/module_10/Makefile.example\", \"w\") as f:\n",
    "    f.write(makefile_content)\n",
    "\n",
    "print(\"\\n‚úì Makefile saved to outputs/module_10/Makefile.example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snakemake for Data Science\n",
    "\n",
    "**Snakemake** is Make for Python, with better features for data science:\n",
    "- Python-based syntax\n",
    "- Automatic parallelization\n",
    "- Cloud execution support\n",
    "- Conda integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create example Snakefile\n",
    "snakefile_content = \"\"\"# Snakefile for Research Project\n",
    "# More powerful than Make, Python-based\n",
    "\n",
    "# Configuration\n",
    "configfile: \"config.yaml\"\n",
    "\n",
    "# Target rule: what we want to produce\n",
    "rule all:\n",
    "    input:\n",
    "        \"results/final_report.pdf\",\n",
    "        \"results/figures/figure1.png\"\n",
    "\n",
    "# Download raw data\n",
    "rule download_data:\n",
    "    output:\n",
    "        \"data/raw/dataset.csv\"\n",
    "    params:\n",
    "        url = config[\"data_url\"]\n",
    "    shell:\n",
    "        \"wget {params.url} -O {output}\"\n",
    "\n",
    "# Clean data\n",
    "rule clean_data:\n",
    "    input:\n",
    "        \"data/raw/dataset.csv\"\n",
    "    output:\n",
    "        \"data/processed/clean_data.csv\"\n",
    "    conda:\n",
    "        \"envs/data_processing.yaml\"\n",
    "    script:\n",
    "        \"scripts/clean_data.py\"\n",
    "\n",
    "# Run analysis\n",
    "rule analyze:\n",
    "    input:\n",
    "        \"data/processed/clean_data.csv\"\n",
    "    output:\n",
    "        \"results/analysis_output.csv\",\n",
    "        \"results/stats.txt\"\n",
    "    params:\n",
    "        alpha = config[\"alpha\"]\n",
    "    conda:\n",
    "        \"envs/analysis.yaml\"\n",
    "    script:\n",
    "        \"scripts/analyze.py\"\n",
    "\n",
    "# Create visualizations\n",
    "rule visualize:\n",
    "    input:\n",
    "        \"results/analysis_output.csv\"\n",
    "    output:\n",
    "        \"results/figures/figure1.png\"\n",
    "    conda:\n",
    "        \"envs/visualization.yaml\"\n",
    "    script:\n",
    "        \"scripts/visualize.py\"\n",
    "\n",
    "# Generate report\n",
    "rule report:\n",
    "    input:\n",
    "        data = \"results/analysis_output.csv\",\n",
    "        figures = \"results/figures/figure1.png\"\n",
    "    output:\n",
    "        \"results/final_report.pdf\"\n",
    "    conda:\n",
    "        \"envs/report.yaml\"\n",
    "    shell:\n",
    "        \"Rscript -e 'rmarkdown::render(\\\"manuscript/main.Rmd\\\", output_dir=\\\"results\\\")'\"\n",
    "\n",
    "# Usage:\n",
    "# snakemake --cores 4          # Run pipeline with 4 cores\n",
    "# snakemake --use-conda        # Use conda environments\n",
    "# snakemake --dag | dot -Tpng  # Visualize workflow\n",
    "# snakemake -n                 # Dry run (show what would run)\n",
    "\"\"\"\n",
    "\n",
    "print(\"üêç EXAMPLE SNAKEFILE\")\n",
    "print(\"=\" * 70)\n",
    "print(snakefile_content)\n",
    "\n",
    "with open(\"../notebooks/outputs/module_10/Snakefile.example\", \"w\") as f:\n",
    "    f.write(snakefile_content)\n",
    "\n",
    "print(\"\\n‚úì Snakefile saved to outputs/module_10/Snakefile.example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Reproducible Random Number Generation\n",
    "\n",
    "### The Problem\n",
    "\n",
    "Many analyses use randomness:\n",
    "- Random sampling\n",
    "- Train/test splits\n",
    "- Monte Carlo simulations\n",
    "- Stochastic algorithms (e.g., SGD)\n",
    "\n",
    "**Without setting seeds, results are different every time!**\n",
    "\n",
    "### Solution: Set Random Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate importance of random seeds\n",
    "print(\"üé≤ RANDOM SEED DEMONSTRATION\\n\")\n",
    "\n",
    "# Without seed\n",
    "print(\"WITHOUT SEED (different each time):\")\n",
    "sample1 = np.random.normal(0, 1, 5)\n",
    "sample2 = np.random.normal(0, 1, 5)\n",
    "print(f\"Run 1: {sample1}\")\n",
    "print(f\"Run 2: {sample2}\")\n",
    "print(f\"Identical? {np.array_equal(sample1, sample2)}\\n\")\n",
    "\n",
    "# With seed\n",
    "print(\"WITH SEED (reproducible):\")\n",
    "np.random.seed(42)\n",
    "sample3 = np.random.normal(0, 1, 5)\n",
    "np.random.seed(42)  # Reset to same seed\n",
    "sample4 = np.random.normal(0, 1, 5)\n",
    "print(f\"Run 1: {sample3}\")\n",
    "print(f\"Run 2: {sample4}\")\n",
    "print(f\"Identical? {np.array_equal(sample3, sample4)}\")\n",
    "\n",
    "print(\"\\n‚úì Always set random seeds for reproducibility!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive seed-setting function\n",
    "def set_all_seeds(seed=42):\n",
    "    \"\"\"\n",
    "    Set random seeds for all common libraries.\n",
    "\n",
    "    This ensures reproducibility across:\n",
    "    - NumPy\n",
    "    - Python's random module\n",
    "    - TensorFlow (if installed)\n",
    "    - PyTorch (if installed)\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    seed : int\n",
    "        Random seed value\n",
    "    \"\"\"\n",
    "    # Python's random module\n",
    "    import random\n",
    "\n",
    "    random.seed(seed)\n",
    "\n",
    "    # NumPy\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # TensorFlow (if available)\n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "\n",
    "        tf.random.set_seed(seed)\n",
    "    except ImportError:\n",
    "        pass\n",
    "\n",
    "    # PyTorch (if available)\n",
    "    try:\n",
    "        import torch\n",
    "\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed(seed)\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    except ImportError:\n",
    "        pass\n",
    "\n",
    "    print(f\"‚úì All random seeds set to {seed}\")\n",
    "\n",
    "\n",
    "# Usage example\n",
    "set_all_seeds(42)\n",
    "\n",
    "# Save function to file\n",
    "seed_script = '''\"\"\"Reproducibility utilities.\"\"\"\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def set_all_seeds(seed=42):\n",
    "    \"\"\"Set random seeds for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "        tf.random.set_seed(seed)\n",
    "    except ImportError:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        import torch\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    except ImportError:\n",
    "        pass\n",
    "    \n",
    "    return seed\n",
    "\n",
    "# Usage:\n",
    "# from reproducibility_utils import set_all_seeds\n",
    "# set_all_seeds(42)\n",
    "'''\n",
    "\n",
    "with open(\"../notebooks/outputs/module_10/reproducibility_utils.py\", \"w\") as f:\n",
    "    f.write(seed_script)\n",
    "\n",
    "print(\"‚úì Seed-setting utility saved to outputs/module_10/reproducibility_utils.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Practices for Random Seeds\n",
    "\n",
    "1. **Set seeds at the beginning** of your script/notebook\n",
    "2. **Document the seed value** in your paper (e.g., \"We used seed=42 for all random operations\")\n",
    "3. **Use consistent seeds** across related scripts\n",
    "4. **Test with multiple seeds** to ensure results are robust (report in sensitivity analysis)\n",
    "5. **Don't repeatedly reset seeds** within a script (breaks randomness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Environment Documentation\n",
    "\n",
    "### Why Document Your Environment?\n",
    "\n",
    "Future users (including future you!) need to know:\n",
    "- What software versions you used\n",
    "- What operating system\n",
    "- What hardware (for GPU code)\n",
    "\n",
    "### Creating a Reproducibility Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_reproducibility_report(output_file=\"reproducibility_report.txt\"):\n",
    "    \"\"\"\n",
    "    Generate comprehensive environment documentation.\n",
    "    \"\"\"\n",
    "    report = []\n",
    "    report.append(\"=\" * 80)\n",
    "    report.append(\"REPRODUCIBILITY REPORT\")\n",
    "    report.append(\"=\" * 80)\n",
    "    report.append(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "\n",
    "    # Python version\n",
    "    report.append(\"PYTHON ENVIRONMENT\")\n",
    "    report.append(\"-\" * 80)\n",
    "    report.append(f\"Python version: {sys.version}\")\n",
    "    report.append(f\"Python executable: {sys.executable}\\n\")\n",
    "\n",
    "    # System information\n",
    "    report.append(\"SYSTEM INFORMATION\")\n",
    "    report.append(\"-\" * 80)\n",
    "    report.append(f\"Platform: {platform.platform()}\")\n",
    "    report.append(f\"System: {platform.system()}\")\n",
    "    report.append(f\"Release: {platform.release()}\")\n",
    "    report.append(f\"Machine: {platform.machine()}\")\n",
    "    report.append(f\"Processor: {platform.processor()}\\n\")\n",
    "\n",
    "    # Installed packages\n",
    "    report.append(\"INSTALLED PACKAGES\")\n",
    "    report.append(\"-\" * 80)\n",
    "\n",
    "    # Key packages\n",
    "    key_packages = [\"numpy\", \"pandas\", \"scipy\", \"matplotlib\", \"seaborn\", \"sklearn\", \"statsmodels\"]\n",
    "\n",
    "    for pkg in key_packages:\n",
    "        try:\n",
    "            module = __import__(pkg)\n",
    "            version = getattr(module, \"__version__\", \"unknown\")\n",
    "            report.append(f\"{pkg}: {version}\")\n",
    "        except ImportError:\n",
    "            report.append(f\"{pkg}: NOT INSTALLED\")\n",
    "\n",
    "    report.append(\"\\nFor complete package list, run: pip freeze\\n\")\n",
    "\n",
    "    # Random seed\n",
    "    report.append(\"REPRODUCIBILITY SETTINGS\")\n",
    "    report.append(\"-\" * 80)\n",
    "    report.append(\"Random seed: 42 (set at beginning of all scripts)\")\n",
    "    report.append(\"NumPy printoptions: precision=3, suppress=True\\n\")\n",
    "\n",
    "    # Data sources\n",
    "    report.append(\"DATA SOURCES\")\n",
    "    report.append(\"-\" * 80)\n",
    "    report.append(\"Raw data: data/raw/dataset.csv (SHA256: ...)\")\n",
    "    report.append(\"See data/README.md for data provenance\\n\")\n",
    "\n",
    "    # Analysis scripts\n",
    "    report.append(\"ANALYSIS WORKFLOW\")\n",
    "    report.append(\"-\" * 80)\n",
    "    report.append(\"To reproduce analysis:\")\n",
    "    report.append(\"  1. Install dependencies: pip install -r requirements.txt\")\n",
    "    report.append(\"  2. Run workflow: make all\")\n",
    "    report.append(\"  OR use Docker: docker build -t analysis . && docker run analysis\\n\")\n",
    "\n",
    "    report.append(\"=\" * 80)\n",
    "\n",
    "    # Write report\n",
    "    report_text = \"\\n\".join(report)\n",
    "\n",
    "    # Print to console\n",
    "    print(report_text)\n",
    "\n",
    "    # Save to file\n",
    "    output_path = f\"../notebooks/outputs/module_10/{output_file}\"\n",
    "    with open(output_path, \"w\") as f:\n",
    "        f.write(report_text)\n",
    "\n",
    "    print(f\"\\n‚úì Report saved to {output_path}\")\n",
    "    return report_text\n",
    "\n",
    "\n",
    "# Generate report\n",
    "generate_reproducibility_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Complete Reproducibility Checklist\n",
    "\n",
    "### Use this checklist for every research project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checklist = \"\"\"COMPUTATIONAL REPRODUCIBILITY CHECKLIST\n",
    "=====================================================================\n",
    "\n",
    "ENVIRONMENT\n",
    "‚òê requirements.txt or environment.yml created with EXACT versions\n",
    "‚òê Python version documented\n",
    "‚òê Dockerfile created (or instructions for container)\n",
    "‚òê Virtual environment used (never install globally)\n",
    "‚òê Environment documentation generated (pip freeze, conda list)\n",
    "\n",
    "CODE\n",
    "‚òê All code in version control (Git)\n",
    "‚òê Random seeds set at beginning of scripts\n",
    "‚òê Seed values documented in paper/README\n",
    "‚òê No hardcoded file paths (use relative paths or config files)\n",
    "‚òê Code is commented and readable\n",
    "‚òê Functions have docstrings\n",
    "\n",
    "DATA\n",
    "‚òê Raw data preserved (never modified)\n",
    "‚òê Data cleaning/processing scripted (not manual)\n",
    "‚òê Large files managed with DVC or Git-LFS\n",
    "‚òê Data provenance documented (where did data come from?)\n",
    "‚òê Data sharing plan (public repository or on request)\n",
    "‚òê License for data specified\n",
    "\n",
    "WORKFLOW\n",
    "‚òê Analysis workflow automated (Makefile or Snakefile)\n",
    "‚òê Pipeline runs from start to finish without manual intervention\n",
    "‚òê Pipeline tested on clean environment\n",
    "‚òê Workflow diagram created (optional but helpful)\n",
    "\n",
    "DOCUMENTATION\n",
    "‚òê README with setup instructions\n",
    "‚òê README with execution instructions\n",
    "‚òê README with expected outputs\n",
    "‚òê Comments explain \"why\", not just \"what\"\n",
    "‚òê Unusual dependencies explained\n",
    "‚òê Known issues documented\n",
    "\n",
    "TESTING\n",
    "‚òê Someone else can run your code (ideally on different machine)\n",
    "‚òê Results match reported values\n",
    "‚òê Code runs without errors\n",
    "‚òê Outputs generated as expected\n",
    "\n",
    "PUBLICATION\n",
    "‚òê Code repository linked in paper\n",
    "‚òê Data repository linked in paper (or availability statement)\n",
    "‚òê Random seeds reported in methods\n",
    "‚òê Software versions reported in methods\n",
    "‚òê Deviations from preregistration documented (if applicable)\n",
    "\n",
    "LONG-TERM PRESERVATION\n",
    "‚òê Code archived with DOI (Zenodo, figshare)\n",
    "‚òê Data archived with DOI\n",
    "‚òê Container image archived (Docker Hub, Singularity Hub)\n",
    "‚òê Preregistration linked (OSF, AsPredicted)\n",
    "\n",
    "=====================================================================\n",
    "GOLD STANDARD: \"One-click reproducibility\"\n",
    "  Goal: Someone can clone your repo, run one command, and get results\n",
    "  Example: docker run myimage\n",
    "        OR: make all\n",
    "=====================================================================\n",
    "\"\"\"\n",
    "\n",
    "print(checklist)\n",
    "\n",
    "with open(\"../notebooks/outputs/module_10/reproducibility_checklist.txt\", \"w\") as f:\n",
    "    f.write(checklist)\n",
    "\n",
    "print(\"\\n‚úì Checklist saved to outputs/module_10/reproducibility_checklist.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Practice Exercise: Create a Reproducible Mini-Project\n",
    "\n",
    "### Task\n",
    "\n",
    "Create a fully reproducible analysis project with:\n",
    "1. Locked dependencies\n",
    "2. Random seed setting\n",
    "3. Automated workflow\n",
    "4. Documentation\n",
    "\n",
    "### Starter Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_structure = \"\"\"REPRODUCIBLE PROJECT TEMPLATE\n",
    "=====================================================================\n",
    "\n",
    "my_research_project/\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ README.md                    # Project overview and instructions\n",
    "‚îú‚îÄ‚îÄ requirements.txt             # Python dependencies (LOCKED versions)\n",
    "‚îú‚îÄ‚îÄ Dockerfile                   # Container definition\n",
    "‚îú‚îÄ‚îÄ Makefile                     # Workflow automation\n",
    "‚îú‚îÄ‚îÄ .gitignore                   # Git ignore file\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ data/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ raw/                     # Original, immutable data\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ .gitkeep\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ processed/               # Cleaned data (generated)\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ .gitkeep\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ README.md                # Data provenance\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ scripts/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 01_download_data.py      # Data acquisition\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 02_clean_data.py         # Data preprocessing\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 03_analyze.py            # Statistical analysis\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 04_visualize.py          # Create figures\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ reproducibility_utils.py # Helper functions\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ notebooks/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 01_exploratory_analysis.ipynb\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ 02_final_analysis.ipynb\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ results/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ figures/                 # Generated plots\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ tables/                  # Generated tables\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ stats/                   # Statistical output\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ manuscript/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ main.Rmd                 # R Markdown manuscript\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ references.bib           # Bibliography\n",
    "‚îÇ\n",
    "‚îî‚îÄ‚îÄ tests/\n",
    "    ‚îî‚îÄ‚îÄ test_analysis.py         # Unit tests (optional)\n",
    "\n",
    "=====================================================================\n",
    "\n",
    "README.md TEMPLATE:\n",
    "-------------------\n",
    "\n",
    "# Project Title\n",
    "\n",
    "## Overview\n",
    "Brief description of research project.\n",
    "\n",
    "## Setup\n",
    "```bash\n",
    "# Clone repository\n",
    "git clone https://github.com/username/project.git\n",
    "cd project\n",
    "\n",
    "# Create virtual environment\n",
    "python -m venv venv\n",
    "source venv/bin/activate  # On Windows: venv\\Scripts\\activate\n",
    "\n",
    "# Install dependencies\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "## Usage\n",
    "```bash\n",
    "# Run entire pipeline\n",
    "make all\n",
    "\n",
    "# OR using Docker\n",
    "docker build -t my-analysis .\n",
    "docker run -v $(pwd)/results:/research/results my-analysis\n",
    "```\n",
    "\n",
    "## Expected Output\n",
    "- `results/figures/figure1.png`: Main result\n",
    "- `results/final_report.pdf`: Complete analysis\n",
    "\n",
    "## Environment\n",
    "- Python 3.10.8\n",
    "- See `requirements.txt` for package versions\n",
    "- Random seed: 42\n",
    "\n",
    "## Citation\n",
    "If you use this code, please cite:\n",
    "[Citation info]\n",
    "\n",
    "## License\n",
    "MIT License\n",
    "\n",
    "=====================================================================\n",
    "\"\"\"\n",
    "\n",
    "print(project_structure)\n",
    "\n",
    "with open(\"../notebooks/outputs/module_10/project_template.txt\", \"w\") as f:\n",
    "    f.write(project_structure)\n",
    "\n",
    "print(\"\\n‚úì Project template saved to outputs/module_10/project_template.txt\")\n",
    "print(\"\\nüí° Exercise: Create this structure for your own research project!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Reproducibility is a spectrum**\n",
    "   - Minimum: Share code\n",
    "   - Better: Lock dependencies\n",
    "   - Best: Containerize + automate + test\n",
    "\n",
    "2. **Containers solve environment problems**\n",
    "   - Docker packages code + dependencies + OS\n",
    "   - \"Works on my machine\" ‚Üí \"Works everywhere\"\n",
    "   - Essential for long-term reproducibility\n",
    "\n",
    "3. **Version control for data, not just code**\n",
    "   - DVC for large datasets and ML pipelines\n",
    "   - Git-LFS for occasional large files\n",
    "   - Never commit large files directly to Git\n",
    "\n",
    "4. **Lock ALL dependencies**\n",
    "   - Use requirements.txt (pip freeze)\n",
    "   - Use environment.yml (conda)\n",
    "   - Include Python version\n",
    "   - Test in clean environment\n",
    "\n",
    "5. **Automate workflows**\n",
    "   - Make or Snakemake for pipelines\n",
    "   - Documents analysis steps\n",
    "   - Enables one-click reproduction\n",
    "   - Only re-runs changed steps\n",
    "\n",
    "6. **Always set random seeds**\n",
    "   - At beginning of scripts\n",
    "   - For all random libraries\n",
    "   - Document seed values\n",
    "   - Test robustness with multiple seeds\n",
    "\n",
    "7. **Document your environment**\n",
    "   - Generate reproducibility report\n",
    "   - Include in supplementary materials\n",
    "   - Update when packages change\n",
    "\n",
    "### Levels of Reproducibility\n",
    "\n",
    "**Level 1: Bare Minimum**\n",
    "- ‚òê Code shared publicly\n",
    "- ‚òê Data available\n",
    "\n",
    "**Level 2: Good Practice**\n",
    "- ‚òê Dependencies documented\n",
    "- ‚òê Random seeds set\n",
    "- ‚òê README with instructions\n",
    "\n",
    "**Level 3: Excellent Practice**\n",
    "- ‚òê Dependencies locked (requirements.txt)\n",
    "- ‚òê Workflow automated (Make/Snakemake)\n",
    "- ‚òê Tested on clean environment\n",
    "\n",
    "**Level 4: Gold Standard**\n",
    "- ‚òê Fully containerized (Docker)\n",
    "- ‚òê One-click reproduction\n",
    "- ‚òê Automated tests\n",
    "- ‚òê Archived with DOI\n",
    "\n",
    "### Impact\n",
    "\n",
    "Reproducible research:\n",
    "- ‚úì Enables verification of findings\n",
    "- ‚úì Facilitates building on prior work\n",
    "- ‚úì Increases citation rates\n",
    "- ‚úì Saves time (for future you!)\n",
    "- ‚úì Builds trust in science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "### Docker\n",
    "- Docker Documentation: https://docs.docker.com\n",
    "- Rocker (R + Docker): https://www.rocker-project.org\n",
    "- Jupyter Docker Stacks: https://jupyter-docker-stacks.readthedocs.io\n",
    "\n",
    "### Version Control\n",
    "- DVC Documentation: https://dvc.org/doc\n",
    "- Git-LFS: https://git-lfs.github.com\n",
    "\n",
    "### Workflow Tools\n",
    "- GNU Make: https://www.gnu.org/software/make/manual/\n",
    "- Snakemake: https://snakemake.readthedocs.io\n",
    "- Nextflow: https://www.nextflow.io (for bioinformatics)\n",
    "\n",
    "### Readings\n",
    "- Sandve et al. (2013). Ten simple rules for reproducible computational research. *PLOS Computational Biology*.\n",
    "- Wilson et al. (2017). Good enough practices in scientific computing. *PLOS Computational Biology*.\n",
    "- Peng (2011). Reproducible research in computational science. *Science*, 334(6060), 1226-1227.\n",
    "- The Turing Way: https://the-turing-way.netlify.app\n",
    "\n",
    "### Tools\n",
    "- CodeOcean: Cloud platform for reproducible code\n",
    "- Binder: Run Jupyter notebooks in browser\n",
    "- Gigantum: Collaborative data science platform\n",
    "- Renku: GitLab-based reproducible research platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "In **Module 11: Research Collaboration & Project Management**, you'll learn:\n",
    "- Git workflows for team collaboration\n",
    "- Project management tools (Trello, GitHub Projects)\n",
    "- Collaborative writing with Google Docs, Overleaf\n",
    "- Code review best practices\n",
    "- Managing co-authorship and contributions\n",
    "- Communication strategies for research teams\n",
    "\n",
    "Transform from solo researcher to effective collaborator! ü§ù"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
