{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 04: Experimental Design\n",
    "\n",
    "Welcome to Module 04! Now that you have a research question and hypothesis, let's learn how to **design rigorous experiments** to test them.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- Understanding different types of variables\n",
    "- Designing controlled experiments\n",
    "- Randomization and control groups\n",
    "- Sample size and statistical power\n",
    "- Common experimental designs (A/B testing, factorial designs)\n",
    "- Threats to validity and how to avoid them\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed Modules 00-03\n",
    "- Understanding of basic statistics\n",
    "\n",
    "## Time Required\n",
    "\n",
    "**40 minutes**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Setup\n",
    "# ========================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create output directory\n",
    "output_dir = \"outputs/notebook_04\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(\"âœ… Setup complete!\")\n",
    "print(f\"Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Variables\n",
    "\n",
    "In experimental research, understanding variables is crucial for proper design.\n",
    "\n",
    "### Types of Variables\n",
    "\n",
    "#### 1. Independent Variable (IV)\n",
    "\n",
    "**Definition**: The variable you manipulate or change\n",
    "\n",
    "**Also called**: \n",
    "- Predictor variable\n",
    "- Treatment variable\n",
    "- Explanatory variable\n",
    "\n",
    "**Examples**:\n",
    "- Algorithm type (LSTM vs ARIMA)\n",
    "- Feature set (with/without RFM features)\n",
    "- Training data size\n",
    "- Learning rate\n",
    "\n",
    "**Key point**: This is what you control in the experiment\n",
    "\n",
    "#### 2. Dependent Variable (DV)\n",
    "\n",
    "**Definition**: The variable you measure (the outcome)\n",
    "\n",
    "**Also called**:\n",
    "- Outcome variable\n",
    "- Response variable\n",
    "- Measured variable\n",
    "\n",
    "**Examples**:\n",
    "- Prediction accuracy\n",
    "- F1-score\n",
    "- Training time\n",
    "- User satisfaction\n",
    "\n",
    "**Key point**: This changes in response to the IV\n",
    "\n",
    "#### 3. Confounding Variables\n",
    "\n",
    "**Definition**: Variables that might affect the DV but aren't the IV\n",
    "\n",
    "**Also called**:\n",
    "- Lurking variables\n",
    "- Third variables\n",
    "\n",
    "**Examples**:\n",
    "- Data quality (when comparing algorithms)\n",
    "- Hardware differences (when measuring speed)\n",
    "- Researcher experience\n",
    "- Time of day for data collection\n",
    "\n",
    "**Key point**: These can lead to false conclusions if not controlled\n",
    "\n",
    "#### 4. Control Variables\n",
    "\n",
    "**Definition**: Confounding variables that you **hold constant**\n",
    "\n",
    "**Examples**:\n",
    "- Use same dataset for all algorithms\n",
    "- Use same hardware for all tests\n",
    "- Use same evaluation metrics\n",
    "\n",
    "**Key point**: By controlling these, you ensure fair comparison\n",
    "\n",
    "### Example: Stock Price Prediction Experiment\n",
    "\n",
    "**Research Question**: Does LSTM outperform ARIMA for stock prediction?\n",
    "\n",
    "- **Independent Variable (IV)**: Algorithm (LSTM vs ARIMA)\n",
    "- **Dependent Variable (DV)**: Prediction accuracy (RMSE)\n",
    "- **Confounding Variables**: \n",
    "  - Stock volatility\n",
    "  - Time period\n",
    "  - Feature selection\n",
    "  - Data quality\n",
    "- **Control Variables** (what we hold constant):\n",
    "  - Same stocks\n",
    "  - Same time period\n",
    "  - Same features\n",
    "  - Same evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Visualize Relationship Between Variables\n",
    "# ========================================\n",
    "\n",
    "# Simulate an experiment: Effect of training data size on accuracy\n",
    "# IV: Training data size\n",
    "# DV: Model accuracy\n",
    "# Confounding: We'll show what happens if not controlled\n",
    "\n",
    "# Scenario 1: Properly controlled (same data quality)\n",
    "data_sizes = np.array([100, 500, 1000, 2000, 5000, 10000])\n",
    "accuracy_controlled = 0.5 + 0.3 * np.log(data_sizes) / np.log(10000)\n",
    "accuracy_controlled += np.random.normal(0, 0.02, len(data_sizes))  # Small random noise\n",
    "\n",
    "# Scenario 2: Confounded (data quality varies)\n",
    "# Worse quality data for larger sizes (unrealistic but illustrative)\n",
    "data_quality_effect = -0.15 * (data_sizes / 10000)\n",
    "accuracy_confounded = accuracy_controlled + data_quality_effect\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Controlled experiment\n",
    "axes[0].plot(data_sizes, accuracy_controlled, \"o-\", color=\"green\", linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel(\"Training Data Size\", fontweight=\"bold\")\n",
    "axes[0].set_ylabel(\"Accuracy\", fontweight=\"bold\")\n",
    "axes[0].set_title(\n",
    "    \"Controlled Experiment\\n(Data quality held constant)\", fontweight=\"bold\", color=\"green\"\n",
    ")\n",
    "axes[0].grid(alpha=0.3)\n",
    "axes[0].set_ylim([0.4, 0.9])\n",
    "\n",
    "# Confounded experiment\n",
    "axes[1].plot(data_sizes, accuracy_confounded, \"o-\", color=\"red\", linewidth=2, markersize=8)\n",
    "axes[1].set_xlabel(\"Training Data Size\", fontweight=\"bold\")\n",
    "axes[1].set_ylabel(\"Accuracy\", fontweight=\"bold\")\n",
    "axes[1].set_title(\n",
    "    \"Confounded Experiment\\n(Data quality varies - WRONG!)\", fontweight=\"bold\", color=\"red\"\n",
    ")\n",
    "axes[1].grid(alpha=0.3)\n",
    "axes[1].set_ylim([0.4, 0.9])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{output_dir}/controlled_vs_confounded.png\", dpi=150, bbox_inches=\"tight\")\n",
    "print(f\"âœ… Visualization saved to: {output_dir}/controlled_vs_confounded.png\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ KEY LESSON:\")\n",
    "print(\"Left: Proper control shows expected relationship (more data = better)\")\n",
    "print(\"Right: Confounding variable masks true effect!\")\n",
    "print(\"\\nAlways control confounding variables!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Experimental Design Principles\n",
    "\n",
    "Good experiments follow these core principles:\n",
    "\n",
    "### 1. Randomization\n",
    "\n",
    "**What**: Randomly assign subjects/samples to different groups\n",
    "\n",
    "**Why**: \n",
    "- Eliminates selection bias\n",
    "- Balances unknown confounds across groups\n",
    "- Allows for statistical inference\n",
    "\n",
    "**Example**: When testing a new algorithm\n",
    "- âŒ **Bad**: Test on easy data samples, compare to hard samples\n",
    "- âœ… **Good**: Randomly split data into train/test, apply both algorithms\n",
    "\n",
    "### 2. Control Group\n",
    "\n",
    "**What**: A baseline group for comparison\n",
    "\n",
    "**Why**: Shows what would happen without the treatment\n",
    "\n",
    "**Example**: Testing a new feature engineering technique\n",
    "- **Treatment group**: Model with new features\n",
    "- **Control group**: Model with original features\n",
    "- **Compare**: Difference in performance\n",
    "\n",
    "### 3. Replication\n",
    "\n",
    "**What**: Repeat the experiment multiple times\n",
    "\n",
    "**Why**:\n",
    "- Reduces impact of random variation\n",
    "- Increases confidence in results\n",
    "- Helps detect real effects vs noise\n",
    "\n",
    "**Example**: \n",
    "- Run model training 10 times with different random seeds\n",
    "- Report mean and standard deviation\n",
    "- Not just one lucky run!\n",
    "\n",
    "### 4. Blinding\n",
    "\n",
    "**What**: Keeping participants/researchers unaware of group assignment\n",
    "\n",
    "**Types**:\n",
    "- **Single-blind**: Participants don't know their group\n",
    "- **Double-blind**: Neither participants nor researchers know\n",
    "\n",
    "**Why**: Prevents bias in measurement or behavior\n",
    "\n",
    "**Example in ML**:\n",
    "- Have someone else evaluate model outputs\n",
    "- Don't tell them which algorithm produced which results\n",
    "- Prevents confirmation bias\n",
    "\n",
    "### 5. Sample Size\n",
    "\n",
    "**What**: Number of observations/samples\n",
    "\n",
    "**Why**: \n",
    "- Too small: Can't detect real effects\n",
    "- Too large: Wastes resources\n",
    "- Just right: Adequate statistical power\n",
    "\n",
    "**How to determine**: Statistical power analysis (see Part 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Common Experimental Designs\n",
    "\n",
    "### Design 1: Simple Randomized Experiment (A/B Test)\n",
    "\n",
    "**Structure**:\n",
    "```\n",
    "Random Assignment\n",
    "       â†“\n",
    "   â”Œâ”€â”€â”€â”´â”€â”€â”€â”\n",
    "   A       B\n",
    "   â†“       â†“\n",
    "Measure  Measure\n",
    "   â†“       â†“\n",
    "   Compare\n",
    "```\n",
    "\n",
    "**When to use**: Comparing two conditions\n",
    "\n",
    "**Example**: \n",
    "- Group A: Algorithm 1\n",
    "- Group B: Algorithm 2\n",
    "- Compare accuracy\n",
    "\n",
    "**Pros**: Simple, easy to analyze\n",
    "**Cons**: Can only test one factor at a time\n",
    "\n",
    "### Design 2: Factorial Design\n",
    "\n",
    "**Structure**: Test multiple factors simultaneously\n",
    "\n",
    "**Example**: 2Ã—2 Factorial (2 factors, 2 levels each)\n",
    "```\n",
    "Factor A: Algorithm (LSTM vs ARIMA)\n",
    "Factor B: Features (Basic vs Advanced)\n",
    "\n",
    "4 Conditions:\n",
    "1. LSTM + Basic\n",
    "2. LSTM + Advanced\n",
    "3. ARIMA + Basic\n",
    "4. ARIMA + Advanced\n",
    "```\n",
    "\n",
    "**When to use**: Want to test interactions between factors\n",
    "\n",
    "**Pros**: \n",
    "- Test multiple factors efficiently\n",
    "- Can detect interactions\n",
    "\n",
    "**Cons**: \n",
    "- More complex\n",
    "- Larger sample size needed\n",
    "\n",
    "### Design 3: Before-After (Within-Subjects)\n",
    "\n",
    "**Structure**: Measure same subjects before and after treatment\n",
    "\n",
    "**Example**:\n",
    "- Measure model performance with current features\n",
    "- Add new features\n",
    "- Measure performance again\n",
    "- Compare difference\n",
    "\n",
    "**Pros**: Controls for individual differences\n",
    "**Cons**: Order effects, can't \"unlearn\" what was learned\n",
    "\n",
    "### Design 4: Crossover Design\n",
    "\n",
    "**Structure**: Each group gets both treatments in different order\n",
    "\n",
    "```\n",
    "Period 1      Period 2\n",
    "Group 1: A  â†’  B\n",
    "Group 2: B  â†’  A\n",
    "```\n",
    "\n",
    "**When to use**: Want benefits of within-subjects but need to control order\n",
    "\n",
    "**Pros**: Controls for order effects\n",
    "**Cons**: More complex, time-consuming\n",
    "\n",
    "### Design 5: Stratified Random Assignment\n",
    "\n",
    "**Structure**: Divide into strata, then randomize within each\n",
    "\n",
    "**Example**:\n",
    "```\n",
    "Stratify by dataset size:\n",
    "- Small datasets: Randomly assign to A or B\n",
    "- Medium datasets: Randomly assign to A or B  \n",
    "- Large datasets: Randomly assign to A or B\n",
    "```\n",
    "\n",
    "**When to use**: Important confounds you want to balance\n",
    "\n",
    "**Pros**: Ensures balance on key variables\n",
    "**Cons**: Need to know important strata in advance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Demonstrate Factorial Design\n",
    "# ========================================\n",
    "\n",
    "# Simulate a 2x2 factorial experiment\n",
    "# Factor A: Algorithm (Random Forest vs XGBoost)\n",
    "# Factor B: Feature Engineering (Basic vs Advanced)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simulate accuracy scores for each condition\n",
    "results = {\n",
    "    \"Algorithm\": [\"Random Forest\"] * 50\n",
    "    + [\"XGBoost\"] * 50\n",
    "    + [\"Random Forest\"] * 50\n",
    "    + [\"XGBoost\"] * 50,\n",
    "    \"Features\": [\"Basic\"] * 100 + [\"Advanced\"] * 100,\n",
    "    \"Accuracy\": (\n",
    "        list(np.random.normal(0.75, 0.05, 50))  # RF + Basic\n",
    "        + list(np.random.normal(0.78, 0.05, 50))  # XGB + Basic\n",
    "        + list(np.random.normal(0.82, 0.05, 50))  # RF + Advanced\n",
    "        + list(np.random.normal(0.88, 0.05, 50))  # XGB + Advanced (interaction!)\n",
    "    ),\n",
    "}\n",
    "\n",
    "df_factorial = pd.DataFrame(results)\n",
    "\n",
    "# Analyze\n",
    "print(\"FACTORIAL EXPERIMENT RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nMean Accuracy by Condition:\")\n",
    "print(df_factorial.groupby([\"Algorithm\", \"Features\"])[\"Accuracy\"].mean())\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Bar chart\n",
    "summary = df_factorial.groupby([\"Algorithm\", \"Features\"])[\"Accuracy\"].mean().reset_index()\n",
    "x = np.arange(len(summary))\n",
    "colors = [\"lightblue\", \"blue\", \"lightcoral\", \"red\"]\n",
    "bars = axes[0].bar(x, summary[\"Accuracy\"], color=colors, alpha=0.7, edgecolor=\"black\")\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(\n",
    "    [f\"{row['Algorithm']}\\n{row['Features']}\" for _, row in summary.iterrows()], fontsize=9\n",
    ")\n",
    "axes[0].set_ylabel(\"Accuracy\", fontweight=\"bold\")\n",
    "axes[0].set_title(\"2Ã—2 Factorial Design Results\", fontweight=\"bold\", fontsize=12)\n",
    "axes[0].set_ylim([0.6, 1.0])\n",
    "axes[0].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    axes[0].text(\n",
    "        bar.get_x() + bar.get_width() / 2.0,\n",
    "        height,\n",
    "        f\"{height:.3f}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "# Plot 2: Interaction plot\n",
    "for algo in [\"Random Forest\", \"XGBoost\"]:\n",
    "    data = summary[summary[\"Algorithm\"] == algo]\n",
    "    axes[1].plot([0, 1], data[\"Accuracy\"], \"o-\", linewidth=2, markersize=10, label=algo)\n",
    "\n",
    "axes[1].set_xticks([0, 1])\n",
    "axes[1].set_xticklabels([\"Basic\", \"Advanced\"])\n",
    "axes[1].set_xlabel(\"Features\", fontweight=\"bold\")\n",
    "axes[1].set_ylabel(\"Accuracy\", fontweight=\"bold\")\n",
    "axes[1].set_title(\n",
    "    \"Interaction Plot\\n(Non-parallel lines = interaction!)\", fontweight=\"bold\", fontsize=12\n",
    ")\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "axes[1].set_ylim([0.7, 0.95])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{output_dir}/factorial_design.png\", dpi=150, bbox_inches=\"tight\")\n",
    "print(f\"\\nâœ… Factorial design plot saved to: {output_dir}/factorial_design.png\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ KEY INSIGHT:\")\n",
    "print(\"The non-parallel lines show an INTERACTION:\")\n",
    "print(\"XGBoost benefits MORE from advanced features than Random Forest does!\")\n",
    "print(\"This insight would be missed with separate A/B tests!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Sample Size and Statistical Power\n",
    "\n",
    "### What is Statistical Power?\n",
    "\n",
    "**Statistical Power**: Probability of detecting an effect when it truly exists\n",
    "\n",
    "**Formula**: Power = 1 - Î² (where Î² = probability of Type II error)\n",
    "\n",
    "**Conventional target**: 0.80 (80% power)\n",
    "\n",
    "### The Four Key Concepts\n",
    "\n",
    "1. **Effect Size**: How big is the difference you expect?\n",
    "   - Small: Cohen's d â‰ˆ 0.2\n",
    "   - Medium: Cohen's d â‰ˆ 0.5\n",
    "   - Large: Cohen's d â‰ˆ 0.8\n",
    "\n",
    "2. **Sample Size (n)**: How many observations?\n",
    "   - What we usually want to calculate\n",
    "\n",
    "3. **Significance Level (Î±)**: Probability of Type I error\n",
    "   - Usually 0.05\n",
    "\n",
    "4. **Power (1-Î²)**: Probability of detecting real effect\n",
    "   - Usually aim for 0.80\n",
    "\n",
    "**Key Relationship**: If you know any 3, you can calculate the 4th!\n",
    "\n",
    "### Sample Size Calculation\n",
    "\n",
    "**General principle**: \n",
    "- Smaller expected effect â†’ Larger sample needed\n",
    "- Higher power desired â†’ Larger sample needed\n",
    "- Lower Î± (more stringent) â†’ Larger sample needed\n",
    "\n",
    "### Rule of Thumb (for t-tests)\n",
    "\n",
    "| Effect Size | Sample Size per Group (Î±=0.05, power=0.80) |\n",
    "|-------------|-------------------------------------------|\n",
    "| Small (d=0.2) | ~400 |\n",
    "| Medium (d=0.5) | ~64 |\n",
    "| Large (d=0.8) | ~26 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Sample Size Calculator\n",
    "# ========================================\n",
    "\n",
    "\n",
    "def calculate_sample_size_ttest(effect_size, power=0.8, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Calculate required sample size for independent t-test.\n",
    "\n",
    "    Args:\n",
    "        effect_size (float): Cohen's d\n",
    "        power (float): Desired statistical power (default 0.8)\n",
    "        alpha (float): Significance level (default 0.05)\n",
    "\n",
    "    Returns:\n",
    "        int: Required sample size per group\n",
    "    \"\"\"\n",
    "    # This is a simplified calculation\n",
    "    # For precise calculations, use statsmodels or G*Power\n",
    "\n",
    "    from scipy.stats import norm\n",
    "\n",
    "    z_alpha = norm.ppf(1 - alpha / 2)  # Two-tailed\n",
    "    z_beta = norm.ppf(power)\n",
    "\n",
    "    n = 2 * ((z_alpha + z_beta) / effect_size) ** 2\n",
    "\n",
    "    return int(np.ceil(n))\n",
    "\n",
    "\n",
    "# Calculate for different effect sizes\n",
    "print(\"SAMPLE SIZE CALCULATOR\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Settings: Î± = 0.05, Power = 0.80\\n\")\n",
    "\n",
    "effect_sizes = {\"Small (0.2)\": 0.2, \"Medium (0.5)\": 0.5, \"Large (0.8)\": 0.8}\n",
    "\n",
    "results = []\n",
    "for label, d in effect_sizes.items():\n",
    "    n = calculate_sample_size_ttest(d)\n",
    "    total = n * 2  # Both groups\n",
    "    results.append({\"Effect Size\": label, \"n per group\": n, \"Total n\": total})\n",
    "    print(f\"Effect Size {label:15s}: {n:4d} per group, {total:4d} total\")\n",
    "\n",
    "# Visualize relationship\n",
    "effect_range = np.linspace(0.1, 1.0, 50)\n",
    "sample_sizes = [calculate_sample_size_ttest(d) for d in effect_range]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(effect_range, sample_sizes, linewidth=2, color=\"steelblue\")\n",
    "ax.fill_between(effect_range, sample_sizes, alpha=0.3, color=\"steelblue\")\n",
    "\n",
    "# Mark common effect sizes\n",
    "for label, d in effect_sizes.items():\n",
    "    n = calculate_sample_size_ttest(d)\n",
    "    ax.plot(d, n, \"ro\", markersize=10)\n",
    "    ax.annotate(\n",
    "        f\"{label}\\nn={n}\",\n",
    "        xy=(d, n),\n",
    "        xytext=(d + 0.05, n + 50),\n",
    "        fontweight=\"bold\",\n",
    "        fontsize=9,\n",
    "        arrowprops=dict(arrowstyle=\"->\", color=\"red\", lw=1.5),\n",
    "    )\n",
    "\n",
    "ax.set_xlabel(\"Effect Size (Cohen's d)\", fontweight=\"bold\", fontsize=11)\n",
    "ax.set_ylabel(\"Required Sample Size (per group)\", fontweight=\"bold\", fontsize=11)\n",
    "ax.set_title(\"Sample Size vs Effect Size\\n(Î±=0.05, Power=0.80)\", fontweight=\"bold\", fontsize=13)\n",
    "ax.grid(alpha=0.3)\n",
    "ax.set_ylim([0, max(sample_sizes) + 100])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{output_dir}/sample_size_calculation.png\", dpi=150, bbox_inches=\"tight\")\n",
    "print(f\"\\nâœ… Sample size plot saved to: {output_dir}/sample_size_calculation.png\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ KEY LESSON:\")\n",
    "print(\"Smaller effects require MUCH larger samples to detect!\")\n",
    "print(\"Always calculate sample size BEFORE collecting data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Threats to Validity\n",
    "\n",
    "Even well-designed experiments can have problems. Here are common threats:\n",
    "\n",
    "### Internal Validity Threats\n",
    "\n",
    "*Does the IV really cause changes in the DV?*\n",
    "\n",
    "#### 1. Selection Bias\n",
    "**Problem**: Groups differ before treatment\n",
    "**Solution**: Random assignment\n",
    "\n",
    "#### 2. History\n",
    "**Problem**: External events affect results\n",
    "**Example**: Software update during experiment\n",
    "**Solution**: Control timing, use control group\n",
    "\n",
    "#### 3. Maturation\n",
    "**Problem**: Participants change over time\n",
    "**Example**: Model improves just due to more training time\n",
    "**Solution**: Control group experiences same time\n",
    "\n",
    "#### 4. Testing Effects\n",
    "**Problem**: Being measured changes behavior\n",
    "**Example**: Model overfits to validation set from repeated testing\n",
    "**Solution**: Hold out true test set\n",
    "\n",
    "#### 5. Instrumentation\n",
    "**Problem**: Measurement tool changes\n",
    "**Example**: Updating evaluation metric mid-experiment\n",
    "**Solution**: Keep measurement consistent\n",
    "\n",
    "### External Validity Threats\n",
    "\n",
    "*Do results generalize beyond this experiment?*\n",
    "\n",
    "#### 1. Sample Bias\n",
    "**Problem**: Sample not representative\n",
    "**Example**: Only test on clean, balanced data\n",
    "**Solution**: Use diverse, realistic datasets\n",
    "\n",
    "#### 2. Setting\n",
    "**Problem**: Lab conditions differ from real world\n",
    "**Example**: Test on small datasets, deploy on massive scale\n",
    "**Solution**: Test at realistic scale\n",
    "\n",
    "#### 3. Time\n",
    "**Problem**: Results specific to time period\n",
    "**Example**: Model tested in stable market, deployed in volatile one\n",
    "**Solution**: Test across different conditions\n",
    "\n",
    "### How to Protect Validity\n",
    "\n",
    "**Checklist**:\n",
    "- â˜‘ Random assignment\n",
    "- â˜‘ Control group\n",
    "- â˜‘ Adequate sample size\n",
    "- â˜‘ Control confounding variables\n",
    "- â˜‘ Blind evaluation when possible\n",
    "- â˜‘ Replicate experiments\n",
    "- â˜‘ Test on diverse datasets\n",
    "- â˜‘ Report all analyses (don't cherry-pick)\n",
    "- â˜‘ Pre-register hypotheses when possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Create Experimental Design Checklist\n",
    "# ========================================\n",
    "\n",
    "checklist = \"\"\"\n",
    "EXPERIMENTAL DESIGN CHECKLIST\n",
    "==============================\n",
    "\n",
    "PLANNING PHASE\n",
    "â–¡ Research question clearly defined\n",
    "â–¡ Hypothesis stated (including direction)\n",
    "â–¡ Independent variable(s) identified\n",
    "â–¡ Dependent variable(s) identified\n",
    "â–¡ Confounding variables identified\n",
    "â–¡ Control variables determined\n",
    "â–¡ Experimental design chosen (A/B, factorial, etc.)\n",
    "â–¡ Sample size calculated (power analysis)\n",
    "â–¡ Randomization method determined\n",
    "â–¡ Control group established\n",
    "\n",
    "EXECUTION PHASE\n",
    "â–¡ Random seed set (for reproducibility)\n",
    "â–¡ Data randomized properly\n",
    "â–¡ Groups balanced on key variables\n",
    "â–¡ Control variables held constant\n",
    "â–¡ Blinding implemented (if applicable)\n",
    "â–¡ Data collection consistent across groups\n",
    "â–¡ All procedures documented\n",
    "â–¡ Unexpected events recorded\n",
    "\n",
    "ANALYSIS PHASE\n",
    "â–¡ Check assumptions (normality, etc.)\n",
    "â–¡ Appropriate statistical test chosen\n",
    "â–¡ Multiple comparison correction (if needed)\n",
    "â–¡ Effect sizes reported (not just p-values)\n",
    "â–¡ Confidence intervals included\n",
    "â–¡ All analyses reported (not selective)\n",
    "â–¡ Sensitivity analyses conducted\n",
    "\n",
    "VALIDITY CHECKS\n",
    "â–¡ Internal validity threats addressed\n",
    "â–¡ External validity considered\n",
    "â–¡ Generalizability discussed\n",
    "â–¡ Limitations acknowledged\n",
    "â–¡ Alternative explanations considered\n",
    "\n",
    "REPORTING\n",
    "â–¡ Methods described in detail\n",
    "â–¡ Sample size justified\n",
    "â–¡ Randomization described\n",
    "â–¡ All conditions reported\n",
    "â–¡ Negative results included\n",
    "â–¡ Code/data shared (when possible)\n",
    "â–¡ Reproducibility information provided\n",
    "\"\"\"\n",
    "\n",
    "# Save checklist\n",
    "with open(f\"{output_dir}/experimental_design_checklist.txt\", \"w\") as f:\n",
    "    f.write(checklist)\n",
    "\n",
    "print(checklist)\n",
    "print(f\"\\nâœ… Checklist saved to: {output_dir}/experimental_design_checklist.txt\")\n",
    "print(\"\\nUse this checklist for every experiment you design!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "Congratulations on completing Module 04!\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "âœ… **Variables**: Understand IV (manipulate), DV (measure), confounds (control)\n",
    "\n",
    "âœ… **Design Principles**: Randomization, control groups, replication, adequate sample size\n",
    "\n",
    "âœ… **Common Designs**: A/B tests, factorial, before-after, crossover, stratified\n",
    "\n",
    "âœ… **Statistical Power**: Calculate sample size before collecting data\n",
    "\n",
    "âœ… **Validity**: Protect against internal and external validity threats\n",
    "\n",
    "### What You Can Do Now\n",
    "\n",
    "- Identify and control variables in experiments\n",
    "- Choose appropriate experimental designs\n",
    "- Calculate required sample sizes\n",
    "- Design factorial experiments to test interactions\n",
    "- Recognize and address threats to validity\n",
    "\n",
    "### Practice Exercise\n",
    "\n",
    "**Exercise**: Design Your Own Experiment\n",
    "\n",
    "Using your research question from Module 03:\n",
    "\n",
    "1. Identify your IV, DV, and potential confounds\n",
    "2. Choose an appropriate experimental design\n",
    "3. List control variables\n",
    "4. Calculate required sample size\n",
    "5. Fill out the experimental design checklist\n",
    "6. Identify potential threats to validity\n",
    "\n",
    "Save this - you'll implement it in Module 09!\n",
    "\n",
    "---\n",
    "\n",
    "### Up Next\n",
    "\n",
    "In **Module 05: Data Collection Methods**, you'll learn:\n",
    "- Primary vs secondary data\n",
    "- Data collection techniques\n",
    "- Ensuring data quality\n",
    "- Data validation strategies\n",
    "- Documentation during collection\n",
    "\n",
    "---\n",
    "\n",
    "**Ready to continue?** Move on to `05_data_collection_methods.ipynb`!\n",
    "\n",
    "**Need to review?** Go back to any challenging section."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
