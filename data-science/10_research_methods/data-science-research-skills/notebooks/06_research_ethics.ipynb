{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 06: Research Ethics\n",
    "\n",
    "Welcome to Module 06! Ethics isn't just a checkbox - it's **fundamental to responsible research**. Let's learn how to conduct ethical research in data science.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- Core ethical principles (Belmont Report)\n",
    "- Privacy and confidentiality in data science\n",
    "- Informed consent and participant rights\n",
    "- Identifying and addressing bias\n",
    "- Fairness in AI/ML systems\n",
    "- Responsible data use and sharing\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed Modules 00-05\n",
    "- Awareness of social impact of data science\n",
    "\n",
    "## Time Required\n",
    "\n",
    "**30 minutes**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Setup\n",
    "# ========================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create output directory\n",
    "output_dir = \"outputs/notebook_06\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(\"âœ… Setup complete!\")\n",
    "print(f\"Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Core Ethical Principles\n",
    "\n",
    "### The Belmont Report (1979)\n",
    "\n",
    "Three foundational principles for ethical research with human subjects:\n",
    "\n",
    "#### 1. Respect for Persons\n",
    "\n",
    "**What it means**:\n",
    "- Treat people as autonomous agents capable of making their own decisions\n",
    "- Protect those with diminished autonomy (children, prisoners, etc.)\n",
    "- Individuals should have choice about participating\n",
    "\n",
    "**In Data Science**:\n",
    "- âœ… Obtain informed consent before collecting personal data\n",
    "- âœ… Allow users to opt-out\n",
    "- âœ… Respect privacy preferences\n",
    "- âœ… Don't manipulate or deceive\n",
    "\n",
    "**Examples**:\n",
    "- **Good**: \"We'd like to use your data for research. Click here to learn more and consent.\"\n",
    "- **Bad**: Scraping personal data without permission\n",
    "\n",
    "#### 2. Beneficence\n",
    "\n",
    "**What it means**:\n",
    "- Maximize benefits\n",
    "- Minimize harms\n",
    "- **Do no harm** (primum non nocere)\n",
    "\n",
    "**In Data Science**:\n",
    "- âœ… Consider potential harms of your research\n",
    "- âœ… Benefits should outweigh risks\n",
    "- âœ… Monitor for unintended consequences\n",
    "- âœ… Have mechanisms to address harms\n",
    "\n",
    "**Examples**:\n",
    "- **Good**: Testing medical AI extensively before deployment\n",
    "- **Bad**: Deploying undertested facial recognition in high-stakes settings\n",
    "\n",
    "**Types of Harm to Consider**:\n",
    "- Physical harm\n",
    "- Psychological harm (stress, anxiety)\n",
    "- Social harm (discrimination, stigma)\n",
    "- Economic harm (job loss, denied services)\n",
    "- Legal harm (wrongful accusation)\n",
    "\n",
    "#### 3. Justice\n",
    "\n",
    "**What it means**:\n",
    "- Fair distribution of benefits and burdens of research\n",
    "- Don't exploit vulnerable populations\n",
    "- Those who bear risks should also receive benefits\n",
    "\n",
    "**In Data Science**:\n",
    "- âœ… Ensure diverse representation in data\n",
    "- âœ… Test models across demographic groups\n",
    "- âœ… Don't use vulnerable populations unfairly\n",
    "- âœ… Consider who benefits and who bears costs\n",
    "\n",
    "**Examples**:\n",
    "- **Bad**: Train on data from wealthy patients, deploy in poor communities\n",
    "- **Good**: Ensure training data represents all groups who will use the system\n",
    "\n",
    "### Additional Ethical Principles\n",
    "\n",
    "#### 4. Transparency\n",
    "- Be open about methods, data sources, limitations\n",
    "- Explain how models make decisions\n",
    "- Disclose conflicts of interest\n",
    "\n",
    "#### 5. Accountability\n",
    "- Take responsibility for your research impacts\n",
    "- Have mechanisms for addressing harms\n",
    "- Document decisions and rationale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Privacy and Confidentiality\n",
    "\n",
    "### Privacy vs Confidentiality\n",
    "\n",
    "**Privacy**: Individuals' right to control their personal information\n",
    "\n",
    "**Confidentiality**: Researchers' duty to protect the information they collect\n",
    "\n",
    "### Key Privacy Principles\n",
    "\n",
    "#### 1. Data Minimization\n",
    "**Principle**: Collect only what you need\n",
    "\n",
    "**Questions to ask**:\n",
    "- Do I really need this data?\n",
    "- Can I use aggregated/anonymized data instead?\n",
    "- How long do I need to keep it?\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "# Bad: Collect everything\n",
    "user_data = ['name', 'email', 'address', 'phone', 'SSN', \n",
    "             'credit_card', 'browsing_history', ...]\n",
    "\n",
    "# Good: Collect only what's needed for research\n",
    "user_data = ['age_range', 'region', 'purchase_frequency']\n",
    "```\n",
    "\n",
    "#### 2. Anonymization\n",
    "\n",
    "**Goal**: Remove personally identifiable information (PII)\n",
    "\n",
    "**PII includes**:\n",
    "- Names\n",
    "- Email addresses\n",
    "- Phone numbers\n",
    "- Social security numbers\n",
    "- IP addresses\n",
    "- Exact locations\n",
    "- Photos of faces\n",
    "\n",
    "**Techniques**:\n",
    "```python\n",
    "# Remove direct identifiers\n",
    "df = df.drop(columns=['name', 'email', 'ssn'])\n",
    "\n",
    "# Replace with pseudonyms\n",
    "df['user_id'] = df['user_id'].apply(lambda x: hashlib.sha256(x.encode()).hexdigest())\n",
    "\n",
    "# Generalize data\n",
    "df['age'] = pd.cut(df['age'], bins=[0, 18, 30, 50, 65, 100], \n",
    "                   labels=['<18', '18-30', '30-50', '50-65', '65+'])\n",
    "\n",
    "# Add noise (differential privacy)\n",
    "df['income'] += np.random.laplace(0, 1000, len(df))\n",
    "```\n",
    "\n",
    "**Warning**: Anonymization is HARD!\n",
    "- Even \"anonymized\" data can be re-identified\n",
    "- Combination of attributes can be unique\n",
    "- Example: ZIP code + birth date + gender identifies 87% of US population\n",
    "\n",
    "#### 3. Secure Storage\n",
    "\n",
    "**Best Practices**:\n",
    "- âœ… Encrypt data at rest and in transit\n",
    "- âœ… Use access controls (who can see what)\n",
    "- âœ… Regular backups\n",
    "- âœ… Secure disposal when done\n",
    "- âœ… Audit logs\n",
    "\n",
    "```python\n",
    "# Example: Basic encryption\n",
    "from cryptography.fernet import Fernet\n",
    "\n",
    "# Generate key (store securely!)\n",
    "key = Fernet.generate_key()\n",
    "cipher = Fernet(key)\n",
    "\n",
    "# Encrypt sensitive data\n",
    "encrypted = cipher.encrypt(b\"Sensitive data\")\n",
    "\n",
    "# Decrypt when needed\n",
    "decrypted = cipher.decrypt(encrypted)\n",
    "```\n",
    "\n",
    "#### 4. Data Retention\n",
    "\n",
    "**Principle**: Don't keep data longer than necessary\n",
    "\n",
    "**Questions**:\n",
    "- How long do I need this data?\n",
    "- What are legal requirements?\n",
    "- When and how will I delete it?\n",
    "\n",
    "**Create a retention policy**:\n",
    "```\n",
    "Raw data: Delete after 30 days\n",
    "Processed data: Keep for duration of study\n",
    "Published results: Keep indefinitely (but anonymized)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Informed Consent\n",
    "\n",
    "### What is Informed Consent?\n",
    "\n",
    "Participants must:\n",
    "1. **Understand** what they're agreeing to\n",
    "2. **Voluntarily** agree to participate\n",
    "3. Have **capacity** to consent\n",
    "\n",
    "### Elements of Informed Consent\n",
    "\n",
    "#### 1. Disclosure\n",
    "Tell participants:\n",
    "- **Purpose**: What is the research about?\n",
    "- **Procedures**: What will happen to their data?\n",
    "- **Risks**: What could go wrong?\n",
    "- **Benefits**: What might they gain?\n",
    "- **Alternatives**: What are their options?\n",
    "- **Privacy**: How will data be protected?\n",
    "- **Voluntary**: They can refuse or withdraw\n",
    "\n",
    "#### 2. Comprehension\n",
    "Ensure they understand:\n",
    "- Use plain language (no jargon)\n",
    "- Appropriate reading level\n",
    "- Visual aids if helpful\n",
    "- Opportunity to ask questions\n",
    "\n",
    "#### 3. Voluntariness\n",
    "No coercion or undue influence:\n",
    "- Free to decline\n",
    "- Can withdraw at any time\n",
    "- No penalties for refusing\n",
    "\n",
    "### Consent Template Example\n",
    "\n",
    "```\n",
    "RESEARCH CONSENT FORM\n",
    "\n",
    "Study Title: [Your Study]\n",
    "Researcher: [Your Name]\n",
    "\n",
    "PURPOSE:\n",
    "We are researching [clear description]. This will help [benefit].\n",
    "\n",
    "WHAT YOU'LL DO:\n",
    "- We will collect [specific data]\n",
    "- This will take approximately [time]\n",
    "- Your data will be used for [purposes]\n",
    "\n",
    "RISKS:\n",
    "Possible risks include [list risks, or \"minimal risk\"]\n",
    "\n",
    "BENEFITS:\n",
    "[Direct benefits, or \"no direct benefit but may help others\"]\n",
    "\n",
    "PRIVACY:\n",
    "- Your data will be [anonymized/confidential]\n",
    "- Stored [how and where]\n",
    "- Accessed only by [who]\n",
    "- Deleted [when]\n",
    "\n",
    "VOLUNTARY:\n",
    "Participation is completely voluntary. You may:\n",
    "- Decline to participate\n",
    "- Withdraw at any time\n",
    "- Skip any questions\n",
    "\n",
    "QUESTIONS:\n",
    "Contact [researcher] at [email] with questions.\n",
    "\n",
    "CONSENT:\n",
    "â˜ I have read and understood this form\n",
    "â˜ I have had opportunity to ask questions\n",
    "â˜ I voluntarily agree to participate\n",
    "\n",
    "Signature: _________________ Date: _______\n",
    "```\n",
    "\n",
    "### Special Cases\n",
    "\n",
    "#### When is consent NOT required?\n",
    "- Publicly available data\n",
    "- Fully anonymized data\n",
    "- Certain educational settings\n",
    "- Some observational studies\n",
    "\n",
    "**But**: Check with your institution's IRB!\n",
    "\n",
    "#### Vulnerable Populations\n",
    "Extra protections needed for:\n",
    "- Children (get parent/guardian consent)\n",
    "- Prisoners\n",
    "- Pregnant women\n",
    "- Cognitively impaired individuals\n",
    "- Economically disadvantaged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Bias and Fairness\n",
    "\n",
    "### Types of Bias in Data Science\n",
    "\n",
    "#### 1. Historical Bias\n",
    "\n",
    "**What**: Bias already present in the world gets encoded in data\n",
    "\n",
    "**Example**: \n",
    "- Hiring data shows men in senior roles â†’ Model learns to prefer men\n",
    "- Historical loan approvals favor certain demographics â†’ Model perpetuates discrimination\n",
    "\n",
    "**Solution**: \n",
    "- Recognize historical inequalities\n",
    "- Don't blindly optimize for historical patterns\n",
    "- Consider fairness constraints\n",
    "\n",
    "#### 2. Representation Bias\n",
    "\n",
    "**What**: Data doesn't represent the full population\n",
    "\n",
    "**Example**:\n",
    "- Facial recognition trained mostly on light-skinned faces\n",
    "- Medical AI trained only on data from one hospital\n",
    "- NLP models trained on English-only text\n",
    "\n",
    "**Solution**:\n",
    "- Ensure diverse, representative data\n",
    "- Report demographic breakdown of data\n",
    "- Test performance across groups\n",
    "\n",
    "#### 3. Measurement Bias\n",
    "\n",
    "**What**: Systematic errors in how data is collected/measured\n",
    "\n",
    "**Example**:\n",
    "- Measuring \"job performance\" only by manager ratings (subjective)\n",
    "- Using arrest rates to predict crime (arrests â‰  crimes)\n",
    "\n",
    "**Solution**:\n",
    "- Use validated measurements\n",
    "- Multiple measurement sources\n",
    "- Acknowledge measurement limitations\n",
    "\n",
    "#### 4. Aggregation Bias\n",
    "\n",
    "**What**: One model for all groups when different groups need different models\n",
    "\n",
    "**Example**:\n",
    "- Diabetes risk model trained on general population doesn't work well for specific ethnic groups\n",
    "- One-size-fits-all recommendation ignoring user diversity\n",
    "\n",
    "**Solution**:\n",
    "- Test model performance by subgroup\n",
    "- Consider group-specific models\n",
    "- Or: features that capture relevant differences\n",
    "\n",
    "#### 5. Evaluation Bias\n",
    "\n",
    "**What**: Testing on non-representative data\n",
    "\n",
    "**Example**:\n",
    "- Test set differs from deployment population\n",
    "- Only evaluating on easy/common cases\n",
    "\n",
    "**Solution**:\n",
    "- Ensure test set represents deployment setting\n",
    "- Stratified sampling\n",
    "- Report performance by subgroup\n",
    "\n",
    "### Fairness Metrics\n",
    "\n",
    "Different notions of fairness (often in tension!):\n",
    "\n",
    "#### 1. Demographic Parity\n",
    "**Definition**: Equal prediction rates across groups\n",
    "\n",
    "```python\n",
    "# Both groups should have similar positive prediction rates\n",
    "pred_rate_group_a = (predictions[group_a] == 1).mean()\n",
    "pred_rate_group_b = (predictions[group_b] == 1).mean()\n",
    "# Should be similar\n",
    "```\n",
    "\n",
    "#### 2. Equal Opportunity\n",
    "**Definition**: Equal true positive rates across groups\n",
    "\n",
    "```python\n",
    "# Among qualified people, equal chance of being predicted positive\n",
    "from sklearn.metrics import recall_score\n",
    "tpr_group_a = recall_score(y_true[group_a], predictions[group_a])\n",
    "tpr_group_b = recall_score(y_true[group_b], predictions[group_b])\n",
    "# Should be similar\n",
    "```\n",
    "\n",
    "#### 3. Predictive Parity\n",
    "**Definition**: Equal precision across groups\n",
    "\n",
    "```python\n",
    "# Among those predicted positive, equal accuracy\n",
    "from sklearn.metrics import precision_score\n",
    "precision_a = precision_score(y_true[group_a], predictions[group_a])\n",
    "precision_b = precision_score(y_true[group_b], predictions[group_b])\n",
    "# Should be similar\n",
    "```\n",
    "\n",
    "**Important**: You usually CAN'T satisfy all fairness criteria at once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Fairness Analysis Example\n",
    "# ========================================\n",
    "\n",
    "# Simulate a dataset with potential bias\n",
    "np.random.seed(42)\n",
    "\n",
    "n = 1000\n",
    "group = np.random.choice([\"Group A\", \"Group B\"], n)\n",
    "\n",
    "# True outcomes (Group B has slightly higher base rate due to historical factors)\n",
    "y_true = np.where(\n",
    "    group == \"Group A\",\n",
    "    np.random.random(n) < 0.3,  # 30% positive rate\n",
    "    np.random.random(n) < 0.4,  # 40% positive rate\n",
    ").astype(int)\n",
    "\n",
    "# Biased predictions (model is less accurate for Group B)\n",
    "predictions = np.where(\n",
    "    group == \"Group A\",\n",
    "    (y_true == 1) & (np.random.random(n) > 0.1),  # 90% recall\n",
    "    (y_true == 1) & (np.random.random(n) > 0.3),  # 70% recall (biased!)\n",
    ").astype(int)\n",
    "\n",
    "# Create DataFrame\n",
    "df_fairness = pd.DataFrame({\"group\": group, \"y_true\": y_true, \"y_pred\": predictions})\n",
    "\n",
    "# Analyze fairness\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "print(\"FAIRNESS ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for grp in [\"Group A\", \"Group B\"]:\n",
    "    mask = df_fairness[\"group\"] == grp\n",
    "    y_true_grp = df_fairness.loc[mask, \"y_true\"]\n",
    "    y_pred_grp = df_fairness.loc[mask, \"y_pred\"]\n",
    "\n",
    "    print(f\"\\n{grp}:\")\n",
    "    print(f\"  Base rate (% positive in reality): {y_true_grp.mean()*100:.1f}%\")\n",
    "    print(f\"  Prediction rate (% predicted positive): {y_pred_grp.mean()*100:.1f}%\")\n",
    "    print(f\"  Precision: {precision_score(y_true_grp, y_pred_grp):.3f}\")\n",
    "    print(f\"  Recall (TPR): {recall_score(y_true_grp, y_pred_grp):.3f}\")\n",
    "    print(f\"  F1-Score: {f1_score(y_true_grp, y_pred_grp):.3f}\")\n",
    "\n",
    "# Calculate disparities\n",
    "group_a_recall = recall_score(\n",
    "    df_fairness.loc[df_fairness[\"group\"] == \"Group A\", \"y_true\"],\n",
    "    df_fairness.loc[df_fairness[\"group\"] == \"Group A\", \"y_pred\"],\n",
    ")\n",
    "group_b_recall = recall_score(\n",
    "    df_fairness.loc[df_fairness[\"group\"] == \"Group B\", \"y_true\"],\n",
    "    df_fairness.loc[df_fairness[\"group\"] == \"Group B\", \"y_pred\"],\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"\\nâš ï¸ FAIRNESS ISSUE DETECTED:\")\n",
    "print(f\"Recall disparity: {abs(group_a_recall - group_b_recall)*100:.1f} percentage points\")\n",
    "print(f\"\\nGroup B has {((group_a_recall - group_b_recall)/group_b_recall)*100:.1f}% lower recall\")\n",
    "print(\"\\nThis means qualified members of Group B are less likely to be\")\n",
    "print(\"identified as positive compared to Group A!\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot 1: Recall by group\n",
    "groups = [\"Group A\", \"Group B\"]\n",
    "recalls = [group_a_recall, group_b_recall]\n",
    "colors = [\"green\" if r > 0.8 else \"orange\" if r > 0.7 else \"red\" for r in recalls]\n",
    "\n",
    "bars = axes[0].bar(groups, recalls, color=colors, alpha=0.7, edgecolor=\"black\")\n",
    "axes[0].set_ylabel(\"Recall (True Positive Rate)\", fontweight=\"bold\")\n",
    "axes[0].set_title(\"Fairness Analysis: Recall by Group\", fontweight=\"bold\")\n",
    "axes[0].set_ylim([0, 1])\n",
    "axes[0].axhline(y=0.8, color=\"gray\", linestyle=\"--\", alpha=0.5, label=\"Target (80%)\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    axes[0].text(\n",
    "        bar.get_x() + bar.get_width() / 2.0,\n",
    "        height,\n",
    "        f\"{height:.1%}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "# Plot 2: Performance metrics\n",
    "metrics = [\"Precision\", \"Recall\", \"F1-Score\"]\n",
    "group_a_metrics = [\n",
    "    precision_score(\n",
    "        df_fairness.loc[df_fairness[\"group\"] == \"Group A\", \"y_true\"],\n",
    "        df_fairness.loc[df_fairness[\"group\"] == \"Group A\", \"y_pred\"],\n",
    "    ),\n",
    "    group_a_recall,\n",
    "    f1_score(\n",
    "        df_fairness.loc[df_fairness[\"group\"] == \"Group A\", \"y_true\"],\n",
    "        df_fairness.loc[df_fairness[\"group\"] == \"Group A\", \"y_pred\"],\n",
    "    ),\n",
    "]\n",
    "group_b_metrics = [\n",
    "    precision_score(\n",
    "        df_fairness.loc[df_fairness[\"group\"] == \"Group B\", \"y_true\"],\n",
    "        df_fairness.loc[df_fairness[\"group\"] == \"Group B\", \"y_pred\"],\n",
    "    ),\n",
    "    group_b_recall,\n",
    "    f1_score(\n",
    "        df_fairness.loc[df_fairness[\"group\"] == \"Group B\", \"y_true\"],\n",
    "        df_fairness.loc[df_fairness[\"group\"] == \"Group B\", \"y_pred\"],\n",
    "    ),\n",
    "]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "axes[1].bar(x - width / 2, group_a_metrics, width, label=\"Group A\", color=\"steelblue\", alpha=0.7)\n",
    "axes[1].bar(x + width / 2, group_b_metrics, width, label=\"Group B\", color=\"coral\", alpha=0.7)\n",
    "\n",
    "axes[1].set_ylabel(\"Score\", fontweight=\"bold\")\n",
    "axes[1].set_title(\"Performance Metrics by Group\", fontweight=\"bold\")\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(metrics)\n",
    "axes[1].legend()\n",
    "axes[1].set_ylim([0, 1])\n",
    "axes[1].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{output_dir}/fairness_analysis.png\", dpi=150, bbox_inches=\"tight\")\n",
    "print(f\"\\nâœ… Fairness analysis plot saved to: {output_dir}/fairness_analysis.png\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ KEY LESSON:\")\n",
    "print(\"Always disaggregate performance metrics by demographic groups!\")\n",
    "print(\"Overall accuracy can hide serious fairness issues.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Responsible Data Use\n",
    "\n",
    "### Dual Use\n",
    "\n",
    "**Problem**: Research can be used for both beneficial and harmful purposes\n",
    "\n",
    "**Example**: \n",
    "- Facial recognition: Could help find missing persons OR enable mass surveillance\n",
    "- Natural language generation: Could help write OR create misinformation at scale\n",
    "\n",
    "**What to do**:\n",
    "- Consider potential misuse\n",
    "- Implement safeguards\n",
    "- Be transparent about limitations\n",
    "- Sometimes: Don't publish certain details\n",
    "\n",
    "### Data Sharing\n",
    "\n",
    "**Benefits of sharing**:\n",
    "- Reproducibility\n",
    "- Enables others to build on your work\n",
    "- Scientific progress\n",
    "\n",
    "**Risks**:\n",
    "- Privacy violations\n",
    "- Misuse\n",
    "- Re-identification\n",
    "\n",
    "**Best practices**:\n",
    "```\n",
    "âœ… Share when safe and beneficial\n",
    "âœ… Anonymize thoroughly\n",
    "âœ… Use data use agreements\n",
    "âœ… Provide clear documentation\n",
    "âœ… Consider tiered access\n",
    "âŒ Don't share sensitive raw data publicly\n",
    "âŒ Don't share without permission\n",
    "```\n",
    "\n",
    "### Algorithmic Transparency\n",
    "\n",
    "**Why it matters**: People affected by algorithms deserve to understand them\n",
    "\n",
    "**Levels of transparency**:\n",
    "1. **Black box**: No information\n",
    "2. **Model transparency**: Share model type, features\n",
    "3. **Process transparency**: Explain how decisions are made\n",
    "4. **Individual transparency**: Explain specific predictions\n",
    "\n",
    "**Example**: Credit scoring\n",
    "- **Bad**: \"Computer says no\" (no explanation)\n",
    "- **Better**: \"Denied due to high debt-to-income ratio\"\n",
    "- **Best**: \"Denied. Reducing debt by $X would likely change outcome\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Create Ethics Checklist\n",
    "# ========================================\n",
    "\n",
    "ethics_checklist = \"\"\"\n",
    "RESEARCH ETHICS CHECKLIST\n",
    "=========================\n",
    "\n",
    "BEFORE STARTING RESEARCH\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "â–¡ Research question is ethically sound\n",
    "â–¡ Benefits outweigh risks\n",
    "â–¡ IRB approval obtained (if required)\n",
    "â–¡ Funding sources disclosed\n",
    "â–¡ Conflicts of interest identified\n",
    "\n",
    "RESPECT FOR PERSONS\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "â–¡ Informed consent process designed\n",
    "â–¡ Consent forms use plain language\n",
    "â–¡ Participants can withdraw at any time\n",
    "â–¡ Extra protections for vulnerable populations\n",
    "â–¡ Deception minimized (or justified and debriefed)\n",
    "\n",
    "DATA PRIVACY\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "â–¡ Collect only necessary data (data minimization)\n",
    "â–¡ PII removed or anonymized\n",
    "â–¡ Data stored securely (encryption, access controls)\n",
    "â–¡ Data retention policy established\n",
    "â–¡ Secure disposal plan in place\n",
    "â–¡ Privacy impact assessment conducted\n",
    "\n",
    "FAIRNESS & BIAS\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "â–¡ Training data is representative\n",
    "â–¡ Data collection methods are fair\n",
    "â–¡ Tested for bias across demographic groups\n",
    "â–¡ Fairness metrics calculated\n",
    "â–¡ Disparate impact assessed\n",
    "â–¡ Mitigation strategies for identified biases\n",
    "\n",
    "BENEFICENCE (DO NO HARM)\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "â–¡ Potential harms identified\n",
    "â–¡ Benefits clearly outweigh risks\n",
    "â–¡ Safeguards against harm implemented\n",
    "â–¡ Monitoring plan for unintended consequences\n",
    "â–¡ Process for addressing harms if they occur\n",
    "\n",
    "JUSTICE\n",
    "â”€â”€â”€â”€â”€â”€â”€\n",
    "â–¡ Burdens and benefits fairly distributed\n",
    "â–¡ Not exploiting vulnerable populations\n",
    "â–¡ Diverse representation in data\n",
    "â–¡ Benefits accessible to all groups\n",
    "â–¡ No systematic exclusion of groups\n",
    "\n",
    "TRANSPARENCY\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "â–¡ Methods clearly documented\n",
    "â–¡ Data sources disclosed\n",
    "â–¡ Limitations acknowledged\n",
    "â–¡ Assumptions stated\n",
    "â–¡ Conflicts of interest disclosed\n",
    "â–¡ Model decisions explainable\n",
    "\n",
    "ACCOUNTABILITY\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "â–¡ Responsibility for decisions clear\n",
    "â–¡ Audit trail maintained\n",
    "â–¡ Process for appeals/corrections\n",
    "â–¡ Regular reviews planned\n",
    "â–¡ Mechanisms for accountability\n",
    "\n",
    "DATA SHARING\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "â–¡ Data sharing plan created\n",
    "â–¡ Privacy protections verified\n",
    "â–¡ Data use agreements in place\n",
    "â–¡ Documentation provided\n",
    "â–¡ Appropriate access controls\n",
    "\n",
    "PUBLICATION & COMMUNICATION\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "â–¡ Results reported honestly\n",
    "â–¡ Negative results included\n",
    "â–¡ Limitations clearly stated\n",
    "â–¡ Dual use concerns considered\n",
    "â–¡ Accessible summaries for affected communities\n",
    "\n",
    "SPECIFIC TO AI/ML\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "â–¡ Model tested on diverse populations\n",
    "â–¡ Performance metrics disaggregated by group\n",
    "â–¡ Failure modes identified\n",
    "â–¡ Human oversight maintained\n",
    "â–¡ Regular retraining/monitoring planned\n",
    "\n",
    "POST-RESEARCH\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "â–¡ Results shared with participants (if appropriate)\n",
    "â–¡ Data properly archived or destroyed\n",
    "â–¡ Lessons learned documented\n",
    "â–¡ Follow-up for long-term impacts\n",
    "\"\"\"\n",
    "\n",
    "# Save checklist\n",
    "with open(f\"{output_dir}/ethics_checklist.txt\", \"w\") as f:\n",
    "    f.write(ethics_checklist)\n",
    "\n",
    "print(ethics_checklist)\n",
    "print(f\"\\nâœ… Checklist saved to: {output_dir}/ethics_checklist.txt\")\n",
    "print(\"\\nUse this checklist for EVERY research project!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "Congratulations on completing Module 06!\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "âœ… **Belmont Principles**: Respect for persons, Beneficence, Justice\n",
    "\n",
    "âœ… **Privacy**: Data minimization, anonymization, secure storage, retention policies\n",
    "\n",
    "âœ… **Informed Consent**: Disclosure, comprehension, voluntariness\n",
    "\n",
    "âœ… **Bias**: Historical, representation, measurement, aggregation, evaluation\n",
    "\n",
    "âœ… **Fairness**: Multiple definitions (demographic parity, equal opportunity, etc.)\n",
    "\n",
    "âœ… **Responsibility**: Consider dual use, share data responsibly, be transparent\n",
    "\n",
    "### What You Can Do Now\n",
    "\n",
    "- Apply ethical principles to research design\n",
    "- Protect participant privacy and confidentiality\n",
    "- Obtain proper informed consent\n",
    "- Identify and measure bias in data and models\n",
    "- Assess fairness across demographic groups\n",
    "- Make responsible decisions about data use and sharing\n",
    "\n",
    "### Practice Exercise\n",
    "\n",
    "**Exercise**: Ethics Review\n",
    "\n",
    "For your research project from previous modules:\n",
    "\n",
    "1. Go through the ethics checklist\n",
    "2. Identify potential ethical issues\n",
    "3. Design mitigation strategies\n",
    "4. Create an informed consent form (if needed)\n",
    "5. Plan fairness evaluation\n",
    "6. Document privacy protections\n",
    "\n",
    "Save this - it's essential for Module 09!\n",
    "\n",
    "---\n",
    "\n",
    "### Up Next\n",
    "\n",
    "In **Module 07: Reproducible Research**, you'll learn:\n",
    "- What reproducibility means and why it matters\n",
    "- Setting up reproducible environments\n",
    "- Code organization best practices\n",
    "- Data version control\n",
    "- Dependency management\n",
    "\n",
    "---\n",
    "\n",
    "**Ready to continue?** Move on to `07_reproducible_research.ipynb`!\n",
    "\n",
    "**Need to review?** Ethics is crucial - take time to understand it fully."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
