{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 05: Data Collection Methods\n",
    "\n",
    "Welcome to Module 05! Now that you know how to design experiments, let's learn how to **collect high-quality data** to support your research.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- Difference between primary and secondary data\n",
    "- Data collection techniques for different research types\n",
    "- Ensuring data quality (the 6 dimensions)\n",
    "- Data validation strategies\n",
    "- Documentation during collection\n",
    "- Common pitfalls and how to avoid them\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed Modules 00-04\n",
    "- Understanding of your research design\n",
    "\n",
    "## Time Required\n",
    "\n",
    "**35 minutes**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Setup\n",
    "# ========================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# Create output directory\n",
    "output_dir = \"outputs/notebook_05\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")\n",
    "print(f\"Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Primary vs Secondary Data\n",
    "\n",
    "Understanding the source of your data is crucial for research design.\n",
    "\n",
    "### Primary Data\n",
    "\n",
    "**Definition**: Data YOU collect directly for YOUR research\n",
    "\n",
    "**Characteristics**:\n",
    "- Original collection\n",
    "- Tailored to your research question\n",
    "- You control quality\n",
    "- Time-consuming and expensive\n",
    "\n",
    "**Examples in Data Science**:\n",
    "- Running experiments with ML models\n",
    "- Conducting user surveys\n",
    "- Collecting sensor data\n",
    "- Recording user interactions\n",
    "- Creating labeled datasets\n",
    "- Running A/B tests\n",
    "\n",
    "**Pros**:\n",
    "- ‚úÖ Exactly what you need\n",
    "- ‚úÖ You know the quality\n",
    "- ‚úÖ Full control over collection\n",
    "- ‚úÖ Can collect missing variables\n",
    "\n",
    "**Cons**:\n",
    "- ‚ùå Time-consuming\n",
    "- ‚ùå Expensive\n",
    "- ‚ùå Requires expertise\n",
    "- ‚ùå May have sampling limitations\n",
    "\n",
    "### Secondary Data\n",
    "\n",
    "**Definition**: Data collected by OTHERS for different purposes\n",
    "\n",
    "**Characteristics**:\n",
    "- Pre-existing\n",
    "- Not designed for your specific question\n",
    "- Quality varies\n",
    "- Usually faster and cheaper\n",
    "\n",
    "**Examples in Data Science**:\n",
    "- Public datasets (Kaggle, UCI ML Repository)\n",
    "- Government data (census, economic indicators)\n",
    "- Published datasets from papers\n",
    "- Company databases\n",
    "- Web-scraped data\n",
    "- API data (Twitter, financial markets)\n",
    "\n",
    "**Pros**:\n",
    "- ‚úÖ Fast and cheap\n",
    "- ‚úÖ Large sample sizes often available\n",
    "- ‚úÖ Historical data\n",
    "- ‚úÖ Professional collection\n",
    "\n",
    "**Cons**:\n",
    "- ‚ùå May not match your needs exactly\n",
    "- ‚ùå Quality unknown or variable\n",
    "- ‚ùå Missing variables you need\n",
    "- ‚ùå Potential bias from original purpose\n",
    "\n",
    "### Which Should You Use?\n",
    "\n",
    "**Use Primary Data When**:\n",
    "- No existing data available\n",
    "- Need specific variables\n",
    "- Quality control is critical\n",
    "- Testing new methods/algorithms\n",
    "\n",
    "**Use Secondary Data When**:\n",
    "- Good datasets exist\n",
    "- Limited time/budget\n",
    "- Need historical data\n",
    "- Replicating previous studies\n",
    "\n",
    "**Best Practice**: Often use BOTH!\n",
    "- Secondary data for exploration\n",
    "- Primary data for confirmation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Decision Tree: Primary vs Secondary Data\n",
    "# ========================================\n",
    "\n",
    "decision_tree = \"\"\"\n",
    "DATA SOURCE DECISION TREE\n",
    "=========================\n",
    "\n",
    "START: Do you need data for your research?\n",
    "           ‚Üì\n",
    "    Does suitable data already exist?\n",
    "           ‚Üì\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "   YES            NO\n",
    "    ‚Üì              ‚Üì\n",
    "Is it accessible?  Must collect\n",
    "    ‚Üì              PRIMARY DATA\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îê             ‚Üì\n",
    "YES    NO          Methods:\n",
    " ‚Üì      ‚Üì          - Experiments\n",
    "Is    Must         - Surveys  \n",
    "quality collect    - Observations\n",
    "good? PRIMARY      - Sensors\n",
    " ‚Üì      DATA\n",
    "‚îå‚î¥‚îê\n",
    "YES  NO\n",
    " ‚Üì    ‚Üì\n",
    "Use  Collect\n",
    "SECONDARY  PRIMARY\n",
    "DATA       DATA\n",
    "\n",
    "DECISION FACTORS:\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "‚ñ° Research question specificity\n",
    "‚ñ° Time constraints  \n",
    "‚ñ° Budget\n",
    "‚ñ° Required quality\n",
    "‚ñ° Sample size needed\n",
    "‚ñ° Variable requirements\n",
    "\"\"\"\n",
    "\n",
    "print(decision_tree)\n",
    "\n",
    "# Save decision tree\n",
    "with open(f\"{output_dir}/data_source_decision.txt\", \"w\") as f:\n",
    "    f.write(decision_tree)\n",
    "\n",
    "print(f\"\\n‚úÖ Decision tree saved to: {output_dir}/data_source_decision.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Data Collection Techniques\n",
    "\n",
    "Different research questions require different collection methods.\n",
    "\n",
    "### For Quantitative Research\n",
    "\n",
    "#### 1. Experiments\n",
    "\n",
    "**What**: Manipulate IV, measure DV under controlled conditions\n",
    "\n",
    "**Example in Data Science**:\n",
    "```python\n",
    "# Test different algorithms\n",
    "for algorithm in [RandomForest, XGBoost, NeuralNet]:\n",
    "    # Train on same data\n",
    "    model = algorithm.fit(X_train, y_train)\n",
    "    # Measure performance\n",
    "    accuracy = model.score(X_test, y_test)\n",
    "    # Record results\n",
    "    results.append({'algo': algorithm, 'acc': accuracy})\n",
    "```\n",
    "\n",
    "**Best for**: Testing causal relationships\n",
    "\n",
    "#### 2. Surveys\n",
    "\n",
    "**What**: Ask structured questions to many people\n",
    "\n",
    "**Question Types**:\n",
    "- **Closed**: Multiple choice, rating scales\n",
    "- **Open**: Free text responses\n",
    "\n",
    "**Example Questions**:\n",
    "- \"How satisfied are you with the model predictions?\" (1-5 scale)\n",
    "- \"How often do you use this feature?\" (Daily/Weekly/Monthly/Never)\n",
    "\n",
    "**Best for**: Gathering opinions, behaviors, demographics at scale\n",
    "\n",
    "**Tips**:\n",
    "- Keep it short (< 10 minutes)\n",
    "- Clear, unambiguous questions\n",
    "- Avoid leading questions\n",
    "- Pilot test first\n",
    "\n",
    "#### 3. Observational Studies\n",
    "\n",
    "**What**: Measure variables without manipulation\n",
    "\n",
    "**Example**: Analyze existing user behavior logs\n",
    "```python\n",
    "# Observe natural behavior\n",
    "user_logs = pd.read_csv('user_activity.csv')\n",
    "# Measure variables\n",
    "session_length = user_logs.groupby('user_id')['duration'].mean()\n",
    "churn_rate = user_logs.groupby('cohort')['churned'].mean()\n",
    "```\n",
    "\n",
    "**Best for**: When experiments are impractical or unethical\n",
    "\n",
    "#### 4. Sensor/Automated Collection\n",
    "\n",
    "**What**: Continuous automated measurement\n",
    "\n",
    "**Examples**:\n",
    "- Server logs\n",
    "- IoT sensor data\n",
    "- Click tracking\n",
    "- Model performance monitoring\n",
    "\n",
    "**Best for**: Large-scale, continuous data\n",
    "\n",
    "### For Qualitative Research\n",
    "\n",
    "#### 1. Interviews\n",
    "\n",
    "**What**: One-on-one conversations\n",
    "\n",
    "**Types**:\n",
    "- **Structured**: Fixed questions\n",
    "- **Semi-structured**: Flexible guide\n",
    "- **Unstructured**: Open conversation\n",
    "\n",
    "**Example**: Interview data scientists about algorithm selection process\n",
    "\n",
    "**Best for**: Deep understanding, exploring new areas\n",
    "\n",
    "#### 2. Focus Groups\n",
    "\n",
    "**What**: Group discussions (6-10 people)\n",
    "\n",
    "**Example**: Discuss user experience with ML system\n",
    "\n",
    "**Best for**: Generating ideas, understanding group dynamics\n",
    "\n",
    "#### 3. Case Studies\n",
    "\n",
    "**What**: In-depth study of specific instances\n",
    "\n",
    "**Example**: Detailed analysis of model deployment at Company X\n",
    "\n",
    "**Best for**: Understanding complex real-world situations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: The 6 Dimensions of Data Quality\n",
    "\n",
    "High-quality data is essential for valid research. Evaluate your data on these six dimensions:\n",
    "\n",
    "### 1. Accuracy\n",
    "\n",
    "**Definition**: Data correctly represents reality\n",
    "\n",
    "**Questions to ask**:\n",
    "- Are measurements correct?\n",
    "- Any data entry errors?\n",
    "- Are labels accurate?\n",
    "\n",
    "**How to check**:\n",
    "- Compare with ground truth (when available)\n",
    "- Look for impossible values\n",
    "- Cross-validate with other sources\n",
    "\n",
    "**Example Issues**:\n",
    "- Age = -5 (impossible)\n",
    "- Price = $999999999 (likely error)\n",
    "- Mislabeled images in training data\n",
    "\n",
    "### 2. Completeness\n",
    "\n",
    "**Definition**: All required data is present\n",
    "\n",
    "**Questions to ask**:\n",
    "- How many missing values?\n",
    "- Are they missing at random?\n",
    "- Can they be imputed?\n",
    "\n",
    "**How to check**:\n",
    "```python\n",
    "# Check missing values\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df)) * 100\n",
    "```\n",
    "\n",
    "**Thresholds**:\n",
    "- < 5% missing: Usually okay\n",
    "- 5-20% missing: Consider carefully\n",
    "- > 20% missing: May need to drop or collect more\n",
    "\n",
    "### 3. Consistency\n",
    "\n",
    "**Definition**: Data is uniform and doesn't contradict itself\n",
    "\n",
    "**Questions to ask**:\n",
    "- Same format throughout?\n",
    "- Consistent units?\n",
    "- No contradictions?\n",
    "\n",
    "**Example Issues**:\n",
    "- Dates: \"2024-01-15\" vs \"01/15/2024\" vs \"15-Jan-24\"\n",
    "- Units: Some heights in cm, others in inches\n",
    "- Contradictions: Age = 25, Birth_Year = 1980\n",
    "\n",
    "### 4. Timeliness\n",
    "\n",
    "**Definition**: Data is current and up-to-date\n",
    "\n",
    "**Questions to ask**:\n",
    "- When was data collected?\n",
    "- Is it still relevant?\n",
    "- Has the phenomenon changed?\n",
    "\n",
    "**Example**:\n",
    "- Using 2015 social media data to study 2024 trends ‚ùå\n",
    "- Stock prediction with 10-year-old data ‚ùå\n",
    "\n",
    "### 5. Validity\n",
    "\n",
    "**Definition**: Data measures what it's supposed to measure\n",
    "\n",
    "**Questions to ask**:\n",
    "- Does this measure my construct?\n",
    "- Are values in valid ranges?\n",
    "- Correct data types?\n",
    "\n",
    "**Example Checks**:\n",
    "```python\n",
    "# Age should be 0-120\n",
    "assert df['age'].between(0, 120).all()\n",
    "\n",
    "# Percentage should be 0-100\n",
    "assert df['percentage'].between(0, 100).all()\n",
    "\n",
    "# Category should be in allowed values\n",
    "valid_categories = ['A', 'B', 'C']\n",
    "assert df['category'].isin(valid_categories).all()\n",
    "```\n",
    "\n",
    "### 6. Uniqueness\n",
    "\n",
    "**Definition**: No unwanted duplicates\n",
    "\n",
    "**Questions to ask**:\n",
    "- Any duplicate records?\n",
    "- Are duplicates intentional or errors?\n",
    "\n",
    "**How to check**:\n",
    "```python\n",
    "# Find duplicates\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"Found {duplicates} duplicate rows\")\n",
    "\n",
    "# View duplicates\n",
    "df[df.duplicated(keep=False)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Data Quality Assessment Tool\n",
    "# ========================================\n",
    "\n",
    "\n",
    "def assess_data_quality(df, required_columns=None, valid_ranges=None):\n",
    "    \"\"\"\n",
    "    Comprehensive data quality assessment.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Data to assess\n",
    "        required_columns (list): Columns that must exist\n",
    "        valid_ranges (dict): Valid ranges for numeric columns\n",
    "\n",
    "    Returns:\n",
    "        dict: Quality assessment report\n",
    "    \"\"\"\n",
    "    report = {\"total_rows\": len(df), \"total_columns\": len(df.columns), \"dimensions\": {}}\n",
    "\n",
    "    # 1. COMPLETENESS\n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = (missing / len(df)) * 100\n",
    "    report[\"dimensions\"][\"completeness\"] = {\n",
    "        \"missing_values\": missing.to_dict(),\n",
    "        \"missing_pct\": missing_pct.to_dict(),\n",
    "        \"worst_column\": missing_pct.idxmax(),\n",
    "        \"worst_pct\": missing_pct.max(),\n",
    "    }\n",
    "\n",
    "    # 2. UNIQUENESS\n",
    "    duplicates = df.duplicated().sum()\n",
    "    report[\"dimensions\"][\"uniqueness\"] = {\n",
    "        \"duplicate_rows\": duplicates,\n",
    "        \"duplicate_pct\": (duplicates / len(df)) * 100,\n",
    "    }\n",
    "\n",
    "    # 3. VALIDITY (if ranges provided)\n",
    "    if valid_ranges:\n",
    "        validity_issues = []\n",
    "        for col, (min_val, max_val) in valid_ranges.items():\n",
    "            if col in df.columns:\n",
    "                invalid = ~df[col].between(min_val, max_val)\n",
    "                invalid_count = invalid.sum()\n",
    "                if invalid_count > 0:\n",
    "                    validity_issues.append(\n",
    "                        {\n",
    "                            \"column\": col,\n",
    "                            \"invalid_count\": invalid_count,\n",
    "                            \"expected_range\": f\"{min_val}-{max_val}\",\n",
    "                        }\n",
    "                    )\n",
    "        report[\"dimensions\"][\"validity\"] = validity_issues\n",
    "\n",
    "    # OVERALL QUALITY SCORE\n",
    "    scores = []\n",
    "\n",
    "    # Completeness score (100 if < 5% missing)\n",
    "    avg_missing = missing_pct.mean()\n",
    "    completeness_score = max(0, 100 - avg_missing * 2)\n",
    "    scores.append(completeness_score)\n",
    "\n",
    "    # Uniqueness score\n",
    "    uniqueness_score = 100 - (duplicates / len(df)) * 100\n",
    "    scores.append(uniqueness_score)\n",
    "\n",
    "    report[\"overall_quality_score\"] = np.mean(scores)\n",
    "\n",
    "    return report\n",
    "\n",
    "\n",
    "# Example: Generate sample data with quality issues\n",
    "np.random.seed(42)\n",
    "sample_data = pd.DataFrame(\n",
    "    {\n",
    "        \"id\": range(1, 101),\n",
    "        \"age\": np.random.randint(18, 80, 100),\n",
    "        \"income\": np.random.randint(20000, 100000, 100),\n",
    "        \"score\": np.random.uniform(0, 100, 100),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Introduce quality issues\n",
    "sample_data.loc[5, \"age\"] = np.nan  # Missing value\n",
    "sample_data.loc[10, \"age\"] = 150  # Invalid value\n",
    "sample_data = pd.concat([sample_data, sample_data.iloc[[0]]])  # Duplicate\n",
    "\n",
    "# Assess quality\n",
    "valid_ranges = {\"age\": (0, 120), \"income\": (0, 1000000), \"score\": (0, 100)}\n",
    "\n",
    "quality_report = assess_data_quality(sample_data, valid_ranges=valid_ranges)\n",
    "\n",
    "print(\"DATA QUALITY ASSESSMENT REPORT\")\n",
    "print(\"=\" * 60)\n",
    "print(\n",
    "    f\"\\nDataset Size: {quality_report['total_rows']} rows √ó {quality_report['total_columns']} columns\"\n",
    ")\n",
    "print(f\"\\nOverall Quality Score: {quality_report['overall_quality_score']:.1f}/100\")\n",
    "print(\"\\nüìä QUALITY DIMENSIONS:\")\n",
    "print(\"\\n1. COMPLETENESS:\")\n",
    "print(f\"   Worst column: {quality_report['dimensions']['completeness']['worst_column']}\")\n",
    "print(f\"   Missing: {quality_report['dimensions']['completeness']['worst_pct']:.1f}%\")\n",
    "print(\"\\n2. UNIQUENESS:\")\n",
    "print(f\"   Duplicates: {quality_report['dimensions']['uniqueness']['duplicate_rows']} rows\")\n",
    "print(f\"   ({quality_report['dimensions']['uniqueness']['duplicate_pct']:.1f}%)\")\n",
    "if quality_report[\"dimensions\"][\"validity\"]:\n",
    "    print(\"\\n3. VALIDITY ISSUES:\")\n",
    "    for issue in quality_report[\"dimensions\"][\"validity\"]:\n",
    "        print(f\"   {issue['column']}: {issue['invalid_count']} invalid values\")\n",
    "        print(f\"   (Expected range: {issue['expected_range']})\")\n",
    "\n",
    "# Save report\n",
    "import json\n",
    "\n",
    "with open(f\"{output_dir}/quality_report.json\", \"w\") as f:\n",
    "    json.dump(quality_report, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úÖ Quality report saved to: {output_dir}/quality_report.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Data Validation Strategies\n",
    "\n",
    "Don't just collect data - **validate** it!\n",
    "\n",
    "### Validation Checks to Implement\n",
    "\n",
    "#### 1. Range Checks\n",
    "```python\n",
    "# Ensure values within expected range\n",
    "def validate_range(df, column, min_val, max_val):\n",
    "    invalid = ~df[column].between(min_val, max_val)\n",
    "    if invalid.any():\n",
    "        print(f\"‚ö†Ô∏è {invalid.sum()} invalid values in {column}\")\n",
    "        print(f\"Expected: {min_val} to {max_val}\")\n",
    "        print(f\"Found: {df.loc[invalid, column].values}\")\n",
    "    return ~invalid\n",
    "```\n",
    "\n",
    "#### 2. Type Checks\n",
    "```python\n",
    "# Ensure correct data types\n",
    "def validate_types(df, type_map):\n",
    "    for column, expected_type in type_map.items():\n",
    "        if df[column].dtype != expected_type:\n",
    "            print(f\"‚ö†Ô∏è {column}: Expected {expected_type}, got {df[column].dtype}\")\n",
    "```\n",
    "\n",
    "#### 3. Format Checks\n",
    "```python\n",
    "# Check string formats (emails, phone numbers, etc.)\n",
    "import re\n",
    "\n",
    "def validate_email(email):\n",
    "    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
    "    return bool(re.match(pattern, email))\n",
    "\n",
    "df['valid_email'] = df['email'].apply(validate_email)\n",
    "```\n",
    "\n",
    "#### 4. Consistency Checks\n",
    "```python\n",
    "# Check logical consistency\n",
    "# Example: Start date should be before end date\n",
    "invalid = df['start_date'] > df['end_date']\n",
    "if invalid.any():\n",
    "    print(f\"‚ö†Ô∏è {invalid.sum()} records with start_date after end_date\")\n",
    "```\n",
    "\n",
    "#### 5. Referential Integrity\n",
    "```python\n",
    "# Check foreign keys exist\n",
    "valid_ids = master_table['id'].unique()\n",
    "invalid = ~df['foreign_id'].isin(valid_ids)\n",
    "if invalid.any():\n",
    "    print(f\"‚ö†Ô∏è {invalid.sum()} records with invalid foreign keys\")\n",
    "```\n",
    "\n",
    "### Automated Validation Pipeline\n",
    "\n",
    "Create a validation pipeline that runs automatically:\n",
    "\n",
    "```python\n",
    "class DataValidator:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.errors = []\n",
    "    \n",
    "    def add_check(self, name, condition, message):\n",
    "        if not condition:\n",
    "            self.errors.append(f\"{name}: {message}\")\n",
    "    \n",
    "    def validate(self):\n",
    "        # Run all checks\n",
    "        self.check_required_columns()\n",
    "        self.check_data_types()\n",
    "        self.check_ranges()\n",
    "        self.check_duplicates()\n",
    "        \n",
    "        if len(self.errors) == 0:\n",
    "            print(\"‚úÖ All validation checks passed!\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"‚ùå Found {len(self.errors)} validation errors:\")\n",
    "            for error in self.errors:\n",
    "                print(f\"  - {error}\")\n",
    "            return False\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Documentation During Collection\n",
    "\n",
    "**Document EVERYTHING as you collect data!**\n",
    "\n",
    "### What to Document\n",
    "\n",
    "#### 1. Data Dictionary\n",
    "```\n",
    "Column Name  | Data Type | Description           | Valid Range | Missing Allowed?\n",
    "-------------|-----------|----------------------|-------------|------------------\n",
    "user_id      | int       | Unique user ID       | > 0         | No\n",
    "age          | int       | User age in years    | 0-120       | Yes\n",
    "signup_date  | date      | Account creation     | Any         | No\n",
    "```\n",
    "\n",
    "#### 2. Collection Metadata\n",
    "- **When**: Date/time of collection\n",
    "- **Who**: Researcher name\n",
    "- **How**: Method used\n",
    "- **Where**: Source/location\n",
    "- **Why**: Purpose\n",
    "\n",
    "#### 3. Processing Log\n",
    "```\n",
    "2024-01-15 10:00 - Collected raw data from API\n",
    "2024-01-15 10:15 - Removed 5 duplicate records\n",
    "2024-01-15 10:20 - Imputed 12 missing age values with median\n",
    "2024-01-15 10:25 - Validated all records\n",
    "```\n",
    "\n",
    "#### 4. Quality Issues Log\n",
    "```\n",
    "Issue ID | Date       | Type        | Description              | Resolution\n",
    "---------|------------|-------------|--------------------------|------------\n",
    "001      | 2024-01-15 | Missing     | 10% of ages missing      | Imputed with median\n",
    "002      | 2024-01-15 | Invalid     | 3 ages > 120             | Removed as outliers\n",
    "003      | 2024-01-16 | Duplicate   | 5 duplicate user IDs     | Kept most recent\n",
    "```\n",
    "\n",
    "### Version Control for Data\n",
    "\n",
    "Track changes to your datasets:\n",
    "```\n",
    "data/\n",
    "‚îú‚îÄ‚îÄ v1.0_raw_collected_2024-01-15.csv\n",
    "‚îú‚îÄ‚îÄ v1.1_cleaned_2024-01-16.csv\n",
    "‚îú‚îÄ‚îÄ v1.2_validated_2024-01-17.csv\n",
    "‚îî‚îÄ‚îÄ CHANGELOG.md\n",
    "```\n",
    "\n",
    "### CHANGELOG.md Example\n",
    "```markdown\n",
    "# Data Changelog\n",
    "\n",
    "## v1.2 - 2024-01-17\n",
    "- Validated all records\n",
    "- Added data quality flags\n",
    "\n",
    "## v1.1 - 2024-01-16\n",
    "- Cleaned missing values\n",
    "- Removed duplicates\n",
    "- Fixed data types\n",
    "\n",
    "## v1.0 - 2024-01-15\n",
    "- Initial raw data collection\n",
    "- 1000 records from API\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Create Data Collection Documentation Template\n",
    "# ========================================\n",
    "\n",
    "documentation_template = \"\"\"\n",
    "DATA COLLECTION DOCUMENTATION\n",
    "==============================\n",
    "\n",
    "PROJECT INFORMATION\n",
    "-------------------\n",
    "Project Name: _____________________________________________\n",
    "Researcher(s): ____________________________________________\n",
    "Date Started: _____________________________________________\n",
    "Date Completed: ___________________________________________\n",
    "\n",
    "DATA SOURCE\n",
    "-----------\n",
    "Type: ‚ñ° Primary  ‚ñ° Secondary\n",
    "Source: ___________________________________________________\n",
    "Collection Method: ________________________________________\n",
    "Access Date: ______________________________________________\n",
    "\n",
    "SAMPLE INFORMATION\n",
    "------------------\n",
    "Total Records: ____________________________________________\n",
    "Time Period: ______________________________________________\n",
    "Geographic Coverage: ______________________________________\n",
    "Sampling Method: __________________________________________\n",
    "\n",
    "DATA DICTIONARY\n",
    "---------------\n",
    "Variable Name | Type | Description | Valid Range | Missing OK?\n",
    "--------------|------|-------------|-------------|------------\n",
    "_______________|______|_____________|_____________|____________\n",
    "_______________|______|_____________|_____________|____________\n",
    "_______________|______|_____________|_____________|____________\n",
    "\n",
    "QUALITY ASSESSMENT\n",
    "------------------\n",
    "Completeness: _______ % (target: > 95%)\n",
    "Duplicates: _________ records (target: 0)\n",
    "Invalid Values: _____ records (target: 0)\n",
    "Overall Quality Score: _____ / 100\n",
    "\n",
    "PROCESSING STEPS\n",
    "----------------\n",
    "1. ________________________________________________________\n",
    "2. ________________________________________________________\n",
    "3. ________________________________________________________\n",
    "\n",
    "KNOWN ISSUES\n",
    "------------\n",
    "Issue 1: __________________________________________________\n",
    "Resolution: _______________________________________________\n",
    "\n",
    "Issue 2: __________________________________________________\n",
    "Resolution: _______________________________________________\n",
    "\n",
    "VALIDATION\n",
    "----------\n",
    "‚ñ° Range checks performed\n",
    "‚ñ° Type checks performed\n",
    "‚ñ° Duplicate checks performed\n",
    "‚ñ° Consistency checks performed\n",
    "‚ñ° Referential integrity verified\n",
    "\n",
    "ETHICAL CONSIDERATIONS\n",
    "----------------------\n",
    "IRB Approval: ‚ñ° Yes  ‚ñ° No  ‚ñ° Not Required\n",
    "Informed Consent: ‚ñ° Yes  ‚ñ° No  ‚ñ° Not Applicable\n",
    "Privacy Protection: ________________________________________\n",
    "Data Anonymization: ________________________________________\n",
    "\n",
    "FILE INFORMATION\n",
    "----------------\n",
    "File Name: ________________________________________________\n",
    "File Format: ______________________________________________\n",
    "File Size: ________________________________________________\n",
    "Location: _________________________________________________\n",
    "Backup Location: __________________________________________\n",
    "\n",
    "VERSION HISTORY\n",
    "---------------\n",
    "v1.0 - ____-__-__ : _______________________________________\n",
    "v1.1 - ____-__-__ : _______________________________________\n",
    "v1.2 - ____-__-__ : _______________________________________\n",
    "\n",
    "NOTES\n",
    "-----\n",
    "_____________________________________________________________\n",
    "_____________________________________________________________\n",
    "_____________________________________________________________\n",
    "\"\"\"\n",
    "\n",
    "# Save template\n",
    "with open(f\"{output_dir}/data_collection_documentation.txt\", \"w\") as f:\n",
    "    f.write(documentation_template)\n",
    "\n",
    "print(documentation_template)\n",
    "print(f\"\\n‚úÖ Template saved to: {output_dir}/data_collection_documentation.txt\")\n",
    "print(\"\\nUse this template for EVERY dataset you collect!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "Congratulations on completing Module 05!\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "‚úÖ **Data Sources**: Primary (collect yourself) vs Secondary (use existing)\n",
    "\n",
    "‚úÖ **Collection Methods**: Experiments, surveys, observations, sensors, interviews\n",
    "\n",
    "‚úÖ **6 Quality Dimensions**: Accuracy, Completeness, Consistency, Timeliness, Validity, Uniqueness\n",
    "\n",
    "‚úÖ **Validation**: Implement automated checks for range, type, format, consistency\n",
    "\n",
    "‚úÖ **Documentation**: Record everything - collection process, issues, resolutions\n",
    "\n",
    "### What You Can Do Now\n",
    "\n",
    "- Choose appropriate data sources for your research\n",
    "- Select collection methods based on research type\n",
    "- Assess data quality across 6 dimensions\n",
    "- Implement validation pipelines\n",
    "- Document data collection thoroughly\n",
    "- Track data versions\n",
    "\n",
    "### Practice Exercise\n",
    "\n",
    "**Exercise**: Plan Your Data Collection\n",
    "\n",
    "Using your research design from Module 04:\n",
    "\n",
    "1. Decide: Primary, secondary, or both?\n",
    "2. Choose collection method(s)\n",
    "3. Create a data dictionary\n",
    "4. List validation checks needed\n",
    "5. Fill out documentation template\n",
    "6. Plan quality assurance steps\n",
    "\n",
    "Save this - you'll use it in Module 09!\n",
    "\n",
    "---\n",
    "\n",
    "### Up Next\n",
    "\n",
    "In **Module 06: Research Ethics**, you'll learn:\n",
    "- Ethical principles in research\n",
    "- Privacy and confidentiality\n",
    "- Informed consent\n",
    "- Bias and fairness\n",
    "- Responsible data use\n",
    "\n",
    "---\n",
    "\n",
    "**Ready to continue?** Move on to `06_research_ethics.ipynb`!\n",
    "\n",
    "**Need to review?** Go back to any section that needs more attention."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
