{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 06: Introduction to Apache Spark\n",
    "\n",
    "**Estimated Time:** 60-75 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this module, you will:\n",
    "- Understand what Apache Spark is and when to use it\n",
    "- Learn Spark architecture (driver, executors, cluster)\n",
    "- Work with PySpark DataFrames\n",
    "- Understand transformations vs actions\n",
    "- Perform common data operations with Spark\n",
    "- Know when to use Spark vs pandas\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is Apache Spark?\n",
    "\n",
    "### Definition\n",
    "\n",
    "**Apache Spark** is a unified analytics engine for large-scale data processing. It provides high-level APIs in Java, Scala, Python (PySpark), and R.\n",
    "\n",
    "### Why Use Spark?\n",
    "\n",
    "1. **Scale**: Process terabytes to petabytes of data\n",
    "2. **Speed**: In-memory computing (up to 100x faster than Hadoop MapReduce)\n",
    "3. **Distributed**: Automatically distributes data and computation\n",
    "4. **Versatile**: Batch, streaming, ML, graph processing\n",
    "5. **Fault-tolerant**: Automatic recovery from failures\n",
    "\n",
    "### When to Use Spark vs pandas?\n",
    "\n",
    "| Criteria | Use pandas | Use Spark |\n",
    "|----------|------------|----------|\n",
    "| **Data Size** | < 10GB | > 10GB |\n",
    "| **Memory** | Fits in RAM | Doesn't fit in RAM |\n",
    "| **Processing** | Single machine | Distributed cluster |\n",
    "| **Speed Need** | Fast enough | Need distributed speed |\n",
    "| **Complexity** | Simple operations | Complex transformations |\n",
    "| **Cost** | Low | Higher (cluster costs) |\n",
    "\n",
    "**Rule of thumb**: Start with pandas, move to Spark when you outgrow it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if PySpark is installed\n",
    "try:\n",
    "    import pyspark\n",
    "\n",
    "    print(f\"[OK] PySpark version: {pyspark.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"[FAIL] PySpark not installed\")\n",
    "    print(\"Install with: pip install pyspark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create Spark session\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"DataEngineeringFundamentals\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"[OK] Spark session created\")\n",
    "print(f\"   Spark version: {spark.version}\")\n",
    "print(f\"   Master: {spark.sparkContext.master}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Spark Architecture\n",
    "\n",
    "### Components\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────┐\n",
    "│         Driver Program                  │\n",
    "│  (SparkContext, SparkSession)          │\n",
    "└─────────────────┬───────────────────────┘\n",
    "                  │\n",
    "        ┌─────────┴─────────┐\n",
    "        │  Cluster Manager  │\n",
    "        │  (YARN, Mesos,    │\n",
    "        │   Standalone)     │\n",
    "        └─────────┬─────────┘\n",
    "                  │\n",
    "    ┌─────────────┼─────────────┐\n",
    "    │             │             │\n",
    "┌───▼───┐    ┌───▼───┐    ┌───▼───┐\n",
    "│Executor│    │Executor│    │Executor│\n",
    "│ Tasks │    │ Tasks │    │ Tasks │\n",
    "│ Cache │    │ Cache │    │ Cache │\n",
    "└───────┘    └───────┘    └───────┘\n",
    "```\n",
    "\n",
    "- **Driver**: Coordinates the application\n",
    "- **Executors**: Perform computations and store data\n",
    "- **Cluster Manager**: Allocates resources\n",
    "- **Tasks**: Units of work sent to executors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Creating Spark DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: From Python list\n",
    "data = [\n",
    "    (1, \"Alice\", 25, \"USA\", 50000),\n",
    "    (2, \"Bob\", 30, \"UK\", 60000),\n",
    "    (3, \"Carol\", 28, \"Canada\", 55000),\n",
    "    (4, \"David\", 35, \"Australia\", 65000),\n",
    "    (5, \"Eve\", 32, \"USA\", 70000),\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"name\", \"age\", \"country\", \"salary\"]\n",
    "\n",
    "df_spark = spark.createDataFrame(data, columns)\n",
    "print(\"Created Spark DataFrame from list\")\n",
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: From pandas DataFrame\n",
    "df_pandas = pd.DataFrame(\n",
    "    {\n",
    "        \"product_id\": [\"P001\", \"P002\", \"P003\", \"P004\"],\n",
    "        \"product_name\": [\"Laptop\", \"Mouse\", \"Keyboard\", \"Monitor\"],\n",
    "        \"price\": [999.99, 29.99, 79.99, 299.99],\n",
    "        \"stock\": [50, 200, 150, 75],\n",
    "    }\n",
    ")\n",
    "\n",
    "df_spark_from_pandas = spark.createDataFrame(df_pandas)\n",
    "print(\"Created Spark DataFrame from pandas\")\n",
    "df_spark_from_pandas.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: Read from file\n",
    "# Create sample CSV first\n",
    "sample_csv_data = pd.DataFrame(\n",
    "    {\n",
    "        \"order_id\": range(1, 11),\n",
    "        \"customer_id\": np.random.randint(1, 6, 10),\n",
    "        \"amount\": np.random.uniform(100, 1000, 10).round(2),\n",
    "        \"date\": pd.date_range(\"2024-01-01\", periods=10),\n",
    "    }\n",
    ")\n",
    "\n",
    "sample_csv_data.to_csv(\"../data/raw/spark_orders.csv\", index=False)\n",
    "\n",
    "# Read with Spark\n",
    "df_spark_csv = spark.read.csv(\n",
    "    \"../data/raw/spark_orders.csv\", header=True, inferSchema=True  # Automatically detect data types\n",
    ")\n",
    "\n",
    "print(\"Read CSV with Spark:\")\n",
    "df_spark_csv.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Transformations vs Actions\n",
    "\n",
    "### Key Concept: Lazy Evaluation\n",
    "\n",
    "Spark uses **lazy evaluation**: Transformations are not executed immediately.\n",
    "\n",
    "**Transformations** (lazy):\n",
    "- Create new DataFrames\n",
    "- Not executed until an action is called\n",
    "- Examples: `select()`, `filter()`, `groupBy()`, `join()`\n",
    "\n",
    "**Actions** (eager):\n",
    "- Trigger computation\n",
    "- Return results to driver\n",
    "- Examples: `show()`, `count()`, `collect()`, `write()`\n",
    "\n",
    "```\n",
    "df.filter()      # Transformation (lazy)\n",
    "  .select()      # Transformation (lazy)\n",
    "  .groupBy()     # Transformation (lazy)\n",
    "  .count()       # ACTION (triggers execution)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations (lazy - not executed yet)\n",
    "df_filtered = df_spark.filter(df_spark.age > 28)  # Lazy\n",
    "df_selected = df_filtered.select(\"name\", \"age\", \"salary\")  # Lazy\n",
    "\n",
    "print(\"Transformations defined but not executed yet\")\n",
    "\n",
    "# Action (triggers execution)\n",
    "print(\"\\nNow executing with show() action:\")\n",
    "df_selected.show()  # Action - NOW it executes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Common Spark Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns\n",
    "df_spark.select(\"name\", \"salary\").show(3)\n",
    "\n",
    "# Filter rows\n",
    "df_spark.filter(df_spark.salary > 60000).show()\n",
    "\n",
    "# Add new column\n",
    "df_with_bonus = df_spark.withColumn(\"bonus\", df_spark.salary * 0.1)\n",
    "df_with_bonus.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by and aggregate\n",
    "df_grouped = df_spark.groupBy(\"country\").agg(\n",
    "    F.count(\"*\").alias(\"employee_count\"),\n",
    "    F.avg(\"salary\").alias(\"avg_salary\"),\n",
    "    F.max(\"salary\").alias(\"max_salary\"),\n",
    ")\n",
    "\n",
    "df_grouped.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort\n",
    "df_spark.orderBy(F.desc(\"salary\")).show()\n",
    "\n",
    "# Drop duplicates\n",
    "df_spark.dropDuplicates([\"country\"]).show()\n",
    "\n",
    "# Rename columns\n",
    "df_renamed = df_spark.withColumnRenamed(\"salary\", \"annual_salary\")\n",
    "df_renamed.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. SQL Queries with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register DataFrame as temporary view\n",
    "df_spark.createOrReplaceTempView(\"employees\")\n",
    "\n",
    "# Query with SQL\n",
    "result = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT country, \n",
    "           COUNT(*) as count,\n",
    "           AVG(salary) as avg_salary\n",
    "    FROM employees\n",
    "    WHERE age > 28\n",
    "    GROUP BY country\n",
    "    ORDER BY avg_salary DESC\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Spark vs pandas Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Create larger dataset\n",
    "large_data = pd.DataFrame(\n",
    "    {\n",
    "        \"id\": range(100000),\n",
    "        \"value\": np.random.randn(100000),\n",
    "        \"category\": np.random.choice([\"A\", \"B\", \"C\", \"D\"], 100000),\n",
    "    }\n",
    ")\n",
    "\n",
    "# pandas timing\n",
    "start = time.time()\n",
    "pandas_result = large_data.groupby(\"category\")[\"value\"].mean()\n",
    "pandas_time = time.time() - start\n",
    "\n",
    "# Spark timing\n",
    "spark_df = spark.createDataFrame(large_data)\n",
    "start = time.time()\n",
    "spark_result = spark_df.groupBy(\"category\").avg(\"value\").collect()\n",
    "spark_time = time.time() - start\n",
    "\n",
    "print(f\"pandas time: {pandas_time:.4f}s\")\n",
    "print(f\"Spark time: {spark_time:.4f}s\")\n",
    "print(f\"\\nNote: For small data (<1M rows), pandas is often faster due to less overhead\")\n",
    "print(f\"Spark shines with datasets that don't fit in memory!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Reading and Writing Data with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to Parquet (optimized columnar format)\n",
    "df_spark.write.parquet(\n",
    "    \"../data/processed/spark_employees.parquet\",\n",
    "    mode=\"overwrite\",  # overwrite, append, error, ignore\n",
    ")\n",
    "\n",
    "print(\"[OK] Written to Parquet\")\n",
    "\n",
    "# Read from Parquet\n",
    "df_read = spark.read.parquet(\"../data/processed/spark_employees.parquet\")\n",
    "print(\"\\nRead from Parquet:\")\n",
    "df_read.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write partitioned data\n",
    "df_spark.write.partitionBy(\"country\").parquet(\n",
    "    \"../data/processed/spark_employees_partitioned\", mode=\"overwrite\"\n",
    ")\n",
    "\n",
    "print(\"[OK] Written partitioned Parquet\")\n",
    "print(\"   Data is partitioned by country\")\n",
    "print(\"   Each partition is a separate file/folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Key Takeaways\n",
    "\n",
    "[OK] **Spark Purpose**: Distributed data processing at scale\n",
    "\n",
    "[OK] **When to Use**: Data > 10GB, distributed processing needed\n",
    "\n",
    "[OK] **Architecture**: Driver + Executors + Cluster Manager\n",
    "\n",
    "[OK] **Lazy Evaluation**: Transformations are lazy, actions trigger execution\n",
    "\n",
    "[OK] **DataFrames**: Similar API to pandas, but distributed\n",
    "\n",
    "[OK] **SQL Support**: Can query DataFrames with SQL\n",
    "\n",
    "[OK] **File Formats**: Parquet is preferred for big data\n",
    "\n",
    "### When to Use What?\n",
    "\n",
    "- **pandas**: < 10GB, single machine, rapid prototyping\n",
    "- **Spark**: > 10GB, distributed processing, production scale\n",
    "- **Start with pandas, scale to Spark when needed**\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In **Module 07: Workflow Orchestration with Airflow**, we'll learn:\n",
    "- What is workflow orchestration\n",
    "- Apache Airflow concepts (DAGs, Operators, Tasks)\n",
    "- Scheduling and dependencies\n",
    "- Monitoring and alerting\n",
    "\n",
    "---\n",
    "\n",
    "**Ready to orchestrate pipelines?** Open `07_workflow_orchestration_airflow.ipynb`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "spark.stop()\n",
    "print(\"[OK] Spark session stopped\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
