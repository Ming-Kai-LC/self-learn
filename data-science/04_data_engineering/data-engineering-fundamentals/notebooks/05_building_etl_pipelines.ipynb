{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 05: Building ETL Pipelines\n",
    "\n",
    "**Estimated Time:** 60-75 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this module, you will:\n",
    "- Design modular ETL pipeline components\n",
    "- Implement configuration-driven pipelines\n",
    "- Add comprehensive logging and error handling\n",
    "- Build reusable pipeline patterns\n",
    "- Test and validate pipelines\n",
    "- Apply best practices for production pipelines\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ETL Pipeline Design Principles\n",
    "\n",
    "### Core Principles\n",
    "\n",
    "1. **Modularity**: Separate concerns (Extract, Transform, Load)\n",
    "2. **Reusability**: Write components that can be reused\n",
    "3. **Configurability**: Use config files, not hardcoded values\n",
    "4. **Idempotency**: Safe to run multiple times\n",
    "5. **Observability**: Log everything for debugging\n",
    "6. **Error Handling**: Fail gracefully, retry when appropriate\n",
    "7. **Testing**: Validate inputs and outputs\n",
    "\n",
    "### Pipeline Architecture\n",
    "\n",
    "```\n",
    "Config → Extract → Validate → Transform → Validate → Load → Log\n",
    "           ↓          ↓           ↓          ↓        ↓      ↓\n",
    "        Logging   Error      Logging    Error    Logging  Metrics\n",
    "                  Handling               Handling\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import json\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"[OK] Libraries loaded and logging configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Building Modular Pipeline Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Extractor:\n",
    "    \"\"\"\n",
    "    Handles data extraction from various sources\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, source_config):\n",
    "        self.config = source_config\n",
    "        self.logger = logging.getLogger(f\"{__name__}.Extractor\")\n",
    "\n",
    "    def extract(self):\n",
    "        \"\"\"\n",
    "        Extract data based on configuration\n",
    "        \"\"\"\n",
    "        source_type = self.config.get(\"type\")\n",
    "        self.logger.info(f\"Extracting from {source_type}\")\n",
    "\n",
    "        try:\n",
    "            if source_type == \"csv\":\n",
    "                df = pd.read_csv(self.config[\"path\"])\n",
    "            elif source_type == \"json\":\n",
    "                df = pd.read_json(self.config[\"path\"])\n",
    "            elif source_type == \"api\":\n",
    "                # Simulated API extraction\n",
    "                df = self._extract_from_api()\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported source type: {source_type}\")\n",
    "\n",
    "            self.logger.info(f\"Extracted {len(df):,} records\")\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Extraction failed: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _extract_from_api(self):\n",
    "        \"\"\"Simulated API extraction\"\"\"\n",
    "        return pd.DataFrame({\"id\": range(100), \"value\": np.random.randn(100)})\n",
    "\n",
    "\n",
    "class Transformer:\n",
    "    \"\"\"\n",
    "    Handles data transformation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, transform_config):\n",
    "        self.config = transform_config\n",
    "        self.logger = logging.getLogger(f\"{__name__}.Transformer\")\n",
    "\n",
    "    def transform(self, df):\n",
    "        \"\"\"\n",
    "        Apply transformations based on configuration\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Starting transformation\")\n",
    "        df_transformed = df.copy()\n",
    "\n",
    "        try:\n",
    "            # Apply configured transformations\n",
    "            for transformation in self.config.get(\"steps\", []):\n",
    "                operation = transformation[\"operation\"]\n",
    "\n",
    "                if operation == \"drop_nulls\":\n",
    "                    df_transformed = df_transformed.dropna()\n",
    "                elif operation == \"add_column\":\n",
    "                    col_name = transformation[\"name\"]\n",
    "                    col_value = transformation[\"value\"]\n",
    "                    df_transformed[col_name] = col_value\n",
    "                elif operation == \"filter\":\n",
    "                    column = transformation[\"column\"]\n",
    "                    condition = transformation[\"condition\"]\n",
    "                    value = transformation[\"value\"]\n",
    "\n",
    "                    if condition == \">\":\n",
    "                        df_transformed = df_transformed[df_transformed[column] > value]\n",
    "                    elif condition == \"<\":\n",
    "                        df_transformed = df_transformed[df_transformed[column] < value]\n",
    "\n",
    "                self.logger.info(f\"Applied {operation}\")\n",
    "\n",
    "            self.logger.info(f\"Transformation complete: {len(df_transformed):,} records\")\n",
    "            return df_transformed\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Transformation failed: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "class Loader:\n",
    "    \"\"\"\n",
    "    Handles data loading to destinations\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, destination_config):\n",
    "        self.config = destination_config\n",
    "        self.logger = logging.getLogger(f\"{__name__}.Loader\")\n",
    "\n",
    "    def load(self, df):\n",
    "        \"\"\"\n",
    "        Load data based on configuration\n",
    "        \"\"\"\n",
    "        dest_type = self.config.get(\"type\")\n",
    "        self.logger.info(f\"Loading to {dest_type}\")\n",
    "\n",
    "        try:\n",
    "            if dest_type == \"csv\":\n",
    "                df.to_csv(self.config[\"path\"], index=False)\n",
    "            elif dest_type == \"parquet\":\n",
    "                df.to_parquet(self.config[\"path\"], compression=\"snappy\", index=False)\n",
    "            elif dest_type == \"json\":\n",
    "                df.to_json(self.config[\"path\"], orient=\"records\", indent=2)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported destination type: {dest_type}\")\n",
    "\n",
    "            self.logger.info(f\"Loaded {len(df):,} records to {self.config['path']}\")\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Loading failed: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "print(\"[OK] Pipeline components defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Configuration-Driven Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data source\n",
    "sample_data = pd.DataFrame(\n",
    "    {\n",
    "        \"id\": range(100),\n",
    "        \"value\": np.random.randn(100),\n",
    "        \"category\": np.random.choice([\"A\", \"B\", \"C\"], 100),\n",
    "        \"amount\": np.random.uniform(10, 100, 100),\n",
    "    }\n",
    ")\n",
    "\n",
    "sample_data.to_csv(\"../data/raw/pipeline_source.csv\", index=False)\n",
    "print(\"[OK] Sample source data created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pipeline configuration\n",
    "pipeline_config = {\n",
    "    \"name\": \"sample_etl_pipeline\",\n",
    "    \"version\": \"1.0.0\",\n",
    "    \"source\": {\"type\": \"csv\", \"path\": \"../data/raw/pipeline_source.csv\"},\n",
    "    \"transformations\": {\n",
    "        \"steps\": [\n",
    "            {\"operation\": \"drop_nulls\"},\n",
    "            {\"operation\": \"filter\", \"column\": \"amount\", \"condition\": \">\", \"value\": 50},\n",
    "            {\n",
    "                \"operation\": \"add_column\",\n",
    "                \"name\": \"processed_at\",\n",
    "                \"value\": datetime.now().isoformat(),\n",
    "            },\n",
    "        ]\n",
    "    },\n",
    "    \"destination\": {\"type\": \"parquet\", \"path\": \"../data/processed/pipeline_output.parquet\"},\n",
    "}\n",
    "\n",
    "print(\"Pipeline Configuration:\")\n",
    "print(json.dumps(pipeline_config, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Complete ETL Pipeline Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ETLPipeline:\n",
    "    \"\"\"\n",
    "    Complete ETL Pipeline orchestrator\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.logger = logging.getLogger(f\"{__name__}.ETLPipeline\")\n",
    "        self.metrics = {\n",
    "            \"start_time\": None,\n",
    "            \"end_time\": None,\n",
    "            \"duration_seconds\": None,\n",
    "            \"records_extracted\": 0,\n",
    "            \"records_transformed\": 0,\n",
    "            \"records_loaded\": 0,\n",
    "            \"status\": \"pending\",\n",
    "        }\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Execute the complete ETL pipeline\n",
    "        \"\"\"\n",
    "        self.metrics[\"start_time\"] = datetime.now()\n",
    "        self.logger.info(f\"Starting pipeline: {self.config['name']}\")\n",
    "\n",
    "        try:\n",
    "            # Extract\n",
    "            extractor = Extractor(self.config[\"source\"])\n",
    "            df = extractor.extract()\n",
    "            self.metrics[\"records_extracted\"] = len(df)\n",
    "\n",
    "            # Validate extraction\n",
    "            self._validate_data(df, stage=\"extract\")\n",
    "\n",
    "            # Transform\n",
    "            transformer = Transformer(self.config[\"transformations\"])\n",
    "            df_transformed = transformer.transform(df)\n",
    "            self.metrics[\"records_transformed\"] = len(df_transformed)\n",
    "\n",
    "            # Validate transformation\n",
    "            self._validate_data(df_transformed, stage=\"transform\")\n",
    "\n",
    "            # Load\n",
    "            loader = Loader(self.config[\"destination\"])\n",
    "            loader.load(df_transformed)\n",
    "            self.metrics[\"records_loaded\"] = len(df_transformed)\n",
    "\n",
    "            # Success\n",
    "            self.metrics[\"status\"] = \"success\"\n",
    "            self.metrics[\"end_time\"] = datetime.now()\n",
    "            self.metrics[\"duration_seconds\"] = (\n",
    "                self.metrics[\"end_time\"] - self.metrics[\"start_time\"]\n",
    "            ).total_seconds()\n",
    "\n",
    "            self.logger.info(f\"Pipeline completed successfully\")\n",
    "            self._log_metrics()\n",
    "\n",
    "            return True, self.metrics\n",
    "\n",
    "        except Exception as e:\n",
    "            self.metrics[\"status\"] = \"failed\"\n",
    "            self.metrics[\"error\"] = str(e)\n",
    "            self.metrics[\"end_time\"] = datetime.now()\n",
    "\n",
    "            self.logger.error(f\"Pipeline failed: {e}\")\n",
    "            return False, self.metrics\n",
    "\n",
    "    def _validate_data(self, df, stage):\n",
    "        \"\"\"\n",
    "        Validate data at each stage\n",
    "        \"\"\"\n",
    "        if df is None or len(df) == 0:\n",
    "            raise ValueError(f\"{stage}: No data to process\")\n",
    "\n",
    "        self.logger.info(f\"{stage}: Validation passed ({len(df):,} records)\")\n",
    "\n",
    "    def _log_metrics(self):\n",
    "        \"\"\"\n",
    "        Log pipeline execution metrics\n",
    "        \"\"\"\n",
    "        self.logger.info(\"=\" * 60)\n",
    "        self.logger.info(\"PIPELINE METRICS\")\n",
    "        self.logger.info(\"=\" * 60)\n",
    "        self.logger.info(f\"Pipeline: {self.config['name']}\")\n",
    "        self.logger.info(f\"Status: {self.metrics['status']}\")\n",
    "        self.logger.info(f\"Duration: {self.metrics['duration_seconds']:.2f}s\")\n",
    "        self.logger.info(f\"Records Extracted: {self.metrics['records_extracted']:,}\")\n",
    "        self.logger.info(f\"Records Transformed: {self.metrics['records_transformed']:,}\")\n",
    "        self.logger.info(f\"Records Loaded: {self.metrics['records_loaded']:,}\")\n",
    "        self.logger.info(\"=\" * 60)\n",
    "\n",
    "\n",
    "print(\"[OK] ETL Pipeline class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the pipeline\n",
    "pipeline = ETLPipeline(pipeline_config)\n",
    "success, metrics = pipeline.run()\n",
    "\n",
    "if success:\n",
    "    print(\"\\n[OK] Pipeline executed successfully!\")\n",
    "else:\n",
    "    print(\"\\n[FAIL] Pipeline execution failed\")\n",
    "\n",
    "print(\"\\nMetrics:\")\n",
    "for key, value in metrics.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Error Handling and Recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobustETLPipeline(ETLPipeline):\n",
    "    \"\"\"\n",
    "    ETL Pipeline with advanced error handling and retry logic\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, max_retries=3):\n",
    "        super().__init__(config)\n",
    "        self.max_retries = max_retries\n",
    "\n",
    "    def run_with_retry(self):\n",
    "        \"\"\"\n",
    "        Run pipeline with retry logic\n",
    "        \"\"\"\n",
    "        for attempt in range(1, self.max_retries + 1):\n",
    "            self.logger.info(f\"Attempt {attempt}/{self.max_retries}\")\n",
    "\n",
    "            success, metrics = self.run()\n",
    "\n",
    "            if success:\n",
    "                return True, metrics\n",
    "\n",
    "            if attempt < self.max_retries:\n",
    "                self.logger.warning(f\"Attempt {attempt} failed, retrying...\")\n",
    "                # Could add exponential backoff here\n",
    "\n",
    "        self.logger.error(f\"Pipeline failed after {self.max_retries} attempts\")\n",
    "        return False, metrics\n",
    "\n",
    "\n",
    "print(\"[OK] Robust pipeline with retry logic defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Pipeline Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pipeline(pipeline_config):\n",
    "    \"\"\"\n",
    "    Test pipeline with sample data\n",
    "    \"\"\"\n",
    "    print(\"Testing Pipeline...\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Test 1: Configuration validation\n",
    "    required_keys = [\"name\", \"source\", \"transformations\", \"destination\"]\n",
    "    for key in required_keys:\n",
    "        assert key in pipeline_config, f\"Missing required config key: {key}\"\n",
    "    print(\"[OK] Test 1: Configuration valid\")\n",
    "\n",
    "    # Test 2: Source file exists\n",
    "    if pipeline_config[\"source\"][\"type\"] in [\"csv\", \"json\"]:\n",
    "        source_path = Path(pipeline_config[\"source\"][\"path\"])\n",
    "        assert source_path.exists(), f\"Source file not found: {source_path}\"\n",
    "    print(\"[OK] Test 2: Source accessible\")\n",
    "\n",
    "    # Test 3: Run pipeline\n",
    "    pipeline = ETLPipeline(pipeline_config)\n",
    "    success, metrics = pipeline.run()\n",
    "    assert success, \"Pipeline execution failed\"\n",
    "    print(\"[OK] Test 3: Pipeline executed successfully\")\n",
    "\n",
    "    # Test 4: Output file created\n",
    "    output_path = Path(pipeline_config[\"destination\"][\"path\"])\n",
    "    assert output_path.exists(), \"Output file not created\"\n",
    "    print(\"[OK] Test 4: Output file created\")\n",
    "\n",
    "    # Test 5: Validate record count\n",
    "    assert metrics[\"records_loaded\"] > 0, \"No records loaded\"\n",
    "    print(f\"[OK] Test 5: Loaded {metrics['records_loaded']:,} records\")\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "\n",
    "# Run tests\n",
    "test_pipeline(pipeline_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Best Practices Summary\n",
    "\n",
    "### Design\n",
    "[OK] Modular components (Extract, Transform, Load)\n",
    "[OK] Configuration-driven (not hardcoded)\n",
    "[OK] Reusable and maintainable code\n",
    "\n",
    "### Reliability\n",
    "[OK] Comprehensive error handling\n",
    "[OK] Retry logic for transient failures\n",
    "[OK] Data validation at each stage\n",
    "\n",
    "### Observability\n",
    "[OK] Detailed logging throughout\n",
    "[OK] Metrics collection and reporting\n",
    "[OK] Clear success/failure indicators\n",
    "\n",
    "### Testing\n",
    "[OK] Unit tests for components\n",
    "[OK] Integration tests for full pipeline\n",
    "[OK] Data quality validation\n",
    "\n",
    "### Documentation\n",
    "[OK] Clear code comments\n",
    "[OK] Configuration documentation\n",
    "[OK] README with setup instructions\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In **Module 06: Introduction to Apache Spark**, we'll:\n",
    "- Learn distributed data processing\n",
    "- Work with PySpark DataFrames\n",
    "- Understand when to use Spark vs pandas\n",
    "- Build Spark-based data pipelines\n",
    "\n",
    "---\n",
    "\n",
    "**Ready for big data processing?** Open `06_introduction_to_apache_spark.ipynb`!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
