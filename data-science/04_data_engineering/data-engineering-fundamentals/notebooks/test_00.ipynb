{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 00: Setup and Introduction to Data Engineering\n",
    "\n",
    "**Estimated Time:** 20-30 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this module, you will:\n",
    "- Verify your development environment is set up correctly\n",
    "- Understand what data engineering is and why it matters\n",
    "- Learn the course structure and learning path\n",
    "- Run your first simple data pipeline example\n",
    "- Get familiar with the tools we'll be using\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Verification\n",
    "\n",
    "Let's make sure all required packages are installed correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Python version\n",
    "import sys\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Python version info: {sys.version_info}\")\n",
    "\n",
    "if sys.version_info < (3, 8):\n",
    "    print(\"\\n[WARNING] WARNING: Python 3.8 or higher is recommended\")\n",
    "else:\n",
    "    print(\"\\n[OK] Python version is compatible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify core data manipulation libraries\n",
    "try:\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    print(f\"[OK] pandas version: {pd.__version__}\")\n",
    "    print(f\"[OK] numpy version: {np.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"[FAIL] Error importing libraries: {e}\")\n",
    "    print(\"\\nPlease run: pip install pandas numpy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify database connectivity libraries\n",
    "try:\n",
    "    import sqlalchemy\n",
    "\n",
    "    print(f\"[OK] SQLAlchemy version: {sqlalchemy.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"[FAIL] Error importing SQLAlchemy: {e}\")\n",
    "    print(\"\\nPlease run: pip install sqlalchemy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify PySpark (Big Data library)\n",
    "try:\n",
    "    import pyspark\n",
    "\n",
    "    print(f\"[OK] PySpark version: {pyspark.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"[WARNING] PySpark not found: {e}\")\n",
    "    print(\"\\nPySpark will be needed in Module 06\")\n",
    "    print(\"Install with: pip install pyspark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify additional utilities\n",
    "try:\n",
    "    import requests\n",
    "    import yaml\n",
    "\n",
    "    print(f\"[OK] requests version: {requests.__version__}\")\n",
    "    print(\"[OK] PyYAML is installed\")\n",
    "except ImportError as e:\n",
    "    print(f\"[WARNING] Some utilities not found: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Check Summary\n",
    "\n",
    "If you see [OK] for pandas, numpy, and SQLAlchemy, you're ready to start!\n",
    "\n",
    "If you see [FAIL] or [WARNING]:\n",
    "1. Make sure you've activated your virtual environment\n",
    "2. Run: `pip install -r requirements.txt`\n",
    "3. Restart the Jupyter kernel (Kernel → Restart)\n",
    "4. Re-run the cells above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. What is Data Engineering?\n",
    "\n",
    "### Definition\n",
    "\n",
    "**Data Engineering** is the practice of designing, building, and maintaining systems that collect, store, process, and deliver data at scale. Data engineers create the infrastructure and pipelines that enable data scientists, analysts, and business users to access reliable, high-quality data.\n",
    "\n",
    "### The Data Engineering Workflow\n",
    "\n",
    "```\n",
    "┌─────────────┐       ┌─────────────┐       ┌─────────────┐       ┌─────────────┐\n",
    "│   EXTRACT   │  -->  │  TRANSFORM  │  -->  │    LOAD     │  -->  │   ANALYZE   │\n",
    "│             │       │             │       │             │       │             │\n",
    "│ Data from   │       │ Clean &     │       │ Store in    │       │ BI, ML,     │\n",
    "│ various     │       │ process     │       │ warehouse/  │       │ Analytics   │\n",
    "│ sources     │       │ data        │       │ lake        │       │             │\n",
    "└─────────────┘       └─────────────┘       └─────────────┘       └─────────────┘\n",
    "```\n",
    "\n",
    "### Why Data Engineering Matters\n",
    "\n",
    "1. **Data Volume is Exploding**: Organizations generate terabytes of data daily\n",
    "2. **Data Quality is Critical**: Bad data leads to bad decisions\n",
    "3. **Speed Matters**: Real-time insights require real-time pipelines\n",
    "4. **Complexity is Increasing**: Multiple data sources, formats, and systems\n",
    "5. **Foundation for AI/ML**: Machine learning models need quality data\n",
    "\n",
    "### Key Responsibilities of Data Engineers\n",
    "\n",
    "- Design and build data pipelines (ETL/ELT)\n",
    "- Ensure data quality and reliability\n",
    "- Optimize data storage and retrieval\n",
    "- Implement data security and governance\n",
    "- Monitor and maintain data systems\n",
    "- Enable data accessibility for stakeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Data Engineer vs. Other Data Roles\n",
    "\n",
    "| Role | Primary Focus | Key Skills | Example Task |\n",
    "|------|---------------|------------|-------------|\n",
    "| **Data Engineer** | Infrastructure & Pipelines | Python, SQL, ETL, Spark, Airflow | Build a pipeline to ingest and transform daily sales data |\n",
    "| **Data Scientist** | Insights & Predictions | Statistics, ML, Python, R | Build a model to predict customer churn |\n",
    "| **Data Analyst** | Reporting & Analysis | SQL, BI Tools, Excel, Statistics | Create a dashboard showing monthly sales trends |\n",
    "| **Analytics Engineer** | Data Transformation | SQL, dbt, modeling | Transform raw data into analysis-ready tables |\n",
    "| **ML Engineer** | Production ML Systems | ML frameworks, deployment, scaling | Deploy and scale a recommendation model |\n",
    "\n",
    "### The Relationship\n",
    "\n",
    "```\n",
    "Data Engineers build the infrastructure\n",
    "        ↓\n",
    "Analytics Engineers model the data\n",
    "        ↓\n",
    "Data Scientists & Analysts use the data\n",
    "        ↓\n",
    "ML Engineers productionize the models\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Your First Data Pipeline\n",
    "\n",
    "Let's build a simple ETL pipeline to understand the core concept!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "print(\"Building your first data pipeline...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: EXTRACT - Get Raw Data\n",
    "\n",
    "In this example, we'll simulate extracting sales data from a source system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate extracting data from a source system\n",
    "def extract_sales_data():\n",
    "    \"\"\"\n",
    "    Simulates extracting sales data from a database or API.\n",
    "    In real scenarios, this would connect to actual data sources.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Generate sample data\n",
    "    n_records = 100\n",
    "    dates = [datetime.now() - timedelta(days=x) for x in range(n_records)]\n",
    "\n",
    "    raw_data = {\n",
    "        \"date\": dates,\n",
    "        \"product_id\": np.random.choice([\"P001\", \"P002\", \"P003\", \"P004\"], n_records),\n",
    "        \"quantity\": np.random.randint(1, 20, n_records),\n",
    "        \"price\": np.random.uniform(10.0, 100.0, n_records),\n",
    "        \"customer_id\": np.random.choice([\"C001\", \"C002\", \"C003\", \"C004\", \"C005\"], n_records),\n",
    "        \"region\": np.random.choice([\"North\", \"South\", \"East\", \"West\"], n_records),\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(raw_data)\n",
    "    print(f\"[OK] EXTRACT: Retrieved {len(df)} records from source system\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# Extract the data\n",
    "raw_sales_data = extract_sales_data()\n",
    "print(\"\\nFirst 5 records:\")\n",
    "raw_sales_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: TRANSFORM - Clean and Process Data\n",
    "\n",
    "Now we'll clean, enhance, and aggregate the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_sales_data(df):\n",
    "    \"\"\"\n",
    "    Transform raw sales data:\n",
    "    - Calculate total revenue\n",
    "    - Format dates\n",
    "    - Add derived columns\n",
    "    - Aggregate by product\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying original\n",
    "    df_transformed = df.copy()\n",
    "\n",
    "    # Calculate revenue\n",
    "    df_transformed[\"revenue\"] = df_transformed[\"quantity\"] * df_transformed[\"price\"]\n",
    "\n",
    "    # Round price and revenue\n",
    "    df_transformed[\"price\"] = df_transformed[\"price\"].round(2)\n",
    "    df_transformed[\"revenue\"] = df_transformed[\"revenue\"].round(2)\n",
    "\n",
    "    # Extract date components\n",
    "    df_transformed[\"year\"] = df_transformed[\"date\"].dt.year\n",
    "    df_transformed[\"month\"] = df_transformed[\"date\"].dt.month\n",
    "    df_transformed[\"day_of_week\"] = df_transformed[\"date\"].dt.day_name()\n",
    "\n",
    "    print(f\"[OK] TRANSFORM: Processed {len(df_transformed)} records\")\n",
    "    print(f\"   - Added revenue calculation\")\n",
    "    print(f\"   - Extracted date components\")\n",
    "    print(f\"   - Total revenue: ${df_transformed['revenue'].sum():,.2f}\")\n",
    "\n",
    "    return df_transformed\n",
    "\n",
    "\n",
    "# Transform the data\n",
    "transformed_sales_data = transform_sales_data(raw_sales_data)\n",
    "print(\"\\nTransformed data sample:\")\n",
    "transformed_sales_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create aggregated summary for reporting\n",
    "def create_product_summary(df):\n",
    "    \"\"\"\n",
    "    Create a summary report by product\n",
    "    \"\"\"\n",
    "    summary = (\n",
    "        df.groupby(\"product_id\")\n",
    "        .agg({\"quantity\": \"sum\", \"revenue\": \"sum\", \"customer_id\": \"nunique\"})\n",
    "        .round(2)\n",
    "    )\n",
    "\n",
    "    summary.columns = [\"total_quantity\", \"total_revenue\", \"unique_customers\"]\n",
    "    summary = summary.sort_values(\"total_revenue\", ascending=False)\n",
    "\n",
    "    print(\"[OK] TRANSFORM: Created product summary\")\n",
    "    return summary\n",
    "\n",
    "\n",
    "product_summary = create_product_summary(transformed_sales_data)\n",
    "print(\"\\nProduct Summary:\")\n",
    "product_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: LOAD - Save to Destination\n",
    "\n",
    "Finally, we'll save the processed data to CSV files (in production, this would be a database or data warehouse)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def load_data(df, summary, output_dir=\"outputs\"):\n",
    "    \"\"\"\n",
    "    Load transformed data to destination (CSV files in this example)\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Save detailed data\n",
    "    detail_file = os.path.join(output_dir, \"sales_detail.csv\")\n",
    "    df.to_csv(detail_file, index=False)\n",
    "    print(f\"[OK] LOAD: Saved detailed data to {detail_file}\")\n",
    "\n",
    "    # Save summary data\n",
    "    summary_file = os.path.join(output_dir, \"product_summary.csv\")\n",
    "    summary.to_csv(summary_file)\n",
    "    print(f\"[OK] LOAD: Saved summary data to {summary_file}\")\n",
    "\n",
    "    print(f\"\\n[DATA] Pipeline complete! Data loaded successfully.\")\n",
    "\n",
    "\n",
    "# Load the data\n",
    "load_data(transformed_sales_data, product_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify the Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify files were created\n",
    "import os\n",
    "\n",
    "output_files = os.listdir(\"outputs\")\n",
    "print(\"Files created:\")\n",
    "for file in output_files:\n",
    "    file_path = os.path.join(\"outputs\", file)\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    print(f\"  - {file} ({file_size:,} bytes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Pipeline Function\n",
    "\n",
    "Let's wrap everything into a single pipeline function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sales_etl_pipeline():\n",
    "    \"\"\"\n",
    "    Complete ETL pipeline for sales data\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"SALES DATA ETL PIPELINE\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "\n",
    "    # Extract\n",
    "    print(\"[1/3] EXTRACT Phase\")\n",
    "    raw_data = extract_sales_data()\n",
    "    print()\n",
    "\n",
    "    # Transform\n",
    "    print(\"[2/3] TRANSFORM Phase\")\n",
    "    transformed_data = transform_sales_data(raw_data)\n",
    "    summary = create_product_summary(transformed_data)\n",
    "    print()\n",
    "\n",
    "    # Load\n",
    "    print(\"[3/3] LOAD Phase\")\n",
    "    load_data(transformed_data, summary)\n",
    "    print()\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ETL Pipeline Completed Successfully!\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    return transformed_data, summary\n",
    "\n",
    "\n",
    "# Run the complete pipeline\n",
    "final_data, final_summary = run_sales_etl_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Course Overview\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "This course consists of 10 modules covering:\n",
    "\n",
    "1. **Module 00** (This Module): Setup and Introduction ✓\n",
    "2. **Module 01**: Introduction to Data Engineering - Concepts and architecture\n",
    "3. **Module 02**: Data Sources and Extraction - APIs, databases, files\n",
    "4. **Module 03**: Data Transformation and Cleaning - pandas mastery\n",
    "5. **Module 04**: Data Loading and Storage - Databases, files, formats\n",
    "6. **Module 05**: Building ETL Pipelines - Design patterns and best practices\n",
    "7. **Module 06**: Introduction to Apache Spark - Distributed data processing\n",
    "8. **Module 07**: Workflow Orchestration - Apache Airflow concepts\n",
    "9. **Module 08**: Data Quality and Validation - Testing and validation\n",
    "10. **Module 09**: End-to-End Pipeline Project - Capstone project\n",
    "\n",
    "### Learning Approach\n",
    "\n",
    "- **Theory First**: Understand concepts before implementation\n",
    "- **Hands-On**: Code examples you can run and modify\n",
    "- **Progressive**: Each module builds on previous ones\n",
    "- **Practical**: Real-world scenarios and best practices\n",
    "\n",
    "### Estimated Time\n",
    "\n",
    "- **Total**: 8-10 hours\n",
    "- **Per Module**: 45-75 minutes\n",
    "- **Recommended Pace**: 2-3 modules per week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Key Concepts from This Module\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "[OK] **Environment Setup**: Your Python environment is ready for data engineering\n",
    "\n",
    "[OK] **Data Engineering Definition**: Building systems to collect, store, and process data\n",
    "\n",
    "[OK] **ETL Process**:\n",
    "- **Extract**: Get data from sources\n",
    "- **Transform**: Clean and process data\n",
    "- **Load**: Store data in destination\n",
    "\n",
    "[OK] **First Pipeline**: Built a complete working data pipeline\n",
    "\n",
    "[OK] **Course Structure**: Know what's coming in the next modules\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. Data engineering is about building reliable data infrastructure\n",
    "2. ETL pipelines are the core of data engineering\n",
    "3. Python + pandas is powerful for data processing\n",
    "4. Real-world pipelines follow the same Extract-Transform-Load pattern\n",
    "5. Code modularity makes pipelines maintainable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Practice Exercise\n",
    "\n",
    "### Challenge: Modify the Pipeline\n",
    "\n",
    "Try these modifications to reinforce your learning:\n",
    "\n",
    "1. **Add a new column**: Calculate `discount` as 10% of revenue\n",
    "2. **Filter data**: Only include sales where quantity > 5\n",
    "3. **New aggregation**: Create a summary by region instead of product\n",
    "4. **Add validation**: Check that all revenues are positive\n",
    "\n",
    "Use the cells below to experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here: Add discount column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here: Filter by quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here: Aggregate by region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Next Steps\n",
    "\n",
    "Congratulations on completing Module 00!\n",
    "\n",
    "### Ready to Continue?\n",
    "\n",
    "In **Module 01: Introduction to Data Engineering**, you'll learn:\n",
    "- Deep dive into data engineering concepts\n",
    "- The modern data stack\n",
    "- ETL vs. ELT patterns\n",
    "- Data pipeline architectures\n",
    "- Common challenges and solutions\n",
    "\n",
    "### Before Moving On\n",
    "\n",
    "Make sure you:\n",
    "- [OK] Have all packages installed (no errors in section 1)\n",
    "- [OK] Successfully ran the complete ETL pipeline\n",
    "- [OK] Understand the basic ETL concept\n",
    "- [OK] Can see the output files in the `outputs/` folder\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [pandas documentation](https://pandas.pydata.org/docs/)\n",
    "- [Python data structures](https://docs.python.org/3/tutorial/datastructures.html)\n",
    "- Main README.md in the project root\n",
    "\n",
    "---\n",
    "\n",
    "**Ready?** Open `01_introduction_to_data_engineering.ipynb` to continue your learning journey!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
