{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 07: Workflow Orchestration with Airflow\n",
    "\n",
    "**Estimated Time:** 45-60 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this module, you will:\n",
    "- Understand what workflow orchestration is\n",
    "- Learn Apache Airflow concepts (DAGs, Operators, Tasks)\n",
    "- Understand task dependencies and scheduling\n",
    "- Know how to monitor and debug workflows\n",
    "- Compare orchestration tools (Airflow, Prefect, Dagster)\n",
    "\n",
    "---\n",
    "\n",
    "**Note**: This module covers Airflow concepts theoretically. Setting up Airflow requires Docker or a dedicated environment. We'll learn the concepts and patterns that apply to any orchestration tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is Workflow Orchestration?\n",
    "\n",
    "### Definition\n",
    "\n",
    "**Workflow Orchestration** is the automated management, coordination, and scheduling of complex data workflows with dependencies.\n",
    "\n",
    "### Why Do We Need It?\n",
    "\n",
    "Without orchestration:\n",
    "- [FAIL] Manual execution of pipelines\n",
    "- [FAIL] Hard to manage dependencies\n",
    "- [FAIL] No visibility into failures\n",
    "- [FAIL] Difficult to schedule recurring jobs\n",
    "- [FAIL] No centralized monitoring\n",
    "\n",
    "With orchestration:\n",
    "- [OK] Automated scheduling\n",
    "- [OK] Dependency management\n",
    "- [OK] Error handling and retries\n",
    "- [OK] Monitoring and alerting\n",
    "- [OK] Historical run data\n",
    "\n",
    "### Common Use Cases\n",
    "\n",
    "- ETL pipeline scheduling\n",
    "- Data warehouse loading\n",
    "- ML model training pipelines\n",
    "- Report generation\n",
    "- Data quality checks\n",
    "- Multi-step data transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Apache Airflow Core Concepts\n",
    "\n",
    "### DAG (Directed Acyclic Graph)\n",
    "\n",
    "A DAG is a collection of tasks with dependencies:\n",
    "\n",
    "```\n",
    "start → extract_data → transform_data → load_data → end\n",
    "                              ↓\n",
    "                        data_quality_check\n",
    "```\n",
    "\n",
    "**Directed**: Tasks flow in one direction\n",
    "\n",
    "**Acyclic**: No loops (task A can't depend on task B if B depends on A)\n",
    "\n",
    "**Graph**: Collection of nodes (tasks) and edges (dependencies)\n",
    "\n",
    "### Key Components\n",
    "\n",
    "1. **DAG**: The workflow definition\n",
    "2. **Operators**: Templates for tasks (PythonOperator, BashOperator, etc.)\n",
    "3. **Tasks**: Instances of operators\n",
    "4. **Task Instances**: Specific runs of tasks\n",
    "5. **Scheduler**: Triggers task execution\n",
    "6. **Executor**: Runs the tasks\n",
    "7. **Webserver**: UI for monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Example Airflow DAG (Conceptual)\n",
    "\n",
    "Here's what an Airflow DAG looks like:\n",
    "\n",
    "```python\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.operators.bash import BashOperator\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Default arguments for all tasks\n",
    "default_args = {\n",
    "    'owner': 'data_team',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': datetime(2024, 1, 1),\n",
    "    'email_on_failure': True,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 3,\n",
    "    'retry_delay': timedelta(minutes=5)\n",
    "}\n",
    "\n",
    "# Define the DAG\n",
    "dag = DAG(\n",
    "    'sales_etl_pipeline',\n",
    "    default_args=default_args,\n",
    "    description='Daily sales data ETL pipeline',\n",
    "    schedule_interval='0 2 * * *',  # Daily at 2 AM\n",
    "    catchup=False\n",
    ")\n",
    "\n",
    "# Define tasks\n",
    "def extract_sales_data():\n",
    "    # Extract logic here\n",
    "    print(\"Extracting sales data...\")\n",
    "    return \"extraction_complete\"\n",
    "\n",
    "def transform_sales_data():\n",
    "    # Transform logic here\n",
    "    print(\"Transforming sales data...\")\n",
    "    return \"transformation_complete\"\n",
    "\n",
    "def load_sales_data():\n",
    "    # Load logic here\n",
    "    print(\"Loading sales data...\")\n",
    "    return \"load_complete\"\n",
    "\n",
    "# Create task instances\n",
    "extract_task = PythonOperator(\n",
    "    task_id='extract_sales',\n",
    "    python_callable=extract_sales_data,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "transform_task = PythonOperator(\n",
    "    task_id='transform_sales',\n",
    "    python_callable=transform_sales_data,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "load_task = PythonOperator(\n",
    "    task_id='load_sales',\n",
    "    python_callable=load_sales_data,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "quality_check_task = BashOperator(\n",
    "    task_id='quality_check',\n",
    "    bash_command='python /path/to/quality_check.py',\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "# Define dependencies\n",
    "extract_task >> transform_task >> load_task >> quality_check_task\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Simulating Workflow Orchestration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple workflow orchestrator simulation\n",
    "from datetime import datetime\n",
    "from enum import Enum\n",
    "import time\n",
    "\n",
    "\n",
    "class TaskStatus(Enum):\n",
    "    PENDING = \"pending\"\n",
    "    RUNNING = \"running\"\n",
    "    SUCCESS = \"success\"\n",
    "    FAILED = \"failed\"\n",
    "\n",
    "\n",
    "class Task:\n",
    "    def __init__(self, task_id, func, retries=3):\n",
    "        self.task_id = task_id\n",
    "        self.func = func\n",
    "        self.status = TaskStatus.PENDING\n",
    "        self.retries = retries\n",
    "        self.start_time = None\n",
    "        self.end_time = None\n",
    "\n",
    "    def execute(self):\n",
    "        self.status = TaskStatus.RUNNING\n",
    "        self.start_time = datetime.now()\n",
    "\n",
    "        for attempt in range(1, self.retries + 1):\n",
    "            try:\n",
    "                print(f\"\\n[{self.task_id}] Attempt {attempt}/{self.retries}\")\n",
    "                result = self.func()\n",
    "                self.status = TaskStatus.SUCCESS\n",
    "                self.end_time = datetime.now()\n",
    "                print(f\"[{self.task_id}] [OK] SUCCESS\")\n",
    "                return result\n",
    "            except Exception as e:\n",
    "                if attempt == self.retries:\n",
    "                    self.status = TaskStatus.FAILED\n",
    "                    self.end_time = datetime.now()\n",
    "                    print(f\"[{self.task_id}] [FAIL] FAILED after {self.retries} attempts\")\n",
    "                    raise\n",
    "                print(f\"[{self.task_id}] [WARNING] Error: {e}, retrying...\")\n",
    "                time.sleep(1)  # Retry delay\n",
    "\n",
    "\n",
    "class SimpleWorkflow:\n",
    "    def __init__(self, workflow_name):\n",
    "        self.workflow_name = workflow_name\n",
    "        self.tasks = []\n",
    "        self.dependencies = {}\n",
    "\n",
    "    def add_task(self, task):\n",
    "        self.tasks.append(task)\n",
    "        self.dependencies[task.task_id] = []\n",
    "\n",
    "    def set_dependency(self, upstream_task_id, downstream_task_id):\n",
    "        \"\"\"Set task dependency: upstream >> downstream\"\"\"\n",
    "        if downstream_task_id in self.dependencies:\n",
    "            self.dependencies[downstream_task_id].append(upstream_task_id)\n",
    "\n",
    "    def run(self):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Running Workflow: {self.workflow_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        executed = set()\n",
    "\n",
    "        while len(executed) < len(self.tasks):\n",
    "            for task in self.tasks:\n",
    "                if task.task_id in executed:\n",
    "                    continue\n",
    "\n",
    "                # Check if all dependencies are met\n",
    "                dependencies_met = all(\n",
    "                    dep_id in executed for dep_id in self.dependencies[task.task_id]\n",
    "                )\n",
    "\n",
    "                if dependencies_met:\n",
    "                    task.execute()\n",
    "                    executed.add(task.task_id)\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Workflow Complete: {self.workflow_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        self._print_summary()\n",
    "\n",
    "    def _print_summary(self):\n",
    "        print(\"\\nTask Summary:\")\n",
    "        for task in self.tasks:\n",
    "            duration = (task.end_time - task.start_time).total_seconds() if task.end_time else 0\n",
    "            print(f\"  {task.task_id}: {task.status.value} ({duration:.2f}s)\")\n",
    "\n",
    "\n",
    "print(\"[OK] Simple workflow orchestrator created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pipeline tasks\n",
    "def extract_data():\n",
    "    print(\"  Extracting data from source...\")\n",
    "    time.sleep(0.5)\n",
    "    return \"data_extracted\"\n",
    "\n",
    "\n",
    "def transform_data():\n",
    "    print(\"  Transforming data...\")\n",
    "    time.sleep(0.5)\n",
    "    return \"data_transformed\"\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    print(\"  Loading data to warehouse...\")\n",
    "    time.sleep(0.5)\n",
    "    return \"data_loaded\"\n",
    "\n",
    "\n",
    "def quality_check():\n",
    "    print(\"  Running quality checks...\")\n",
    "    time.sleep(0.3)\n",
    "    return \"quality_passed\"\n",
    "\n",
    "\n",
    "def send_notification():\n",
    "    print(\"  Sending success notification...\")\n",
    "    return \"notification_sent\"\n",
    "\n",
    "\n",
    "# Create workflow\n",
    "workflow = SimpleWorkflow(\"Daily Sales ETL\")\n",
    "\n",
    "# Add tasks\n",
    "extract_task = Task(\"extract\", extract_data)\n",
    "transform_task = Task(\"transform\", transform_data)\n",
    "load_task = Task(\"load\", load_data)\n",
    "quality_task = Task(\"quality_check\", quality_check)\n",
    "notify_task = Task(\"notify\", send_notification)\n",
    "\n",
    "workflow.add_task(extract_task)\n",
    "workflow.add_task(transform_task)\n",
    "workflow.add_task(load_task)\n",
    "workflow.add_task(quality_task)\n",
    "workflow.add_task(notify_task)\n",
    "\n",
    "# Set dependencies\n",
    "workflow.set_dependency(\"extract\", \"transform\")\n",
    "workflow.set_dependency(\"transform\", \"load\")\n",
    "workflow.set_dependency(\"load\", \"quality_check\")\n",
    "workflow.set_dependency(\"quality_check\", \"notify\")\n",
    "\n",
    "# Run workflow\n",
    "workflow.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Scheduling Patterns\n",
    "\n",
    "### Cron Expressions\n",
    "\n",
    "Airflow uses cron expressions for scheduling:\n",
    "\n",
    "```\n",
    "┌─── minute (0 - 59)\n",
    "│ ┌─── hour (0 - 23)\n",
    "│ │ ┌─── day of month (1 - 31)\n",
    "│ │ │ ┌─── month (1 - 12)\n",
    "│ │ │ │ ┌─── day of week (0 - 6, Sunday = 0)\n",
    "│ │ │ │ │\n",
    "* * * * *\n",
    "```\n",
    "\n",
    "### Common Schedules\n",
    "\n",
    "| Schedule | Cron Expression | Meaning |\n",
    "|----------|----------------|----------|\n",
    "| Every minute | `* * * * *` | Every minute |\n",
    "| Every hour | `0 * * * *` | At minute 0 of every hour |\n",
    "| Daily at 2 AM | `0 2 * * *` | 2:00 AM every day |\n",
    "| Every Monday at 9 AM | `0 9 * * 1` | 9:00 AM every Monday |\n",
    "| First day of month | `0 0 1 * *` | Midnight on the 1st |\n",
    "| Every 15 minutes | `*/15 * * * *` | Every 15 minutes |\n",
    "\n",
    "### Airflow Presets\n",
    "\n",
    "```python\n",
    "@daily      # 0 0 * * *\n",
    "@hourly     # 0 * * * *\n",
    "@weekly     # 0 0 * * 0\n",
    "@monthly    # 0 0 1 * *\n",
    "@yearly     # 0 0 1 1 *\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Orchestration Tools Comparison\n",
    "\n",
    "### Apache Airflow\n",
    "\n",
    "**Pros**:\n",
    "- [OK] Most popular and mature\n",
    "- [OK] Rich ecosystem of operators\n",
    "- [OK] Great UI\n",
    "- [OK] Strong community\n",
    "\n",
    "**Cons**:\n",
    "- [FAIL] Complex setup\n",
    "- [FAIL] Requires infrastructure management\n",
    "- [FAIL] Steep learning curve\n",
    "\n",
    "**Best For**: Enterprise workflows, complex dependencies\n",
    "\n",
    "---\n",
    "\n",
    "### Prefect\n",
    "\n",
    "**Pros**:\n",
    "- [OK] Modern Python-first approach\n",
    "- [OK] Easier to set up\n",
    "- [OK] Better error handling\n",
    "- [OK] Cloud-native\n",
    "\n",
    "**Cons**:\n",
    "- [FAIL] Less mature than Airflow\n",
    "- [FAIL] Smaller ecosystem\n",
    "\n",
    "**Best For**: Python-centric teams, modern data stacks\n",
    "\n",
    "---\n",
    "\n",
    "### Dagster\n",
    "\n",
    "**Pros**:\n",
    "- [OK] Development-focused\n",
    "- [OK] Strong typing\n",
    "- [OK] Testing built-in\n",
    "- [OK] Data-aware orchestration\n",
    "\n",
    "**Cons**:\n",
    "- [FAIL] Newer, smaller community\n",
    "- [FAIL] Different paradigm (learning curve)\n",
    "\n",
    "**Best For**: Software engineering teams, data apps\n",
    "\n",
    "---\n",
    "\n",
    "### Others\n",
    "\n",
    "- **Luigi** (Spotify): Older, simpler, less features\n",
    "- **Argo Workflows** (Kubernetes-native)\n",
    "- **Temporal** (General workflow engine)\n",
    "- **AWS Step Functions** (AWS-specific)\n",
    "- **dbt** (SQL transformations only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Best Practices\n",
    "\n",
    "### DAG Design\n",
    "\n",
    "1. **Keep DAGs simple**: One DAG per business process\n",
    "2. **Make tasks idempotent**: Safe to run multiple times\n",
    "3. **Use operators wisely**: Don't put too much logic in operators\n",
    "4. **Handle failures**: Add retries and alerts\n",
    "5. **Document**: Add descriptions to DAGs and tasks\n",
    "\n",
    "### Performance\n",
    "\n",
    "1. **Parallelize**: Run independent tasks in parallel\n",
    "2. **Pool resources**: Limit concurrent tasks\n",
    "3. **Optimize sensors**: Don't poll too frequently\n",
    "4. **Monitor**: Track execution times and resource usage\n",
    "\n",
    "### Maintenance\n",
    "\n",
    "1. **Version control**: Keep DAGs in Git\n",
    "2. **Testing**: Test DAGs before deployment\n",
    "3. **Backfilling**: Handle historical data loads carefully\n",
    "4. **Cleanup**: Remove old task instances and logs\n",
    "\n",
    "### Security\n",
    "\n",
    "1. **Use connections**: Store credentials securely\n",
    "2. **Variables**: Use Airflow variables for config\n",
    "3. **Secrets backend**: Integrate with secret managers\n",
    "4. **RBAC**: Control access to DAGs and features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Key Takeaways\n",
    "\n",
    "[OK] **Orchestration**: Automated management of complex workflows\n",
    "\n",
    "[OK] **DAG**: Directed Acyclic Graph - workflow with dependencies\n",
    "\n",
    "[OK] **Airflow Components**: DAG, Operators, Tasks, Scheduler, Executor\n",
    "\n",
    "[OK] **Scheduling**: Use cron expressions or presets (@daily, @hourly)\n",
    "\n",
    "[OK] **Tools**: Airflow (mature), Prefect (modern), Dagster (dev-focused)\n",
    "\n",
    "[OK] **Best Practices**: Idempotency, retries, monitoring, testing\n",
    "\n",
    "### When to Use Orchestration?\n",
    "\n",
    "- Multiple dependent tasks\n",
    "- Scheduled recurring jobs\n",
    "- Need for monitoring and alerts\n",
    "- Complex data pipelines\n",
    "- Production environments\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In **Module 08: Data Quality and Validation**, we'll:\n",
    "- Learn data quality dimensions\n",
    "- Implement validation checks\n",
    "- Use data quality frameworks\n",
    "- Test data pipelines\n",
    "- Set up data contracts\n",
    "\n",
    "---\n",
    "\n",
    "**Ready to ensure data quality?** Open `08_data_quality_validation.ipynb`!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
