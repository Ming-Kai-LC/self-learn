{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 02: Data Sources and Extraction\n",
    "\n",
    "**Estimated Time:** 45-60 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this module, you will:\n",
    "- Extract data from various file formats (CSV, JSON, Excel, Parquet)\n",
    "- Connect to and query databases using SQLAlchemy\n",
    "- Make API requests to extract data from REST APIs\n",
    "- Implement error handling and retry logic\n",
    "- Understand best practices for data extraction\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Types of Data Sources\n",
    "\n",
    "Data engineers work with various data sources:\n",
    "\n",
    "### Common Data Sources\n",
    "\n",
    "1. **Files**\n",
    "   - CSV, TSV (Comma/Tab Separated Values)\n",
    "   - JSON (JavaScript Object Notation)\n",
    "   - Parquet (Columnar format)\n",
    "   - Excel (.xlsx, .xls)\n",
    "   - XML\n",
    "   - Log files\n",
    "\n",
    "2. **Databases**\n",
    "   - Relational (PostgreSQL, MySQL, SQL Server)\n",
    "   - NoSQL (MongoDB, Cassandra, DynamoDB)\n",
    "   - Data Warehouses (Snowflake, BigQuery, Redshift)\n",
    "\n",
    "3. **APIs**\n",
    "   - REST APIs (most common)\n",
    "   - GraphQL\n",
    "   - SOAP (legacy)\n",
    "\n",
    "4. **Streaming**\n",
    "   - Kafka, Kinesis, Pub/Sub\n",
    "   - WebSockets\n",
    "   - Message queues (RabbitMQ, SQS)\n",
    "\n",
    "5. **Cloud Storage**\n",
    "   - S3, Google Cloud Storage, Azure Blob\n",
    "\n",
    "In this module, we'll focus on files, databases, and APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from sqlalchemy import create_engine, text\n",
    "import time\n",
    "import os\n",
    "\n",
    "print(\"[OK] Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Extracting Data from Files\n",
    "\n",
    "### 2.1 CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's create some sample CSV data\n",
    "sample_csv_data = \"\"\"user_id,name,email,signup_date,country\n",
    "1,Alice Smith,alice@example.com,2024-01-15,USA\n",
    "2,Bob Jones,bob@example.com,2024-01-16,UK\n",
    "3,Carol Davis,carol@example.com,2024-01-17,Canada\n",
    "4,David Wilson,david@example.com,2024-01-18,Australia\n",
    "5,Eve Martinez,eve@example.com,2024-01-19,Spain\"\"\"\n",
    "\n",
    "# Save to file\n",
    "with open(\"../data/raw/users.csv\", \"w\") as f:\n",
    "    f.write(sample_csv_data)\n",
    "\n",
    "print(\"[OK] Sample CSV file created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data from CSV\n",
    "def extract_csv(file_path):\n",
    "    \"\"\"\n",
    "    Extract data from a CSV file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"[OK] Successfully read {len(df)} records from {file_path}\")\n",
    "        print(f\"   Columns: {list(df.columns)}\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"[FAIL] File not found: {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"[FAIL] Error reading CSV: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Extract\n",
    "users_df = extract_csv(\"../data/raw/users.csv\")\n",
    "users_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced CSV reading options\n",
    "def extract_csv_advanced(file_path, **kwargs):\n",
    "    \"\"\"\n",
    "    Advanced CSV extraction with options:\n",
    "    - chunksize: Read in chunks for large files\n",
    "    - usecols: Only read specific columns\n",
    "    - parse_dates: Automatically parse date columns\n",
    "    - dtype: Specify data types\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(\n",
    "        file_path,\n",
    "        parse_dates=[\"signup_date\"],  # Parse date columns\n",
    "        dtype={\"user_id\": int},  # Specify data types\n",
    "        **kwargs,\n",
    "    )\n",
    "    print(f\"[OK] Read {len(df)} records with advanced options\")\n",
    "    print(f\"   Data types: {df.dtypes.to_dict()}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "users_df_advanced = extract_csv_advanced(\"../data/raw/users.csv\")\n",
    "print(\"\\nData preview:\")\n",
    "users_df_advanced.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 JSON Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample JSON data\n",
    "sample_json_data = [\n",
    "    {\n",
    "        \"product_id\": \"P001\",\n",
    "        \"name\": \"Laptop\",\n",
    "        \"price\": 999.99,\n",
    "        \"category\": \"Electronics\",\n",
    "        \"in_stock\": True,\n",
    "    },\n",
    "    {\n",
    "        \"product_id\": \"P002\",\n",
    "        \"name\": \"Mouse\",\n",
    "        \"price\": 29.99,\n",
    "        \"category\": \"Electronics\",\n",
    "        \"in_stock\": True,\n",
    "    },\n",
    "    {\n",
    "        \"product_id\": \"P003\",\n",
    "        \"name\": \"Keyboard\",\n",
    "        \"price\": 79.99,\n",
    "        \"category\": \"Electronics\",\n",
    "        \"in_stock\": False,\n",
    "    },\n",
    "    {\n",
    "        \"product_id\": \"P004\",\n",
    "        \"name\": \"Monitor\",\n",
    "        \"price\": 299.99,\n",
    "        \"category\": \"Electronics\",\n",
    "        \"in_stock\": True,\n",
    "    },\n",
    "]\n",
    "\n",
    "with open(\"../data/raw/products.json\", \"w\") as f:\n",
    "    json.dump(sample_json_data, f, indent=2)\n",
    "\n",
    "print(\"[OK] Sample JSON file created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract JSON data\n",
    "def extract_json(file_path):\n",
    "    \"\"\"\n",
    "    Extract data from a JSON file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Method 1: Using pandas\n",
    "        df = pd.read_json(file_path)\n",
    "        print(f\"[OK] Successfully read {len(df)} records from JSON\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"[FAIL] Error reading JSON: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "products_df = extract_json(\"../data/raw/products.json\")\n",
    "products_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For nested JSON (common with APIs)\n",
    "nested_json_data = {\n",
    "    \"metadata\": {\"timestamp\": \"2024-01-20T10:00:00Z\", \"source\": \"sales_api\"},\n",
    "    \"data\": [\n",
    "        {\"order_id\": 1, \"customer\": {\"id\": 101, \"name\": \"Alice\"}, \"total\": 150.00},\n",
    "        {\"order_id\": 2, \"customer\": {\"id\": 102, \"name\": \"Bob\"}, \"total\": 200.00},\n",
    "    ],\n",
    "}\n",
    "\n",
    "with open(\"../data/raw/orders.json\", \"w\") as f:\n",
    "    json.dump(nested_json_data, f, indent=2)\n",
    "\n",
    "\n",
    "# Extract nested JSON\n",
    "def extract_nested_json(file_path):\n",
    "    \"\"\"\n",
    "    Extract nested JSON and flatten it\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Extract the data array and flatten nested structures\n",
    "    df = pd.json_normalize(data[\"data\"])\n",
    "    print(f\"[OK] Extracted and flattened {len(df)} records\")\n",
    "    return df\n",
    "\n",
    "\n",
    "orders_df = extract_nested_json(\"../data/raw/orders.json\")\n",
    "orders_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Excel Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample Excel file\n",
    "sales_data = {\n",
    "    \"date\": pd.date_range(\"2024-01-01\", periods=10),\n",
    "    \"product\": [\"A\", \"B\", \"C\", \"A\", \"B\"] * 2,\n",
    "    \"quantity\": np.random.randint(1, 50, 10),\n",
    "    \"revenue\": np.random.uniform(100, 1000, 10).round(2),\n",
    "}\n",
    "\n",
    "sales_df = pd.DataFrame(sales_data)\n",
    "sales_df.to_excel(\"../data/raw/sales.xlsx\", sheet_name=\"Sales\", index=False)\n",
    "\n",
    "print(\"[OK] Sample Excel file created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract from Excel\n",
    "def extract_excel(file_path, sheet_name=0):\n",
    "    \"\"\"\n",
    "    Extract data from Excel file\n",
    "\n",
    "    sheet_name: can be sheet name (str) or index (int)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "        print(f\"[OK] Successfully read {len(df)} records from Excel\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"[FAIL] Error reading Excel: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "sales_df_extracted = extract_excel(\"../data/raw/sales.xlsx\", sheet_name=\"Sales\")\n",
    "sales_df_extracted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Parquet Files (Columnar Format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample Parquet file\n",
    "large_data = {\n",
    "    \"id\": range(1000),\n",
    "    \"value\": np.random.randn(1000),\n",
    "    \"category\": np.random.choice([\"A\", \"B\", \"C\"], 1000),\n",
    "}\n",
    "\n",
    "large_df = pd.DataFrame(large_data)\n",
    "large_df.to_parquet(\"../data/raw/large_dataset.parquet\", compression=\"snappy\")\n",
    "\n",
    "print(\"[OK] Sample Parquet file created\")\n",
    "print(f\"   Records: {len(large_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract from Parquet\n",
    "def extract_parquet(file_path, columns=None):\n",
    "    \"\"\"\n",
    "    Extract data from Parquet file\n",
    "\n",
    "    Parquet advantages:\n",
    "    - Columnar format (fast for analytical queries)\n",
    "    - Built-in compression\n",
    "    - Can read specific columns only\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_parquet(file_path, columns=columns)\n",
    "        print(f\"[OK] Successfully read {len(df):,} records from Parquet\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"[FAIL] Error reading Parquet: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Read all columns\n",
    "parquet_df = extract_parquet(\"../data/raw/large_dataset.parquet\")\n",
    "\n",
    "# Read specific columns only (more efficient)\n",
    "parquet_df_subset = extract_parquet(\"../data/raw/large_dataset.parquet\", columns=[\"id\", \"category\"])\n",
    "\n",
    "print(\"\\nFull data shape:\", parquet_df.shape)\n",
    "print(\"Subset data shape:\", parquet_df_subset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Extracting Data from Databases\n",
    "\n",
    "We'll use SQLite for this example (no server needed), but the same principles apply to PostgreSQL, MySQL, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample SQLite database\n",
    "from sqlalchemy import create_engine\n",
    "import sqlite3\n",
    "\n",
    "# Create engine\n",
    "db_path = \"../data/raw/sample_db.sqlite\"\n",
    "engine = create_engine(f\"sqlite:///{db_path}\")\n",
    "\n",
    "# Create sample table\n",
    "customers_data = {\n",
    "    \"customer_id\": range(1, 11),\n",
    "    \"name\": [f\"Customer {i}\" for i in range(1, 11)],\n",
    "    \"email\": [f\"customer{i}@example.com\" for i in range(1, 11)],\n",
    "    \"country\": np.random.choice([\"USA\", \"UK\", \"Canada\", \"Australia\"], 10),\n",
    "    \"lifetime_value\": np.random.uniform(100, 10000, 10).round(2),\n",
    "}\n",
    "\n",
    "customers_df = pd.DataFrame(customers_data)\n",
    "customers_df.to_sql(\"customers\", engine, if_exists=\"replace\", index=False)\n",
    "\n",
    "print(\"[OK] Sample database created with 'customers' table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract from database using SQL query\n",
    "def extract_from_database(engine, query):\n",
    "    \"\"\"\n",
    "    Extract data from database using SQL query\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_sql(query, engine)\n",
    "        print(f\"[OK] Successfully extracted {len(df)} records from database\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"[FAIL] Database extraction error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Simple query\n",
    "query1 = \"SELECT * FROM customers\"\n",
    "result1 = extract_from_database(engine, query1)\n",
    "result1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More complex query with filtering and aggregation\n",
    "query2 = \"\"\"\n",
    "SELECT \n",
    "    country,\n",
    "    COUNT(*) as customer_count,\n",
    "    AVG(lifetime_value) as avg_lifetime_value,\n",
    "    MAX(lifetime_value) as max_lifetime_value\n",
    "FROM customers\n",
    "GROUP BY country\n",
    "ORDER BY avg_lifetime_value DESC\n",
    "\"\"\"\n",
    "\n",
    "result2 = extract_from_database(engine, query2)\n",
    "result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract with parameters (prevents SQL injection)\n",
    "def extract_with_parameters(engine, query, params):\n",
    "    \"\"\"\n",
    "    Extract data using parameterized queries (secure)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_sql(query, engine, params=params)\n",
    "        print(f\"[OK] Extracted {len(df)} records using parameterized query\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"[FAIL] Error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Secure parameterized query\n",
    "query = \"SELECT * FROM customers WHERE country = :country AND lifetime_value > :min_value\"\n",
    "params = {\"country\": \"USA\", \"min_value\": 1000}\n",
    "\n",
    "filtered_customers = extract_with_parameters(engine, query, params)\n",
    "filtered_customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Extracting Data from APIs\n",
    "\n",
    "### 4.1 Simple GET Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data from a public API\n",
    "def extract_from_api(url, params=None):\n",
    "    \"\"\"\n",
    "    Extract data from a REST API\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status()  # Raise exception for bad status codes\n",
    "\n",
    "        data = response.json()\n",
    "        print(f\"[OK] Successfully fetched data from API\")\n",
    "        print(f\"   Status code: {response.status_code}\")\n",
    "\n",
    "        return data\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"[FAIL] API request error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Example: JSONPlaceholder (fake API for testing)\n",
    "api_url = \"https://jsonplaceholder.typicode.com/posts\"\n",
    "posts_data = extract_from_api(api_url)\n",
    "\n",
    "if posts_data:\n",
    "    # Convert to DataFrame\n",
    "    posts_df = pd.DataFrame(posts_data)\n",
    "    print(f\"\\nExtracted {len(posts_df)} posts\")\n",
    "    posts_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 API with Authentication and Headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract from API with authentication\n",
    "def extract_from_api_with_auth(url, api_key=None, headers=None, params=None):\n",
    "    \"\"\"\n",
    "    Extract data from API with authentication\n",
    "    \"\"\"\n",
    "    # Build headers\n",
    "    if headers is None:\n",
    "        headers = {}\n",
    "\n",
    "    if api_key:\n",
    "        headers[\"Authorization\"] = f\"Bearer {api_key}\"\n",
    "\n",
    "    headers.setdefault(\"Content-Type\", \"application/json\")\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        print(f\"[OK] API call successful\")\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"[FAIL] API error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Example (without real API key)\n",
    "# api_data = extract_from_api_with_auth(\n",
    "#     url=\"https://api.example.com/data\",\n",
    "#     api_key=\"your-api-key-here\",\n",
    "#     params={'limit': 100}\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Paginated API Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data from paginated API\n",
    "def extract_paginated_api(base_url, max_pages=5):\n",
    "    \"\"\"\n",
    "    Extract data from paginated API endpoints\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{base_url}?_page={page}&_limit=10\"\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            data = response.json()\n",
    "\n",
    "            if not data:  # No more data\n",
    "                break\n",
    "\n",
    "            all_data.extend(data)\n",
    "            print(f\"  Page {page}: Fetched {len(data)} records\")\n",
    "\n",
    "            # Be nice to the API - add delay\n",
    "            time.sleep(0.1)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[FAIL] Error on page {page}: {e}\")\n",
    "            break\n",
    "\n",
    "    print(f\"\\n[OK] Total records fetched: {len(all_data)}\")\n",
    "    return all_data\n",
    "\n",
    "\n",
    "# Fetch paginated data\n",
    "api_url = \"https://jsonplaceholder.typicode.com/posts\"\n",
    "all_posts = extract_paginated_api(api_url, max_pages=3)\n",
    "\n",
    "posts_df = pd.DataFrame(all_posts)\n",
    "posts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Error Handling and Retry Logic\n",
    "\n",
    "Production data pipelines need robust error handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement retry logic with exponential backoff\n",
    "import time\n",
    "from functools import wraps\n",
    "\n",
    "\n",
    "def retry_with_backoff(max_retries=3, initial_delay=1, backoff_factor=2):\n",
    "    \"\"\"\n",
    "    Decorator to retry a function with exponential backoff\n",
    "    \"\"\"\n",
    "\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            delay = initial_delay\n",
    "\n",
    "            for attempt in range(max_retries):\n",
    "                try:\n",
    "                    return func(*args, **kwargs)\n",
    "                except Exception as e:\n",
    "                    if attempt == max_retries - 1:\n",
    "                        print(f\"[FAIL] Failed after {max_retries} attempts: {e}\")\n",
    "                        raise\n",
    "\n",
    "                    print(f\"[WARNING] Attempt {attempt + 1} failed: {e}\")\n",
    "                    print(f\"   Retrying in {delay} seconds...\")\n",
    "                    time.sleep(delay)\n",
    "                    delay *= backoff_factor\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    return decorator\n",
    "\n",
    "\n",
    "@retry_with_backoff(max_retries=3, initial_delay=1)\n",
    "def extract_with_retry(url):\n",
    "    \"\"\"\n",
    "    Extract data with automatic retry\n",
    "    \"\"\"\n",
    "    response = requests.get(url, timeout=5)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "# Test with a valid URL\n",
    "try:\n",
    "    data = extract_with_retry(\"https://jsonplaceholder.typicode.com/users/1\")\n",
    "    print(\"[OK] Successfully extracted data with retry logic\")\n",
    "    print(json.dumps(data, indent=2)[:200], \"...\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Best Practices for Data Extraction\n",
    "\n",
    "### 1. Always Use Connection Pooling\n",
    "- Reuse database connections\n",
    "- Don't create new connections for each query\n",
    "\n",
    "### 2. Implement Proper Error Handling\n",
    "- Catch specific exceptions\n",
    "- Log errors properly\n",
    "- Use retry logic for transient failures\n",
    "\n",
    "### 3. Be Mindful of API Rate Limits\n",
    "- Add delays between requests\n",
    "- Implement exponential backoff\n",
    "- Cache responses when appropriate\n",
    "\n",
    "### 4. Extract Incrementally When Possible\n",
    "- Use timestamps or IDs to track what's been extracted\n",
    "- Don't re-extract all data every time\n",
    "\n",
    "### 5. Validate Data Early\n",
    "- Check for expected columns\n",
    "- Verify data types\n",
    "- Count records\n",
    "\n",
    "### 6. Use Appropriate File Formats\n",
    "- CSV: Simple, human-readable\n",
    "- Parquet: Large datasets, analytical workloads\n",
    "- JSON: Nested/hierarchical data\n",
    "\n",
    "### 7. Monitor and Log\n",
    "- Track extraction times\n",
    "- Log record counts\n",
    "- Alert on failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete extraction example with best practices\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class DataExtractor:\n",
    "    \"\"\"\n",
    "    Production-ready data extractor with best practices\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.extraction_stats = {}\n",
    "\n",
    "    def extract(self, source_type, **kwargs):\n",
    "        \"\"\"\n",
    "        Extract data from various sources\n",
    "        \"\"\"\n",
    "        start_time = datetime.now()\n",
    "        logger.info(f\"Starting extraction from {source_type}\")\n",
    "\n",
    "        try:\n",
    "            if source_type == \"csv\":\n",
    "                df = pd.read_csv(kwargs[\"file_path\"])\n",
    "            elif source_type == \"json\":\n",
    "                df = pd.read_json(kwargs[\"file_path\"])\n",
    "            elif source_type == \"database\":\n",
    "                df = pd.read_sql(kwargs[\"query\"], kwargs[\"engine\"])\n",
    "            elif source_type == \"api\":\n",
    "                response = requests.get(kwargs[\"url\"])\n",
    "                response.raise_for_status()\n",
    "                df = pd.DataFrame(response.json())\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported source type: {source_type}\")\n",
    "\n",
    "            # Calculate stats\n",
    "            duration = (datetime.now() - start_time).total_seconds()\n",
    "            record_count = len(df)\n",
    "\n",
    "            self.extraction_stats[source_type] = {\n",
    "                \"records\": record_count,\n",
    "                \"duration_seconds\": duration,\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "            }\n",
    "\n",
    "            logger.info(f\"[OK] Extracted {record_count:,} records in {duration:.2f}s\")\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"[FAIL] Extraction failed: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "# Use the extractor\n",
    "extractor = DataExtractor()\n",
    "\n",
    "# Extract from CSV\n",
    "users = extractor.extract(\"csv\", file_path=\"../data/raw/users.csv\")\n",
    "\n",
    "# Extract from JSON\n",
    "products = extractor.extract(\"json\", file_path=\"../data/raw/products.json\")\n",
    "\n",
    "# View extraction statistics\n",
    "print(\"\\nExtraction Statistics:\")\n",
    "for source, stats in extractor.extraction_stats.items():\n",
    "    print(f\"  {source}: {stats['records']:,} records in {stats['duration_seconds']:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Practice Exercise\n",
    "\n",
    "Create a unified extractor function that can handle multiple source types and includes:\n",
    "1. Error handling\n",
    "2. Logging\n",
    "3. Data validation\n",
    "4. Statistics tracking\n",
    "\n",
    "Try implementing it below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Create a function that extracts data from any source type\n",
    "# and validates that it has at least 1 record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Key Takeaways\n",
    "\n",
    "[OK] **File Extraction**: CSV, JSON, Excel, Parquet - each has different use cases\n",
    "\n",
    "[OK] **Database Extraction**: Use SQLAlchemy for database-agnostic queries\n",
    "\n",
    "[OK] **API Extraction**: Handle pagination, authentication, and rate limits\n",
    "\n",
    "[OK] **Error Handling**: Retry logic with exponential backoff is crucial\n",
    "\n",
    "[OK] **Best Practices**: Log everything, validate early, extract incrementally\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In **Module 03: Data Transformation and Cleaning**, we'll take the extracted data and:\n",
    "- Clean and validate it\n",
    "- Handle missing values\n",
    "- Transform data types\n",
    "- Merge and aggregate datasets\n",
    "\n",
    "---\n",
    "\n",
    "**Ready to transform data?** Open `03_data_transformation_cleaning.ipynb`!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
