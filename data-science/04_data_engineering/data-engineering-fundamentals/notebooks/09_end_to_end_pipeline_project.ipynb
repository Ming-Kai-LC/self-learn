{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 09: End-to-End Pipeline Project\n",
    "\n",
    "**Estimated Time:** 90-120 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this module, you will:\n",
    "- Build a complete production-ready data pipeline\n",
    "- Apply all concepts from previous modules\n",
    "- Implement extraction, transformation, validation, and loading\n",
    "- Add comprehensive logging and error handling\n",
    "- Test and document the pipeline\n",
    "- Understand deployment considerations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project: E-Commerce Sales Analytics Pipeline\n",
    "\n",
    "### Business Requirements\n",
    "\n",
    "Build a daily ETL pipeline that:\n",
    "1. **Extracts** sales data from multiple sources (CSV files, API)\n",
    "2. **Transforms** and enriches the data\n",
    "3. **Validates** data quality\n",
    "4. **Loads** to a data warehouse (Parquet files)\n",
    "5. **Generates** summary reports\n",
    "\n",
    "### Data Sources\n",
    "\n",
    "- **Orders**: Customer orders (CSV)\n",
    "- **Products**: Product catalog (CSV)\n",
    "- **Customers**: Customer information (API simulation)\n",
    "\n",
    "### Success Criteria\n",
    "\n",
    "- [OK] All data sources successfully extracted\n",
    "- [OK] Data quality checks pass (>95% quality score)\n",
    "- [OK] Output matches expected schema\n",
    "- [OK] Pipeline completes in < 5 minutes\n",
    "- [OK] Comprehensive logging\n",
    "- [OK] Error handling and recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "import json\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[logging.FileHandler(\"../data/processed/pipeline.log\"), logging.StreamHandler()],\n",
    ")\n",
    "logger = logging.getLogger(\"SalesPipeline\")\n",
    "\n",
    "print(\"[OK] Libraries loaded and logging configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Generate Sample Data Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample orders data\n",
    "np.random.seed(42)\n",
    "\n",
    "n_orders = 1000\n",
    "orders = pd.DataFrame(\n",
    "    {\n",
    "        \"order_id\": range(1, n_orders + 1),\n",
    "        \"customer_id\": np.random.randint(1, 201, n_orders),\n",
    "        \"product_id\": np.random.randint(1, 51, n_orders),\n",
    "        \"quantity\": np.random.randint(1, 10, n_orders),\n",
    "        \"order_date\": [\n",
    "            datetime(2024, 1, 1) + timedelta(days=np.random.randint(0, 90)) for _ in range(n_orders)\n",
    "        ],\n",
    "        \"status\": np.random.choice(\n",
    "            [\"pending\", \"processing\", \"shipped\", \"delivered\", \"cancelled\"], n_orders\n",
    "        ),\n",
    "    }\n",
    ")\n",
    "\n",
    "orders.to_csv(\"../data/raw/orders.csv\", index=False)\n",
    "print(f\"[OK] Generated {len(orders):,} orders\")\n",
    "\n",
    "# Generate sample products data\n",
    "products = pd.DataFrame(\n",
    "    {\n",
    "        \"product_id\": range(1, 51),\n",
    "        \"product_name\": [f\"Product {i}\" for i in range(1, 51)],\n",
    "        \"category\": np.random.choice([\"Electronics\", \"Clothing\", \"Home\", \"Books\", \"Sports\"], 50),\n",
    "        \"price\": np.random.uniform(10, 500, 50).round(2),\n",
    "        \"cost\": np.random.uniform(5, 300, 50).round(2),\n",
    "    }\n",
    ")\n",
    "\n",
    "products.to_csv(\"../data/raw/products.csv\", index=False)\n",
    "print(f\"[OK] Generated {len(products)} products\")\n",
    "\n",
    "# Generate sample customers (simulating API data)\n",
    "customers = pd.DataFrame(\n",
    "    {\n",
    "        \"customer_id\": range(1, 201),\n",
    "        \"name\": [f\"Customer {i}\" for i in range(1, 201)],\n",
    "        \"email\": [f\"customer{i}@example.com\" for i in range(1, 201)],\n",
    "        \"country\": np.random.choice([\"USA\", \"UK\", \"Canada\", \"Australia\", \"Germany\"], 200),\n",
    "        \"signup_date\": [\n",
    "            datetime(2023, 1, 1) + timedelta(days=np.random.randint(0, 365)) for _ in range(200)\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "# Save as JSON (simulating API response)\n",
    "customers.to_json(\"../data/raw/customers.json\", orient=\"records\", indent=2)\n",
    "print(f\"[OK] Generated {len(customers)} customers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Build the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SalesDataPipeline:\n",
    "    \"\"\"\n",
    "    End-to-end sales data pipeline\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.logger = logging.getLogger(f\"{__name__}.Pipeline\")\n",
    "        self.metrics = {\n",
    "            \"start_time\": None,\n",
    "            \"end_time\": None,\n",
    "            \"duration_seconds\": None,\n",
    "            \"records_extracted\": {},\n",
    "            \"records_transformed\": 0,\n",
    "            \"records_loaded\": 0,\n",
    "            \"quality_score\": 0.0,\n",
    "            \"status\": \"pending\",\n",
    "            \"errors\": [],\n",
    "        }\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Execute complete pipeline\"\"\"\n",
    "        self.metrics[\"start_time\"] = datetime.now()\n",
    "        self.logger.info(\"=\" * 80)\n",
    "        self.logger.info(\"SALES DATA PIPELINE STARTED\")\n",
    "        self.logger.info(\"=\" * 80)\n",
    "\n",
    "        try:\n",
    "            # Phase 1: Extract\n",
    "            orders_df, products_df, customers_df = self._extract()\n",
    "\n",
    "            # Phase 2: Transform\n",
    "            enriched_df = self._transform(orders_df, products_df, customers_df)\n",
    "\n",
    "            # Phase 3: Validate\n",
    "            self._validate(enriched_df)\n",
    "\n",
    "            # Phase 4: Load\n",
    "            self._load(enriched_df)\n",
    "\n",
    "            # Phase 5: Generate Reports\n",
    "            self._generate_reports(enriched_df)\n",
    "\n",
    "            # Success\n",
    "            self.metrics[\"status\"] = \"success\"\n",
    "\n",
    "        except Exception as e:\n",
    "            self.metrics[\"status\"] = \"failed\"\n",
    "            self.metrics[\"errors\"].append(str(e))\n",
    "            self.logger.error(f\"Pipeline failed: {e}\", exc_info=True)\n",
    "            raise\n",
    "\n",
    "        finally:\n",
    "            self.metrics[\"end_time\"] = datetime.now()\n",
    "            self.metrics[\"duration_seconds\"] = (\n",
    "                self.metrics[\"end_time\"] - self.metrics[\"start_time\"]\n",
    "            ).total_seconds()\n",
    "            self._log_metrics()\n",
    "\n",
    "    def _extract(self):\n",
    "        \"\"\"Extract data from all sources\"\"\"\n",
    "        self.logger.info(\"[EXTRACT] Starting data extraction\")\n",
    "\n",
    "        # Extract orders (CSV)\n",
    "        orders_df = pd.read_csv(\"../data/raw/orders.csv\")\n",
    "        orders_df[\"order_date\"] = pd.to_datetime(orders_df[\"order_date\"])\n",
    "        self.metrics[\"records_extracted\"][\"orders\"] = len(orders_df)\n",
    "        self.logger.info(f\"[EXTRACT] Orders: {len(orders_df):,} records\")\n",
    "\n",
    "        # Extract products (CSV)\n",
    "        products_df = pd.read_csv(\"../data/raw/products.csv\")\n",
    "        self.metrics[\"records_extracted\"][\"products\"] = len(products_df)\n",
    "        self.logger.info(f\"[EXTRACT] Products: {len(products_df):,} records\")\n",
    "\n",
    "        # Extract customers (JSON - simulated API)\n",
    "        customers_df = pd.read_json(\"../data/raw/customers.json\")\n",
    "        customers_df[\"signup_date\"] = pd.to_datetime(customers_df[\"signup_date\"])\n",
    "        self.metrics[\"records_extracted\"][\"customers\"] = len(customers_df)\n",
    "        self.logger.info(f\"[EXTRACT] Customers: {len(customers_df):,} records\")\n",
    "\n",
    "        self.logger.info(\"[EXTRACT] [OK] Extraction complete\")\n",
    "        return orders_df, products_df, customers_df\n",
    "\n",
    "    def _transform(self, orders_df, products_df, customers_df):\n",
    "        \"\"\"Transform and enrich data\"\"\"\n",
    "        self.logger.info(\"[TRANSFORM] Starting data transformation\")\n",
    "\n",
    "        # Join orders with products\n",
    "        enriched = orders_df.merge(products_df, on=\"product_id\", how=\"left\")\n",
    "        self.logger.info(\"[TRANSFORM] Joined orders with products\")\n",
    "\n",
    "        # Join with customers\n",
    "        enriched = enriched.merge(customers_df, on=\"customer_id\", how=\"left\")\n",
    "        self.logger.info(\"[TRANSFORM] Joined with customers\")\n",
    "\n",
    "        # Calculate revenue and profit\n",
    "        enriched[\"revenue\"] = (enriched[\"quantity\"] * enriched[\"price\"]).round(2)\n",
    "        enriched[\"cost_total\"] = (enriched[\"quantity\"] * enriched[\"cost\"]).round(2)\n",
    "        enriched[\"profit\"] = (enriched[\"revenue\"] - enriched[\"cost_total\"]).round(2)\n",
    "        self.logger.info(\"[TRANSFORM] Calculated revenue and profit\")\n",
    "\n",
    "        # Add derived columns\n",
    "        enriched[\"year\"] = enriched[\"order_date\"].dt.year\n",
    "        enriched[\"month\"] = enriched[\"order_date\"].dt.month\n",
    "        enriched[\"quarter\"] = enriched[\"order_date\"].dt.quarter\n",
    "        enriched[\"is_delivered\"] = enriched[\"status\"] == \"delivered\"\n",
    "        self.logger.info(\"[TRANSFORM] Added derived columns\")\n",
    "\n",
    "        # Clean data\n",
    "        enriched = enriched.dropna()\n",
    "        self.logger.info(f\"[TRANSFORM] Removed null values\")\n",
    "\n",
    "        self.metrics[\"records_transformed\"] = len(enriched)\n",
    "        self.logger.info(f\"[TRANSFORM] [OK] Transformation complete: {len(enriched):,} records\")\n",
    "\n",
    "        return enriched\n",
    "\n",
    "    def _validate(self, df):\n",
    "        \"\"\"Validate data quality\"\"\"\n",
    "        self.logger.info(\"[VALIDATE] Starting data validation\")\n",
    "\n",
    "        validation_checks = []\n",
    "\n",
    "        # Check 1: No nulls in critical columns\n",
    "        critical_cols = [\"order_id\", \"customer_id\", \"product_id\", \"revenue\"]\n",
    "        for col in critical_cols:\n",
    "            null_count = df[col].isnull().sum()\n",
    "            passed = null_count == 0\n",
    "            validation_checks.append(passed)\n",
    "            status = \"[OK]\" if passed else \"[FAIL]\"\n",
    "            self.logger.info(f\"[VALIDATE] {status} No nulls in {col}: {passed}\")\n",
    "\n",
    "        # Check 2: Revenue is positive\n",
    "        revenue_valid = (df[\"revenue\"] >= 0).all()\n",
    "        validation_checks.append(revenue_valid)\n",
    "        status = \"[OK]\" if revenue_valid else \"[FAIL]\"\n",
    "        self.logger.info(f\"[VALIDATE] {status} All revenue >= 0: {revenue_valid}\")\n",
    "\n",
    "        # Check 3: Quantity is positive\n",
    "        qty_valid = (df[\"quantity\"] > 0).all()\n",
    "        validation_checks.append(qty_valid)\n",
    "        status = \"[OK]\" if qty_valid else \"[FAIL]\"\n",
    "        self.logger.info(f\"[VALIDATE] {status} All quantity > 0: {qty_valid}\")\n",
    "\n",
    "        # Calculate quality score\n",
    "        quality_score = sum(validation_checks) / len(validation_checks) * 100\n",
    "        self.metrics[\"quality_score\"] = quality_score\n",
    "\n",
    "        self.logger.info(f\"[VALIDATE] Quality Score: {quality_score:.1f}%\")\n",
    "\n",
    "        if quality_score < 95:\n",
    "            raise ValueError(f\"Data quality check failed: {quality_score:.1f}% < 95%\")\n",
    "\n",
    "        self.logger.info(\"[VALIDATE] [OK] Validation passed\")\n",
    "\n",
    "    def _load(self, df):\n",
    "        \"\"\"Load data to warehouse\"\"\"\n",
    "        self.logger.info(\"[LOAD] Starting data load\")\n",
    "\n",
    "        # Load to Parquet (partitioned by year/month)\n",
    "        output_path = \"../data/processed/sales_warehouse.parquet\"\n",
    "        df.to_parquet(output_path, compression=\"snappy\", index=False)\n",
    "\n",
    "        self.metrics[\"records_loaded\"] = len(df)\n",
    "        self.logger.info(f\"[LOAD] [OK] Loaded {len(df):,} records to {output_path}\")\n",
    "\n",
    "    def _generate_reports(self, df):\n",
    "        \"\"\"Generate summary reports\"\"\"\n",
    "        self.logger.info(\"[REPORT] Generating summary reports\")\n",
    "\n",
    "        # Summary by category\n",
    "        category_summary = (\n",
    "            df.groupby(\"category\")\n",
    "            .agg({\"order_id\": \"count\", \"revenue\": \"sum\", \"profit\": \"sum\"})\n",
    "            .round(2)\n",
    "        )\n",
    "        category_summary.columns = [\"orders\", \"revenue\", \"profit\"]\n",
    "        category_summary.to_csv(\"../data/processed/category_summary.csv\")\n",
    "        self.logger.info(\"[REPORT] Category summary saved\")\n",
    "\n",
    "        # Summary by country\n",
    "        country_summary = (\n",
    "            df.groupby(\"country\")\n",
    "            .agg({\"order_id\": \"count\", \"revenue\": \"sum\", \"customer_id\": \"nunique\"})\n",
    "            .round(2)\n",
    "        )\n",
    "        country_summary.columns = [\"orders\", \"revenue\", \"unique_customers\"]\n",
    "        country_summary.to_csv(\"../data/processed/country_summary.csv\")\n",
    "        self.logger.info(\"[REPORT] Country summary saved\")\n",
    "\n",
    "        self.logger.info(\"[REPORT] [OK] Reports generated\")\n",
    "\n",
    "    def _log_metrics(self):\n",
    "        \"\"\"Log pipeline execution metrics\"\"\"\n",
    "        self.logger.info(\"=\" * 80)\n",
    "        self.logger.info(\"PIPELINE EXECUTION SUMMARY\")\n",
    "        self.logger.info(\"=\" * 80)\n",
    "        self.logger.info(f\"Status: {self.metrics['status']}\")\n",
    "        self.logger.info(f\"Duration: {self.metrics['duration_seconds']:.2f}s\")\n",
    "        self.logger.info(f\"Records Extracted: {self.metrics['records_extracted']}\")\n",
    "        self.logger.info(f\"Records Transformed: {self.metrics['records_transformed']:,}\")\n",
    "        self.logger.info(f\"Records Loaded: {self.metrics['records_loaded']:,}\")\n",
    "        self.logger.info(f\"Quality Score: {self.metrics['quality_score']:.1f}%\")\n",
    "        if self.metrics[\"errors\"]:\n",
    "            self.logger.info(f\"Errors: {self.metrics['errors']}\")\n",
    "        self.logger.info(\"=\" * 80)\n",
    "\n",
    "\n",
    "print(\"[OK] Pipeline class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Run the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure pipeline\n",
    "config = {\"name\": \"sales_analytics_pipeline\", \"version\": \"1.0.0\", \"schedule\": \"daily at 2:00 AM\"}\n",
    "\n",
    "# Create and run pipeline\n",
    "pipeline = SalesDataPipeline(config)\n",
    "pipeline.run()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"[OK] PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Verify Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and verify warehouse data\n",
    "warehouse_df = pd.read_parquet(\"../data/processed/sales_warehouse.parquet\")\n",
    "\n",
    "print(\"Warehouse Data:\")\n",
    "print(f\"  Total Records: {len(warehouse_df):,}\")\n",
    "print(f\"  Columns: {list(warehouse_df.columns)}\")\n",
    "print(f\"  Date Range: {warehouse_df['order_date'].min()} to {warehouse_df['order_date'].max()}\")\n",
    "print(f\"  Total Revenue: ${warehouse_df['revenue'].sum():,.2f}\")\n",
    "print(f\"  Total Profit: ${warehouse_df['profit'].sum():,.2f}\")\n",
    "\n",
    "print(\"\\nSample Records:\")\n",
    "warehouse_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View summary reports\n",
    "category_summary = pd.read_csv(\"../data/processed/category_summary.csv\")\n",
    "country_summary = pd.read_csv(\"../data/processed/country_summary.csv\")\n",
    "\n",
    "print(\"Category Summary:\")\n",
    "print(category_summary.sort_values(\"revenue\", ascending=False))\n",
    "\n",
    "print(\"\\nCountry Summary:\")\n",
    "print(country_summary.sort_values(\"revenue\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Deployment Considerations\n",
    "\n",
    "### Production Deployment Checklist\n",
    "\n",
    "#### Infrastructure\n",
    "- [ ] Set up production environment (cloud or on-premise)\n",
    "- [ ] Configure database/data warehouse\n",
    "- [ ] Set up orchestration tool (Airflow, Prefect)\n",
    "- [ ] Configure monitoring and alerting\n",
    "\n",
    "#### Security\n",
    "- [ ] Store credentials in secret manager\n",
    "- [ ] Implement access controls (RBAC)\n",
    "- [ ] Enable encryption at rest and in transit\n",
    "- [ ] Set up audit logging\n",
    "\n",
    "#### Reliability\n",
    "- [ ] Implement comprehensive error handling\n",
    "- [ ] Set up retry logic with exponential backoff\n",
    "- [ ] Create alerting for failures\n",
    "- [ ] Implement circuit breakers\n",
    "- [ ] Set up data backups\n",
    "\n",
    "#### Monitoring\n",
    "- [ ] Track execution metrics\n",
    "- [ ] Monitor data quality scores\n",
    "- [ ] Set up dashboards\n",
    "- [ ] Configure alerts for anomalies\n",
    "- [ ] Track resource usage\n",
    "\n",
    "#### Testing\n",
    "- [ ] Unit tests for all transformations\n",
    "- [ ] Integration tests for full pipeline\n",
    "- [ ] Data quality tests\n",
    "- [ ] Performance tests\n",
    "- [ ] Disaster recovery tests\n",
    "\n",
    "#### Documentation\n",
    "- [ ] Document pipeline architecture\n",
    "- [ ] Create runbooks for common issues\n",
    "- [ ] Document data lineage\n",
    "- [ ] Write onboarding guide\n",
    "- [ ] Maintain change log\n",
    "\n",
    "### Scaling Considerations\n",
    "\n",
    "1. **Horizontal Scaling**: Add more workers\n",
    "2. **Vertical Scaling**: Increase resources per worker\n",
    "3. **Partitioning**: Split data by date/region\n",
    "4. **Caching**: Cache frequently accessed data\n",
    "5. **Incremental Processing**: Process only new/changed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Course Conclusion\n",
    "\n",
    "### What You've Learned\n",
    "\n",
    "Throughout this course, you've covered:\n",
    "\n",
    "1. **Module 00**: Setup and introduction to data engineering\n",
    "2. **Module 01**: Data engineering concepts, ETL vs ELT, architectures\n",
    "3. **Module 02**: Data extraction from files, databases, and APIs\n",
    "4. **Module 03**: Data transformation and cleaning with pandas\n",
    "5. **Module 04**: Data loading strategies and file formats\n",
    "6. **Module 05**: Building modular ETL pipelines\n",
    "7. **Module 06**: Apache Spark for distributed data processing\n",
    "8. **Module 07**: Workflow orchestration with Airflow concepts\n",
    "9. **Module 08**: Data quality and validation frameworks\n",
    "10. **Module 09**: Complete end-to-end pipeline project\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "[OK] **Data Engineering Fundamentals**: ETL, data pipelines, data quality\n",
    "\n",
    "[OK] **Tools & Technologies**: pandas, Spark, Airflow, Parquet, SQL\n",
    "\n",
    "[OK] **Best Practices**: Modular design, logging, testing, error handling\n",
    "\n",
    "[OK] **Production Skills**: Validation, monitoring, deployment\n",
    "\n",
    "### Next Steps in Your Journey\n",
    "\n",
    "1. **Build More Pipelines**: Practice with different data sources\n",
    "2. **Learn Cloud Platforms**: AWS, GCP, or Azure data services\n",
    "3. **Master Orchestration**: Set up real Airflow or Prefect\n",
    "4. **Deep Dive into Spark**: Learn advanced Spark optimization\n",
    "5. **Data Modeling**: Study Kimball, Data Vault methodologies\n",
    "6. **Streaming**: Learn Kafka, Flink for real-time pipelines\n",
    "7. **Contribute**: Open source data engineering projects\n",
    "8. **Stay Current**: Follow data engineering blogs and communities\n",
    "\n",
    "### Recommended Resources\n",
    "\n",
    "**Books**:\n",
    "- Fundamentals of Data Engineering (Joe Reis & Matt Housley)\n",
    "- Designing Data-Intensive Applications (Martin Kleppmann)\n",
    "- The Data Warehouse Toolkit (Ralph Kimball)\n",
    "\n",
    "**Online**:\n",
    "- DataCamp Data Engineering Track\n",
    "- r/dataengineering (Reddit)\n",
    "- Data Engineering Weekly Newsletter\n",
    "- Seattle Data Guy (Blog)\n",
    "\n",
    "**Practice**:\n",
    "- Kaggle datasets\n",
    "- Public APIs\n",
    "- Personal projects\n",
    "\n",
    "---\n",
    "\n",
    "## Congratulations!\n",
    "\n",
    "You've completed the Data Engineering Fundamentals course! \n",
    "\n",
    "You now have the skills to:\n",
    "- Design and build data pipelines\n",
    "- Extract, transform, and load data at scale\n",
    "- Ensure data quality and reliability\n",
    "- Deploy production data systems\n",
    "\n",
    "**Keep building, keep learning, and welcome to the world of data engineering!** [SUCCESS]\n",
    "\n",
    "---\n",
    "\n",
    "*Questions or feedback? Check the main README.md or create an issue in the repository.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
