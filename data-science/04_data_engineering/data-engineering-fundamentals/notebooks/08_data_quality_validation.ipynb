{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 08: Data Quality and Validation\n",
    "\n",
    "**Estimated Time:** 45-60 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this module, you will:\n",
    "- Understand data quality dimensions\n",
    "- Implement data validation strategies\n",
    "- Use validation frameworks (Pandera, Great Expectations)\n",
    "- Design data quality checks\n",
    "- Test data pipelines\n",
    "- Understand data contracts\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Quality Dimensions\n",
    "\n",
    "### The Six Dimensions of Data Quality\n",
    "\n",
    "1. **Accuracy**: Is the data correct?\n",
    "   - Values match reality\n",
    "   - No errors or mistakes\n",
    "\n",
    "2. **Completeness**: Is all required data present?\n",
    "   - No missing values where required\n",
    "   - All records are captured\n",
    "\n",
    "3. **Consistency**: Does data agree across systems?\n",
    "   - Same data, same value everywhere\n",
    "   - No contradictions\n",
    "\n",
    "4. **Timeliness**: Is data up-to-date?\n",
    "   - Available when needed\n",
    "   - Fresh and current\n",
    "\n",
    "5. **Validity**: Does data conform to rules?\n",
    "   - Follows format requirements\n",
    "   - Within acceptable ranges\n",
    "\n",
    "6. **Uniqueness**: No unwanted duplicates?\n",
    "   - Each record appears once\n",
    "   - Primary keys are unique\n",
    "\n",
    "### Impact of Poor Data Quality\n",
    "\n",
    "- [FAIL] Bad business decisions\n",
    "- [FAIL] Wasted resources\n",
    "- [FAIL] Loss of customer trust\n",
    "- [FAIL] Regulatory compliance issues\n",
    "- [FAIL] Failed ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "print(\"[OK] Libraries loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Basic Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data with quality issues\n",
    "data = {\n",
    "    \"customer_id\": [1, 2, 3, 4, 5, 5],  # Duplicate\n",
    "    \"name\": [\"Alice\", \"Bob\", None, \"David\", \"Eve\", \"Frank\"],  # Missing value\n",
    "    \"email\": [\n",
    "        \"alice@ex.com\",\n",
    "        \"invalid-email\",\n",
    "        \"carol@ex.com\",\n",
    "        \"david@ex.com\",\n",
    "        \"eve@ex.com\",\n",
    "        \"frank@ex.com\",\n",
    "    ],  # Invalid format\n",
    "    \"age\": [25, 30, -5, 200, 35, 28],  # Invalid values\n",
    "    \"revenue\": [1000.0, 1500.0, 2000.0, None, 3000.0, 1200.0],  # Missing value\n",
    "    \"signup_date\": [\n",
    "        \"2024-01-01\",\n",
    "        \"2024-02-01\",\n",
    "        \"2024-03-01\",\n",
    "        \"2024-04-01\",\n",
    "        \"2024-05-01\",\n",
    "        \"2024-06-01\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Sample Data:\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic validation checks\n",
    "def basic_data_validation(df):\n",
    "    \"\"\"\n",
    "    Perform basic data quality checks\n",
    "    \"\"\"\n",
    "    validation_results = []\n",
    "\n",
    "    # Check 1: Missing values\n",
    "    missing_counts = df.isnull().sum()\n",
    "    for col, count in missing_counts.items():\n",
    "        validation_results.append(\n",
    "            {\n",
    "                \"check\": \"Missing Values\",\n",
    "                \"column\": col,\n",
    "                \"passed\": count == 0,\n",
    "                \"details\": f\"{count} missing values\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Check 2: Duplicate primary keys\n",
    "    if \"customer_id\" in df.columns:\n",
    "        duplicates = df[\"customer_id\"].duplicated().sum()\n",
    "        validation_results.append(\n",
    "            {\n",
    "                \"check\": \"Unique IDs\",\n",
    "                \"column\": \"customer_id\",\n",
    "                \"passed\": duplicates == 0,\n",
    "                \"details\": f\"{duplicates} duplicates found\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Check 3: Value ranges\n",
    "    if \"age\" in df.columns:\n",
    "        invalid_ages = ((df[\"age\"] < 0) | (df[\"age\"] > 120)).sum()\n",
    "        validation_results.append(\n",
    "            {\n",
    "                \"check\": \"Age Range\",\n",
    "                \"column\": \"age\",\n",
    "                \"passed\": invalid_ages == 0,\n",
    "                \"details\": f\"{invalid_ages} values outside 0-120 range\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Check 4: Email format\n",
    "    if \"email\" in df.columns:\n",
    "        email_pattern = r\"^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$\"\n",
    "        valid_emails = df[\"email\"].str.match(email_pattern, na=False)\n",
    "        invalid_count = (~valid_emails).sum()\n",
    "        validation_results.append(\n",
    "            {\n",
    "                \"check\": \"Email Format\",\n",
    "                \"column\": \"email\",\n",
    "                \"passed\": invalid_count == 0,\n",
    "                \"details\": f\"{invalid_count} invalid email formats\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(validation_results)\n",
    "\n",
    "\n",
    "# Run validation\n",
    "results = basic_data_validation(df)\n",
    "print(\"\\nValidation Results:\")\n",
    "print(\"=\" * 80)\n",
    "for _, row in results.iterrows():\n",
    "    status = \"[OK] PASS\" if row[\"passed\"] else \"[FAIL] FAIL\"\n",
    "    print(f\"{status} | {row['check']:15} | {row['column']:15} | {row['details']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "passed = results[\"passed\"].sum()\n",
    "total = len(results)\n",
    "print(f\"Overall: {passed}/{total} checks passed ({(passed/total*100):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Using Pandera for Schema Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pandera if needed\n",
    "# pip install pandera\n",
    "\n",
    "try:\n",
    "    import pandera as pa\n",
    "    from pandera import Column, Check, DataFrameSchema\n",
    "\n",
    "    print(\"[OK] Pandera loaded\")\n",
    "except ImportError:\n",
    "    print(\"[WARNING] Pandera not installed. Install with: pip install pandera\")\n",
    "    print(\"   Continuing with conceptual examples...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a data schema with Pandera (conceptual example)\n",
    "\"\"\"\n",
    "schema = DataFrameSchema({\n",
    "    \"customer_id\": Column(int, checks=[\n",
    "        Check.greater_than(0),\n",
    "        Check(lambda s: ~s.duplicated().any(), error=\"Duplicate IDs found\")\n",
    "    ]),\n",
    "    \"name\": Column(str, nullable=False),\n",
    "    \"email\": Column(str, checks=[\n",
    "        Check(lambda s: s.str.contains(\"@\").all(), error=\"Invalid email format\")\n",
    "    ]),\n",
    "    \"age\": Column(int, checks=[\n",
    "        Check.in_range(min_value=0, max_value=120)\n",
    "    ]),\n",
    "    \"revenue\": Column(float, checks=[\n",
    "        Check.greater_than_or_equal_to(0)\n",
    "    ], nullable=False),\n",
    "    \"signup_date\": Column(str)  # In practice, would be datetime\n",
    "})\n",
    "\n",
    "# Validate DataFrame against schema\n",
    "try:\n",
    "    validated_df = schema.validate(df)\n",
    "    print(\"[OK] Data validation passed!\")\n",
    "except pa.errors.SchemaError as e:\n",
    "    print(\"[FAIL] Data validation failed:\")\n",
    "    print(e)\n",
    "\"\"\"\n",
    "\n",
    "print(\"Schema definition example (conceptual)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Custom Validation Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataValidator:\n",
    "    \"\"\"\n",
    "    Custom data validation framework\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.results = []\n",
    "\n",
    "    def check_not_null(self, column, error_message=None):\n",
    "        \"\"\"Check column has no null values\"\"\"\n",
    "        null_count = self.df[column].isnull().sum()\n",
    "        passed = null_count == 0\n",
    "\n",
    "        self.results.append(\n",
    "            {\n",
    "                \"check\": \"Not Null\",\n",
    "                \"column\": column,\n",
    "                \"passed\": passed,\n",
    "                \"message\": error_message or f\"{null_count} null values found\",\n",
    "            }\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def check_unique(self, column, error_message=None):\n",
    "        \"\"\"Check column has unique values\"\"\"\n",
    "        dup_count = self.df[column].duplicated().sum()\n",
    "        passed = dup_count == 0\n",
    "\n",
    "        self.results.append(\n",
    "            {\n",
    "                \"check\": \"Unique\",\n",
    "                \"column\": column,\n",
    "                \"passed\": passed,\n",
    "                \"message\": error_message or f\"{dup_count} duplicates found\",\n",
    "            }\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def check_range(self, column, min_val=None, max_val=None, error_message=None):\n",
    "        \"\"\"Check column values are within range\"\"\"\n",
    "        mask = pd.Series([True] * len(self.df))\n",
    "\n",
    "        if min_val is not None:\n",
    "            mask &= self.df[column] >= min_val\n",
    "        if max_val is not None:\n",
    "            mask &= self.df[column] <= max_val\n",
    "\n",
    "        invalid_count = (~mask).sum()\n",
    "        passed = invalid_count == 0\n",
    "\n",
    "        self.results.append(\n",
    "            {\n",
    "                \"check\": \"Range\",\n",
    "                \"column\": column,\n",
    "                \"passed\": passed,\n",
    "                \"message\": error_message\n",
    "                or f\"{invalid_count} values outside range [{min_val}, {max_val}]\",\n",
    "            }\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def check_custom(self, column, condition_func, check_name=\"Custom\", error_message=None):\n",
    "        \"\"\"Check custom condition\"\"\"\n",
    "        try:\n",
    "            passed = condition_func(self.df[column])\n",
    "            message = error_message or (\"Passed\" if passed else \"Failed\")\n",
    "        except Exception as e:\n",
    "            passed = False\n",
    "            message = str(e)\n",
    "\n",
    "        self.results.append(\n",
    "            {\"check\": check_name, \"column\": column, \"passed\": passed, \"message\": message}\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def get_report(self):\n",
    "        \"\"\"Get validation report\"\"\"\n",
    "        df_results = pd.DataFrame(self.results)\n",
    "\n",
    "        print(\"\\nData Validation Report\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        for _, row in df_results.iterrows():\n",
    "            status = \"[OK]\" if row[\"passed\"] else \"[FAIL]\"\n",
    "            print(f\"{status} {row['check']:15} | {row['column']:15} | {row['message']}\")\n",
    "\n",
    "        print(\"=\" * 80)\n",
    "        passed = df_results[\"passed\"].sum()\n",
    "        total = len(df_results)\n",
    "        print(f\"Summary: {passed}/{total} checks passed ({(passed/total*100):.1f}%)\")\n",
    "\n",
    "        return df_results\n",
    "\n",
    "    def validate(self, raise_on_error=False):\n",
    "        \"\"\"Validate and optionally raise exception\"\"\"\n",
    "        report = self.get_report()\n",
    "        all_passed = report[\"passed\"].all()\n",
    "\n",
    "        if raise_on_error and not all_passed:\n",
    "            failed = report[~report[\"passed\"]]\n",
    "            raise ValueError(f\"Validation failed:\\n{failed}\")\n",
    "\n",
    "        return all_passed\n",
    "\n",
    "\n",
    "print(\"[OK] Custom validation framework created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the custom validator\n",
    "validator = DataValidator(df)\n",
    "\n",
    "# Chain validation checks\n",
    "(\n",
    "    validator.check_not_null(\"name\")\n",
    "    .check_unique(\"customer_id\")\n",
    "    .check_range(\"age\", min_val=0, max_val=120)\n",
    "    .check_not_null(\"revenue\")\n",
    "    .check_custom(\n",
    "        \"email\",\n",
    "        lambda s: s.str.contains(\"@\").all(),\n",
    "        check_name=\"Email Format\",\n",
    "        error_message=\"All emails must contain @\",\n",
    "    )\n",
    "    .validate(raise_on_error=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Data Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_data(df):\n",
    "    \"\"\"\n",
    "    Generate data quality profile\n",
    "    \"\"\"\n",
    "    profile = {\n",
    "        \"total_rows\": len(df),\n",
    "        \"total_columns\": len(df.columns),\n",
    "        \"memory_usage_mb\": df.memory_usage(deep=True).sum() / 1024 / 1024,\n",
    "        \"columns\": {},\n",
    "    }\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_profile = {\n",
    "            \"dtype\": str(df[col].dtype),\n",
    "            \"null_count\": int(df[col].isnull().sum()),\n",
    "            \"null_percentage\": float(df[col].isnull().sum() / len(df) * 100),\n",
    "            \"unique_count\": int(df[col].nunique()),\n",
    "            \"duplicate_count\": int(df[col].duplicated().sum()),\n",
    "        }\n",
    "\n",
    "        # Numeric statistics\n",
    "        if df[col].dtype in [\"int64\", \"float64\"]:\n",
    "            col_profile.update(\n",
    "                {\n",
    "                    \"min\": float(df[col].min()) if not df[col].isnull().all() else None,\n",
    "                    \"max\": float(df[col].max()) if not df[col].isnull().all() else None,\n",
    "                    \"mean\": float(df[col].mean()) if not df[col].isnull().all() else None,\n",
    "                    \"median\": float(df[col].median()) if not df[col].isnull().all() else None,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        profile[\"columns\"][col] = col_profile\n",
    "\n",
    "    return profile\n",
    "\n",
    "\n",
    "# Generate profile\n",
    "import json\n",
    "\n",
    "profile = profile_data(df)\n",
    "\n",
    "print(\"Data Quality Profile:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total Rows: {profile['total_rows']:,}\")\n",
    "print(f\"Total Columns: {profile['total_columns']}\")\n",
    "print(f\"Memory Usage: {profile['memory_usage_mb']:.2f} MB\")\n",
    "print(\"\\nColumn Details:\")\n",
    "print(json.dumps(profile[\"columns\"], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Data Contracts\n",
    "\n",
    "A **Data Contract** is an agreement between data producers and consumers about:\n",
    "\n",
    "1. **Schema**: Column names, types, constraints\n",
    "2. **Freshness**: How often data is updated\n",
    "3. **Quality**: Acceptable data quality thresholds\n",
    "4. **SLAs**: Service level agreements\n",
    "\n",
    "### Example Data Contract\n",
    "\n",
    "```yaml\n",
    "# customer_data_contract.yaml\n",
    "dataset: customers\n",
    "owner: data_team\n",
    "consumers:\n",
    "  - analytics_team\n",
    "  - ml_team\n",
    "\n",
    "schema:\n",
    "  customer_id:\n",
    "    type: integer\n",
    "    nullable: false\n",
    "    unique: true\n",
    "  name:\n",
    "    type: string\n",
    "    nullable: false\n",
    "  email:\n",
    "    type: string\n",
    "    nullable: false\n",
    "    format: email\n",
    "  age:\n",
    "    type: integer\n",
    "    min: 0\n",
    "    max: 120\n",
    "\n",
    "quality:\n",
    "  completeness: 0.95  # 95% of required fields must be filled\n",
    "  uniqueness: 1.0     # 100% unique IDs\n",
    "  validity: 0.99      # 99% valid emails\n",
    "\n",
    "freshness:\n",
    "  update_frequency: daily\n",
    "  max_delay_hours: 2\n",
    "\n",
    "sla:\n",
    "  availability: 0.999  # 99.9% uptime\n",
    "  response_time_ms: 100\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Testing Data Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit tests for data transformations\n",
    "def test_data_transformation():\n",
    "    \"\"\"\n",
    "    Test data transformation functions\n",
    "    \"\"\"\n",
    "    # Sample input\n",
    "    input_data = pd.DataFrame({\"value\": [10, 20, 30]})\n",
    "\n",
    "    # Apply transformation\n",
    "    def transform(df):\n",
    "        df[\"doubled\"] = df[\"value\"] * 2\n",
    "        return df\n",
    "\n",
    "    result = transform(input_data)\n",
    "\n",
    "    # Assertions\n",
    "    assert \"doubled\" in result.columns, \"Missing 'doubled' column\"\n",
    "    assert (result[\"doubled\"] == result[\"value\"] * 2).all(), \"Incorrect transformation\"\n",
    "    assert len(result) == len(input_data), \"Row count changed\"\n",
    "\n",
    "    print(\"[OK] All transformation tests passed\")\n",
    "\n",
    "\n",
    "test_data_transformation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integration test for full pipeline\n",
    "def test_pipeline_integration():\n",
    "    \"\"\"\n",
    "    Test complete ETL pipeline\n",
    "    \"\"\"\n",
    "\n",
    "    # Simulate pipeline\n",
    "    def run_pipeline():\n",
    "        # Extract\n",
    "        df = pd.DataFrame({\"id\": [1, 2, 3], \"value\": [10, 20, 30]})\n",
    "\n",
    "        # Transform\n",
    "        df[\"value\"] = df[\"value\"] * 2\n",
    "\n",
    "        # Validate\n",
    "        assert df[\"value\"].min() >= 0, \"Negative values found\"\n",
    "\n",
    "        return df\n",
    "\n",
    "    result = run_pipeline()\n",
    "\n",
    "    # Validate output\n",
    "    assert len(result) > 0, \"Empty result\"\n",
    "    assert \"id\" in result.columns, \"Missing ID column\"\n",
    "    assert \"value\" in result.columns, \"Missing value column\"\n",
    "\n",
    "    print(\"[OK] Pipeline integration test passed\")\n",
    "\n",
    "\n",
    "test_pipeline_integration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Key Takeaways\n",
    "\n",
    "[OK] **Quality Dimensions**: Accuracy, completeness, consistency, timeliness, validity, uniqueness\n",
    "\n",
    "[OK] **Validation**: Check data at every stage (extract, transform, load)\n",
    "\n",
    "[OK] **Frameworks**: Pandera, Great Expectations, custom validators\n",
    "\n",
    "[OK] **Profiling**: Understand data characteristics before building pipelines\n",
    "\n",
    "[OK] **Data Contracts**: Agreements between producers and consumers\n",
    "\n",
    "[OK] **Testing**: Unit tests for transformations, integration tests for pipelines\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Validate early**: Check data as soon as it's extracted\n",
    "2. **Fail fast**: Stop processing bad data immediately\n",
    "3. **Log issues**: Record all quality problems\n",
    "4. **Monitor metrics**: Track quality over time\n",
    "5. **Automate checks**: Build quality checks into pipelines\n",
    "6. **Document rules**: Make validation rules explicit\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In **Module 09: End-to-End Pipeline Project**, we'll:\n",
    "- Build a complete production-ready data pipeline\n",
    "- Apply all concepts from previous modules\n",
    "- Include extraction, transformation, loading\n",
    "- Add validation, logging, and error handling\n",
    "- Deploy and test the pipeline\n",
    "\n",
    "---\n",
    "\n",
    "**Ready for the capstone project?** Open `09_end_to_end_pipeline_project.ipynb`!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
