{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 03: Data Transformation and Cleaning\n",
    "\n",
    "**Estimated Time:** 60-75 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this module, you will:\n",
    "- Clean messy data and handle missing values\n",
    "- Transform data types and formats\n",
    "- Perform string manipulation and regex operations\n",
    "- Work with dates and times effectively\n",
    "- Merge, join, and aggregate datasets\n",
    "- Apply data normalization techniques\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Importance of Data Transformation\n",
    "\n",
    "Raw data is rarely ready for analysis. Transformation involves:\n",
    "\n",
    "### Common Transformation Tasks\n",
    "- **Cleaning**: Remove duplicates, handle nulls, fix errors\n",
    "- **Type Conversion**: Ensure correct data types\n",
    "- **Normalization**: Standardize formats and values\n",
    "- **Enrichment**: Add derived columns\n",
    "- **Aggregation**: Summarize data\n",
    "- **Joining**: Combine multiple datasets\n",
    "\n",
    "### Why Transformation Matters\n",
    "- Garbage in, garbage out - clean data is critical\n",
    "- Consistent formats enable reliable analysis\n",
    "- Proper types prevent errors downstream\n",
    "- Derived metrics add business value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "\n",
    "print(\"[OK] Libraries loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Handling Missing Values\n",
    "\n",
    "Missing data is one of the most common data quality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data with missing values\n",
    "data = {\n",
    "    \"customer_id\": [1, 2, 3, 4, 5, 6],\n",
    "    \"name\": [\"Alice\", \"Bob\", None, \"David\", \"Eve\", \"Frank\"],\n",
    "    \"email\": [\"alice@ex.com\", None, \"carol@ex.com\", \"david@ex.com\", None, \"frank@ex.com\"],\n",
    "    \"age\": [25, 30, np.nan, 40, 35, 28],\n",
    "    \"revenue\": [1000.0, 1500.0, 2000.0, np.nan, 3000.0, 1200.0],\n",
    "    \"country\": [\"USA\", \"UK\", \"USA\", \"Canada\", None, \"USA\"],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original Data:\")\n",
    "print(df)\n",
    "print(\"\\nMissing Values Count:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 1: Drop rows with any missing values\n",
    "df_dropped_rows = df.dropna()\n",
    "print(f\"After dropping rows with NaN: {len(df_dropped_rows)} rows remain (from {len(df)})\")\n",
    "df_dropped_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 2: Drop columns with missing values\n",
    "df_dropped_cols = df.dropna(axis=1)\n",
    "print(f\"After dropping columns with NaN: {len(df_dropped_cols.columns)} columns remain\")\n",
    "df_dropped_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 3: Fill missing values (most common in production)\n",
    "df_filled = df.copy()\n",
    "\n",
    "# Fill numeric with mean/median\n",
    "df_filled[\"age\"] = df_filled[\"age\"].fillna(df_filled[\"age\"].median())\n",
    "df_filled[\"revenue\"] = df_filled[\"revenue\"].fillna(df_filled[\"revenue\"].mean())\n",
    "\n",
    "# Fill categorical with mode or specific value\n",
    "df_filled[\"name\"] = df_filled[\"name\"].fillna(\"Unknown\")\n",
    "df_filled[\"email\"] = df_filled[\"email\"].fillna(\"no-email@example.com\")\n",
    "df_filled[\"country\"] = df_filled[\"country\"].fillna(\"Unknown\")\n",
    "\n",
    "print(\"After filling missing values:\")\n",
    "print(df_filled)\n",
    "print(\"\\nRemaining NaN count:\", df_filled.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Data Type Conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data with type issues\n",
    "messy_data = {\n",
    "    \"id\": [\"1\", \"2\", \"3\", \"4\"],\n",
    "    \"price\": [\"$100.50\", \"$200.00\", \"$150.75\", \"$300.00\"],\n",
    "    \"quantity\": [\"10\", \"20\", \"15\", \"25\"],\n",
    "    \"date\": [\"2024-01-01\", \"2024-01-02\", \"2024-01-03\", \"2024-01-04\"],\n",
    "    \"active\": [\"yes\", \"no\", \"yes\", \"yes\"],\n",
    "}\n",
    "\n",
    "df_messy = pd.DataFrame(messy_data)\n",
    "print(\"Original Types:\")\n",
    "print(df_messy.dtypes)\n",
    "print(\"\\nData:\")\n",
    "df_messy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data types\n",
    "df_cleaned = df_messy.copy()\n",
    "\n",
    "# Convert ID to integer\n",
    "df_cleaned[\"id\"] = df_cleaned[\"id\"].astype(int)\n",
    "\n",
    "# Remove $ and convert to float\n",
    "df_cleaned[\"price\"] = df_cleaned[\"price\"].str.replace(\"$\", \"\").astype(float)\n",
    "\n",
    "# Convert quantity to integer\n",
    "df_cleaned[\"quantity\"] = df_cleaned[\"quantity\"].astype(int)\n",
    "\n",
    "# Convert to datetime\n",
    "df_cleaned[\"date\"] = pd.to_datetime(df_cleaned[\"date\"])\n",
    "\n",
    "# Convert yes/no to boolean\n",
    "df_cleaned[\"active\"] = df_cleaned[\"active\"].map({\"yes\": True, \"no\": False})\n",
    "\n",
    "print(\"Cleaned Types:\")\n",
    "print(df_cleaned.dtypes)\n",
    "print(\"\\nCleaned Data:\")\n",
    "df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. String Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data with messy strings\n",
    "text_data = {\n",
    "    \"name\": [\"  alice smith  \", \"BOB JONES\", \"carol DAVIS\", \"david-wilson\"],\n",
    "    \"email\": [\"ALICE@EXAMPLE.COM\", \"bob@Example.com\", \"Carol@example.COM\", \"david@EXAMPLE.com\"],\n",
    "    \"phone\": [\"(555) 123-4567\", \"555-234-5678\", \"5552345678\", \"+1-555-345-6789\"],\n",
    "}\n",
    "\n",
    "df_text = pd.DataFrame(text_data)\n",
    "print(\"Original Text Data:\")\n",
    "df_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and standardize strings\n",
    "df_text_clean = df_text.copy()\n",
    "\n",
    "# Strip whitespace and title case names\n",
    "df_text_clean[\"name\"] = df_text_clean[\"name\"].str.strip().str.title().str.replace(\"-\", \" \")\n",
    "\n",
    "# Lowercase emails\n",
    "df_text_clean[\"email\"] = df_text_clean[\"email\"].str.lower()\n",
    "\n",
    "# Standardize phone numbers (remove all non-numeric except +)\n",
    "df_text_clean[\"phone\"] = df_text_clean[\"phone\"].str.replace(r\"[^0-9+]\", \"\", regex=True)\n",
    "\n",
    "print(\"Cleaned Text Data:\")\n",
    "df_text_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced string operations with regex\n",
    "sample_text = pd.Series(\n",
    "    [\"Order #12345 total: $500.00\", \"Order #67890 total: $1,234.56\", \"Order #11111 total: $99.99\"]\n",
    ")\n",
    "\n",
    "# Extract order numbers\n",
    "order_numbers = sample_text.str.extract(r\"#(\\d+)\")\n",
    "print(\"Extracted Order Numbers:\")\n",
    "print(order_numbers)\n",
    "\n",
    "# Extract amounts\n",
    "amounts = sample_text.str.extract(r\"\\$([\\d,]+\\.\\d{2})\")\n",
    "amounts = amounts[0].str.replace(\",\", \"\").astype(float)\n",
    "print(\"\\nExtracted Amounts:\")\n",
    "print(amounts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Date and Time Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data with dates\n",
    "date_data = {\n",
    "    \"transaction_id\": range(1, 6),\n",
    "    \"date\": [\"2024-01-15\", \"2024-02-20\", \"2024-03-10\", \"2024-04-05\", \"2024-05-25\"],\n",
    "    \"timestamp\": [\n",
    "        \"2024-01-15 10:30:00\",\n",
    "        \"2024-02-20 14:45:00\",\n",
    "        \"2024-03-10 09:15:00\",\n",
    "        \"2024-04-05 16:20:00\",\n",
    "        \"2024-05-25 11:00:00\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "df_dates = pd.DataFrame(date_data)\n",
    "df_dates[\"date\"] = pd.to_datetime(df_dates[\"date\"])\n",
    "df_dates[\"timestamp\"] = pd.to_datetime(df_dates[\"timestamp\"])\n",
    "\n",
    "print(\"Original Date Data:\")\n",
    "df_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract date components\n",
    "df_dates[\"year\"] = df_dates[\"date\"].dt.year\n",
    "df_dates[\"month\"] = df_dates[\"date\"].dt.month\n",
    "df_dates[\"month_name\"] = df_dates[\"date\"].dt.month_name()\n",
    "df_dates[\"day\"] = df_dates[\"date\"].dt.day\n",
    "df_dates[\"day_of_week\"] = df_dates[\"date\"].dt.day_name()\n",
    "df_dates[\"quarter\"] = df_dates[\"date\"].dt.quarter\n",
    "\n",
    "# Extract time components\n",
    "df_dates[\"hour\"] = df_dates[\"timestamp\"].dt.hour\n",
    "df_dates[\"minute\"] = df_dates[\"timestamp\"].dt.minute\n",
    "\n",
    "# Calculate days since first transaction\n",
    "df_dates[\"days_since_first\"] = (df_dates[\"date\"] - df_dates[\"date\"].min()).dt.days\n",
    "\n",
    "print(\"Date Data with Extracted Components:\")\n",
    "df_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Merging and Joining Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample datasets to merge\n",
    "customers = pd.DataFrame(\n",
    "    {\n",
    "        \"customer_id\": [1, 2, 3, 4],\n",
    "        \"name\": [\"Alice\", \"Bob\", \"Carol\", \"David\"],\n",
    "        \"country\": [\"USA\", \"UK\", \"Canada\", \"Australia\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "orders = pd.DataFrame(\n",
    "    {\n",
    "        \"order_id\": [101, 102, 103, 104, 105],\n",
    "        \"customer_id\": [1, 2, 1, 3, 5],  # Note: customer 5 doesn't exist\n",
    "        \"amount\": [100, 200, 150, 300, 250],\n",
    "        \"date\": pd.date_range(\"2024-01-01\", periods=5),\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Customers:\")\n",
    "print(customers)\n",
    "print(\"\\nOrders:\")\n",
    "print(orders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner join (only matching records)\n",
    "inner_merged = pd.merge(orders, customers, on=\"customer_id\", how=\"inner\")\n",
    "print(\"Inner Join (only matching customers):\")\n",
    "print(inner_merged)\n",
    "print(f\"\\nResult: {len(inner_merged)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left join (all orders, matched customers)\n",
    "left_merged = pd.merge(orders, customers, on=\"customer_id\", how=\"left\")\n",
    "print(\"Left Join (all orders):\")\n",
    "print(left_merged)\n",
    "print(f\"\\nResult: {len(left_merged)} rows (NaN for unmatched customer)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outer join (all records from both)\n",
    "outer_merged = pd.merge(orders, customers, on=\"customer_id\", how=\"outer\")\n",
    "print(\"Outer Join (all orders and customers):\")\n",
    "print(outer_merged)\n",
    "print(f\"\\nResult: {len(outer_merged)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Aggregation and Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sales data\n",
    "sales = pd.DataFrame(\n",
    "    {\n",
    "        \"date\": pd.date_range(\"2024-01-01\", periods=20),\n",
    "        \"product\": np.random.choice([\"A\", \"B\", \"C\"], 20),\n",
    "        \"region\": np.random.choice([\"North\", \"South\", \"East\", \"West\"], 20),\n",
    "        \"quantity\": np.random.randint(1, 100, 20),\n",
    "        \"revenue\": np.random.uniform(100, 1000, 20).round(2),\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Sales Data (first 10 rows):\")\n",
    "sales.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by product and aggregate\n",
    "product_summary = (\n",
    "    sales.groupby(\"product\")\n",
    "    .agg({\"quantity\": [\"sum\", \"mean\", \"count\"], \"revenue\": [\"sum\", \"mean\", \"max\"]})\n",
    "    .round(2)\n",
    ")\n",
    "\n",
    "print(\"Product Summary:\")\n",
    "product_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by multiple columns\n",
    "region_product_summary = (\n",
    "    sales.groupby([\"region\", \"product\"])\n",
    "    .agg({\"revenue\": \"sum\", \"quantity\": \"sum\"})\n",
    "    .round(2)\n",
    "    .sort_values(\"revenue\", ascending=False)\n",
    ")\n",
    "\n",
    "print(\"Region & Product Summary:\")\n",
    "region_product_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot table for cross-tabulation\n",
    "pivot = sales.pivot_table(\n",
    "    values=\"revenue\", index=\"product\", columns=\"region\", aggfunc=\"sum\", fill_value=0\n",
    ").round(2)\n",
    "\n",
    "print(\"Revenue Pivot Table (Product x Region):\")\n",
    "pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data for normalization\n",
    "scores = pd.DataFrame(\n",
    "    {\n",
    "        \"student\": [\"Alice\", \"Bob\", \"Carol\", \"David\", \"Eve\"],\n",
    "        \"math_score\": [95, 80, 70, 85, 90],\n",
    "        \"english_score\": [88, 92, 78, 85, 95],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Original Scores:\")\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min-Max Normalization (scale to 0-1)\n",
    "def min_max_normalize(series):\n",
    "    return (series - series.min()) / (series.max() - series.min())\n",
    "\n",
    "\n",
    "scores[\"math_normalized\"] = min_max_normalize(scores[\"math_score\"])\n",
    "scores[\"english_normalized\"] = min_max_normalize(scores[\"english_score\"])\n",
    "\n",
    "print(\"Min-Max Normalized Scores:\")\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-score Normalization (standardization)\n",
    "def z_score_normalize(series):\n",
    "    return (series - series.mean()) / series.std()\n",
    "\n",
    "\n",
    "scores[\"math_zscore\"] = z_score_normalize(scores[\"math_score\"])\n",
    "scores[\"english_zscore\"] = z_score_normalize(scores[\"english_score\"])\n",
    "\n",
    "print(\"Z-Score Normalized Scores:\")\n",
    "print(scores[[\"student\", \"math_zscore\", \"english_zscore\"]].round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Complete Transformation Pipeline Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create messy realistic dataset\n",
    "messy_sales = pd.DataFrame(\n",
    "    {\n",
    "        \"order_id\": [\"ORD-001\", \"ORD-002\", \"ORD-003\", \"ORD-004\", \"ORD-005\"],\n",
    "        \"customer_name\": [\"  alice SMITH  \", \"bob jones\", None, \"CAROL davis\", \"david-wilson\"],\n",
    "        \"order_date\": [\"2024-01-15\", \"2024/02/20\", \"2024-03-10\", \"2024-04-05\", \"2024-05-25\"],\n",
    "        \"total\": [\"$1,234.56\", \"$567.89\", \"$890.12\", None, \"$2,345.67\"],\n",
    "        \"status\": [\"delivered\", \"PENDING\", \"delivered\", \"cancelled\", \"delivered\"],\n",
    "        \"country\": [\"USA\", \"uk\", \"USA\", \"canada\", None],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Messy Sales Data:\")\n",
    "print(messy_sales)\n",
    "print(\"\\nData Types:\")\n",
    "print(messy_sales.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete transformation pipeline\n",
    "def transform_sales_data(df):\n",
    "    \"\"\"\n",
    "    Complete transformation pipeline for sales data\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "\n",
    "    # 1. Handle missing values\n",
    "    df_clean[\"customer_name\"] = df_clean[\"customer_name\"].fillna(\"Unknown Customer\")\n",
    "    df_clean[\"country\"] = df_clean[\"country\"].fillna(\"Unknown\")\n",
    "    df_clean[\"total\"] = df_clean[\"total\"].fillna(\"$0.00\")\n",
    "\n",
    "    # 2. Clean and standardize strings\n",
    "    df_clean[\"customer_name\"] = (\n",
    "        df_clean[\"customer_name\"].str.strip().str.title().str.replace(\"-\", \" \")\n",
    "    )\n",
    "\n",
    "    df_clean[\"status\"] = df_clean[\"status\"].str.lower().str.strip()\n",
    "    df_clean[\"country\"] = df_clean[\"country\"].str.upper().str.strip()\n",
    "\n",
    "    # 3. Convert data types\n",
    "    # Clean currency and convert to float\n",
    "    df_clean[\"total\"] = df_clean[\"total\"].str.replace(\"$\", \"\").str.replace(\",\", \"\").astype(float)\n",
    "\n",
    "    # Standardize date format and convert to datetime\n",
    "    df_clean[\"order_date\"] = pd.to_datetime(df_clean[\"order_date\"].str.replace(\"/\", \"-\"))\n",
    "\n",
    "    # 4. Add derived columns\n",
    "    df_clean[\"year\"] = df_clean[\"order_date\"].dt.year\n",
    "    df_clean[\"month\"] = df_clean[\"order_date\"].dt.month\n",
    "    df_clean[\"quarter\"] = df_clean[\"order_date\"].dt.quarter\n",
    "    df_clean[\"is_delivered\"] = df_clean[\"status\"] == \"delivered\"\n",
    "\n",
    "    # 5. Sort by date\n",
    "    df_clean = df_clean.sort_values(\"order_date\").reset_index(drop=True)\n",
    "\n",
    "    print(\"[OK] Transformation complete!\")\n",
    "    print(f\"   Records processed: {len(df_clean)}\")\n",
    "    print(f\"   Missing values remaining: {df_clean.isnull().sum().sum()}\")\n",
    "\n",
    "    return df_clean\n",
    "\n",
    "\n",
    "# Transform the data\n",
    "clean_sales = transform_sales_data(messy_sales)\n",
    "print(\"\\nCleaned Sales Data:\")\n",
    "print(clean_sales)\n",
    "print(\"\\nNew Data Types:\")\n",
    "print(clean_sales.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Key Takeaways\n",
    "\n",
    "[OK] **Missing Values**: Fill, drop, or interpolate based on context\n",
    "\n",
    "[OK] **Type Conversion**: Always ensure correct data types\n",
    "\n",
    "[OK] **String Cleaning**: Standardize formats (case, whitespace, special chars)\n",
    "\n",
    "[OK] **Date Operations**: Extract components for analysis\n",
    "\n",
    "[OK] **Merging**: Understand different join types\n",
    "\n",
    "[OK] **Aggregation**: Group and summarize for insights\n",
    "\n",
    "[OK] **Normalization**: Scale data when needed\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In **Module 04: Data Loading and Storage**, we'll:\n",
    "- Load transformed data to various destinations\n",
    "- Work with different file formats\n",
    "- Understand batch vs incremental loading\n",
    "- Optimize for performance\n",
    "\n",
    "---\n",
    "\n",
    "**Ready to load data?** Open `04_data_loading_storage.ipynb`!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
