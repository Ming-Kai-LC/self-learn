{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 04: Data Loading and Storage\n",
    "\n",
    "**Estimated Time:** 45-60 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this module, you will:\n",
    "- Load data to various file formats (CSV, JSON, Parquet)\n",
    "- Write data to SQL databases\n",
    "- Understand batch vs. incremental loading strategies\n",
    "- Implement upsert operations\n",
    "- Learn data partitioning strategies\n",
    "- Optimize loading performance\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Load Phase in ETL\n",
    "\n",
    "The Load phase is where transformed data lands in its final destination:\n",
    "\n",
    "### Common Destinations\n",
    "- **Files**: CSV, Parquet, JSON (for data lakes or sharing)\n",
    "- **Databases**: PostgreSQL, MySQL, SQL Server\n",
    "- **Data Warehouses**: Snowflake, BigQuery, Redshift\n",
    "- **Object Storage**: S3, GCS, Azure Blob\n",
    "- **APIs**: REST endpoints for downstream systems\n",
    "\n",
    "### Loading Strategies\n",
    "1. **Full Load**: Replace all data\n",
    "2. **Incremental Load**: Only new/changed records\n",
    "3. **Upsert**: Update existing, insert new\n",
    "4. **Append**: Add new records without updates\n",
    "\n",
    "### Key Considerations\n",
    "- Performance: How fast can you load?\n",
    "- Atomicity: All or nothing?\n",
    "- Idempotency: Safe to run multiple times?\n",
    "- Data quality: Validation before loading?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from sqlalchemy import create_engine, text\n",
    "import os\n",
    "\n",
    "print(\"[OK] Libraries loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Loading to File Formats\n",
    "\n",
    "### 2.1 CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample transformed data\n",
    "sales_data = pd.DataFrame(\n",
    "    {\n",
    "        \"date\": pd.date_range(\"2024-01-01\", periods=100),\n",
    "        \"product_id\": np.random.choice([\"P001\", \"P002\", \"P003\"], 100),\n",
    "        \"quantity\": np.random.randint(1, 50, 100),\n",
    "        \"revenue\": np.random.uniform(100, 1000, 100).round(2),\n",
    "        \"customer_id\": np.random.randint(1, 50, 100),\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Sample Data: {len(sales_data)} records\")\n",
    "sales_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load to CSV\n",
    "output_path = \"../data/processed/sales_data.csv\"\n",
    "\n",
    "sales_data.to_csv(output_path, index=False)\n",
    "print(f\"[OK] Loaded {len(sales_data)} records to {output_path}\")\n",
    "\n",
    "# Verify file size\n",
    "file_size = os.path.getsize(output_path)\n",
    "print(f\"   File size: {file_size:,} bytes ({file_size/1024:.2f} KB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load with options\n",
    "sales_data.to_csv(\n",
    "    \"../data/processed/sales_data_advanced.csv\",\n",
    "    index=False,\n",
    "    date_format=\"%Y-%m-%d\",  # Format dates\n",
    "    float_format=\"%.2f\",  # 2 decimal places for floats\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "\n",
    "print(\"[OK] Loaded with advanced options\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Parquet Files (Recommended for Large Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load to Parquet\n",
    "parquet_path = \"../data/processed/sales_data.parquet\"\n",
    "\n",
    "sales_data.to_parquet(\n",
    "    parquet_path, engine=\"pyarrow\", compression=\"snappy\", index=False  # Fast compression\n",
    ")\n",
    "\n",
    "# Compare file sizes\n",
    "csv_size = os.path.getsize(output_path)\n",
    "parquet_size = os.path.getsize(parquet_path)\n",
    "\n",
    "print(f\"[OK] Loaded to Parquet\")\n",
    "print(f\"   CSV size: {csv_size:,} bytes\")\n",
    "print(f\"   Parquet size: {parquet_size:,} bytes\")\n",
    "print(f\"   Compression ratio: {(1 - parquet_size/csv_size)*100:.1f}% smaller\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 JSON Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load to JSON\n",
    "json_path = \"../data/processed/sales_data.json\"\n",
    "\n",
    "sales_data.to_json(json_path, orient=\"records\", date_format=\"iso\", indent=2)  # List of objects\n",
    "\n",
    "print(f\"[OK] Loaded to JSON\")\n",
    "print(f\"   File size: {os.path.getsize(json_path):,} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Loading to Databases\n",
    "\n",
    "### 3.1 Basic Database Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SQLite database for demo\n",
    "db_path = \"../data/processed/warehouse.db\"\n",
    "engine = create_engine(f\"sqlite:///{db_path}\")\n",
    "\n",
    "print(\"[OK] Database connection created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data to database - Method 1: Replace\n",
    "sales_data.to_sql(\n",
    "    name=\"sales\", con=engine, if_exists=\"replace\", index=False  # Drop and recreate table\n",
    ")\n",
    "\n",
    "print(\"[OK] Data loaded to 'sales' table (replace mode)\")\n",
    "\n",
    "# Verify\n",
    "count = pd.read_sql(\"SELECT COUNT(*) as count FROM sales\", engine)\n",
    "print(f\"   Records in database: {count['count'][0]:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data to database - Method 2: Append\n",
    "new_sales = pd.DataFrame(\n",
    "    {\n",
    "        \"date\": [datetime.now()],\n",
    "        \"product_id\": [\"P001\"],\n",
    "        \"quantity\": [25],\n",
    "        \"revenue\": [500.00],\n",
    "        \"customer_id\": [999],\n",
    "    }\n",
    ")\n",
    "\n",
    "new_sales.to_sql(name=\"sales\", con=engine, if_exists=\"append\", index=False)  # Add to existing table\n",
    "\n",
    "print(\"[OK] New data appended\")\n",
    "\n",
    "# Verify\n",
    "count = pd.read_sql(\"SELECT COUNT(*) as count FROM sales\", engine)\n",
    "print(f\"   Records in database: {count['count'][0]:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Incremental Loading (Load Only New Records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def incremental_load(df, table_name, engine, date_column=\"date\"):\n",
    "    \"\"\"\n",
    "    Load only records newer than the last load\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get the latest date from the database\n",
    "        query = f\"SELECT MAX({date_column}) as max_date FROM {table_name}\"\n",
    "        result = pd.read_sql(query, engine)\n",
    "        max_date = result[\"max_date\"][0]\n",
    "\n",
    "        if max_date:\n",
    "            # Filter for records after max_date\n",
    "            df_incremental = df[df[date_column] > max_date]\n",
    "            print(f\"[DATA] Latest date in DB: {max_date}\")\n",
    "            print(f\"[DATA] New records to load: {len(df_incremental)}\")\n",
    "        else:\n",
    "            # No existing data, load all\n",
    "            df_incremental = df\n",
    "            print(\"[DATA] No existing data, loading all records\")\n",
    "\n",
    "        if len(df_incremental) > 0:\n",
    "            df_incremental.to_sql(table_name, engine, if_exists=\"append\", index=False)\n",
    "            print(f\"[OK] Loaded {len(df_incremental)} new records\")\n",
    "        else:\n",
    "            print(\"ℹ️ No new records to load\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # Table doesn't exist yet\n",
    "        print(f\"[DATA] Table doesn't exist, creating and loading all records\")\n",
    "        df.to_sql(table_name, engine, if_exists=\"replace\", index=False)\n",
    "        print(f\"[OK] Created table and loaded {len(df)} records\")\n",
    "\n",
    "\n",
    "# Test incremental load\n",
    "future_sales = pd.DataFrame(\n",
    "    {\n",
    "        \"date\": pd.date_range(\"2024-05-01\", periods=10),\n",
    "        \"product_id\": np.random.choice([\"P001\", \"P002\"], 10),\n",
    "        \"quantity\": np.random.randint(1, 50, 10),\n",
    "        \"revenue\": np.random.uniform(100, 1000, 10).round(2),\n",
    "        \"customer_id\": np.random.randint(1, 50, 10),\n",
    "    }\n",
    ")\n",
    "\n",
    "incremental_load(future_sales, \"sales_incremental\", engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Upsert Operations (Update or Insert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsert_data(df, table_name, engine, primary_key):\n",
    "    \"\"\"\n",
    "    Upsert: Update if exists, Insert if new\n",
    "\n",
    "    Note: This is a simplified version. Production systems use\n",
    "    database-specific UPSERT syntax (INSERT ... ON CONFLICT, MERGE, etc.)\n",
    "    \"\"\"\n",
    "    # Create temporary table\n",
    "    temp_table = f\"{table_name}_temp\"\n",
    "    df.to_sql(temp_table, engine, if_exists=\"replace\", index=False)\n",
    "\n",
    "    # Check if main table exists\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            # Delete existing records that will be updated\n",
    "            delete_query = text(\n",
    "                f\"\"\"\n",
    "                DELETE FROM {table_name}\n",
    "                WHERE {primary_key} IN (SELECT {primary_key} FROM {temp_table})\n",
    "            \"\"\"\n",
    "            )\n",
    "            result = conn.execute(delete_query)\n",
    "            conn.commit()\n",
    "            deleted = result.rowcount\n",
    "\n",
    "            # Insert all records from temp table\n",
    "            insert_query = text(\n",
    "                f\"\"\"\n",
    "                INSERT INTO {table_name}\n",
    "                SELECT * FROM {temp_table}\n",
    "            \"\"\"\n",
    "            )\n",
    "            conn.execute(insert_query)\n",
    "            conn.commit()\n",
    "\n",
    "        print(f\"[OK] Upsert complete\")\n",
    "        print(f\"   Updated: {deleted} records\")\n",
    "        print(f\"   Total processed: {len(df)} records\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # Table doesn't exist, create it\n",
    "        df.to_sql(table_name, engine, if_exists=\"replace\", index=False)\n",
    "        print(f\"[OK] Created table and inserted {len(df)} records\")\n",
    "\n",
    "    # Drop temp table\n",
    "    with engine.connect() as conn:\n",
    "        conn.execute(text(f\"DROP TABLE IF EXISTS {temp_table}\"))\n",
    "        conn.commit()\n",
    "\n",
    "\n",
    "# Test upsert\n",
    "customer_data = pd.DataFrame(\n",
    "    {\n",
    "        \"customer_id\": [1, 2, 3],\n",
    "        \"name\": [\"Alice\", \"Bob\", \"Carol\"],\n",
    "        \"email\": [\"alice@ex.com\", \"bob@ex.com\", \"carol@ex.com\"],\n",
    "        \"lifetime_value\": [1000, 2000, 1500],\n",
    "    }\n",
    ")\n",
    "\n",
    "upsert_data(customer_data, \"customers\", engine, \"customer_id\")\n",
    "\n",
    "# Update some customers\n",
    "updated_customers = pd.DataFrame(\n",
    "    {\n",
    "        \"customer_id\": [2, 3, 4],  # 2 & 3 exist, 4 is new\n",
    "        \"name\": [\"Bob Updated\", \"Carol Updated\", \"David\"],\n",
    "        \"email\": [\"bob@ex.com\", \"carol@ex.com\", \"david@ex.com\"],\n",
    "        \"lifetime_value\": [2500, 1800, 500],\n",
    "    }\n",
    ")\n",
    "\n",
    "upsert_data(updated_customers, \"customers\", engine, \"customer_id\")\n",
    "\n",
    "# Verify\n",
    "result = pd.read_sql(\"SELECT * FROM customers ORDER BY customer_id\", engine)\n",
    "print(\"\\nFinal customer data:\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Data Partitioning Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create larger dataset for partitioning\n",
    "large_sales = pd.DataFrame(\n",
    "    {\n",
    "        \"date\": pd.date_range(\"2023-01-01\", \"2024-12-31\", freq=\"H\"),\n",
    "        \"product_id\": np.random.choice([\"P001\", \"P002\", \"P003\"], 17544),  # 2 years of hourly data\n",
    "        \"revenue\": np.random.uniform(100, 1000, 17544).round(2),\n",
    "    }\n",
    ")\n",
    "\n",
    "large_sales[\"year\"] = large_sales[\"date\"].dt.year\n",
    "large_sales[\"month\"] = large_sales[\"date\"].dt.month\n",
    "\n",
    "print(f\"Large dataset: {len(large_sales):,} records\")\n",
    "print(f\"Date range: {large_sales['date'].min()} to {large_sales['date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition by year and month\n",
    "def save_partitioned_data(df, base_path, partition_cols):\n",
    "    \"\"\"\n",
    "    Save data partitioned by specified columns\n",
    "\n",
    "    Example structure: base_path/year=2024/month=01/data.parquet\n",
    "    \"\"\"\n",
    "    for group_keys, group_df in df.groupby(partition_cols):\n",
    "        # Create partition path\n",
    "        if len(partition_cols) == 1:\n",
    "            group_keys = [group_keys]\n",
    "\n",
    "        partition_path = base_path\n",
    "        for col, value in zip(partition_cols, group_keys):\n",
    "            partition_path = os.path.join(partition_path, f\"{col}={value}\")\n",
    "\n",
    "        os.makedirs(partition_path, exist_ok=True)\n",
    "\n",
    "        # Save partition\n",
    "        file_path = os.path.join(partition_path, \"data.parquet\")\n",
    "        group_df.to_parquet(file_path, index=False)\n",
    "\n",
    "        print(f\"[OK] Saved partition: {partition_path} ({len(group_df):,} records)\")\n",
    "\n",
    "\n",
    "# Save partitioned data\n",
    "partition_base = \"../data/processed/sales_partitioned\"\n",
    "save_partitioned_data(large_sales, partition_base, [\"year\", \"month\"])\n",
    "\n",
    "print(\"\\n[DATA] Partitioning complete!\")\n",
    "print(f\"   Benefits: Faster queries when filtering by year/month\")\n",
    "print(f\"   Benefits: Can process one partition at a time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Performance Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Create test data\n",
    "test_data = pd.DataFrame(\n",
    "    {\n",
    "        \"id\": range(10000),\n",
    "        \"value\": np.random.randn(10000),\n",
    "        \"category\": np.random.choice([\"A\", \"B\", \"C\"], 10000),\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Test data: {len(test_data):,} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare loading speeds\n",
    "def benchmark_loading(df, method=\"default\"):\n",
    "    \"\"\"\n",
    "    Benchmark different loading methods\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "\n",
    "    if method == \"default\":\n",
    "        df.to_sql(\"benchmark\", engine, if_exists=\"replace\", index=False)\n",
    "    elif method == \"batched\":\n",
    "        df.to_sql(\"benchmark\", engine, if_exists=\"replace\", index=False, chunksize=1000)\n",
    "\n",
    "    elapsed = time.time() - start\n",
    "    return elapsed\n",
    "\n",
    "\n",
    "# Benchmark\n",
    "time_default = benchmark_loading(test_data, \"default\")\n",
    "print(f\"Default loading: {time_default:.3f}s\")\n",
    "\n",
    "time_batched = benchmark_loading(test_data, \"batched\")\n",
    "print(f\"Batched loading: {time_batched:.3f}s\")\n",
    "\n",
    "if time_batched < time_default:\n",
    "    print(f\"\\n[OK] Batched is {((time_default - time_batched) / time_default * 100):.1f}% faster\")\n",
    "else:\n",
    "    print(f\"\\n[OK] Default is {((time_batched - time_default) / time_batched * 100):.1f}% faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Best Practices for Data Loading\n",
    "\n",
    "### 1. Choose the Right Format\n",
    "- **CSV**: Simple, human-readable, universal\n",
    "- **Parquet**: Large datasets, analytics, compression\n",
    "- **JSON**: Nested/hierarchical data, APIs\n",
    "\n",
    "### 2. Use Appropriate Loading Strategy\n",
    "- **Full Load**: Simple but slow for large datasets\n",
    "- **Incremental**: Faster, requires tracking\n",
    "- **Upsert**: Best for dimensional data\n",
    "\n",
    "### 3. Implement Data Validation\n",
    "- Validate before loading\n",
    "- Check row counts\n",
    "- Verify data types\n",
    "- Log any discrepancies\n",
    "\n",
    "### 4. Handle Failures Gracefully\n",
    "- Use transactions when possible\n",
    "- Implement retry logic\n",
    "- Log errors\n",
    "- Alert on failures\n",
    "\n",
    "### 5. Partition Large Datasets\n",
    "- By date (year/month/day)\n",
    "- By category/region\n",
    "- Enables parallel processing\n",
    "- Faster queries\n",
    "\n",
    "### 6. Monitor Performance\n",
    "- Track loading times\n",
    "- Monitor resource usage\n",
    "- Optimize bottlenecks\n",
    "- Scale as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Complete Loading Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    \"\"\"\n",
    "    Production-ready data loader with best practices\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, engine=None):\n",
    "        self.engine = engine\n",
    "        self.load_stats = {}\n",
    "\n",
    "    def load(self, df, destination_type, **kwargs):\n",
    "        \"\"\"\n",
    "        Load data to various destinations\n",
    "        \"\"\"\n",
    "        start = time.time()\n",
    "\n",
    "        try:\n",
    "            if destination_type == \"csv\":\n",
    "                df.to_csv(kwargs[\"file_path\"], index=False)\n",
    "            elif destination_type == \"parquet\":\n",
    "                df.to_parquet(kwargs[\"file_path\"], compression=\"snappy\", index=False)\n",
    "            elif destination_type == \"database\":\n",
    "                df.to_sql(\n",
    "                    kwargs[\"table_name\"],\n",
    "                    self.engine,\n",
    "                    if_exists=kwargs.get(\"if_exists\", \"replace\"),\n",
    "                    index=False,\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported destination: {destination_type}\")\n",
    "\n",
    "            elapsed = time.time() - start\n",
    "\n",
    "            self.load_stats[destination_type] = {\n",
    "                \"records\": len(df),\n",
    "                \"duration_seconds\": elapsed,\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "            }\n",
    "\n",
    "            print(f\"[OK] Loaded {len(df):,} records to {destination_type} in {elapsed:.2f}s\")\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[FAIL] Load failed: {e}\")\n",
    "            return False\n",
    "\n",
    "\n",
    "# Use the loader\n",
    "loader = DataLoader(engine=engine)\n",
    "\n",
    "# Load to multiple destinations\n",
    "sample_data = sales_data.head(50)\n",
    "\n",
    "loader.load(sample_data, \"csv\", file_path=\"../data/processed/final_output.csv\")\n",
    "loader.load(sample_data, \"parquet\", file_path=\"../data/processed/final_output.parquet\")\n",
    "loader.load(sample_data, \"database\", table_name=\"final_sales\", if_exists=\"replace\")\n",
    "\n",
    "print(\"\\n[DATA] Loading Statistics:\")\n",
    "for dest, stats in loader.load_stats.items():\n",
    "    print(f\"   {dest}: {stats['records']:,} records in {stats['duration_seconds']:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Key Takeaways\n",
    "\n",
    "[OK] **File Formats**: Choose based on use case (CSV, Parquet, JSON)\n",
    "\n",
    "[OK] **Loading Strategies**: Full, incremental, upsert, append\n",
    "\n",
    "[OK] **Incremental Loading**: Only load new/changed data\n",
    "\n",
    "[OK] **Upsert**: Update existing, insert new records\n",
    "\n",
    "[OK] **Partitioning**: Split data for better performance\n",
    "\n",
    "[OK] **Validation**: Always verify data before and after loading\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In **Module 05: Building ETL Pipelines**, we'll:\n",
    "- Combine Extract, Transform, and Load into complete pipelines\n",
    "- Learn pipeline design patterns\n",
    "- Implement error handling and logging\n",
    "- Build modular, reusable pipeline components\n",
    "\n",
    "---\n",
    "\n",
    "**Ready to build complete pipelines?** Open `05_building_etl_pipelines.ipynb`!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
