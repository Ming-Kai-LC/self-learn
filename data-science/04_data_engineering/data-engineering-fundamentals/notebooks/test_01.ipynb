{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 01: Introduction to Data Engineering\n",
    "\n",
    "**Estimated Time:** 45-60 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this module, you will:\n",
    "- Understand the comprehensive role of a data engineer\n",
    "- Learn about the modern data stack and its components\n",
    "- Understand ETL vs. ELT patterns and when to use each\n",
    "- Explore different data pipeline architectures\n",
    "- Recognize common data engineering challenges and solutions\n",
    "- Understand the data engineering lifecycle\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is Data Engineering?\n",
    "\n",
    "### Definition Expanded\n",
    "\n",
    "Data engineering is a multifaceted discipline that encompasses:\n",
    "\n",
    "1. **Data Architecture**: Designing how data flows through systems\n",
    "2. **Data Infrastructure**: Building and maintaining data platforms\n",
    "3. **Data Pipeline Development**: Creating automated data workflows\n",
    "4. **Data Quality**: Ensuring accuracy, completeness, and reliability\n",
    "5. **Data Integration**: Combining data from multiple sources\n",
    "6. **Performance Optimization**: Making data systems fast and efficient\n",
    "\n",
    "### The Data Engineering Spectrum\n",
    "\n",
    "```\n",
    "Operational Data          →    Data Engineering    →    Analytical Data\n",
    "(Live transactions)            (Transformation)         (Ready for analysis)\n",
    "\n",
    "Examples:                      Processes:               Examples:\n",
    "- User clicks                 - Extract                 - User behavior\n",
    "- Sales records               - Transform               - Sales trends\n",
    "- Sensor data                 - Load                    - Predictive models\n",
    "- App logs                    - Validate                - Business dashboards\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize a typical day in data flow\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Simulate hourly data volume through a pipeline\n",
    "hours = list(range(24))\n",
    "data_flow = {\n",
    "    \"hour\": hours,\n",
    "    \"source_records\": [10000 + (i * 500) + ((i % 8) * 2000) for i in hours],\n",
    "    \"after_validation\": [\n",
    "        int(x * 0.95) for x in [10000 + (i * 500) + ((i % 8) * 2000) for i in hours]\n",
    "    ],\n",
    "    \"after_transformation\": [\n",
    "        int(x * 0.90) for x in [10000 + (i * 500) + ((i % 8) * 2000) for i in hours]\n",
    "    ],\n",
    "}\n",
    "\n",
    "df_flow = pd.DataFrame(data_flow)\n",
    "print(\"Typical Data Flow Through a Pipeline (24 hours)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total source records: {df_flow['source_records'].sum():,}\")\n",
    "print(f\"Records after validation: {df_flow['after_validation'].sum():,}\")\n",
    "print(f\"Final transformed records: {df_flow['after_transformation'].sum():,}\")\n",
    "print(\n",
    "    f\"Data loss percentage: {((df_flow['source_records'].sum() - df_flow['after_transformation'].sum()) / df_flow['source_records'].sum() * 100):.2f}%\"\n",
    ")\n",
    "print(\"\\nNote: Data loss is often due to duplicates, invalid records, or filtering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. The Modern Data Stack\n",
    "\n",
    "The modern data stack consists of specialized tools for each stage of the data lifecycle:\n",
    "\n",
    "### Layer 1: Data Sources\n",
    "- **Transactional Databases**: PostgreSQL, MySQL, Oracle\n",
    "- **SaaS Applications**: Salesforce, HubSpot, Google Analytics\n",
    "- **APIs**: REST, GraphQL\n",
    "- **Streaming**: Kafka, Kinesis\n",
    "- **Files**: CSV, JSON, Parquet, logs\n",
    "\n",
    "### Layer 2: Data Ingestion\n",
    "- **Batch**: Apache Airflow, Prefect, Dagster\n",
    "- **Streaming**: Apache Kafka, Apache Flink, AWS Kinesis\n",
    "- **ELT Tools**: Fivetran, Airbyte, Stitch\n",
    "\n",
    "### Layer 3: Data Storage\n",
    "- **Data Warehouses**: Snowflake, BigQuery, Redshift\n",
    "- **Data Lakes**: S3, Azure Data Lake, GCS\n",
    "- **Lakehouses**: Databricks, Delta Lake\n",
    "\n",
    "### Layer 4: Data Transformation\n",
    "- **SQL-based**: dbt (data build tool)\n",
    "- **Python-based**: pandas, PySpark\n",
    "- **Notebooks**: Jupyter, Databricks\n",
    "\n",
    "### Layer 5: Data Orchestration\n",
    "- **Workflow Management**: Apache Airflow, Prefect, Dagster\n",
    "- **Job Scheduling**: Cron, Cloud Scheduler\n",
    "\n",
    "### Layer 6: Data Quality & Governance\n",
    "- **Quality**: Great Expectations, deequ, Soda\n",
    "- **Cataloging**: Amundsen, DataHub, Alation\n",
    "- **Lineage**: OpenLineage, Marquez\n",
    "\n",
    "### Layer 7: Data Consumption\n",
    "- **BI Tools**: Tableau, Looker, Power BI, Metabase\n",
    "- **ML Platforms**: MLflow, Kubeflow, SageMaker\n",
    "- **APIs**: REST, GraphQL for data access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Simulating the modern data stack layers\n",
    "import json\n",
    "\n",
    "data_stack = {\n",
    "    \"sources\": [\"PostgreSQL\", \"Salesforce API\", \"S3 Logs\"],\n",
    "    \"ingestion\": \"Apache Airflow\",\n",
    "    \"storage\": \"Snowflake Data Warehouse\",\n",
    "    \"transformation\": \"dbt + Python\",\n",
    "    \"quality\": \"Great Expectations\",\n",
    "    \"consumption\": [\"Tableau Dashboard\", \"ML Model API\"],\n",
    "}\n",
    "\n",
    "print(\"Example Modern Data Stack Configuration:\")\n",
    "print(json.dumps(data_stack, indent=2))\n",
    "\n",
    "print(\"\\n[DATA] Data Flow:\")\n",
    "print(\"Sources → Airflow → Snowflake → dbt → Quality Checks → Tableau/ML\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. ETL vs. ELT: Understanding the Difference\n",
    "\n",
    "### ETL (Extract, Transform, Load)\n",
    "\n",
    "**Traditional approach**: Transform data BEFORE loading into the warehouse\n",
    "\n",
    "```\n",
    "Source → Extract → Transform → Load → Data Warehouse\n",
    "         (Raw)     (Cleaned)   (Ready)\n",
    "```\n",
    "\n",
    "**When to use ETL:**\n",
    "- Limited warehouse compute capacity\n",
    "- Data privacy requirements (mask before storing)\n",
    "- Complex transformations best done in specialized tools\n",
    "- On-premise systems with legacy constraints\n",
    "\n",
    "**Pros:**\n",
    "- [OK] Reduced warehouse storage\n",
    "- [OK] Data already cleaned before storage\n",
    "- [OK] Compliance: sensitive data can be masked early\n",
    "\n",
    "**Cons:**\n",
    "- [FAIL] Slower - transformation bottleneck\n",
    "- [FAIL] No access to raw data\n",
    "- [FAIL] Less flexible for ad-hoc analysis\n",
    "\n",
    "---\n",
    "\n",
    "### ELT (Extract, Load, Transform)\n",
    "\n",
    "**Modern approach**: Load raw data first, transform in the warehouse\n",
    "\n",
    "```\n",
    "Source → Extract → Load → Transform → Data Warehouse\n",
    "         (Raw)     (Raw)   (Cleaned)   (Multiple views)\n",
    "```\n",
    "\n",
    "**When to use ELT:**\n",
    "- Cloud data warehouses (Snowflake, BigQuery)\n",
    "- Need for data flexibility\n",
    "- Multiple transformation use cases\n",
    "- Fast-changing requirements\n",
    "\n",
    "**Pros:**\n",
    "- [OK] Faster initial load\n",
    "- [OK] Raw data always available\n",
    "- [OK] Leverage warehouse compute power\n",
    "- [OK] More flexible transformations\n",
    "\n",
    "**Cons:**\n",
    "- [FAIL] Requires powerful warehouse\n",
    "- [FAIL] More storage needed\n",
    "- [FAIL] Potential for messy data if not governed\n",
    "\n",
    "---\n",
    "\n",
    "### Comparison Table\n",
    "\n",
    "| Aspect | ETL | ELT |\n",
    "|--------|-----|-----|\n",
    "| **Transform Location** | External processing | Inside warehouse |\n",
    "| **Speed** | Slower (transformation bottleneck) | Faster (parallel processing) |\n",
    "| **Flexibility** | Limited (fixed transformations) | High (ad-hoc SQL queries) |\n",
    "| **Cost** | Lower storage, higher compute | Higher storage, lower compute |\n",
    "| **Best For** | On-premise, legacy systems | Cloud, modern data warehouses |\n",
    "| **Raw Data Access** | Not available | Always available |\n",
    "| **Tools** | Informatica, Talend, SSIS | dbt, Snowflake, BigQuery |\n",
    "\n",
    "### The Trend\n",
    "\n",
    "**Today's Standard**: ELT is becoming dominant due to:\n",
    "- Cheap cloud storage\n",
    "- Powerful cloud data warehouses\n",
    "- Need for data flexibility\n",
    "- Faster time-to-insight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulating ETL vs ELT performance difference\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def simulate_etl_approach(records):\n",
    "    \"\"\"Simulates traditional ETL - transform before load\"\"\"\n",
    "    print(\"ETL Approach:\")\n",
    "    start = time.time()\n",
    "\n",
    "    # Extract\n",
    "    print(\"  [1/3] Extracting data...\")\n",
    "    time.sleep(0.1)\n",
    "\n",
    "    # Transform (bottleneck in ETL)\n",
    "    print(\"  [2/3] Transforming data (sequential processing)...\")\n",
    "    time.sleep(0.5)  # Simulate longer transformation\n",
    "\n",
    "    # Load\n",
    "    print(\"  [3/3] Loading transformed data...\")\n",
    "    time.sleep(0.1)\n",
    "\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"  [OK] Complete in {elapsed:.2f}s\")\n",
    "    return elapsed\n",
    "\n",
    "\n",
    "def simulate_elt_approach(records):\n",
    "    \"\"\"Simulates modern ELT - load then transform\"\"\"\n",
    "    print(\"\\nELT Approach:\")\n",
    "    start = time.time()\n",
    "\n",
    "    # Extract\n",
    "    print(\"  [1/3] Extracting data...\")\n",
    "    time.sleep(0.1)\n",
    "\n",
    "    # Load (faster)\n",
    "    print(\"  [2/3] Loading raw data...\")\n",
    "    time.sleep(0.1)\n",
    "\n",
    "    # Transform (leverages warehouse parallelism)\n",
    "    print(\"  [3/3] Transforming in warehouse (parallel)...\")\n",
    "    time.sleep(0.2)  # Faster due to warehouse compute\n",
    "\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"  [OK] Complete in {elapsed:.2f}s\")\n",
    "    return elapsed\n",
    "\n",
    "\n",
    "# Compare\n",
    "print(\"Comparing ETL vs ELT Performance\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "etl_time = simulate_etl_approach(10000)\n",
    "elt_time = simulate_elt_approach(10000)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(f\"Performance Difference: {((etl_time - elt_time) / etl_time * 100):.1f}% faster with ELT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Data Pipeline Architectures\n",
    "\n",
    "### 1. Batch Processing Architecture\n",
    "\n",
    "Process data in scheduled batches (hourly, daily, weekly)\n",
    "\n",
    "```\n",
    "Source DB → Batch Extract (Airflow) → Transform → Load → Data Warehouse\n",
    "           (Every 6 hours)            (pandas)          (Snowflake)\n",
    "```\n",
    "\n",
    "**Use Cases:**\n",
    "- Daily reports and analytics\n",
    "- Historical data processing\n",
    "- Non-time-sensitive transformations\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Streaming Architecture\n",
    "\n",
    "Process data in real-time as it arrives\n",
    "\n",
    "```\n",
    "Events → Kafka → Stream Processor (Flink) → Real-time DB → Dashboard\n",
    "        (Queue)   (Continuous)               (Redis)       (Live)\n",
    "```\n",
    "\n",
    "**Use Cases:**\n",
    "- Fraud detection\n",
    "- Real-time recommendations\n",
    "- Live dashboards\n",
    "- IoT data processing\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Lambda Architecture\n",
    "\n",
    "Combines batch and streaming (complex but comprehensive)\n",
    "\n",
    "```\n",
    "           ┌─→ Batch Layer (Accurate, slow) ─┐\n",
    "Data Source ─┤                                  ├─→ Serving Layer → Query\n",
    "           └─→ Speed Layer (Fast, approximate)─┘\n",
    "```\n",
    "\n",
    "**Use Cases:**\n",
    "- Need both real-time AND accurate historical data\n",
    "- Complex analytics requirements\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Kappa Architecture\n",
    "\n",
    "Simplified - everything is a stream\n",
    "\n",
    "```\n",
    "Data Source → Kafka → Stream Processing → Storage → Query\n",
    "             (All data as streams)\n",
    "```\n",
    "\n",
    "**Use Cases:**\n",
    "- When batch processing can be replaced with streaming\n",
    "- Simpler to maintain than Lambda\n",
    "\n",
    "---\n",
    "\n",
    "### Which Architecture to Choose?\n",
    "\n",
    "| Factor | Batch | Streaming | Lambda | Kappa |\n",
    "|--------|-------|-----------|--------|-------|\n",
    "| **Latency Requirement** | Hours/Days | Seconds | Both | Seconds |\n",
    "| **Complexity** | Low | Medium | High | Medium |\n",
    "| **Cost** | Low | Medium | High | Medium |\n",
    "| **Accuracy** | High | Good | High | Good |\n",
    "| **Typical Usage** | 80% | 15% | 4% | 1% |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulating different pipeline patterns\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "\n",
    "class BatchPipeline:\n",
    "    def __init__(self, batch_size=1000):\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer = []\n",
    "\n",
    "    def process(self, records):\n",
    "        \"\"\"Process records in batches\"\"\"\n",
    "        print(f\"\\n[Batch Pipeline] Processing {len(records)} records\")\n",
    "        batches = [\n",
    "            records[i : i + self.batch_size] for i in range(0, len(records), self.batch_size)\n",
    "        ]\n",
    "        print(f\"  Split into {len(batches)} batches of {self.batch_size} records\")\n",
    "        return f\"Processed {len(batches)} batches\"\n",
    "\n",
    "\n",
    "class StreamPipeline:\n",
    "    def __init__(self):\n",
    "        self.processed_count = 0\n",
    "\n",
    "    def process(self, record):\n",
    "        \"\"\"Process records one at a time (or micro-batches)\"\"\"\n",
    "        self.processed_count += 1\n",
    "        if self.processed_count % 100 == 0:\n",
    "            print(f\"  [Stream] Processed {self.processed_count} records in real-time...\")\n",
    "        return record\n",
    "\n",
    "\n",
    "# Demo\n",
    "print(\"=\" * 60)\n",
    "print(\"Pipeline Architecture Demo\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create sample data\n",
    "sample_records = list(range(5000))\n",
    "\n",
    "# Batch processing\n",
    "batch_pipeline = BatchPipeline(batch_size=1000)\n",
    "result = batch_pipeline.process(sample_records)\n",
    "print(f\"  Result: {result}\")\n",
    "\n",
    "# Stream processing simulation\n",
    "print(\"\\n[Stream Pipeline] Processing records as they arrive\")\n",
    "stream_pipeline = StreamPipeline()\n",
    "for i, record in enumerate(sample_records):\n",
    "    stream_pipeline.process(record)\n",
    "    if i >= 300:  # Simulate first 300 for demo\n",
    "        print(f\"  ... (continuing to process remaining records)\")\n",
    "        stream_pipeline.processed_count = len(sample_records)\n",
    "        break\n",
    "\n",
    "print(f\"\\n  Total streamed: {stream_pipeline.processed_count} records\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Key Difference:\")\n",
    "print(\"  Batch: Waits for all data, processes in chunks\")\n",
    "print(\"  Stream: Processes each record immediately as it arrives\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Common Data Engineering Challenges\n",
    "\n",
    "### Challenge 1: Data Quality Issues\n",
    "\n",
    "**Problems:**\n",
    "- Missing values\n",
    "- Duplicate records\n",
    "- Incorrect data types\n",
    "- Inconsistent formats\n",
    "- Schema changes\n",
    "\n",
    "**Solutions:**\n",
    "- Data validation frameworks (Great Expectations)\n",
    "- Schema enforcement\n",
    "- Automated quality checks\n",
    "- Data contracts between teams\n",
    "\n",
    "---\n",
    "\n",
    "### Challenge 2: Scalability\n",
    "\n",
    "**Problems:**\n",
    "- Data volume growing exponentially\n",
    "- Pipelines becoming slower\n",
    "- Infrastructure costs rising\n",
    "\n",
    "**Solutions:**\n",
    "- Distributed processing (Spark, Flink)\n",
    "- Incremental loading strategies\n",
    "- Data partitioning\n",
    "- Caching and optimization\n",
    "\n",
    "---\n",
    "\n",
    "### Challenge 3: Pipeline Failures\n",
    "\n",
    "**Problems:**\n",
    "- Source systems unavailable\n",
    "- Network issues\n",
    "- Code bugs\n",
    "- Resource exhaustion\n",
    "\n",
    "**Solutions:**\n",
    "- Retry logic with exponential backoff\n",
    "- Circuit breakers\n",
    "- Dead letter queues\n",
    "- Comprehensive monitoring and alerting\n",
    "- Idempotent operations\n",
    "\n",
    "---\n",
    "\n",
    "### Challenge 4: Changing Requirements\n",
    "\n",
    "**Problems:**\n",
    "- Business needs evolve\n",
    "- New data sources added\n",
    "- Schema changes\n",
    "\n",
    "**Solutions:**\n",
    "- Modular pipeline design\n",
    "- Configuration-driven pipelines\n",
    "- Version control and CI/CD\n",
    "- Backward compatibility\n",
    "\n",
    "---\n",
    "\n",
    "### Challenge 5: Data Governance\n",
    "\n",
    "**Problems:**\n",
    "- Who owns what data?\n",
    "- Privacy regulations (GDPR, CCPA)\n",
    "- Data lineage tracking\n",
    "- Access control\n",
    "\n",
    "**Solutions:**\n",
    "- Data catalog (Amundsen, DataHub)\n",
    "- Lineage tracking tools\n",
    "- Role-based access control\n",
    "- Data classification\n",
    "\n",
    "---\n",
    "\n",
    "### Challenge 6: Monitoring and Debugging\n",
    "\n",
    "**Problems:**\n",
    "- Pipeline failures hard to diagnose\n",
    "- No visibility into data flow\n",
    "- Performance degradation over time\n",
    "\n",
    "**Solutions:**\n",
    "- Comprehensive logging\n",
    "- Metrics and dashboards\n",
    "- Data quality metrics\n",
    "- Alerting systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Implementing basic data quality checks\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def validate_data_quality(df, column_rules):\n",
    "    \"\"\"\n",
    "    Validate data quality based on rules\n",
    "\n",
    "    column_rules: dict of {column_name: {rule_type: rule_value}}\n",
    "    \"\"\"\n",
    "    validation_results = []\n",
    "\n",
    "    for column, rules in column_rules.items():\n",
    "        if column not in df.columns:\n",
    "            validation_results.append(\n",
    "                {\n",
    "                    \"column\": column,\n",
    "                    \"check\": \"column_exists\",\n",
    "                    \"passed\": False,\n",
    "                    \"message\": f\"Column {column} not found\",\n",
    "                }\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        # Check for null values\n",
    "        if \"allow_null\" in rules and not rules[\"allow_null\"]:\n",
    "            null_count = df[column].isnull().sum()\n",
    "            validation_results.append(\n",
    "                {\n",
    "                    \"column\": column,\n",
    "                    \"check\": \"no_nulls\",\n",
    "                    \"passed\": null_count == 0,\n",
    "                    \"message\": (\n",
    "                        f\"Found {null_count} null values\" if null_count > 0 else \"No nulls found\"\n",
    "                    ),\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # Check data type\n",
    "        if \"dtype\" in rules:\n",
    "            expected_type = rules[\"dtype\"]\n",
    "            actual_type = str(df[column].dtype)\n",
    "            validation_results.append(\n",
    "                {\n",
    "                    \"column\": column,\n",
    "                    \"check\": \"data_type\",\n",
    "                    \"passed\": actual_type.startswith(expected_type),\n",
    "                    \"message\": f\"Expected {expected_type}, got {actual_type}\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # Check value range\n",
    "        if \"min_value\" in rules:\n",
    "            min_val = df[column].min()\n",
    "            passed = min_val >= rules[\"min_value\"]\n",
    "            validation_results.append(\n",
    "                {\n",
    "                    \"column\": column,\n",
    "                    \"check\": \"min_value\",\n",
    "                    \"passed\": passed,\n",
    "                    \"message\": (\n",
    "                        f'Min value {min_val} >= {rules[\"min_value\"]}'\n",
    "                        if passed\n",
    "                        else f'Min value {min_val} < {rules[\"min_value\"]}'\n",
    "                    ),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return validation_results\n",
    "\n",
    "\n",
    "# Create sample data with quality issues\n",
    "data = {\n",
    "    \"user_id\": [1, 2, 3, None, 5],  # Has null\n",
    "    \"age\": [25, 30, -5, 40, 150],  # Has invalid values\n",
    "    \"revenue\": [100.5, 200.0, 150.0, 300.0, 250.0],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define quality rules\n",
    "quality_rules = {\n",
    "    \"user_id\": {\"allow_null\": False, \"dtype\": \"int\"},\n",
    "    \"age\": {\"allow_null\": False, \"min_value\": 0, \"dtype\": \"int\"},\n",
    "    \"revenue\": {\"allow_null\": False, \"min_value\": 0, \"dtype\": \"float\"},\n",
    "}\n",
    "\n",
    "# Run validation\n",
    "results = validate_data_quality(df, quality_rules)\n",
    "\n",
    "# Display results\n",
    "print(\"Data Quality Validation Results:\")\n",
    "print(\"=\" * 70)\n",
    "for result in results:\n",
    "    status = \"[OK] PASS\" if result[\"passed\"] else \"[FAIL] FAIL\"\n",
    "    print(f\"{status} | {result['column']:15} | {result['check']:15} | {result['message']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "total = len(results)\n",
    "passed = sum(1 for r in results if r[\"passed\"])\n",
    "print(f\"Overall: {passed}/{total} checks passed ({(passed/total*100):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. The Data Engineering Lifecycle\n",
    "\n",
    "Understanding the full lifecycle helps you see where each task fits:\n",
    "\n",
    "### Phase 1: Requirements Gathering\n",
    "- Understand business needs\n",
    "- Identify data sources\n",
    "- Define success metrics\n",
    "- Determine latency requirements\n",
    "\n",
    "### Phase 2: Design\n",
    "- Choose architecture (batch/stream)\n",
    "- Select tech stack\n",
    "- Design data models\n",
    "- Plan for scalability\n",
    "\n",
    "### Phase 3: Implementation\n",
    "- Build extraction logic\n",
    "- Implement transformations\n",
    "- Set up storage layer\n",
    "- Create data quality checks\n",
    "\n",
    "### Phase 4: Testing\n",
    "- Unit tests for transformations\n",
    "- Integration tests\n",
    "- Data quality validation\n",
    "- Performance testing\n",
    "\n",
    "### Phase 5: Deployment\n",
    "- CI/CD pipelines\n",
    "- Monitoring setup\n",
    "- Alerting configuration\n",
    "- Documentation\n",
    "\n",
    "### Phase 6: Maintenance\n",
    "- Monitor performance\n",
    "- Handle incidents\n",
    "- Optimize queries\n",
    "- Scale infrastructure\n",
    "\n",
    "### Phase 7: Evolution\n",
    "- Add new data sources\n",
    "- Implement new features\n",
    "- Refactor for performance\n",
    "- Adapt to changing needs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Key Takeaways\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "[OK] **Data Engineering Role**: Building and maintaining data infrastructure\n",
    "\n",
    "[OK] **Modern Data Stack**: 7 layers from source to consumption\n",
    "\n",
    "[OK] **ETL vs ELT**: \n",
    "- ETL: Transform before loading (traditional)\n",
    "- ELT: Load then transform (modern)\n",
    "- ELT is winning due to cloud warehouses\n",
    "\n",
    "[OK] **Architectures**:\n",
    "- Batch: Scheduled processing (most common)\n",
    "- Streaming: Real-time processing\n",
    "- Lambda/Kappa: Hybrid approaches\n",
    "\n",
    "[OK] **Challenges**: Quality, scalability, failures, governance\n",
    "\n",
    "[OK] **Solutions**: Validation, monitoring, modular design, automation\n",
    "\n",
    "### Important Principles\n",
    "\n",
    "1. **Start simple, scale as needed** - Don't over-engineer\n",
    "2. **Data quality is paramount** - Garbage in, garbage out\n",
    "3. **Design for failure** - Systems will fail, plan for it\n",
    "4. **Automate everything** - Manual processes don't scale\n",
    "5. **Monitor and measure** - You can't improve what you don't measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Practice Questions\n",
    "\n",
    "Test your understanding:\n",
    "\n",
    "1. **When would you choose ETL over ELT?**\n",
    "   - Consider: data privacy, warehouse costs, transformation complexity\n",
    "\n",
    "2. **What architecture would you use for:**\n",
    "   - Daily sales reports?\n",
    "   - Real-time fraud detection?\n",
    "   - Monthly financial analysis?\n",
    "\n",
    "3. **How would you handle:**\n",
    "   - A source system that goes down during extraction?\n",
    "   - Schema changes in source data?\n",
    "   - 10x increase in data volume?\n",
    "\n",
    "4. **What's the difference between:**\n",
    "   - Data Engineer vs. Data Scientist?\n",
    "   - Data Warehouse vs. Data Lake?\n",
    "   - Batch vs. Stream processing?\n",
    "\n",
    "Think about these - we'll apply the answers in upcoming modules!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Next Steps\n",
    "\n",
    "Congratulations! You now understand the fundamental concepts of data engineering.\n",
    "\n",
    "### Coming Up in Module 02\n",
    "\n",
    "In **Module 02: Data Sources and Extraction**, you'll learn:\n",
    "- How to extract data from different sources\n",
    "- Working with files (CSV, JSON, Parquet)\n",
    "- Connecting to databases\n",
    "- Calling REST APIs\n",
    "- Error handling and retries\n",
    "- Hands-on extraction examples\n",
    "\n",
    "### Resources for Further Learning\n",
    "\n",
    "- [The Data Engineering Cookbook](https://github.com/andkret/Cookbook)\n",
    "- [Fundamentals of Data Engineering (Book)](https://www.oreilly.com/library/view/fundamentals-of-data/9781098108298/)\n",
    "- [Data Engineering Weekly Newsletter](https://www.dataengineeringweekly.com/)\n",
    "\n",
    "---\n",
    "\n",
    "**Ready to start extracting data?**\n",
    "\n",
    "Open `02_data_sources_and_extraction.ipynb` to continue!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
