{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 03: Stream Processing Fundamentals\n",
    "\n",
    "**Estimated Time:** 90 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this module, you will:\n",
    "- Understand stream processing concepts and patterns\n",
    "- Differentiate between stateless and stateful operations\n",
    "- Master windowing techniques (tumbling, sliding, session)\n",
    "- Work with event time vs processing time\n",
    "- Handle late-arriving data with watermarks\n",
    "- Build real-time aggregations and transformations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is Stream Processing?\n",
    "\n",
    "### Stream Processing vs Batch Processing\n",
    "\n",
    "**Batch Processing (Traditional):**\n",
    "```\n",
    "Data Collection (1 hour) → Process → Results\n",
    "    [...........]          [compute]   [output]\n",
    "    Wait, wait, wait...    ALL at once  \n",
    "\n",
    "Characteristics:\n",
    "- Process data in fixed-size chunks\n",
    "- High latency (minutes to hours)\n",
    "- Complete dataset available\n",
    "- Examples: Daily reports, ETL jobs\n",
    "```\n",
    "\n",
    "**Stream Processing (Real-time):**\n",
    "```\n",
    "Continuous Data → Process Each Event → Continuous Results\n",
    "   [→][→][→][→]      [compute]           [→][→][→][→]\n",
    "   Process immediately, one by one or in micro-batches\n",
    "\n",
    "Characteristics:\n",
    "- Process data as it arrives\n",
    "- Low latency (milliseconds to seconds)\n",
    "- Infinite, unbounded dataset\n",
    "- Examples: Fraud detection, monitoring\n",
    "```\n",
    "\n",
    "### Stream Processing Operations\n",
    "\n",
    "**1. Stateless Operations:**\n",
    "```\n",
    "Input Stream:    [1] → [2] → [3] → [4] → [5]\n",
    "                  ↓     ↓     ↓     ↓     ↓\n",
    "Operation:       *2    *2    *2    *2    *2  (independent)\n",
    "                  ↓     ↓     ↓     ↓     ↓\n",
    "Output Stream:   [2] → [4] → [6] → [8] → [10]\n",
    "\n",
    "Examples: filter, map, flatMap\n",
    "```\n",
    "\n",
    "**2. Stateful Operations:**\n",
    "```\n",
    "Input Stream:    [1] → [2] → [3] → [4] → [5]\n",
    "                  ↓     ↓     ↓     ↓     ↓\n",
    "State:          sum=1 sum=3 sum=6 sum=10 sum=15\n",
    "                  ↓     ↓     ↓     ↓     ↓\n",
    "Output Stream:   [1] → [3] → [6] → [10] → [15]\n",
    "\n",
    "Examples: count, sum, aggregations, joins\n",
    "```\n",
    "\n",
    "### Real-World Use Cases\n",
    "\n",
    "| Use Case | Input | Processing | Output |\n",
    "|----------|-------|------------|--------|\n",
    "| Fraud Detection | Transactions | Pattern matching | Alerts |\n",
    "| Real-time Analytics | User clicks | Aggregation | Dashboard |\n",
    "| IoT Monitoring | Sensor data | Threshold checks | Notifications |\n",
    "| Recommendation | User behavior | ML inference | Recommendations |\n",
    "| Log Monitoring | Log events | Filtering + aggregation | Metrics |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Import libraries\n",
    "from confluent_kafka import Producer, Consumer, KafkaException\n",
    "from confluent_kafka.admin import AdminClient, NewTopic\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict, deque\n",
    "import random\n",
    "import threading\n",
    "\n",
    "admin_client = AdminClient({\"bootstrap.servers\": \"localhost:9092\"})\n",
    "print(\"[OK] Ready for stream processing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create topics for stream processing examples\n",
    "TOPICS = {\n",
    "    \"click-stream\": \"Raw user click events\",\n",
    "    \"transactions\": \"Financial transactions\",\n",
    "    \"sensor-data\": \"IoT sensor readings\",\n",
    "    \"processed-clicks\": \"Processed click events\",\n",
    "}\n",
    "\n",
    "new_topics = [\n",
    "    NewTopic(topic=name, num_partitions=3, replication_factor=1) for name in TOPICS.keys()\n",
    "]\n",
    "\n",
    "try:\n",
    "    futures = admin_client.create_topics(new_topics)\n",
    "    for topic, future in futures.items():\n",
    "        try:\n",
    "            future.result()\n",
    "            print(f\"[OK] Created topic '{topic}'\")\n",
    "        except KafkaException as e:\n",
    "            if \"TOPIC_ALREADY_EXISTS\" in str(e):\n",
    "                print(f\"[OK] Topic '{topic}' exists\")\n",
    "except Exception as e:\n",
    "    print(f\"[FAIL] Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Stateless Stream Processing\n",
    "\n",
    "### Filter Operation\n",
    "\n",
    "**Concept**: Select events that match a condition\n",
    "```\n",
    "Input:  [1] [2] [3] [4] [5] [6] [7] [8] [9]\n",
    "         ↓   ↓   ↓   ↓   ↓   ↓   ↓   ↓   ↓\n",
    "Filter: even numbers only\n",
    "         ↓       ↓       ↓       ↓\n",
    "Output: [2]     [4]     [6]     [8]\n",
    "```\n",
    "\n",
    "### Map Operation\n",
    "\n",
    "**Concept**: Transform each event independently\n",
    "```\n",
    "Input:  [a] [b] [c] [d]\n",
    "         ↓   ↓   ↓   ↓\n",
    "Map:    uppercase\n",
    "         ↓   ↓   ↓   ↓\n",
    "Output: [A] [B] [C] [D]\n",
    "```\n",
    "\n",
    "### FlatMap Operation\n",
    "\n",
    "**Concept**: Transform one event into multiple events\n",
    "```\n",
    "Input:  [\"hello world\"] [\"foo bar\"]\n",
    "         ↓               ↓\n",
    "FlatMap: split by space\n",
    "         ↓               ↓\n",
    "Output: [\"hello\"][\"world\"] [\"foo\"][\"bar\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Stateless stream processor - Filter and Transform\n",
    "class StatelessProcessor:\n",
    "    \"\"\"Process events without maintaining state\"\"\"\n",
    "\n",
    "    def __init__(self, input_topic, output_topic):\n",
    "        self.input_topic = input_topic\n",
    "        self.output_topic = output_topic\n",
    "\n",
    "        self.consumer = Consumer(\n",
    "            {\n",
    "                \"bootstrap.servers\": \"localhost:9092\",\n",
    "                \"group.id\": \"stateless-processor\",\n",
    "                \"auto.offset.reset\": \"earliest\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "        self.producer = Producer({\"bootstrap.servers\": \"localhost:9092\"})\n",
    "\n",
    "    def filter_event(self, event):\n",
    "        \"\"\"Filter: Only process events from premium users\"\"\"\n",
    "        return event.get(\"user_tier\") == \"premium\"\n",
    "\n",
    "    def transform_event(self, event):\n",
    "        \"\"\"Map: Enrich event with additional fields\"\"\"\n",
    "        return {\n",
    "            **event,\n",
    "            \"processed_at\": datetime.now().isoformat(),\n",
    "            \"is_premium\": True,\n",
    "            \"priority\": \"high\",\n",
    "        }\n",
    "\n",
    "    def process(self, duration_seconds=10):\n",
    "        \"\"\"Run the stream processor\"\"\"\n",
    "        self.consumer.subscribe([self.input_topic])\n",
    "\n",
    "        processed_count = 0\n",
    "        filtered_count = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        print(f\"[OK] Starting stateless processor...\\n\")\n",
    "\n",
    "        try:\n",
    "            while time.time() - start_time < duration_seconds:\n",
    "                msg = self.consumer.poll(timeout=1.0)\n",
    "\n",
    "                if msg is None:\n",
    "                    continue\n",
    "\n",
    "                if msg.error():\n",
    "                    continue\n",
    "\n",
    "                # Deserialize\n",
    "                event = json.loads(msg.value().decode(\"utf-8\"))\n",
    "\n",
    "                # Filter\n",
    "                if not self.filter_event(event):\n",
    "                    filtered_count += 1\n",
    "                    continue\n",
    "\n",
    "                # Transform\n",
    "                transformed = self.transform_event(event)\n",
    "\n",
    "                # Produce to output topic\n",
    "                self.producer.produce(\n",
    "                    topic=self.output_topic, key=event.get(\"user_id\"), value=json.dumps(transformed)\n",
    "                )\n",
    "                self.producer.poll(0)\n",
    "\n",
    "                processed_count += 1\n",
    "\n",
    "                if processed_count <= 5:\n",
    "                    print(f\"[{processed_count}] Processed: {event['user_id']} - {event['action']}\")\n",
    "\n",
    "        finally:\n",
    "            self.producer.flush()\n",
    "            self.consumer.close()\n",
    "\n",
    "            print(f\"\\n[DATA] Processor Summary:\")\n",
    "            print(f\"  Processed: {processed_count}\")\n",
    "            print(f\"  Filtered out: {filtered_count}\")\n",
    "            print(f\"  Total events: {processed_count + filtered_count}\")\n",
    "\n",
    "\n",
    "print(\"[OK] StatelessProcessor class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample click events\n",
    "def generate_click_events(num_events=50):\n",
    "    \"\"\"Generate sample click stream events\"\"\"\n",
    "    producer = Producer({\"bootstrap.servers\": \"localhost:9092\"})\n",
    "\n",
    "    actions = [\"view_page\", \"click_button\", \"add_to_cart\", \"checkout\"]\n",
    "    tiers = [\"free\", \"premium\", \"free\", \"premium\", \"free\"]  # Mix of tiers\n",
    "\n",
    "    for i in range(num_events):\n",
    "        event = {\n",
    "            \"event_id\": f\"evt_{i}\",\n",
    "            \"user_id\": f\"user_{random.randint(1, 10)}\",\n",
    "            \"user_tier\": random.choice(tiers),\n",
    "            \"action\": random.choice(actions),\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"page\": f\"/page{random.randint(1, 5)}\",\n",
    "        }\n",
    "\n",
    "        producer.produce(topic=\"click-stream\", value=json.dumps(event))\n",
    "        producer.poll(0)\n",
    "        time.sleep(0.05)\n",
    "\n",
    "    producer.flush()\n",
    "    print(f\"[OK] Generated {num_events} click events\")\n",
    "\n",
    "\n",
    "# Generate events in background\n",
    "generator_thread = threading.Thread(target=generate_click_events, args=(50,))\n",
    "generator_thread.start()\n",
    "\n",
    "# Wait a moment for events to start flowing\n",
    "time.sleep(1)\n",
    "\n",
    "# Run stateless processor\n",
    "processor = StatelessProcessor(\"click-stream\", \"processed-clicks\")\n",
    "processor.process(duration_seconds=8)\n",
    "\n",
    "generator_thread.join()\n",
    "print(\"\\n[SUCCESS] Stateless processing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Stateful Stream Processing\n",
    "\n",
    "### State Management\n",
    "\n",
    "**Stateful operations need to remember previous events:**\n",
    "```\n",
    "Example: Count clicks per user\n",
    "\n",
    "Event 1: {user: 'Alice', action: 'click'}\n",
    "State: {'Alice': 1}\n",
    "\n",
    "Event 2: {user: 'Bob', action: 'click'}\n",
    "State: {'Alice': 1, 'Bob': 1}\n",
    "\n",
    "Event 3: {user: 'Alice', action: 'click'}\n",
    "State: {'Alice': 2, 'Bob': 1}  ← Updated state!\n",
    "```\n",
    "\n",
    "### Types of Stateful Operations\n",
    "\n",
    "**1. Aggregations:**\n",
    "- Count: How many events?\n",
    "- Sum: Total of values\n",
    "- Average: Mean of values\n",
    "- Min/Max: Extremes\n",
    "\n",
    "**2. Joins:**\n",
    "- Stream-Stream: Join two event streams\n",
    "- Stream-Table: Enrich stream with reference data\n",
    "\n",
    "**3. Pattern Detection:**\n",
    "- Sequence detection: A followed by B within time window\n",
    "- Anomaly detection: Values outside normal range\n",
    "\n",
    "### State Storage\n",
    "\n",
    "```\n",
    "Stream Processor\n",
    "┌────────────────────────────────┐\n",
    "│  Processing Logic              │\n",
    "│  ┌──────────────┐              │\n",
    "│  │ Local State  │              │\n",
    "│  │ (in-memory)  │              │\n",
    "│  └──────┬───────┘              │\n",
    "│         │                      │\n",
    "│         ↓                      │\n",
    "│  ┌──────────────┐              │\n",
    "│  │ State Store  │              │\n",
    "│  │ (persistent) │              │\n",
    "│  └──────────────┘              │\n",
    "└────────────────────────────────┘\n",
    "\n",
    "Benefits:\n",
    "- Fast access (in-memory)\n",
    "- Fault tolerant (persisted)\n",
    "- Recoverable (from checkpoints)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Stateful stream processor - Aggregations\n",
    "class StatefulAggregator:\n",
    "    \"\"\"Process events with state (counting, summing, etc.)\"\"\"\n",
    "\n",
    "    def __init__(self, input_topic):\n",
    "        self.input_topic = input_topic\n",
    "\n",
    "        # State: counts per user\n",
    "        self.user_counts = defaultdict(int)\n",
    "\n",
    "        # State: total spend per user\n",
    "        self.user_spend = defaultdict(float)\n",
    "\n",
    "        # State: actions per user\n",
    "        self.user_actions = defaultdict(list)\n",
    "\n",
    "        self.consumer = Consumer(\n",
    "            {\n",
    "                \"bootstrap.servers\": \"localhost:9092\",\n",
    "                \"group.id\": \"stateful-aggregator\",\n",
    "                \"auto.offset.reset\": \"earliest\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def update_state(self, event):\n",
    "        \"\"\"Update internal state with new event\"\"\"\n",
    "        user_id = event[\"user_id\"]\n",
    "\n",
    "        # Increment count\n",
    "        self.user_counts[user_id] += 1\n",
    "\n",
    "        # Track spend\n",
    "        if \"amount\" in event:\n",
    "            self.user_spend[user_id] += event[\"amount\"]\n",
    "\n",
    "        # Track actions\n",
    "        self.user_actions[user_id].append(event.get(\"action\", \"unknown\"))\n",
    "\n",
    "    def get_user_summary(self, user_id):\n",
    "        \"\"\"Get aggregated state for a user\"\"\"\n",
    "        return {\n",
    "            \"user_id\": user_id,\n",
    "            \"event_count\": self.user_counts[user_id],\n",
    "            \"total_spend\": self.user_spend[user_id],\n",
    "            \"actions\": self.user_actions[user_id][-5:],  # Last 5 actions\n",
    "        }\n",
    "\n",
    "    def process(self, duration_seconds=10):\n",
    "        \"\"\"Run the stateful processor\"\"\"\n",
    "        self.consumer.subscribe([self.input_topic])\n",
    "\n",
    "        events_processed = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        print(f\"[OK] Starting stateful aggregator...\\n\")\n",
    "\n",
    "        try:\n",
    "            while time.time() - start_time < duration_seconds:\n",
    "                msg = self.consumer.poll(timeout=1.0)\n",
    "\n",
    "                if msg is None:\n",
    "                    continue\n",
    "\n",
    "                if msg.error():\n",
    "                    continue\n",
    "\n",
    "                event = json.loads(msg.value().decode(\"utf-8\"))\n",
    "\n",
    "                # Update state\n",
    "                self.update_state(event)\n",
    "                events_processed += 1\n",
    "\n",
    "                # Show progress\n",
    "                if events_processed % 10 == 0:\n",
    "                    print(\n",
    "                        f\"[{events_processed}] Processed events, tracking {len(self.user_counts)} users\"\n",
    "                    )\n",
    "\n",
    "        finally:\n",
    "            self.consumer.close()\n",
    "\n",
    "            # Show final state\n",
    "            print(f\"\\n[DATA] Final Aggregation Results:\\n\")\n",
    "\n",
    "            # Top 5 users by event count\n",
    "            top_users = sorted(self.user_counts.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "            for user_id, count in top_users:\n",
    "                summary = self.get_user_summary(user_id)\n",
    "                print(f\"{user_id}:\")\n",
    "                print(f\"  Events: {summary['event_count']}\")\n",
    "                print(f\"  Spend: ${summary['total_spend']:.2f}\")\n",
    "                print(f\"  Recent actions: {summary['actions']}\")\n",
    "                print()\n",
    "\n",
    "\n",
    "print(\"[OK] StatefulAggregator class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate transaction events\n",
    "def generate_transactions(num_events=100):\n",
    "    \"\"\"Generate sample transaction events\"\"\"\n",
    "    producer = Producer({\"bootstrap.servers\": \"localhost:9092\"})\n",
    "\n",
    "    actions = [\"view\", \"add_to_cart\", \"purchase\", \"return\"]\n",
    "\n",
    "    for i in range(num_events):\n",
    "        action = random.choice(actions)\n",
    "        event = {\n",
    "            \"event_id\": f\"txn_{i}\",\n",
    "            \"user_id\": f\"user_{random.randint(1, 5)}\",  # 5 users\n",
    "            \"action\": action,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "        }\n",
    "\n",
    "        # Add amount for purchases\n",
    "        if action == \"purchase\":\n",
    "            event[\"amount\"] = random.randint(10, 200)\n",
    "\n",
    "        producer.produce(topic=\"transactions\", value=json.dumps(event))\n",
    "        producer.poll(0)\n",
    "        time.sleep(0.03)\n",
    "\n",
    "    producer.flush()\n",
    "    print(f\"[OK] Generated {num_events} transaction events\")\n",
    "\n",
    "\n",
    "# Generate and process\n",
    "generator_thread = threading.Thread(target=generate_transactions, args=(100,))\n",
    "generator_thread.start()\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "aggregator = StatefulAggregator(\"transactions\")\n",
    "aggregator.process(duration_seconds=8)\n",
    "\n",
    "generator_thread.join()\n",
    "print(\"[SUCCESS] Stateful aggregation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Windowing\n",
    "\n",
    "### Why Windowing?\n",
    "\n",
    "**Problem**: Infinite streams need bounded computations\n",
    "```\n",
    "Question: \"Count events per hour\"\n",
    "Stream: [e1][e2][e3]... (infinite)\n",
    "\n",
    "Solution: Divide stream into windows!\n",
    "```\n",
    "\n",
    "### Types of Windows\n",
    "\n",
    "**1. Tumbling Windows (Fixed, Non-overlapping):**\n",
    "```\n",
    "Window size: 1 hour\n",
    "\n",
    "00:00 ────────────── 01:00 ────────────── 02:00\n",
    "  [  Window 1    ]     [  Window 2    ]\n",
    "  Count: 10 events     Count: 15 events\n",
    "\n",
    "Properties:\n",
    "- Fixed size\n",
    "- No overlap\n",
    "- Each event in exactly ONE window\n",
    "```\n",
    "\n",
    "**2. Sliding Windows (Overlapping):**\n",
    "```\n",
    "Window size: 1 hour, Slide: 15 minutes\n",
    "\n",
    "00:00 ──────── 00:15 ──────── 00:30 ──────── 00:45 ──────── 01:00\n",
    "  [    W1     ]\n",
    "       [    W2     ]\n",
    "            [    W3     ]\n",
    "                 [    W4     ]\n",
    "\n",
    "Properties:\n",
    "- Fixed size\n",
    "- Windows overlap\n",
    "- Each event in MULTIPLE windows\n",
    "- Good for: Moving averages, trend detection\n",
    "```\n",
    "\n",
    "**3. Session Windows (Gap-based):**\n",
    "```\n",
    "Inactivity gap: 5 minutes\n",
    "\n",
    "Events: [e1]─2min─[e2]─1min─[e3]───7min───[e4]─3min─[e5]\n",
    "        └──────── Session 1 ──────┘       └── Session 2 ──┘\n",
    "\n",
    "Properties:\n",
    "- Dynamic size\n",
    "- Ends after inactivity gap\n",
    "- Good for: User sessions, activity bursts\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Tumbling Window Aggregator\n",
    "class TumblingWindowAggregator:\n",
    "    \"\"\"Aggregate events in fixed-size, non-overlapping windows\"\"\"\n",
    "\n",
    "    def __init__(self, window_size_seconds=10):\n",
    "        self.window_size = window_size_seconds\n",
    "        self.windows = {}  # window_id -> events\n",
    "        self.current_window_start = None\n",
    "\n",
    "    def get_window_id(self, timestamp):\n",
    "        \"\"\"Determine which window this event belongs to\"\"\"\n",
    "        event_time = datetime.fromisoformat(timestamp)\n",
    "        epoch = int(event_time.timestamp())\n",
    "        window_id = (epoch // self.window_size) * self.window_size\n",
    "        return window_id\n",
    "\n",
    "    def add_event(self, event):\n",
    "        \"\"\"Add event to appropriate window\"\"\"\n",
    "        window_id = self.get_window_id(event[\"timestamp\"])\n",
    "\n",
    "        if window_id not in self.windows:\n",
    "            self.windows[window_id] = []\n",
    "\n",
    "        self.windows[window_id].append(event)\n",
    "\n",
    "    def get_window_results(self, window_id):\n",
    "        \"\"\"Get aggregated results for a window\"\"\"\n",
    "        events = self.windows.get(window_id, [])\n",
    "\n",
    "        if not events:\n",
    "            return None\n",
    "\n",
    "        # Aggregate\n",
    "        action_counts = defaultdict(int)\n",
    "        for event in events:\n",
    "            action_counts[event.get(\"action\", \"unknown\")] += 1\n",
    "\n",
    "        window_start = datetime.fromtimestamp(window_id)\n",
    "        window_end = window_start + timedelta(seconds=self.window_size)\n",
    "\n",
    "        return {\n",
    "            \"window_start\": window_start.isoformat(),\n",
    "            \"window_end\": window_end.isoformat(),\n",
    "            \"event_count\": len(events),\n",
    "            \"action_counts\": dict(action_counts),\n",
    "        }\n",
    "\n",
    "    def print_all_windows(self):\n",
    "        \"\"\"Display results for all windows\"\"\"\n",
    "        print(\"\\n[DATA] Tumbling Window Results:\\n\")\n",
    "\n",
    "        for window_id in sorted(self.windows.keys()):\n",
    "            result = self.get_window_results(window_id)\n",
    "            if result:\n",
    "                print(f\"Window {window_id}:\")\n",
    "                print(f\"  Time: {result['window_start'][:19]} to {result['window_end'][11:19]}\")\n",
    "                print(f\"  Events: {result['event_count']}\")\n",
    "                print(f\"  Actions: {result['action_counts']}\")\n",
    "                print()\n",
    "\n",
    "\n",
    "print(\"[OK] TumblingWindowAggregator class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test tumbling windows\n",
    "def generate_sensor_data(num_events=60, window_aggregator=None):\n",
    "    \"\"\"Generate sensor events over time\"\"\"\n",
    "    actions = [\"temp_reading\", \"humidity_reading\", \"motion_detected\"]\n",
    "\n",
    "    for i in range(num_events):\n",
    "        event = {\n",
    "            \"event_id\": f\"sensor_{i}\",\n",
    "            \"sensor_id\": f\"sensor_{random.randint(1, 3)}\",\n",
    "            \"action\": random.choice(actions),\n",
    "            \"value\": random.randint(20, 30),\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "        }\n",
    "\n",
    "        if window_aggregator:\n",
    "            window_aggregator.add_event(event)\n",
    "\n",
    "        time.sleep(0.1)  # 100ms between events\n",
    "\n",
    "    print(f\"[OK] Generated {num_events} sensor events\")\n",
    "\n",
    "\n",
    "# Create aggregator with 10-second windows\n",
    "window_agg = TumblingWindowAggregator(window_size_seconds=10)\n",
    "\n",
    "print(\"[OK] Generating events with 10-second tumbling windows...\\n\")\n",
    "generate_sensor_data(num_events=60, window_aggregator=window_agg)\n",
    "\n",
    "# Show results\n",
    "window_agg.print_all_windows()\n",
    "\n",
    "print(\"[SUCCESS] Tumbling window aggregation complete!\")\n",
    "print(\"\\n[OK] Notice: Events are grouped into non-overlapping 10-second windows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Event Time vs Processing Time\n",
    "\n",
    "### The Two Notions of Time\n",
    "\n",
    "**Event Time**: When the event actually occurred\n",
    "```\n",
    "Mobile phone disconnected: 10:00:00 AM\n",
    "Event created: 10:00:00 AM ← Event Time\n",
    "```\n",
    "\n",
    "**Processing Time**: When the system processes the event\n",
    "```\n",
    "Phone reconnects: 10:30:00 AM\n",
    "Event reaches Kafka: 10:30:01 AM\n",
    "Processed: 10:30:02 AM ← Processing Time\n",
    "```\n",
    "\n",
    "### Why Event Time Matters\n",
    "\n",
    "**Problem with Processing Time:**\n",
    "```\n",
    "Events:     A(10:00) → B(10:01) → C(10:02)\n",
    "            Network delay...\n",
    "Arrive:     C(10:05) → A(10:07) → B(10:08)\n",
    "            ↑ Out of order!\n",
    "\n",
    "Processing Time Windows (10:05-10:10):\n",
    "  Would count: C, A, B ← Wrong grouping!\n",
    "\n",
    "Event Time Windows (10:00-10:05):\n",
    "  Would count: A, B, C ← Correct grouping!\n",
    "```\n",
    "\n",
    "### Watermarks\n",
    "\n",
    "**Definition**: A watermark is an assertion that no events with timestamp < T will arrive\n",
    "\n",
    "```\n",
    "Events arrive:    E1(10:00) E2(10:01) E3(10:03)\n",
    "Watermark:        ────────────────────────────→ 10:02\n",
    "                  \"No events before 10:02 will arrive\"\n",
    "\n",
    "Late event:       E4(10:01) ← Timestamp before watermark!\n",
    "                  Options:\n",
    "                  1. Drop (ignore)\n",
    "                  2. Accept (adjust results)\n",
    "                  3. Side output (special handling)\n",
    "```\n",
    "\n",
    "### Handling Late Data\n",
    "\n",
    "**Strategy 1: Allowed Lateness**\n",
    "```\n",
    "Window: 10:00-10:10\n",
    "Watermark at 10:12 (2min delay)\n",
    "Allowed lateness: 5 minutes\n",
    "\n",
    "Accept events until: 10:12 + 5min = 10:17\n",
    "```\n",
    "\n",
    "**Strategy 2: Side Outputs**\n",
    "```\n",
    "On-time events → Main output\n",
    "Late events → Side output (for investigation)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate late-arriving events\n",
    "class EventTimeProcessor:\n",
    "    \"\"\"Process events using event time with watermarks\"\"\"\n",
    "\n",
    "    def __init__(self, window_size_seconds=10, allowed_lateness_seconds=5):\n",
    "        self.window_size = window_size_seconds\n",
    "        self.allowed_lateness = allowed_lateness_seconds\n",
    "        self.windows = {}\n",
    "        self.watermark = None\n",
    "        self.late_events = []\n",
    "\n",
    "    def update_watermark(self, event_time):\n",
    "        \"\"\"Update watermark (event_time - 2 seconds)\"\"\"\n",
    "        event_dt = datetime.fromisoformat(event_time)\n",
    "        new_watermark = event_dt - timedelta(seconds=2)\n",
    "\n",
    "        if self.watermark is None or new_watermark > self.watermark:\n",
    "            self.watermark = new_watermark\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def get_window_id(self, timestamp):\n",
    "        \"\"\"Get window ID from event timestamp\"\"\"\n",
    "        event_time = datetime.fromisoformat(timestamp)\n",
    "        epoch = int(event_time.timestamp())\n",
    "        return (epoch // self.window_size) * self.window_size\n",
    "\n",
    "    def is_late(self, event_time):\n",
    "        \"\"\"Check if event is late\"\"\"\n",
    "        if self.watermark is None:\n",
    "            return False\n",
    "\n",
    "        event_dt = datetime.fromisoformat(event_time)\n",
    "        return event_dt < self.watermark\n",
    "\n",
    "    def process_event(self, event):\n",
    "        \"\"\"Process event using event time\"\"\"\n",
    "        event_time = event[\"timestamp\"]\n",
    "\n",
    "        # Update watermark\n",
    "        self.update_watermark(event_time)\n",
    "\n",
    "        # Check if late\n",
    "        if self.is_late(event_time):\n",
    "            event_dt = datetime.fromisoformat(event_time)\n",
    "            lateness = (self.watermark - event_dt).total_seconds()\n",
    "\n",
    "            if lateness <= self.allowed_lateness:\n",
    "                # Accept late event\n",
    "                window_id = self.get_window_id(event_time)\n",
    "                if window_id not in self.windows:\n",
    "                    self.windows[window_id] = []\n",
    "                self.windows[window_id].append(event)\n",
    "                return \"late_accepted\"\n",
    "            else:\n",
    "                # Too late, drop\n",
    "                self.late_events.append(event)\n",
    "                return \"too_late\"\n",
    "        else:\n",
    "            # On-time event\n",
    "            window_id = self.get_window_id(event_time)\n",
    "            if window_id not in self.windows:\n",
    "                self.windows[window_id] = []\n",
    "            self.windows[window_id].append(event)\n",
    "            return \"on_time\"\n",
    "\n",
    "    def print_stats(self):\n",
    "        \"\"\"Print processing statistics\"\"\"\n",
    "        print(\"\\n[DATA] Event Time Processing Results:\\n\")\n",
    "        print(f\"Watermark: {self.watermark.isoformat() if self.watermark else 'None'}\")\n",
    "        print(f\"Windows processed: {len(self.windows)}\")\n",
    "        print(f\"Late events dropped: {len(self.late_events)}\")\n",
    "\n",
    "        total_events = sum(len(events) for events in self.windows.values())\n",
    "        print(f\"Total events in windows: {total_events}\")\n",
    "\n",
    "\n",
    "print(\"[OK] EventTimeProcessor class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate events with realistic delays\n",
    "def generate_events_with_delays():\n",
    "    \"\"\"Generate events with some arriving late\"\"\"\n",
    "    processor = EventTimeProcessor(window_size_seconds=10, allowed_lateness_seconds=3)\n",
    "\n",
    "    base_time = datetime.now()\n",
    "    events = []\n",
    "\n",
    "    # Create events with timestamps\n",
    "    for i in range(20):\n",
    "        event_time = base_time + timedelta(seconds=i)\n",
    "        events.append(\n",
    "            {\"id\": i, \"timestamp\": event_time.isoformat(), \"value\": random.randint(1, 100)}\n",
    "        )\n",
    "\n",
    "    # Shuffle to simulate out-of-order arrival\n",
    "    random.shuffle(events)\n",
    "\n",
    "    # Process events\n",
    "    on_time = 0\n",
    "    late_accepted = 0\n",
    "    too_late = 0\n",
    "\n",
    "    print(\"[OK] Processing events with out-of-order arrival...\\n\")\n",
    "\n",
    "    for i, event in enumerate(events):\n",
    "        result = processor.process_event(event)\n",
    "\n",
    "        if result == \"on_time\":\n",
    "            on_time += 1\n",
    "        elif result == \"late_accepted\":\n",
    "            late_accepted += 1\n",
    "            print(f\"[{i+1}] Event {event['id']} arrived LATE but ACCEPTED\")\n",
    "        elif result == \"too_late\":\n",
    "            too_late += 1\n",
    "            print(f\"[{i+1}] Event {event['id']} arrived TOO LATE, DROPPED\")\n",
    "\n",
    "        time.sleep(0.05)\n",
    "\n",
    "    processor.print_stats()\n",
    "\n",
    "    print(f\"\\n[DATA] Event Classification:\")\n",
    "    print(f\"  On-time: {on_time}\")\n",
    "    print(f\"  Late (accepted): {late_accepted}\")\n",
    "    print(f\"  Too late (dropped): {too_late}\")\n",
    "\n",
    "\n",
    "generate_events_with_delays()\n",
    "\n",
    "print(\"\\n[SUCCESS] Event time processing complete!\")\n",
    "print(\"\\n[OK] Key insight: Event time ensures correct results despite out-of-order arrival\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Mini-Project: Real-Time Analytics Dashboard\n",
    "\n",
    "Let's build a complete stream processor that:\n",
    "- Filters events\n",
    "- Aggregates in tumbling windows\n",
    "- Handles late data\n",
    "- Produces real-time metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete real-time analytics processor\n",
    "class RealTimeAnalytics:\n",
    "    \"\"\"\n",
    "    Real-time analytics processor combining:\n",
    "    - Filtering\n",
    "    - Tumbling windows\n",
    "    - Event time processing\n",
    "    - Stateful aggregations\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, window_size_seconds=10):\n",
    "        self.window_size = window_size_seconds\n",
    "        self.windows = {}\n",
    "        self.stats = {\"total_events\": 0, \"filtered_events\": 0, \"windows_completed\": 0}\n",
    "\n",
    "    def should_process(self, event):\n",
    "        \"\"\"Filter: Only process high-value events\"\"\"\n",
    "        return event.get(\"value\", 0) > 50\n",
    "\n",
    "    def get_window_id(self, timestamp):\n",
    "        \"\"\"Assign event to window\"\"\"\n",
    "        event_time = datetime.fromisoformat(timestamp)\n",
    "        epoch = int(event_time.timestamp())\n",
    "        return (epoch // self.window_size) * self.window_size\n",
    "\n",
    "    def process_event(self, event):\n",
    "        \"\"\"Process incoming event\"\"\"\n",
    "        self.stats[\"total_events\"] += 1\n",
    "\n",
    "        # Filter\n",
    "        if not self.should_process(event):\n",
    "            self.stats[\"filtered_events\"] += 1\n",
    "            return False\n",
    "\n",
    "        # Assign to window\n",
    "        window_id = self.get_window_id(event[\"timestamp\"])\n",
    "\n",
    "        if window_id not in self.windows:\n",
    "            self.windows[window_id] = {\n",
    "                \"events\": [],\n",
    "                \"count\": 0,\n",
    "                \"sum\": 0,\n",
    "                \"max\": float(\"-inf\"),\n",
    "                \"min\": float(\"inf\"),\n",
    "            }\n",
    "\n",
    "        # Update window state\n",
    "        window = self.windows[window_id]\n",
    "        window[\"events\"].append(event)\n",
    "        window[\"count\"] += 1\n",
    "        window[\"sum\"] += event.get(\"value\", 0)\n",
    "        window[\"max\"] = max(window[\"max\"], event.get(\"value\", 0))\n",
    "        window[\"min\"] = min(window[\"min\"], event.get(\"value\", 0))\n",
    "\n",
    "        return True\n",
    "\n",
    "    def get_window_metrics(self, window_id):\n",
    "        \"\"\"Calculate metrics for a window\"\"\"\n",
    "        if window_id not in self.windows:\n",
    "            return None\n",
    "\n",
    "        window = self.windows[window_id]\n",
    "\n",
    "        if window[\"count\"] == 0:\n",
    "            return None\n",
    "\n",
    "        window_start = datetime.fromtimestamp(window_id)\n",
    "        window_end = window_start + timedelta(seconds=self.window_size)\n",
    "\n",
    "        return {\n",
    "            \"window_start\": window_start.isoformat(),\n",
    "            \"window_end\": window_end.isoformat(),\n",
    "            \"count\": window[\"count\"],\n",
    "            \"sum\": window[\"sum\"],\n",
    "            \"avg\": window[\"sum\"] / window[\"count\"],\n",
    "            \"max\": window[\"max\"],\n",
    "            \"min\": window[\"min\"],\n",
    "        }\n",
    "\n",
    "    def print_dashboard(self):\n",
    "        \"\"\"Print real-time dashboard\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"           REAL-TIME ANALYTICS DASHBOARD\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        print(f\"\\nOverall Statistics:\")\n",
    "        print(f\"  Total events received: {self.stats['total_events']}\")\n",
    "        print(f\"  Events filtered out: {self.stats['filtered_events']}\")\n",
    "        print(f\"  Events processed: {self.stats['total_events'] - self.stats['filtered_events']}\")\n",
    "        print(f\"  Active windows: {len(self.windows)}\")\n",
    "\n",
    "        print(f\"\\nWindow Metrics:\")\n",
    "        print(f\"{'Window':<20} {'Count':<8} {'Avg':<10} {'Max':<8} {'Min':<8}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "        for window_id in sorted(self.windows.keys()):\n",
    "            metrics = self.get_window_metrics(window_id)\n",
    "            if metrics:\n",
    "                window_str = metrics[\"window_start\"][11:19]\n",
    "                print(\n",
    "                    f\"{window_str:<20} {metrics['count']:<8} {metrics['avg']:<10.2f} \"\n",
    "                    f\"{metrics['max']:<8} {metrics['min']:<8}\"\n",
    "                )\n",
    "\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "\n",
    "print(\"[OK] RealTimeAnalytics class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the analytics processor\n",
    "def generate_analytics_events(analytics, num_events=100):\n",
    "    \"\"\"Generate events for analytics\"\"\"\n",
    "    base_time = datetime.now()\n",
    "\n",
    "    for i in range(num_events):\n",
    "        event = {\n",
    "            \"id\": i,\n",
    "            \"timestamp\": (base_time + timedelta(seconds=i * 0.5)).isoformat(),\n",
    "            \"value\": random.randint(1, 100),\n",
    "            \"sensor_id\": f\"sensor_{random.randint(1, 5)}\",\n",
    "        }\n",
    "\n",
    "        analytics.process_event(event)\n",
    "\n",
    "        # Print dashboard every 20 events\n",
    "        if (i + 1) % 20 == 0:\n",
    "            analytics.print_dashboard()\n",
    "            time.sleep(0.5)\n",
    "\n",
    "\n",
    "# Create and run analytics\n",
    "analytics = RealTimeAnalytics(window_size_seconds=10)\n",
    "\n",
    "print(\"[OK] Starting real-time analytics...\\n\")\n",
    "generate_analytics_events(analytics, num_events=100)\n",
    "\n",
    "# Final dashboard\n",
    "analytics.print_dashboard()\n",
    "\n",
    "print(\"\\n[SUCCESS] Real-time analytics complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Key Takeaways\n",
    "\n",
    "[OK] **Stream Processing**: Process data continuously as it arrives\n",
    "\n",
    "[OK] **Stateless vs Stateful**: Stateless operations are independent; stateful maintain state\n",
    "\n",
    "[OK] **Windowing**: Divide infinite streams into bounded computations\n",
    "\n",
    "[OK] **Window Types**: Tumbling (non-overlapping), Sliding (overlapping), Session (gap-based)\n",
    "\n",
    "[OK] **Event Time**: Use event timestamps for correctness\n",
    "\n",
    "[OK] **Watermarks**: Handle late data gracefully\n",
    "\n",
    "### Design Patterns\n",
    "\n",
    "**1. Filter-Map-Reduce:**\n",
    "```\n",
    "Stream → Filter → Map → Reduce → Output\n",
    "```\n",
    "\n",
    "**2. Windowed Aggregation:**\n",
    "```\n",
    "Stream → Assign to Windows → Aggregate → Output\n",
    "```\n",
    "\n",
    "**3. Event Time Processing:**\n",
    "```\n",
    "Stream → Extract Event Time → Window → Watermark → Output\n",
    "```\n",
    "\n",
    "### Production Considerations\n",
    "\n",
    "1. **Choose appropriate window size** based on latency requirements\n",
    "2. **Handle late data** with allowed lateness\n",
    "3. **Use event time** for correctness\n",
    "4. **Monitor watermark lag** to detect issues\n",
    "5. **Checkpoint state** for fault tolerance\n",
    "6. **Scale with partitions** for high throughput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Practice Exercises\n",
    "\n",
    "1. **Implement sliding window** aggregation (overlapping windows)\n",
    "2. **Create session window** processor (gap-based windows)\n",
    "3. **Build anomaly detector** that alerts when values exceed threshold\n",
    "4. **Implement join** of two streams (correlation)\n",
    "5. **Add watermark visualization** to see progression over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your practice code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Next Steps\n",
    "\n",
    "Congratulations on completing Module 03!\n",
    "\n",
    "### What You've Learned\n",
    "\n",
    "- [OK] Stream processing fundamentals\n",
    "- [OK] Stateless and stateful operations\n",
    "- [OK] Windowing techniques\n",
    "- [OK] Event time vs processing time\n",
    "- [OK] Late data handling with watermarks\n",
    "\n",
    "### Coming Up in Module 04: Apache Flink Basics\n",
    "\n",
    "You'll learn:\n",
    "- Apache Flink architecture\n",
    "- DataStream API\n",
    "- Flink operators and transformations\n",
    "- Connectors (Kafka source/sink)\n",
    "- Running Flink jobs\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Stream Processing Concepts](https://www.oreilly.com/library/view/streaming-systems/9781491983867/)\n",
    "- [Windowing in Stream Processing](https://www.confluent.io/blog/windowing-in-kafka-streams/)\n",
    "- [Event Time and Watermarks](https://nightlies.apache.org/flink/flink-docs-master/docs/concepts/time/)\n",
    "\n",
    "---\n",
    "\n",
    "**Ready for Apache Flink?** Open `04_apache_flink_basics.ipynb` to continue!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
