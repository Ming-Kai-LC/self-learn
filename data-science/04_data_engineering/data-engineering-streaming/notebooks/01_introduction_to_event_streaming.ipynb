{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 01: Introduction to Event Streaming\n",
    "\n",
    "**Estimated Time:** 60 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this module, you will:\n",
    "- Understand event-driven architecture principles\n",
    "- Learn Apache Kafka architecture and core concepts\n",
    "- Work with brokers, topics, and partitions\n",
    "- Master producer and consumer patterns\n",
    "- Understand consumer groups and parallel processing\n",
    "- Build a multi-producer, multi-consumer system\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Event-Driven Architecture (EDA)\n",
    "\n",
    "### What is an Event?\n",
    "\n",
    "An **event** is a record of something that happened in your system:\n",
    "- **State change**: User updated profile, order was shipped\n",
    "- **Action**: Button clicked, file uploaded, payment processed\n",
    "- **Measurement**: Temperature recorded, CPU usage captured\n",
    "\n",
    "### Anatomy of an Event\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"event_id\": \"evt_123456\",\n",
    "  \"event_type\": \"user.profile.updated\",\n",
    "  \"timestamp\": \"2024-01-15T10:30:00Z\",\n",
    "  \"user_id\": \"user_789\",\n",
    "  \"data\": {\n",
    "    \"field\": \"email\",\n",
    "    \"old_value\": \"old@example.com\",\n",
    "    \"new_value\": \"new@example.com\"\n",
    "  },\n",
    "  \"metadata\": {\n",
    "    \"source\": \"web-app\",\n",
    "    \"version\": \"1.0\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "### Event-Driven vs Request-Response\n",
    "\n",
    "**Traditional Request-Response:**\n",
    "```\n",
    "Client → [Request] → Server → [Process] → Response → Client\n",
    "         (waiting...)                     (blocked)\n",
    "\n",
    "- Synchronous\n",
    "- Tight coupling\n",
    "- Client waits for response\n",
    "```\n",
    "\n",
    "**Event-Driven:**\n",
    "```\n",
    "Producer → [Event] → Event Stream → [Consumer 1]\n",
    "                                  → [Consumer 2]\n",
    "                                  → [Consumer 3]\n",
    "\n",
    "- Asynchronous\n",
    "- Loose coupling\n",
    "- Fire and forget\n",
    "```\n",
    "\n",
    "### Benefits of Event-Driven Architecture\n",
    "\n",
    "1. **Decoupling**: Producers don't know about consumers\n",
    "2. **Scalability**: Add consumers independently\n",
    "3. **Flexibility**: New consumers can be added without changing producers\n",
    "4. **Resilience**: If a consumer fails, events are still stored\n",
    "5. **Auditability**: Complete event history is preserved\n",
    "\n",
    "### Real-World Use Cases\n",
    "\n",
    "| Industry | Use Case | Events |\n",
    "|----------|----------|--------|\n",
    "| E-commerce | Order processing | OrderPlaced, PaymentProcessed, OrderShipped |\n",
    "| Finance | Fraud detection | TransactionInitiated, AccountAccessed, LargeWithdrawal |\n",
    "| IoT | Smart home | TemperatureChanged, MotionDetected, DoorOpened |\n",
    "| Social Media | Activity feed | PostCreated, CommentAdded, UserFollowed |\n",
    "| Gaming | Player actions | PlayerMoved, ItemPurchased, LevelCompleted |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Apache Kafka Architecture\n",
    "\n",
    "### What is Kafka?\n",
    "\n",
    "Apache Kafka is a **distributed event streaming platform** that:\n",
    "- Publishes and subscribes to streams of events\n",
    "- Stores events durably and reliably\n",
    "- Processes events in real-time\n",
    "\n",
    "### Kafka Architecture Components\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│                    Kafka Cluster                        │\n",
    "│                                                         │\n",
    "│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐    │\n",
    "│  │  Broker 1   │  │  Broker 2   │  │  Broker 3   │    │\n",
    "│  │             │  │             │  │             │    │\n",
    "│  │ Topic: user │  │ Topic: user │  │ Topic: user │    │\n",
    "│  │ Partition 0 │  │ Partition 1 │  │ Partition 2 │    │\n",
    "│  └─────────────┘  └─────────────┘  └─────────────┘    │\n",
    "│                                                         │\n",
    "│  ┌──────────────────────────────────────────────┐     │\n",
    "│  │           Zookeeper (Coordination)           │     │\n",
    "│  └──────────────────────────────────────────────┘     │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "     ↑                                           ↓\n",
    " Producers                                  Consumers\n",
    "```\n",
    "\n",
    "### Core Components\n",
    "\n",
    "1. **Broker**: A Kafka server that stores and serves events\n",
    "2. **Topic**: A category/feed of events (like a table in a database)\n",
    "3. **Partition**: Subdivision of a topic for parallel processing\n",
    "4. **Producer**: Application that writes events to topics\n",
    "5. **Consumer**: Application that reads events from topics\n",
    "6. **Zookeeper**: Manages cluster coordination (being replaced by KRaft)\n",
    "\n",
    "### Topics and Partitions\n",
    "\n",
    "**Topic Structure:**\n",
    "```\n",
    "Topic: \"user-events\" (3 partitions)\n",
    "\n",
    "Partition 0:  [evt1] → [evt4] → [evt7] → [evt10] →\n",
    "Partition 1:  [evt2] → [evt5] → [evt8] → [evt11] →\n",
    "Partition 2:  [evt3] → [evt6] → [evt9] → [evt12] →\n",
    "              ↑offset 0  offset 1  offset 2\n",
    "```\n",
    "\n",
    "**Key Concepts:**\n",
    "- Events in a partition are **ordered**\n",
    "- Each event has an **offset** (position in partition)\n",
    "- Partitions enable **parallelism**\n",
    "- Events with the same **key** go to the same partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore Kafka cluster metadata\n",
    "from confluent_kafka.admin import AdminClient, NewTopic\n",
    "import json\n",
    "\n",
    "# Connect to Kafka\n",
    "admin_client = AdminClient({\"bootstrap.servers\": \"localhost:9092\"})\n",
    "\n",
    "# Get cluster metadata\n",
    "metadata = admin_client.list_topics(timeout=5)\n",
    "\n",
    "print(\"[OK] Kafka Cluster Information\\n\")\n",
    "print(f\"Cluster ID: {metadata.cluster_id}\")\n",
    "print(f\"Controller Broker: {metadata.controller_id}\")\n",
    "print(f\"\\nBrokers in cluster:\")\n",
    "\n",
    "for broker_id, broker_metadata in metadata.brokers.items():\n",
    "    print(f\"  - Broker {broker_id}: {broker_metadata.host}:{broker_metadata.port}\")\n",
    "\n",
    "print(f\"\\nExisting Topics ({len(metadata.topics)}):\")\n",
    "for topic_name in list(metadata.topics.keys())[:10]:  # Show first 10\n",
    "    topic = metadata.topics[topic_name]\n",
    "    print(f\"  - {topic_name}: {len(topic.partitions)} partitions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Creating and Managing Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new topic with specific configuration\n",
    "from confluent_kafka.admin import AdminClient, NewTopic, KafkaException\n",
    "\n",
    "TOPIC_NAME = \"user-events\"\n",
    "NUM_PARTITIONS = 3\n",
    "REPLICATION_FACTOR = 1  # For local dev; use 3 in production\n",
    "\n",
    "# Define new topic\n",
    "new_topic = NewTopic(\n",
    "    topic=TOPIC_NAME,\n",
    "    num_partitions=NUM_PARTITIONS,\n",
    "    replication_factor=REPLICATION_FACTOR,\n",
    "    config={\n",
    "        \"retention.ms\": \"604800000\",  # 7 days in milliseconds\n",
    "        \"compression.type\": \"gzip\",\n",
    "        \"max.message.bytes\": \"1048576\",  # 1 MB\n",
    "    },\n",
    ")\n",
    "\n",
    "# Create topic\n",
    "admin_client = AdminClient({\"bootstrap.servers\": \"localhost:9092\"})\n",
    "\n",
    "try:\n",
    "    # Returns a dict of futures\n",
    "    futures = admin_client.create_topics([new_topic])\n",
    "\n",
    "    # Wait for operation to complete\n",
    "    for topic, future in futures.items():\n",
    "        try:\n",
    "            future.result()  # Block until topic is created\n",
    "            print(f\"[OK] Topic '{topic}' created successfully\")\n",
    "            print(f\"     Partitions: {NUM_PARTITIONS}\")\n",
    "            print(f\"     Replication: {REPLICATION_FACTOR}\")\n",
    "        except KafkaException as e:\n",
    "            if \"TOPIC_ALREADY_EXISTS\" in str(e):\n",
    "                print(f\"[WARNING] Topic '{topic}' already exists\")\n",
    "            else:\n",
    "                print(f\"[FAIL] Failed to create topic '{topic}': {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"[FAIL] Error creating topic: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect topic configuration\n",
    "from confluent_kafka.admin import ConfigResource\n",
    "\n",
    "# Get topic configuration\n",
    "resource = ConfigResource(\"topic\", TOPIC_NAME)\n",
    "futures = admin_client.describe_configs([resource])\n",
    "\n",
    "for res, future in futures.items():\n",
    "    try:\n",
    "        configs = future.result()\n",
    "        print(f\"[DATA] Configuration for topic '{res.name}':\\n\")\n",
    "\n",
    "        # Show key configurations\n",
    "        important_configs = [\n",
    "            \"retention.ms\",\n",
    "            \"compression.type\",\n",
    "            \"max.message.bytes\",\n",
    "            \"cleanup.policy\",\n",
    "            \"segment.ms\",\n",
    "        ]\n",
    "\n",
    "        for config_name in important_configs:\n",
    "            if config_name in configs:\n",
    "                config = configs[config_name]\n",
    "                print(f\"  {config_name}: {config.value}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[FAIL] Could not get config: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Producers: Writing Events to Kafka\n",
    "\n",
    "### Producer Basics\n",
    "\n",
    "**How Producers Work:**\n",
    "```\n",
    "Producer → [Serialization] → [Partitioner] → [Buffer] → Broker\n",
    "              (to bytes)       (which partition?)  (batch)\n",
    "```\n",
    "\n",
    "**Key Producer Concepts:**\n",
    "1. **Serialization**: Convert data to bytes (JSON, Avro, Protobuf)\n",
    "2. **Partitioning**: Decide which partition receives the event\n",
    "3. **Batching**: Group events for efficiency\n",
    "4. **Acknowledgments**: Confirm delivery (acks=0, 1, all)\n",
    "5. **Idempotence**: Prevent duplicates (enable.idempotence=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Producer Example\n",
    "from confluent_kafka import Producer\n",
    "from datetime import datetime\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Producer configuration\n",
    "producer_config = {\n",
    "    \"bootstrap.servers\": \"localhost:9092\",\n",
    "    \"client.id\": \"user-events-producer\",\n",
    "    \"acks\": \"all\",  # Wait for all replicas to acknowledge\n",
    "    \"retries\": 3,\n",
    "    \"enable.idempotence\": True,  # Prevent duplicates\n",
    "}\n",
    "\n",
    "producer = Producer(producer_config)\n",
    "\n",
    "\n",
    "# Delivery callback\n",
    "def delivery_report(err, msg):\n",
    "    \"\"\"Callback for message delivery reports\"\"\"\n",
    "    if err:\n",
    "        print(f\"[FAIL] Delivery failed: {err}\")\n",
    "    else:\n",
    "        print(\n",
    "            f\"[OK] Message delivered to {msg.topic()} [partition {msg.partition()}] at offset {msg.offset()}\"\n",
    "        )\n",
    "\n",
    "\n",
    "# Simulate user events\n",
    "user_actions = [\"login\", \"logout\", \"profile_update\", \"purchase\", \"view_page\"]\n",
    "user_ids = [f\"user_{i}\" for i in range(1, 6)]\n",
    "\n",
    "print(\"Producing user events...\\n\")\n",
    "\n",
    "for i in range(10):\n",
    "    event = {\n",
    "        \"event_id\": f\"evt_{i}\",\n",
    "        \"event_type\": f\"user.{random.choice(user_actions)}\",\n",
    "        \"user_id\": random.choice(user_ids),\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"data\": {\n",
    "            \"session_id\": f\"session_{random.randint(1000, 9999)}\",\n",
    "            \"ip_address\": f\"192.168.1.{random.randint(1, 255)}\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Produce with key (ensures same user goes to same partition)\n",
    "    producer.produce(\n",
    "        topic=TOPIC_NAME,\n",
    "        key=event[\"user_id\"],  # Partition by user_id\n",
    "        value=json.dumps(event),\n",
    "        callback=delivery_report,\n",
    "    )\n",
    "\n",
    "    # Trigger delivery callbacks (non-blocking)\n",
    "    producer.poll(0)\n",
    "\n",
    "# Wait for all messages to be delivered\n",
    "print(\"\\nFlushing remaining messages...\")\n",
    "producer.flush()\n",
    "print(\"[SUCCESS] All events produced!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Partitioning\n",
    "\n",
    "**Partitioning Strategies:**\n",
    "\n",
    "1. **Key-based (default)**:\n",
    "   ```python\n",
    "   # Events with same key → same partition\n",
    "   producer.produce(topic, key='user_123', value=event)\n",
    "   # Ensures ordering for each user\n",
    "   ```\n",
    "\n",
    "2. **Round-robin (no key)**:\n",
    "   ```python\n",
    "   # Events distributed evenly across partitions\n",
    "   producer.produce(topic, value=event)\n",
    "   # Good for throughput, no ordering guarantees\n",
    "   ```\n",
    "\n",
    "3. **Custom partitioner**:\n",
    "   ```python\n",
    "   # Implement your own partitioning logic\n",
    "   def custom_partitioner(key, all_partitions, available_partitions):\n",
    "       return hash(key) % len(all_partitions)\n",
    "   ```\n",
    "\n",
    "**Why Partitioning Matters:**\n",
    "- Enables **parallel processing**\n",
    "- Provides **ordering guarantees** within a partition\n",
    "- Allows **consumer groups** to scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate partition distribution\n",
    "from collections import defaultdict\n",
    "\n",
    "# Track which partition each user went to\n",
    "user_partitions = defaultdict(set)\n",
    "\n",
    "\n",
    "def partition_tracker(err, msg):\n",
    "    \"\"\"Track partition assignments\"\"\"\n",
    "    if not err:\n",
    "        user_id = msg.key().decode(\"utf-8\")\n",
    "        partition = msg.partition()\n",
    "        user_partitions[user_id].add(partition)\n",
    "\n",
    "\n",
    "# Produce events with partition tracking\n",
    "producer = Producer(producer_config)\n",
    "\n",
    "for i in range(30):\n",
    "    user_id = random.choice(user_ids)\n",
    "    event = {\"event_id\": f\"evt_{i}\", \"user_id\": user_id, \"action\": random.choice(user_actions)}\n",
    "\n",
    "    producer.produce(\n",
    "        topic=TOPIC_NAME, key=user_id, value=json.dumps(event), callback=partition_tracker\n",
    "    )\n",
    "    producer.poll(0)\n",
    "\n",
    "producer.flush()\n",
    "\n",
    "# Show partition distribution\n",
    "print(\"[DATA] Partition Distribution by User:\\n\")\n",
    "for user_id, partitions in sorted(user_partitions.items()):\n",
    "    print(f\"  {user_id}: Partition {list(partitions)[0]}\")\n",
    "\n",
    "print(\"\\n[OK] Notice: Each user consistently goes to the same partition!\")\n",
    "print(\"     This ensures ordering for events from the same user.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Consumers: Reading Events from Kafka\n",
    "\n",
    "### Consumer Basics\n",
    "\n",
    "**How Consumers Work:**\n",
    "```\n",
    "Broker → [Fetch] → Consumer → [Deserialize] → [Process] → [Commit Offset]\n",
    "          (poll)              (bytes to data)             (mark as read)\n",
    "```\n",
    "\n",
    "**Key Consumer Concepts:**\n",
    "1. **Polling**: Fetch events from Kafka\n",
    "2. **Offset**: Position in partition (which events have been read)\n",
    "3. **Offset Commit**: Save progress (auto or manual)\n",
    "4. **Consumer Group**: Multiple consumers working together\n",
    "5. **Rebalancing**: Reassign partitions when consumers join/leave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Consumer Example\n",
    "from confluent_kafka import Consumer, KafkaException\n",
    "\n",
    "# Consumer configuration\n",
    "consumer_config = {\n",
    "    \"bootstrap.servers\": \"localhost:9092\",\n",
    "    \"group.id\": \"user-events-consumer-group\",\n",
    "    \"auto.offset.reset\": \"earliest\",  # Start from beginning\n",
    "    \"enable.auto.commit\": True,\n",
    "    \"auto.commit.interval.ms\": 5000,\n",
    "}\n",
    "\n",
    "consumer = Consumer(consumer_config)\n",
    "\n",
    "# Subscribe to topic\n",
    "consumer.subscribe([TOPIC_NAME])\n",
    "\n",
    "print(f\"[OK] Consumer subscribed to '{TOPIC_NAME}'\")\n",
    "print(f\"     Consumer Group: {consumer_config['group.id']}\\n\")\n",
    "print(\"Reading events (will stop after 15 messages)...\\n\")\n",
    "\n",
    "messages_read = 0\n",
    "max_messages = 15\n",
    "\n",
    "try:\n",
    "    while messages_read < max_messages:\n",
    "        # Poll for messages\n",
    "        msg = consumer.poll(timeout=2.0)\n",
    "\n",
    "        if msg is None:\n",
    "            print(\"[WARNING] No more messages available\")\n",
    "            break\n",
    "\n",
    "        if msg.error():\n",
    "            print(f\"[FAIL] Consumer error: {msg.error()}\")\n",
    "            continue\n",
    "\n",
    "        # Successfully received a message\n",
    "        key = msg.key().decode(\"utf-8\") if msg.key() else None\n",
    "        value = json.loads(msg.value().decode(\"utf-8\"))\n",
    "\n",
    "        print(f\"[{messages_read + 1}] Event: {value['event_type']}\")\n",
    "        print(f\"    User: {value['user_id']}\")\n",
    "        print(f\"    Partition: {msg.partition()}, Offset: {msg.offset()}\")\n",
    "        print()\n",
    "\n",
    "        messages_read += 1\n",
    "\n",
    "finally:\n",
    "    consumer.close()\n",
    "    print(f\"[SUCCESS] Read {messages_read} events\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Consumer Groups and Parallel Processing\n",
    "\n",
    "### What is a Consumer Group?\n",
    "\n",
    "A **consumer group** is a set of consumers that cooperate to consume events from a topic.\n",
    "\n",
    "**Single Consumer Group:**\n",
    "```\n",
    "Topic: user-events (3 partitions)\n",
    "\n",
    "Partition 0  ────→  Consumer A  ┐\n",
    "Partition 1  ────→  Consumer B  ├─ Group: analytics\n",
    "Partition 2  ────→  Consumer C  ┘\n",
    "\n",
    "- Each partition assigned to ONE consumer\n",
    "- Consumers share the workload\n",
    "- Parallel processing\n",
    "```\n",
    "\n",
    "**Multiple Consumer Groups:**\n",
    "```\n",
    "Topic: user-events\n",
    "\n",
    "Partition 0  ──┬──→  Consumer A (Group: analytics)\n",
    "               └──→  Consumer X (Group: fraud-detection)\n",
    "\n",
    "Partition 1  ──┬──→  Consumer B (Group: analytics)\n",
    "               └──→  Consumer Y (Group: fraud-detection)\n",
    "\n",
    "- Each group gets ALL events independently\n",
    "- Different groups can process the same events\n",
    "```\n",
    "\n",
    "### Consumer Group Rules\n",
    "\n",
    "1. **One partition per consumer** (in the same group)\n",
    "2. **Multiple consumers per group** (up to number of partitions)\n",
    "3. **Different groups are independent**\n",
    "4. **Rebalancing happens** when consumers join/leave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate multiple consumers in the same group\n",
    "import threading\n",
    "import time\n",
    "\n",
    "\n",
    "def consume_events(consumer_id, group_id, topic, max_messages=10):\n",
    "    \"\"\"Consumer function for threading\"\"\"\n",
    "    config = {\n",
    "        \"bootstrap.servers\": \"localhost:9092\",\n",
    "        \"group.id\": group_id,\n",
    "        \"auto.offset.reset\": \"earliest\",\n",
    "        \"enable.auto.commit\": True,\n",
    "    }\n",
    "\n",
    "    consumer = Consumer(config)\n",
    "    consumer.subscribe([topic])\n",
    "\n",
    "    messages_read = 0\n",
    "    partitions_read = set()\n",
    "\n",
    "    print(f\"[{consumer_id}] Started consuming from group '{group_id}'\")\n",
    "\n",
    "    try:\n",
    "        while messages_read < max_messages:\n",
    "            msg = consumer.poll(timeout=2.0)\n",
    "\n",
    "            if msg is None:\n",
    "                break\n",
    "\n",
    "            if msg.error():\n",
    "                continue\n",
    "\n",
    "            partitions_read.add(msg.partition())\n",
    "\n",
    "            value = json.loads(msg.value().decode(\"utf-8\"))\n",
    "            print(\n",
    "                f\"[{consumer_id}] Partition {msg.partition()}, Offset {msg.offset()}: {value['event_type']}\"\n",
    "            )\n",
    "\n",
    "            messages_read += 1\n",
    "            time.sleep(0.1)  # Simulate processing\n",
    "\n",
    "    finally:\n",
    "        consumer.close()\n",
    "        print(\n",
    "            f\"[{consumer_id}] Finished. Read {messages_read} events from partitions {sorted(partitions_read)}\"\n",
    "        )\n",
    "\n",
    "\n",
    "# Create 3 consumers in the same group\n",
    "print(\"Starting 3 consumers in the same group...\\n\")\n",
    "\n",
    "threads = []\n",
    "for i in range(3):\n",
    "    consumer_id = f\"Consumer-{i+1}\"\n",
    "    thread = threading.Thread(\n",
    "        target=consume_events, args=(consumer_id, \"parallel-consumer-group\", TOPIC_NAME, 10)\n",
    "    )\n",
    "    threads.append(thread)\n",
    "    thread.start()\n",
    "\n",
    "# Wait for all consumers to finish\n",
    "for thread in threads:\n",
    "    thread.join()\n",
    "\n",
    "print(\"\\n[SUCCESS] All consumers finished!\")\n",
    "print(\"\\n[OK] Notice: Each consumer read from different partitions!\")\n",
    "print(\"     This is how Kafka achieves parallel processing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Offset Management\n",
    "\n",
    "### What are Offsets?\n",
    "\n",
    "**Offsets** track which events have been read:\n",
    "```\n",
    "Partition 0: [evt0][evt1][evt2][evt3][evt4][evt5]\n",
    "              ↑     ↑     ↑     ↑\n",
    "            offset offset offset offset\n",
    "              0      1      2      3\n",
    "\n",
    "Consumer commits offset 3 → Next read starts at offset 3\n",
    "```\n",
    "\n",
    "### Offset Commit Strategies\n",
    "\n",
    "1. **Auto-commit (default)**:\n",
    "   - Kafka commits offsets automatically\n",
    "   - Simple but may lose/duplicate events on failure\n",
    "   \n",
    "2. **Manual commit**:\n",
    "   - You control when offsets are committed\n",
    "   - Better control, at-least-once or exactly-once\n",
    "\n",
    "3. **Manual commit after processing**:\n",
    "   - Commit only after successful processing\n",
    "   - Prevents data loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual offset commit example\n",
    "consumer_config_manual = {\n",
    "    \"bootstrap.servers\": \"localhost:9092\",\n",
    "    \"group.id\": \"manual-commit-group\",\n",
    "    \"auto.offset.reset\": \"earliest\",\n",
    "    \"enable.auto.commit\": False,  # Disable auto-commit\n",
    "}\n",
    "\n",
    "consumer = Consumer(consumer_config_manual)\n",
    "consumer.subscribe([TOPIC_NAME])\n",
    "\n",
    "print(\"[OK] Consumer with manual offset commit\\n\")\n",
    "\n",
    "messages_processed = 0\n",
    "max_messages = 10\n",
    "\n",
    "try:\n",
    "    while messages_processed < max_messages:\n",
    "        msg = consumer.poll(timeout=2.0)\n",
    "\n",
    "        if msg is None:\n",
    "            break\n",
    "\n",
    "        if msg.error():\n",
    "            continue\n",
    "\n",
    "        # Process the message\n",
    "        value = json.loads(msg.value().decode(\"utf-8\"))\n",
    "        print(f\"[{messages_processed + 1}] Processing: {value['event_type']}\")\n",
    "\n",
    "        # Simulate processing that might fail\n",
    "        try:\n",
    "            # Your processing logic here\n",
    "            time.sleep(0.05)\n",
    "\n",
    "            # Only commit if processing succeeds\n",
    "            consumer.commit(msg)\n",
    "            print(f\"     [OK] Committed offset {msg.offset()} on partition {msg.partition()}\")\n",
    "\n",
    "            messages_processed += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"     [FAIL] Processing failed: {e}\")\n",
    "            print(f\"     Will retry this message on next poll\")\n",
    "            break  # Don't commit, will re-read this message\n",
    "\n",
    "finally:\n",
    "    consumer.close()\n",
    "    print(f\"\\n[SUCCESS] Processed {messages_processed} events with manual commits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Mini-Project: Event-Driven User Activity System\n",
    "\n",
    "Let's build a complete system with:\n",
    "- Multiple producers generating different event types\n",
    "- Multiple consumers in different groups\n",
    "- Real-time analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a topic for the mini-project\n",
    "PROJECT_TOPIC = \"user-activity-stream\"\n",
    "\n",
    "new_topic = NewTopic(topic=PROJECT_TOPIC, num_partitions=3, replication_factor=1)\n",
    "\n",
    "try:\n",
    "    futures = admin_client.create_topics([new_topic])\n",
    "    for topic, future in futures.items():\n",
    "        try:\n",
    "            future.result()\n",
    "            print(f\"[OK] Created topic '{topic}' for mini-project\")\n",
    "        except KafkaException as e:\n",
    "            if \"TOPIC_ALREADY_EXISTS\" in str(e):\n",
    "                print(f\"[OK] Topic '{topic}' already exists\")\n",
    "except Exception as e:\n",
    "    print(f\"[FAIL] Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Producer 1: Web application events\n",
    "def web_event_producer(num_events=20):\n",
    "    \"\"\"Simulate web application events\"\"\"\n",
    "    producer = Producer({\"bootstrap.servers\": \"localhost:9092\"})\n",
    "\n",
    "    actions = [\"page_view\", \"button_click\", \"form_submit\", \"search\"]\n",
    "    pages = [\"/home\", \"/products\", \"/cart\", \"/checkout\", \"/profile\"]\n",
    "\n",
    "    for i in range(num_events):\n",
    "        user_id = f\"user_{random.randint(1, 10)}\"\n",
    "        event = {\n",
    "            \"source\": \"web-app\",\n",
    "            \"event_id\": f\"web_{i}\",\n",
    "            \"event_type\": random.choice(actions),\n",
    "            \"user_id\": user_id,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"data\": {\"page\": random.choice(pages), \"session_duration\": random.randint(10, 300)},\n",
    "        }\n",
    "\n",
    "        producer.produce(topic=PROJECT_TOPIC, key=user_id, value=json.dumps(event))\n",
    "        producer.poll(0)\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    producer.flush()\n",
    "    print(f\"[WEB] Produced {num_events} web events\")\n",
    "\n",
    "\n",
    "# Producer 2: Mobile app events\n",
    "def mobile_event_producer(num_events=20):\n",
    "    \"\"\"Simulate mobile app events\"\"\"\n",
    "    producer = Producer({\"bootstrap.servers\": \"localhost:9092\"})\n",
    "\n",
    "    actions = [\"app_open\", \"app_close\", \"notification_tap\", \"in_app_purchase\"]\n",
    "\n",
    "    for i in range(num_events):\n",
    "        user_id = f\"user_{random.randint(1, 10)}\"\n",
    "        event = {\n",
    "            \"source\": \"mobile-app\",\n",
    "            \"event_id\": f\"mobile_{i}\",\n",
    "            \"event_type\": random.choice(actions),\n",
    "            \"user_id\": user_id,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"data\": {\"device\": random.choice([\"iOS\", \"Android\"]), \"app_version\": \"2.1.0\"},\n",
    "        }\n",
    "\n",
    "        producer.produce(topic=PROJECT_TOPIC, key=user_id, value=json.dumps(event))\n",
    "        producer.poll(0)\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    producer.flush()\n",
    "    print(f\"[MOBILE] Produced {num_events} mobile events\")\n",
    "\n",
    "\n",
    "# Run both producers in parallel\n",
    "print(\"Starting event producers...\\n\")\n",
    "\n",
    "web_thread = threading.Thread(target=web_event_producer, args=(20,))\n",
    "mobile_thread = threading.Thread(target=mobile_event_producer, args=(20,))\n",
    "\n",
    "web_thread.start()\n",
    "mobile_thread.start()\n",
    "\n",
    "web_thread.join()\n",
    "mobile_thread.join()\n",
    "\n",
    "print(\"\\n[SUCCESS] All producers finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consumer 1: Analytics - Count events by type\n",
    "def analytics_consumer(duration_seconds=10):\n",
    "    \"\"\"Analyze event patterns\"\"\"\n",
    "    consumer = Consumer(\n",
    "        {\n",
    "            \"bootstrap.servers\": \"localhost:9092\",\n",
    "            \"group.id\": \"analytics-group\",\n",
    "            \"auto.offset.reset\": \"earliest\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    consumer.subscribe([PROJECT_TOPIC])\n",
    "\n",
    "    event_counts = defaultdict(int)\n",
    "    source_counts = defaultdict(int)\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(\"[ANALYTICS] Starting event analysis...\")\n",
    "\n",
    "    try:\n",
    "        while time.time() - start_time < duration_seconds:\n",
    "            msg = consumer.poll(timeout=1.0)\n",
    "\n",
    "            if msg is None:\n",
    "                continue\n",
    "\n",
    "            if msg.error():\n",
    "                continue\n",
    "\n",
    "            event = json.loads(msg.value().decode(\"utf-8\"))\n",
    "            event_counts[event[\"event_type\"]] += 1\n",
    "            source_counts[event[\"source\"]] += 1\n",
    "\n",
    "    finally:\n",
    "        consumer.close()\n",
    "\n",
    "        print(\"\\n[ANALYTICS] Event Analysis Results:\")\n",
    "        print(\"\\nBy Event Type:\")\n",
    "        for event_type, count in sorted(event_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "            print(f\"  {event_type}: {count}\")\n",
    "\n",
    "        print(\"\\nBy Source:\")\n",
    "        for source, count in source_counts.items():\n",
    "            print(f\"  {source}: {count}\")\n",
    "\n",
    "\n",
    "# Consumer 2: Monitoring - Detect unusual activity\n",
    "def monitoring_consumer(duration_seconds=10):\n",
    "    \"\"\"Monitor for unusual patterns\"\"\"\n",
    "    consumer = Consumer(\n",
    "        {\n",
    "            \"bootstrap.servers\": \"localhost:9092\",\n",
    "            \"group.id\": \"monitoring-group\",\n",
    "            \"auto.offset.reset\": \"earliest\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    consumer.subscribe([PROJECT_TOPIC])\n",
    "\n",
    "    user_activity = defaultdict(list)\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(\"[MONITORING] Starting activity monitoring...\")\n",
    "\n",
    "    try:\n",
    "        while time.time() - start_time < duration_seconds:\n",
    "            msg = consumer.poll(timeout=1.0)\n",
    "\n",
    "            if msg is None:\n",
    "                continue\n",
    "\n",
    "            if msg.error():\n",
    "                continue\n",
    "\n",
    "            event = json.loads(msg.value().decode(\"utf-8\"))\n",
    "            user_id = event[\"user_id\"]\n",
    "            user_activity[user_id].append(event[\"event_type\"])\n",
    "\n",
    "            # Alert if user has > 5 events\n",
    "            if len(user_activity[user_id]) == 6:\n",
    "                print(f\"  [ALERT] High activity detected for {user_id}\")\n",
    "\n",
    "    finally:\n",
    "        consumer.close()\n",
    "\n",
    "        print(\"\\n[MONITORING] Activity Report:\")\n",
    "        for user_id, events in sorted(user_activity.items(), key=lambda x: len(x[1]), reverse=True)[\n",
    "            :5\n",
    "        ]:\n",
    "            print(f\"  {user_id}: {len(events)} events\")\n",
    "\n",
    "\n",
    "# Run both consumers in parallel\n",
    "print(\"\\nStarting consumers...\\n\")\n",
    "\n",
    "analytics_thread = threading.Thread(target=analytics_consumer, args=(10,))\n",
    "monitoring_thread = threading.Thread(target=monitoring_consumer, args=(10,))\n",
    "\n",
    "analytics_thread.start()\n",
    "monitoring_thread.start()\n",
    "\n",
    "analytics_thread.join()\n",
    "monitoring_thread.join()\n",
    "\n",
    "print(\"\\n[SUCCESS] Mini-project complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Key Takeaways\n",
    "\n",
    "[OK] **Event-Driven Architecture**: Decoupled, scalable, flexible approach to system design\n",
    "\n",
    "[OK] **Kafka Components**: Brokers, topics, partitions, producers, consumers\n",
    "\n",
    "[OK] **Topics and Partitions**: Enable parallelism and ordering guarantees\n",
    "\n",
    "[OK] **Producers**: Write events with keys for partitioning, support batching and idempotence\n",
    "\n",
    "[OK] **Consumers**: Read events, manage offsets, work in groups for parallel processing\n",
    "\n",
    "[OK] **Consumer Groups**: Enable scalability and independent processing\n",
    "\n",
    "### Important Patterns\n",
    "\n",
    "1. **Partitioning by Key**:\n",
    "   - Same key → same partition → ordering guarantee\n",
    "   - Use user_id, session_id, or correlation_id as keys\n",
    "\n",
    "2. **Consumer Groups**:\n",
    "   - One group per application\n",
    "   - Multiple consumers per group for parallelism\n",
    "   - Different groups for different use cases\n",
    "\n",
    "3. **Offset Management**:\n",
    "   - Auto-commit for simple use cases\n",
    "   - Manual commit after processing for reliability\n",
    "   - Store offsets externally for exactly-once processing\n",
    "\n",
    "4. **Error Handling**:\n",
    "   - Retry on transient errors\n",
    "   - Dead letter queues for poison messages\n",
    "   - Monitoring and alerting\n",
    "\n",
    "### Production Best Practices\n",
    "\n",
    "1. **Set replication factor ≥ 3** for fault tolerance\n",
    "2. **Monitor consumer lag** to detect processing issues\n",
    "3. **Use Schema Registry** for data governance (covered in Module 03)\n",
    "4. **Enable compression** (gzip, snappy, lz4)\n",
    "5. **Configure retention** based on your needs\n",
    "6. **Use monitoring tools** (Kafka UI, Prometheus, Grafana)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Practice Exercises\n",
    "\n",
    "Try these exercises to reinforce your learning:\n",
    "\n",
    "1. **Create a topic** with 5 partitions and produce 100 events\n",
    "2. **Run 5 consumers** in the same group and observe partition assignment\n",
    "3. **Implement manual offset commit** with error handling\n",
    "4. **Create two consumer groups** reading from the same topic\n",
    "5. **Monitor partition distribution** by analyzing consumer assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your practice code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Next Steps\n",
    "\n",
    "Congratulations on completing Module 01!\n",
    "\n",
    "### What You've Learned\n",
    "\n",
    "- [OK] Event-driven architecture fundamentals\n",
    "- [OK] Apache Kafka core concepts\n",
    "- [OK] Producer and consumer patterns\n",
    "- [OK] Consumer groups and parallelism\n",
    "- [OK] Offset management strategies\n",
    "\n",
    "### Coming Up in Module 02: Kafka Deep Dive\n",
    "\n",
    "You'll learn:\n",
    "- Kafka internals: log structure, segments, indexes\n",
    "- Replication and fault tolerance\n",
    "- Performance tuning (batching, compression, linger.ms)\n",
    "- Monitoring and metrics\n",
    "- Troubleshooting common issues\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Kafka Documentation](https://kafka.apache.org/documentation/)\n",
    "- [Confluent Kafka Python](https://docs.confluent.io/kafka-clients/python/current/overview.html)\n",
    "- [Event-Driven Architecture](https://martinfowler.com/articles/201701-event-driven.html)\n",
    "- [Kafka: The Definitive Guide](https://www.confluent.io/resources/kafka-the-definitive-guide/)\n",
    "\n",
    "---\n",
    "\n",
    "**Ready to dive deeper?** Open `02_kafka_deep_dive.ipynb` to continue!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
