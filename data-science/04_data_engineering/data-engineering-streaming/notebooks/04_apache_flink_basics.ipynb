{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 04: Apache Flink Basics\n",
    "\n",
    "**Estimated Time:** 90 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this module, you will:\n",
    "- Understand Apache Flink architecture and components\n",
    "- Work with the PyFlink DataStream API\n",
    "- Implement stream transformations and operators\n",
    "- Connect Flink to Kafka (source and sink)\n",
    "- Build and run your first Flink streaming job\n",
    "- Monitor Flink jobs via the web dashboard\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is Apache Flink?\n",
    "\n",
    "### Overview\n",
    "\n",
    "Apache Flink is a **distributed stream processing framework** for:\n",
    "- Processing unbounded (streaming) and bounded (batch) data\n",
    "- Stateful computations with exactly-once semantics\n",
    "- Event-time processing with watermarks\n",
    "- Low-latency, high-throughput data processing\n",
    "\n",
    "### Flink vs Other Systems\n",
    "\n",
    "| Feature | Flink | Spark Streaming | Kafka Streams |\n",
    "|---------|-------|-----------------|---------------|\n",
    "| Processing Model | True streaming | Micro-batch | True streaming |\n",
    "| Latency | Milliseconds | Seconds | Milliseconds |\n",
    "| State Management | Advanced | Basic | Good |\n",
    "| Exactly-Once | Yes | Yes | Yes |\n",
    "| Deployment | Standalone cluster | Spark cluster | Embedded library |\n",
    "| Windowing | Advanced | Good | Good |\n",
    "\n",
    "### Flink Architecture\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│                   Flink Cluster                         │\n",
    "│                                                         │\n",
    "│  ┌──────────────────┐        ┌──────────────────┐      │\n",
    "│  │  JobManager      │        │  TaskManager     │      │\n",
    "│  │  (Master)        │        │  (Worker)        │      │\n",
    "│  │                  │        │                  │      │\n",
    "│  │  - Job scheduling│───────→│  - Execute tasks │      │\n",
    "│  │  - Checkpointing │        │  - Manage state  │      │\n",
    "│  │  - Coordination  │        │  - Shuffle data  │      │\n",
    "│  └──────────────────┘        └──────────────────┘      │\n",
    "│                                                         │\n",
    "│                              ┌──────────────────┐      │\n",
    "│                              │  TaskManager     │      │\n",
    "│                              │  (Worker)        │      │\n",
    "│                              └──────────────────┘      │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "         ↑                                        ↓\n",
    "    Input Sources                            Output Sinks\n",
    "    (Kafka, files)                          (Kafka, DB)\n",
    "```\n",
    "\n",
    "### Key Components\n",
    "\n",
    "1. **JobManager**: Master node that coordinates job execution\n",
    "2. **TaskManager**: Worker nodes that execute tasks and store state\n",
    "3. **Job**: User-defined streaming application\n",
    "4. **Task**: Unit of execution (parallel operator instance)\n",
    "5. **Checkpoint**: Snapshot of state for fault tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Import PyFlink libraries\n",
    "try:\n",
    "    from pyflink.datastream import StreamExecutionEnvironment\n",
    "    from pyflink.table import StreamTableEnvironment, EnvironmentSettings\n",
    "    from pyflink.datastream.connectors.kafka import (\n",
    "        KafkaSource,\n",
    "        KafkaOffsetsInitializer,\n",
    "        KafkaSink,\n",
    "        KafkaRecordSerializationSchema,\n",
    "    )\n",
    "    from pyflink.common import Types, WatermarkStrategy, Encoder\n",
    "    from pyflink.datastream.functions import MapFunction, FilterFunction, FlatMapFunction\n",
    "\n",
    "    print(\"[OK] PyFlink libraries loaded\")\n",
    "except ImportError as e:\n",
    "    print(f\"[WARNING] PyFlink not available: {e}\")\n",
    "    print(\"         Some examples will use simplified implementations\")\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "from confluent_kafka import Producer, Consumer\n",
    "from confluent_kafka.admin import AdminClient, NewTopic\n",
    "import time\n",
    "import random\n",
    "\n",
    "print(\"[OK] Libraries ready for Flink examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. PyFlink DataStream API\n",
    "\n",
    "### Execution Environment\n",
    "\n",
    "The execution environment is the entry point for Flink programs:\n",
    "\n",
    "```python\n",
    "# Create execution environment\n",
    "env = StreamExecutionEnvironment.get_execution_environment()\n",
    "\n",
    "# Configure parallelism\n",
    "env.set_parallelism(4)\n",
    "\n",
    "# Enable checkpointing\n",
    "env.enable_checkpointing(10000)  # Every 10 seconds\n",
    "```\n",
    "\n",
    "### DataStream Operations\n",
    "\n",
    "**Basic Transformation Pattern:**\n",
    "```\n",
    "Source → Transformation(s) → Sink\n",
    "\n",
    "env.from_source(...) \\       # Read data\n",
    "   .map(...) \\                # Transform\n",
    "   .filter(...) \\             # Filter\n",
    "   .key_by(...) \\             # Partition\n",
    "   .window(...) \\             # Window\n",
    "   .reduce(...) \\             # Aggregate\n",
    "   .sink_to(...)              # Write data\n",
    "```\n",
    "\n",
    "### Common Transformations\n",
    "\n",
    "| Operation | Description | Example |\n",
    "|-----------|-------------|----------|\n",
    "| `map()` | 1-to-1 transformation | Convert Celsius to Fahrenheit |\n",
    "| `filter()` | Select matching elements | Filter premium users |\n",
    "| `flat_map()` | 1-to-N transformation | Split sentence into words |\n",
    "| `key_by()` | Partition by key | Group by user_id |\n",
    "| `reduce()` | Combine elements | Sum, count, max |\n",
    "| `aggregate()` | Custom aggregation | Moving average |\n",
    "| `window()` | Group into windows | 1-minute tumbling window |\n",
    "| `union()` | Merge streams | Combine web + mobile events |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Flink job structure (conceptual)\n",
    "print(\n",
    "    \"\"\"[DATA] Flink Job Structure:\n",
    "\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "\n",
    "# 1. Create execution environment\n",
    "env = StreamExecutionEnvironment.get_execution_environment()\n",
    "env.set_parallelism(2)\n",
    "\n",
    "# 2. Define source\n",
    "stream = env.from_collection([\n",
    "    {'temperature': 20, 'sensor': 'A'},\n",
    "    {'temperature': 25, 'sensor': 'B'},\n",
    "    {'temperature': 30, 'sensor': 'A'}\n",
    "])\n",
    "\n",
    "# 3. Apply transformations\n",
    "result = stream \\\\\n",
    "    .map(lambda x: {'temp_f': x['temperature'] * 9/5 + 32, 'sensor': x['sensor']}) \\\\\n",
    "    .filter(lambda x: x['temp_f'] > 70)\n",
    "\n",
    "# 4. Define sink (output)\n",
    "result.print()\n",
    "\n",
    "# 5. Execute job\n",
    "env.execute('Temperature Conversion Job')\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "print(\"[OK] This shows the basic structure of a Flink streaming job\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Flink Transformations\n",
    "\n",
    "### Map Transformation\n",
    "\n",
    "**Concept**: Transform each element independently\n",
    "```\n",
    "Input:  [1, 2, 3, 4, 5]\n",
    "Map(x → x * 2)\n",
    "Output: [2, 4, 6, 8, 10]\n",
    "```\n",
    "\n",
    "### Filter Transformation\n",
    "\n",
    "**Concept**: Select elements matching a condition\n",
    "```\n",
    "Input:  [1, 2, 3, 4, 5, 6]\n",
    "Filter(x → x % 2 == 0)\n",
    "Output: [2, 4, 6]\n",
    "```\n",
    "\n",
    "### FlatMap Transformation\n",
    "\n",
    "**Concept**: Transform one element into zero or more elements\n",
    "```\n",
    "Input:  ['hello world', 'foo bar']\n",
    "FlatMap(s → s.split(' '))\n",
    "Output: ['hello', 'world', 'foo', 'bar']\n",
    "```\n",
    "\n",
    "### KeyBy Transformation\n",
    "\n",
    "**Concept**: Partition stream by key for parallel processing\n",
    "```\n",
    "Input:  [{user: 'A', val: 1}, {user: 'B', val: 2}, {user: 'A', val: 3}]\n",
    "KeyBy('user')\n",
    "Result: \n",
    "  Partition 0: [{user: 'A', val: 1}, {user: 'A', val: 3}]\n",
    "  Partition 1: [{user: 'B', val: 2}]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate transformations with Python (Flink-style)\n",
    "class FlinkStyleProcessor:\n",
    "    \"\"\"Simulate Flink transformations with Python\"\"\"\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def map(self, func):\n",
    "        \"\"\"Apply function to each element\"\"\"\n",
    "        self.data = [func(x) for x in self.data]\n",
    "        return self\n",
    "\n",
    "    def filter(self, func):\n",
    "        \"\"\"Keep elements matching condition\"\"\"\n",
    "        self.data = [x for x in self.data if func(x)]\n",
    "        return self\n",
    "\n",
    "    def flat_map(self, func):\n",
    "        \"\"\"Transform one element into multiple\"\"\"\n",
    "        result = []\n",
    "        for x in self.data:\n",
    "            result.extend(func(x))\n",
    "        self.data = result\n",
    "        return self\n",
    "\n",
    "    def key_by(self, key_func):\n",
    "        \"\"\"Group by key\"\"\"\n",
    "        from collections import defaultdict\n",
    "\n",
    "        grouped = defaultdict(list)\n",
    "        for x in self.data:\n",
    "            key = key_func(x)\n",
    "            grouped[key].append(x)\n",
    "        return grouped\n",
    "\n",
    "    def collect(self):\n",
    "        \"\"\"Get results\"\"\"\n",
    "        return self.data\n",
    "\n",
    "\n",
    "# Example: Transform temperature data\n",
    "data = [\n",
    "    {\"temperature\": 20, \"sensor\": \"A\", \"unit\": \"C\"},\n",
    "    {\"temperature\": 25, \"sensor\": \"B\", \"unit\": \"C\"},\n",
    "    {\"temperature\": 15, \"sensor\": \"A\", \"unit\": \"C\"},\n",
    "    {\"temperature\": 30, \"sensor\": \"C\", \"unit\": \"C\"},\n",
    "]\n",
    "\n",
    "print(\"[DATA] Original data:\")\n",
    "for d in data:\n",
    "    print(f\"  {d}\")\n",
    "\n",
    "# Apply transformations\n",
    "result = (\n",
    "    FlinkStyleProcessor(data)\n",
    "    .map(lambda x: {**x, \"temperature\": x[\"temperature\"] * 9 / 5 + 32, \"unit\": \"F\"})\n",
    "    .filter(lambda x: x[\"temperature\"] > 70)\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "print(\"\\n[DATA] After map (C to F) and filter (> 70F):\")\n",
    "for d in result:\n",
    "    print(f\"  {d}\")\n",
    "\n",
    "# KeyBy example\n",
    "grouped = FlinkStyleProcessor(result).key_by(lambda x: x[\"sensor\"])\n",
    "\n",
    "print(\"\\n[DATA] After key_by('sensor'):\")\n",
    "for sensor, readings in grouped.items():\n",
    "    print(f\"  Sensor {sensor}: {len(readings)} readings\")\n",
    "\n",
    "print(\"\\n[OK] Transformations complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Kafka Integration\n",
    "\n",
    "### Kafka Source\n",
    "\n",
    "**Reading from Kafka:**\n",
    "```python\n",
    "from pyflink.datastream.connectors.kafka import KafkaSource\n",
    "\n",
    "kafka_source = KafkaSource.builder() \\\\\n",
    "    .set_bootstrap_servers('localhost:9092') \\\\\n",
    "    .set_topics('input-topic') \\\\\n",
    "    .set_group_id('flink-consumer') \\\\\n",
    "    .set_starting_offsets(KafkaOffsetsInitializer.earliest()) \\\\\n",
    "    .build()\n",
    "\n",
    "stream = env.from_source(\n",
    "    kafka_source,\n",
    "    WatermarkStrategy.no_watermarks(),\n",
    "    'Kafka Source'\n",
    ")\n",
    "```\n",
    "\n",
    "### Kafka Sink\n",
    "\n",
    "**Writing to Kafka:**\n",
    "```python\n",
    "from pyflink.datastream.connectors.kafka import KafkaSink\n",
    "\n",
    "kafka_sink = KafkaSink.builder() \\\\\n",
    "    .set_bootstrap_servers('localhost:9092') \\\\\n",
    "    .set_record_serializer(...) \\\\\n",
    "    .build()\n",
    "\n",
    "stream.sink_to(kafka_sink)\n",
    "```\n",
    "\n",
    "### End-to-End Flow\n",
    "\n",
    "```\n",
    "Kafka Topic          Flink Job          Kafka Topic\n",
    "  (Input)                                (Output)\n",
    "     │                                       ↑\n",
    "     │                                       │\n",
    "     └→ KafkaSource → Transform → KafkaSink ┘\n",
    "                         ↓\n",
    "                    map, filter,\n",
    "                    aggregate, etc.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate Kafka → Flink → Kafka pipeline\n",
    "class FlinkKafkaPipeline:\n",
    "    \"\"\"\n",
    "    Simulates a Flink job that:\n",
    "    1. Reads from Kafka\n",
    "    2. Processes events\n",
    "    3. Writes to Kafka\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_topic, output_topic, group_id=\"flink-processor\"):\n",
    "        self.input_topic = input_topic\n",
    "        self.output_topic = output_topic\n",
    "\n",
    "        # Kafka consumer (source)\n",
    "        self.consumer = Consumer(\n",
    "            {\n",
    "                \"bootstrap.servers\": \"localhost:9092\",\n",
    "                \"group.id\": group_id,\n",
    "                \"auto.offset.reset\": \"earliest\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Kafka producer (sink)\n",
    "        self.producer = Producer({\"bootstrap.servers\": \"localhost:9092\"})\n",
    "\n",
    "    def process_event(self, event):\n",
    "        \"\"\"\n",
    "        Transform event (Flink processing logic)\n",
    "        \"\"\"\n",
    "        # Example: Enrich event with processing timestamp\n",
    "        return {**event, \"processed_at\": datetime.now().isoformat(), \"processor\": \"flink-job-1\"}\n",
    "\n",
    "    def run(self, duration_seconds=10):\n",
    "        \"\"\"Run the processing pipeline\"\"\"\n",
    "        self.consumer.subscribe([self.input_topic])\n",
    "\n",
    "        processed_count = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        print(f\"[OK] Flink pipeline started\")\n",
    "        print(f\"     Input: {self.input_topic}\")\n",
    "        print(f\"     Output: {self.output_topic}\\n\")\n",
    "\n",
    "        try:\n",
    "            while time.time() - start_time < duration_seconds:\n",
    "                msg = self.consumer.poll(timeout=1.0)\n",
    "\n",
    "                if msg is None:\n",
    "                    continue\n",
    "\n",
    "                if msg.error():\n",
    "                    continue\n",
    "\n",
    "                # Read event\n",
    "                event = json.loads(msg.value().decode(\"utf-8\"))\n",
    "\n",
    "                # Process (transform)\n",
    "                processed = self.process_event(event)\n",
    "\n",
    "                # Write to output topic\n",
    "                self.producer.produce(\n",
    "                    topic=self.output_topic, key=event.get(\"user_id\"), value=json.dumps(processed)\n",
    "                )\n",
    "                self.producer.poll(0)\n",
    "\n",
    "                processed_count += 1\n",
    "\n",
    "                if processed_count <= 5:\n",
    "                    print(f\"[{processed_count}] Processed: {event.get('event_type', 'unknown')}\")\n",
    "\n",
    "        finally:\n",
    "            self.producer.flush()\n",
    "            self.consumer.close()\n",
    "\n",
    "            print(f\"\\n[SUCCESS] Processed {processed_count} events\")\n",
    "\n",
    "\n",
    "print(\"[OK] FlinkKafkaPipeline class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create topics for Flink pipeline\n",
    "admin_client = AdminClient({\"bootstrap.servers\": \"localhost:9092\"})\n",
    "\n",
    "topics = [\n",
    "    NewTopic(\"flink-input\", num_partitions=3, replication_factor=1),\n",
    "    NewTopic(\"flink-output\", num_partitions=3, replication_factor=1),\n",
    "]\n",
    "\n",
    "try:\n",
    "    futures = admin_client.create_topics(topics)\n",
    "    for topic, future in futures.items():\n",
    "        try:\n",
    "            future.result()\n",
    "            print(f\"[OK] Created topic '{topic}'\")\n",
    "        except Exception as e:\n",
    "            if \"TOPIC_ALREADY_EXISTS\" in str(e):\n",
    "                print(f\"[OK] Topic '{topic}' exists\")\n",
    "except Exception as e:\n",
    "    print(f\"[WARNING] {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample events and run pipeline\n",
    "import threading\n",
    "\n",
    "\n",
    "def generate_flink_input(num_events=50):\n",
    "    \"\"\"Generate events for Flink processing\"\"\"\n",
    "    producer = Producer({\"bootstrap.servers\": \"localhost:9092\"})\n",
    "\n",
    "    event_types = [\"click\", \"view\", \"purchase\", \"search\"]\n",
    "\n",
    "    for i in range(num_events):\n",
    "        event = {\n",
    "            \"event_id\": f\"evt_{i}\",\n",
    "            \"user_id\": f\"user_{random.randint(1, 10)}\",\n",
    "            \"event_type\": random.choice(event_types),\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"value\": random.randint(1, 100),\n",
    "        }\n",
    "\n",
    "        producer.produce(topic=\"flink-input\", value=json.dumps(event))\n",
    "        producer.poll(0)\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    producer.flush()\n",
    "    print(f\"\\n[OK] Generated {num_events} input events\")\n",
    "\n",
    "\n",
    "# Start event generator in background\n",
    "generator = threading.Thread(target=generate_flink_input, args=(50,))\n",
    "generator.start()\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "# Run Flink pipeline\n",
    "pipeline = FlinkKafkaPipeline(\"flink-input\", \"flink-output\")\n",
    "pipeline.run(duration_seconds=8)\n",
    "\n",
    "generator.join()\n",
    "\n",
    "print(\"\\n[SUCCESS] Flink pipeline complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Stateful Processing in Flink\n",
    "\n",
    "### State Types\n",
    "\n",
    "**1. ValueState**: Stores a single value\n",
    "```python\n",
    "# Example: Track last value per key\n",
    "class MyFunction(RichMapFunction):\n",
    "    def open(self, context):\n",
    "        self.state = context.get_state(\n",
    "            ValueStateDescriptor('last-value', Types.INT())\n",
    "        )\n",
    "```\n",
    "\n",
    "**2. ListState**: Stores a list of values\n",
    "```python\n",
    "# Example: Track last N values\n",
    "self.state = context.get_list_state(\n",
    "    ListStateDescriptor('history', Types.INT())\n",
    ")\n",
    "```\n",
    "\n",
    "**3. MapState**: Stores key-value pairs\n",
    "```python\n",
    "# Example: Count by category\n",
    "self.state = context.get_map_state(\n",
    "    MapStateDescriptor('counts', Types.STRING(), Types.INT())\n",
    ")\n",
    "```\n",
    "\n",
    "### Checkpointing\n",
    "\n",
    "**Purpose**: Save state periodically for fault tolerance\n",
    "```\n",
    "Timeline:\n",
    "  ├── Process events ──┬── Checkpoint 1 (save state)\n",
    "  ├── Process events ──┬── Checkpoint 2 (save state)\n",
    "  ├── [FAILURE] ───────┘\n",
    "  └── Restore from Checkpoint 2 → Resume\n",
    "\n",
    "Configuration:\n",
    "env.enable_checkpointing(10000)  # Every 10 seconds\n",
    "env.get_checkpoint_config().set_min_pause_between_checkpoints(5000)\n",
    "```\n",
    "\n",
    "### Exactly-Once Semantics\n",
    "\n",
    "**How Flink achieves exactly-once:**\n",
    "```\n",
    "1. Checkpoint barrier flows through pipeline\n",
    "2. All operators save state at barrier\n",
    "3. Kafka offsets committed atomically\n",
    "4. On failure: restore from last checkpoint\n",
    "\n",
    "Result: Each event processed exactly once!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate stateful processing\n",
    "class StatefulCounter:\n",
    "    \"\"\"Maintain counts per key (like Flink state)\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.state = {}  # key -> count\n",
    "        self.checkpoints = []  # Saved state snapshots\n",
    "\n",
    "    def process(self, key, value):\n",
    "        \"\"\"Update state for key\"\"\"\n",
    "        if key not in self.state:\n",
    "            self.state[key] = 0\n",
    "        self.state[key] += value\n",
    "        return self.state[key]\n",
    "\n",
    "    def checkpoint(self):\n",
    "        \"\"\"Save current state (checkpoint)\"\"\"\n",
    "        snapshot = self.state.copy()\n",
    "        self.checkpoints.append(snapshot)\n",
    "        return len(self.checkpoints) - 1\n",
    "\n",
    "    def restore(self, checkpoint_id):\n",
    "        \"\"\"Restore from checkpoint\"\"\"\n",
    "        if checkpoint_id < len(self.checkpoints):\n",
    "            self.state = self.checkpoints[checkpoint_id].copy()\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"Get current state\"\"\"\n",
    "        return self.state.copy()\n",
    "\n",
    "\n",
    "# Example: Process events with checkpointing\n",
    "counter = StatefulCounter()\n",
    "\n",
    "events = [\n",
    "    (\"user_1\", 10),\n",
    "    (\"user_2\", 5),\n",
    "    (\"user_1\", 15),  # Checkpoint here\n",
    "    (\"user_3\", 20),\n",
    "    (\"user_1\", 25),\n",
    "]\n",
    "\n",
    "print(\"[DATA] Processing events with state:\\n\")\n",
    "\n",
    "for i, (key, value) in enumerate(events):\n",
    "    result = counter.process(key, value)\n",
    "    print(f\"Event {i+1}: {key} + {value} = {result}\")\n",
    "\n",
    "    # Checkpoint after event 3\n",
    "    if i == 2:\n",
    "        checkpoint_id = counter.checkpoint()\n",
    "        print(f\"  [CHECKPOINT] Saved state snapshot {checkpoint_id}\")\n",
    "\n",
    "print(f\"\\n[DATA] Final state: {counter.get_state()}\")\n",
    "\n",
    "# Simulate failure and recovery\n",
    "print(\"\\n[WARNING] Simulating failure...\")\n",
    "print(\"[OK] Restoring from checkpoint 0...\")\n",
    "counter.restore(0)\n",
    "print(f\"[DATA] Restored state: {counter.get_state()}\")\n",
    "\n",
    "print(\"\\n[OK] This demonstrates how Flink maintains and recovers state\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Flink Web Dashboard\n",
    "\n",
    "### Accessing the Dashboard\n",
    "\n",
    "**URL**: http://localhost:8082\n",
    "\n",
    "**Key Screens:**\n",
    "\n",
    "1. **Overview**: Cluster status, running jobs\n",
    "2. **Jobs**: All jobs (running, finished, failed)\n",
    "3. **Task Managers**: Worker nodes, available slots\n",
    "4. **Job Details**: \n",
    "   - Execution graph\n",
    "   - Metrics (records processed, backpressure)\n",
    "   - Checkpoints\n",
    "   - Exceptions\n",
    "\n",
    "### Key Metrics\n",
    "\n",
    "| Metric | Description | Good Value |\n",
    "|--------|-------------|------------|\n",
    "| Records In | Events received | Steady |\n",
    "| Records Out | Events produced | Steady |\n",
    "| Backpressure | Downstream slow | Low/None |\n",
    "| Checkpoint Duration | Time to checkpoint | < 1 second |\n",
    "| State Size | Total state | Manageable |\n",
    "\n",
    "### Troubleshooting\n",
    "\n",
    "**Problem: High Backpressure**\n",
    "```\n",
    "Cause: Downstream operator is slow\n",
    "Solution: \n",
    "  - Increase parallelism\n",
    "  - Optimize processing logic\n",
    "  - Add more TaskManagers\n",
    "```\n",
    "\n",
    "**Problem: Checkpoint Failures**\n",
    "```\n",
    "Cause: State too large or timeout\n",
    "Solution:\n",
    "  - Increase checkpoint timeout\n",
    "  - Reduce state size\n",
    "  - Use RocksDB state backend\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instructions for monitoring Flink\n",
    "print(\n",
    "    \"\"\"[DATA] Monitoring Your Flink Job:\n",
    "\n",
    "1. Access Flink Dashboard:\n",
    "   URL: http://localhost:8082\n",
    "\n",
    "2. Check Running Jobs:\n",
    "   - Go to \"Running Jobs\" tab\n",
    "   - Click on your job name\n",
    "   - View execution graph\n",
    "\n",
    "3. Monitor Metrics:\n",
    "   - Records In/Out: Event throughput\n",
    "   - Backpressure: Green = good, Red = problem\n",
    "   - Checkpoint: Should complete successfully\n",
    "\n",
    "4. View Task Managers:\n",
    "   - Check available slots\n",
    "   - Monitor memory usage\n",
    "   - View task distribution\n",
    "\n",
    "5. Debug Issues:\n",
    "   - Check \"Exceptions\" tab\n",
    "   - View task logs\n",
    "   - Analyze checkpoint history\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "print(\"[OK] Open http://localhost:8082 in your browser to explore!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Mini-Project: Word Count Stream\n",
    "\n",
    "Build a classic streaming word count application!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Count Stream Processor\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class WordCountStream:\n",
    "    \"\"\"Streaming word count with tumbling windows\"\"\"\n",
    "\n",
    "    def __init__(self, window_size_seconds=10):\n",
    "        self.window_size = window_size_seconds\n",
    "        self.windows = {}  # window_id -> word counts\n",
    "\n",
    "    def get_window_id(self, timestamp):\n",
    "        \"\"\"Determine window for event\"\"\"\n",
    "        event_time = datetime.fromisoformat(timestamp)\n",
    "        epoch = int(event_time.timestamp())\n",
    "        return (epoch // self.window_size) * self.window_size\n",
    "\n",
    "    def process_sentence(self, sentence, timestamp):\n",
    "        \"\"\"Process a sentence: split into words and count\"\"\"\n",
    "        window_id = self.get_window_id(timestamp)\n",
    "\n",
    "        if window_id not in self.windows:\n",
    "            self.windows[window_id] = defaultdict(int)\n",
    "\n",
    "        # FlatMap: split into words\n",
    "        words = sentence.lower().split()\n",
    "\n",
    "        # Map: count each word\n",
    "        for word in words:\n",
    "            # Filter: only alphanumeric\n",
    "            word = \"\".join(c for c in word if c.isalnum())\n",
    "            if word:\n",
    "                self.windows[window_id][word] += 1\n",
    "\n",
    "    def get_top_words(self, window_id, n=10):\n",
    "        \"\"\"Get top N words in a window\"\"\"\n",
    "        if window_id not in self.windows:\n",
    "            return []\n",
    "\n",
    "        word_counts = self.windows[window_id]\n",
    "        return sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:n]\n",
    "\n",
    "    def print_results(self):\n",
    "        \"\"\"Print word count results\"\"\"\n",
    "        print(\"\\n[DATA] Word Count Results:\\n\")\n",
    "\n",
    "        for window_id in sorted(self.windows.keys()):\n",
    "            window_time = datetime.fromtimestamp(window_id)\n",
    "            top_words = self.get_top_words(window_id, n=5)\n",
    "\n",
    "            print(f\"Window {window_time.strftime('%H:%M:%S')}:\")\n",
    "            for word, count in top_words:\n",
    "                print(f\"  {word:15s}: {count}\")\n",
    "            print()\n",
    "\n",
    "\n",
    "# Generate sample text stream\n",
    "sample_sentences = [\n",
    "    \"Apache Flink is a stream processing framework\",\n",
    "    \"Flink provides stateful computations over data streams\",\n",
    "    \"Stream processing enables real-time analytics\",\n",
    "    \"Kafka and Flink work great together\",\n",
    "    \"Event-driven architectures use stream processing\",\n",
    "    \"Flink supports exactly-once semantics\",\n",
    "    \"Real-time data processing is critical\",\n",
    "    \"Apache Kafka is a distributed streaming platform\",\n",
    "    \"Flink can process millions of events per second\",\n",
    "    \"Stream processing frameworks are powerful tools\",\n",
    "]\n",
    "\n",
    "# Process sentences\n",
    "word_counter = WordCountStream(window_size_seconds=5)\n",
    "\n",
    "print(\"[OK] Processing text stream...\\n\")\n",
    "\n",
    "base_time = datetime.now()\n",
    "for i, sentence in enumerate(sample_sentences):\n",
    "    timestamp = (base_time + timedelta(seconds=i)).isoformat()\n",
    "    word_counter.process_sentence(sentence, timestamp)\n",
    "    print(f\"[{i+1}] Processed: {sentence[:50]}...\")\n",
    "    time.sleep(0.2)\n",
    "\n",
    "# Show results\n",
    "word_counter.print_results()\n",
    "\n",
    "print(\"[SUCCESS] Word count stream processing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Key Takeaways\n",
    "\n",
    "[OK] **Flink Architecture**: JobManager coordinates, TaskManagers execute\n",
    "\n",
    "[OK] **DataStream API**: Fluent API for building streaming pipelines\n",
    "\n",
    "[OK] **Transformations**: map, filter, flatMap, keyBy, reduce, window\n",
    "\n",
    "[OK] **Kafka Integration**: Read from and write to Kafka seamlessly\n",
    "\n",
    "[OK] **Stateful Processing**: Manage state with checkpoints for fault tolerance\n",
    "\n",
    "[OK] **Exactly-Once**: Flink guarantees each event processed exactly once\n",
    "\n",
    "### Common Patterns\n",
    "\n",
    "**1. ETL Pipeline:**\n",
    "```\n",
    "Kafka → Flink (Extract, Transform) → Kafka/Database\n",
    "```\n",
    "\n",
    "**2. Real-Time Analytics:**\n",
    "```\n",
    "Events → Flink (Aggregate in windows) → Metrics Dashboard\n",
    "```\n",
    "\n",
    "**3. Event Enrichment:**\n",
    "```\n",
    "Stream → Flink (Join with reference data) → Enriched Stream\n",
    "```\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Use keyed streams** for stateful operations\n",
    "2. **Enable checkpointing** for production jobs\n",
    "3. **Monitor backpressure** to detect bottlenecks\n",
    "4. **Choose parallelism** based on workload\n",
    "5. **Test with small data** before scaling up\n",
    "6. **Use event time** for correctness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Practice Exercises\n",
    "\n",
    "1. **Build a filter pipeline**: Read from Kafka, filter events, write to new topic\n",
    "2. **Implement aggregation**: Count events per user in 1-minute windows\n",
    "3. **Create enrichment job**: Add timestamp and metadata to events\n",
    "4. **Monitor via dashboard**: Deploy a job and observe metrics\n",
    "5. **Test fault tolerance**: Simulate failure and verify recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your practice code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Next Steps\n",
    "\n",
    "Congratulations on completing Module 04!\n",
    "\n",
    "### What You've Learned\n",
    "\n",
    "- [OK] Apache Flink architecture and components\n",
    "- [OK] PyFlink DataStream API\n",
    "- [OK] Stream transformations and operators\n",
    "- [OK] Kafka source and sink connectors\n",
    "- [OK] Stateful processing and checkpointing\n",
    "\n",
    "### Coming Up in Module 05: Advanced Stream Processing\n",
    "\n",
    "You'll learn:\n",
    "- Advanced windowing (session, sliding)\n",
    "- Watermarks and late data handling\n",
    "- Stream joins (window joins, interval joins)\n",
    "- Custom functions and operators\n",
    "- Performance optimization\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Flink Documentation](https://nightlies.apache.org/flink/flink-docs-master/)\n",
    "- [PyFlink Tutorial](https://nightlies.apache.org/flink/flink-docs-master/docs/dev/python/overview/)\n",
    "- [Flink Kafka Connector](https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/datastream/kafka/)\n",
    "- [Flink Best Practices](https://flink.apache.org/news/2020/07/28/flink-best-practices.html)\n",
    "\n",
    "---\n",
    "\n",
    "**Ready for advanced topics?** Open `05_advanced_stream_processing.ipynb` to continue!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
