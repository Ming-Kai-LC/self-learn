{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 00: Setup and Introduction to Streaming\n",
    "\n",
    "**Estimated Time:** 30 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this module, you will:\n",
    "- Verify your streaming environment is set up correctly\n",
    "- Understand the difference between batch and stream processing\n",
    "- Learn core streaming concepts (events, topics, consumers)\n",
    "- Connect to Kafka and Flink clusters\n",
    "- Run your first streaming \"Hello World\" example\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Verification\n",
    "\n",
    "Let's verify that all required components are installed and running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Python version\n",
    "import sys\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Python version info: {sys.version_info}\")\n",
    "\n",
    "if sys.version_info < (3, 8):\n",
    "    print(\"\\n[WARNING] Python 3.8 or higher is recommended\")\n",
    "else:\n",
    "    print(\"\\n[OK] Python version is compatible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify core streaming libraries\n",
    "try:\n",
    "    from confluent_kafka import Producer, Consumer, KafkaException\n",
    "    from confluent_kafka.admin import AdminClient\n",
    "\n",
    "    print(\"[OK] confluent-kafka library installed\")\n",
    "except ImportError as e:\n",
    "    print(f\"[FAIL] confluent-kafka not installed: {e}\")\n",
    "    print(\"\\nPlease run: pip install confluent-kafka\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if Docker is running\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    result = subprocess.run([\"docker\", \"ps\"], capture_output=True, text=True, timeout=5)\n",
    "    if result.returncode == 0:\n",
    "        print(\"[OK] Docker is running\")\n",
    "        print(f\"\\nRunning containers:\")\n",
    "        print(result.stdout[:500])  # Show first 500 chars\n",
    "    else:\n",
    "        print(\"[FAIL] Docker command failed\")\n",
    "        print(\"Please start Docker Desktop\")\n",
    "except Exception as e:\n",
    "    print(f\"[FAIL] Cannot connect to Docker: {e}\")\n",
    "    print(\"\\nPlease ensure Docker Desktop is installed and running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Kafka connection\n",
    "from confluent_kafka.admin import AdminClient\n",
    "\n",
    "kafka_config = {\"bootstrap.servers\": \"localhost:9092\"}\n",
    "\n",
    "try:\n",
    "    admin_client = AdminClient(kafka_config)\n",
    "    # Test connection by listing topics\n",
    "    metadata = admin_client.list_topics(timeout=5)\n",
    "    print(\"[OK] Successfully connected to Kafka\")\n",
    "    print(f\"\\nKafka cluster has {len(metadata.topics)} topics\")\n",
    "    if metadata.topics:\n",
    "        print(f\"Topics: {list(metadata.topics.keys())[:5]}\")\n",
    "except Exception as e:\n",
    "    print(f\"[FAIL] Cannot connect to Kafka: {e}\")\n",
    "    print(\"\\nPlease ensure Kafka is running:\")\n",
    "    print(\"  docker-compose up -d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Batch vs Stream Processing\n",
    "\n",
    "### What's the Difference?\n",
    "\n",
    "**Batch Processing:**\n",
    "- Processes data in **fixed-size chunks**\n",
    "- Runs on a **schedule** (hourly, daily, weekly)\n",
    "- Higher **latency** (minutes to hours)\n",
    "- Examples: Daily reports, monthly analytics, ETL jobs\n",
    "\n",
    "**Stream Processing:**\n",
    "- Processes data **continuously** as it arrives\n",
    "- Runs **24/7** in real-time\n",
    "- Lower **latency** (milliseconds to seconds)\n",
    "- Examples: Fraud detection, real-time dashboards, monitoring\n",
    "\n",
    "### Visual Comparison\n",
    "\n",
    "```\n",
    "BATCH PROCESSING:\n",
    "Events → [Buffer] → Process Every Hour → Results\n",
    "   |        |           ↓\n",
    "  1hr      1hr        Latency: 1 hour\n",
    "\n",
    "STREAM PROCESSING:\n",
    "Events → Process Immediately → Results\n",
    "   |            ↓\n",
    "  Continuous   Latency: milliseconds\n",
    "```\n",
    "\n",
    "### When to Use Each?\n",
    "\n",
    "| Use Case | Batch | Stream |\n",
    "|----------|-------|--------|\n",
    "| Fraud detection | ❌ | [OK] |\n",
    "| Monthly reports | [OK] | ❌ |\n",
    "| Real-time recommendations | ❌ | [OK] |\n",
    "| Historical analysis | [OK] | ❌ |\n",
    "| Live dashboards | ❌ | [OK] |\n",
    "| Data warehousing | [OK] | ❌ |\n",
    "| IoT monitoring | ❌ | [OK] |\n",
    "\n",
    "**Key Insight**: Many modern systems use **both** - stream for real-time, batch for historical analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Core Streaming Concepts\n",
    "\n",
    "### Events\n",
    "An **event** is something that happened at a point in time.\n",
    "- User clicked a button\n",
    "- Sensor recorded a temperature\n",
    "- Payment was processed\n",
    "\n",
    "### Topics\n",
    "A **topic** is a category/feed of events.\n",
    "- \"user-clicks\" topic\n",
    "- \"temperature-readings\" topic\n",
    "- \"payments\" topic\n",
    "\n",
    "### Producers\n",
    "**Producers** write events to topics.\n",
    "- Web application sends user clicks\n",
    "- IoT device sends sensor data\n",
    "- Payment service sends transactions\n",
    "\n",
    "### Consumers\n",
    "**Consumers** read events from topics.\n",
    "- Analytics service reads clicks\n",
    "- Monitoring service reads sensor data\n",
    "- Fraud detection reads payments\n",
    "\n",
    "### Architecture\n",
    "\n",
    "```\n",
    "Producers                 Kafka Cluster              Consumers\n",
    "   |                           |                         |\n",
    "[Web App] ──┐                  |                    ┌──[Analytics]\n",
    "            ├─→ Topic: clicks ─┤                    |\n",
    "[Mobile]  ──┘                  |                    ├──[Dashboard]\n",
    "                                |                    |\n",
    "[Sensors] ───→ Topic: sensors ─┤                    └──[Alerts]\n",
    "                                |\n",
    "[PaymentAPI]→ Topic: payments─┤\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Your First Streaming Example\n",
    "\n",
    "Let's create a simple producer-consumer example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from confluent_kafka import Producer, Consumer, KafkaException\n",
    "import time\n",
    "\n",
    "# Configuration\n",
    "TOPIC_NAME = \"hello-streaming\"\n",
    "\n",
    "producer_config = {\"bootstrap.servers\": \"localhost:9092\", \"client.id\": \"hello-producer\"}\n",
    "\n",
    "consumer_config = {\n",
    "    \"bootstrap.servers\": \"localhost:9092\",\n",
    "    \"group.id\": \"hello-consumer-group\",\n",
    "    \"auto.offset.reset\": \"earliest\",\n",
    "    \"enable.auto.commit\": True,\n",
    "}\n",
    "\n",
    "print(\"[OK] Configuration ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a Producer and send events\n",
    "def delivery_callback(err, msg):\n",
    "    \"\"\"Callback for message delivery confirmation\"\"\"\n",
    "    if err:\n",
    "        print(f\"[FAIL] Message delivery failed: {err}\")\n",
    "    else:\n",
    "        print(\n",
    "            f\"[OK] Message delivered to {msg.topic()} [{msg.partition()}] at offset {msg.offset()}\"\n",
    "        )\n",
    "\n",
    "\n",
    "# Create producer\n",
    "producer = Producer(producer_config)\n",
    "\n",
    "# Send 5 events\n",
    "print(\"Sending events to Kafka...\\n\")\n",
    "for i in range(5):\n",
    "    event = {\n",
    "        \"event_id\": i,\n",
    "        \"message\": f\"Hello from event {i}\",\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "    }\n",
    "\n",
    "    # Convert to JSON string\n",
    "    event_json = json.dumps(event)\n",
    "\n",
    "    # Produce event\n",
    "    producer.produce(topic=TOPIC_NAME, key=str(i), value=event_json, callback=delivery_callback)\n",
    "\n",
    "    # Trigger delivery callbacks\n",
    "    producer.poll(0)\n",
    "\n",
    "    time.sleep(0.5)  # Small delay\n",
    "\n",
    "# Wait for all messages to be delivered\n",
    "producer.flush()\n",
    "\n",
    "print(\"\\n[SUCCESS] All events sent to Kafka!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create a Consumer and read events\n",
    "print(\"Reading events from Kafka...\\n\")\n",
    "\n",
    "consumer = Consumer(consumer_config)\n",
    "consumer.subscribe([TOPIC_NAME])\n",
    "\n",
    "messages_read = 0\n",
    "max_messages = 5\n",
    "\n",
    "try:\n",
    "    while messages_read < max_messages:\n",
    "        # Poll for messages (timeout in seconds)\n",
    "        msg = consumer.poll(timeout=2.0)\n",
    "\n",
    "        if msg is None:\n",
    "            print(\"[WARNING] No message received, waiting...\")\n",
    "            continue\n",
    "\n",
    "        if msg.error():\n",
    "            print(f\"[FAIL] Consumer error: {msg.error()}\")\n",
    "            continue\n",
    "\n",
    "        # Successfully received a message\n",
    "        key = msg.key().decode(\"utf-8\") if msg.key() else None\n",
    "        value = msg.value().decode(\"utf-8\")\n",
    "        event = json.loads(value)\n",
    "\n",
    "        print(f\"[OK] Received: {event['message']} (ID: {event['event_id']})\")\n",
    "        print(f\"     Key: {key}, Partition: {msg.partition()}, Offset: {msg.offset()}\")\n",
    "\n",
    "        messages_read += 1\n",
    "\n",
    "finally:\n",
    "    consumer.close()\n",
    "    print(f\"\\n[SUCCESS] Read {messages_read} events from Kafka!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened?\n",
    "\n",
    "1. **Producer** created 5 events and sent them to Kafka topic `hello-streaming`\n",
    "2. Events were **stored** in Kafka (durable, replicated)\n",
    "3. **Consumer** read the events from the topic\n",
    "4. Each event has:\n",
    "   - **Key**: Used for partitioning\n",
    "   - **Value**: The actual event data (JSON)\n",
    "   - **Partition**: Which partition stored it\n",
    "   - **Offset**: Position in the partition\n",
    "\n",
    "This is the foundation of **event streaming**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Event Time vs Processing Time\n",
    "\n",
    "In streaming, there are different concepts of \"time\":\n",
    "\n",
    "### Event Time\n",
    "**When the event actually happened** in the real world.\n",
    "- User clicked at 10:00:00 AM\n",
    "- Sensor reading at 10:00:05 AM\n",
    "\n",
    "### Processing Time\n",
    "**When the system processes the event**.\n",
    "- Event arrives at Kafka at 10:00:10 AM\n",
    "- Consumer processes it at 10:00:15 AM\n",
    "\n",
    "### Why Does This Matter?\n",
    "\n",
    "```\n",
    "Event Happens → Network Delay → Kafka → Processing Delay → Result\n",
    "  10:00:00         (5 sec)      10:00:05     (3 sec)      10:00:08\n",
    "     ↑                                                        ↑\n",
    "  Event Time                                           Processing Time\n",
    "```\n",
    "\n",
    "**Challenge**: Events can arrive **out of order** or **late**!\n",
    "- Event A happens at 10:00:00 but arrives at 10:00:10\n",
    "- Event B happens at 10:00:05 but arrives at 10:00:08\n",
    "\n",
    "**Solution**: Use **watermarks** and **event time processing** (covered in Module 05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Event time vs Processing time\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def simulate_event_delay():\n",
    "    \"\"\"\n",
    "    Simulate events with delays\n",
    "    \"\"\"\n",
    "    events = []\n",
    "\n",
    "    # Create events\n",
    "    for i in range(3):\n",
    "        event_time = datetime.now()\n",
    "\n",
    "        # Simulate network delay\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        processing_time = datetime.now()\n",
    "\n",
    "        delay_ms = (processing_time - event_time).total_seconds() * 1000\n",
    "\n",
    "        events.append(\n",
    "            {\n",
    "                \"event_id\": i,\n",
    "                \"event_time\": event_time.isoformat(),\n",
    "                \"processing_time\": processing_time.isoformat(),\n",
    "                \"delay_ms\": round(delay_ms, 2),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        print(f\"Event {i}: Delay = {delay_ms:.2f} ms\")\n",
    "\n",
    "    return events\n",
    "\n",
    "\n",
    "events = simulate_event_delay()\n",
    "print(f\"\\n[DATA] Average delay: {sum(e['delay_ms'] for e in events) / len(events):.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Accessing Web UIs\n",
    "\n",
    "Several web interfaces are available for monitoring:\n",
    "\n",
    "### Kafka UI\n",
    "- URL: http://localhost:8080\n",
    "- View topics, messages, consumer groups\n",
    "- Browse event data\n",
    "\n",
    "### Flink Dashboard\n",
    "- URL: http://localhost:8082\n",
    "- Monitor running jobs\n",
    "- View metrics and checkpoints\n",
    "\n",
    "### Schema Registry\n",
    "- URL: http://localhost:8081\n",
    "- Manage Avro schemas\n",
    "- API endpoint for schemas\n",
    "\n",
    "Open these URLs in your browser to explore!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Key Takeaways\n",
    "\n",
    "[OK] **Environment Setup**: Kafka and Flink are running locally via Docker\n",
    "\n",
    "[OK] **Batch vs Stream**: Stream processing offers low latency, continuous processing\n",
    "\n",
    "[OK] **Core Concepts**: Events, topics, producers, consumers\n",
    "\n",
    "[OK] **First Example**: Successfully produced and consumed events\n",
    "\n",
    "[OK] **Time Semantics**: Event time vs processing time matters for correctness\n",
    "\n",
    "### Important Points\n",
    "\n",
    "1. **Events are immutable** - Once written, they don't change\n",
    "2. **Topics are logs** - Events are appended, not updated\n",
    "3. **Multiple consumers** can read the same topic independently\n",
    "4. **Kafka persists** events (configurable retention)\n",
    "5. **Order is guaranteed** within a partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Practice Exercise\n",
    "\n",
    "Try modifying the producer-consumer example:\n",
    "\n",
    "1. **Change the event structure**: Add more fields (user_id, action, etc.)\n",
    "2. **Send more events**: Increase from 5 to 20\n",
    "3. **Add event time**: Include timestamp in the event payload\n",
    "4. **Multiple consumers**: Run the consumer cell twice (different group IDs)\n",
    "\n",
    "Use the cell below to experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here: Experiment with producer/consumer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Next Steps\n",
    "\n",
    "Congratulations on completing Module 00!\n",
    "\n",
    "### Ready to Continue?\n",
    "\n",
    "In **Module 01: Introduction to Event Streaming**, you'll learn:\n",
    "- Event-driven architecture principles\n",
    "- Kafka architecture deep dive\n",
    "- Partitions, replication, and fault tolerance\n",
    "- Consumer groups and coordination\n",
    "- Build your first mini-project\n",
    "\n",
    "### Before Moving On\n",
    "\n",
    "Make sure you:\n",
    "- [OK] Have Docker containers running\n",
    "- [OK] Successfully connected to Kafka\n",
    "- [OK] Produced and consumed events\n",
    "- [OK] Can access web UIs (Kafka UI, Flink Dashboard)\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Kafka Documentation](https://kafka.apache.org/documentation/)\n",
    "- [Confluent Python Client](https://docs.confluent.io/kafka-clients/python/current/overview.html)\n",
    "- [Event-Driven Architecture](https://martinfowler.com/articles/201701-event-driven.html)\n",
    "\n",
    "---\n",
    "\n",
    "**Ready?** Open `01_introduction_to_event_streaming.ipynb` to continue your streaming journey!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
