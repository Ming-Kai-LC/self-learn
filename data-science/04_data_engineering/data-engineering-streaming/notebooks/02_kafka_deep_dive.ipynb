{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 02: Kafka Deep Dive\n",
    "\n",
    "**Estimated Time:** 75 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this module, you will:\n",
    "- Understand Kafka's internal log structure and storage\n",
    "- Master replication and fault tolerance mechanisms\n",
    "- Learn performance tuning techniques\n",
    "- Configure producers and consumers for optimal performance\n",
    "- Monitor Kafka clusters and troubleshoot issues\n",
    "- Implement reliability patterns (idempotence, transactions)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Kafka Internal Architecture\n",
    "\n",
    "### The Log: Kafka's Core Data Structure\n",
    "\n",
    "Kafka stores events in an **append-only log**:\n",
    "\n",
    "```\n",
    "Partition Log (Append-Only)\n",
    "┌────────────────────────────────────────────────────────┐\n",
    "│ [0] [1] [2] [3] [4] [5] [6] [7] [8] [9] ...           │\n",
    "│  ↑                           ↑              ↑          │\n",
    "│  Old                      Current       New (append)   │\n",
    "└────────────────────────────────────────────────────────┘\n",
    "\n",
    "Properties:\n",
    "- Immutable: Events never change\n",
    "- Ordered: Sequential offset numbers\n",
    "- Durable: Written to disk\n",
    "- Fast: Sequential I/O is efficient\n",
    "```\n",
    "\n",
    "### Log Segments\n",
    "\n",
    "Partitions are divided into **segments** for efficient management:\n",
    "\n",
    "```\n",
    "Partition Directory: /var/lib/kafka/topic-0/\n",
    "\n",
    "├── 00000000000000000000.log    (offsets 0-999)\n",
    "├── 00000000000000000000.index  (index for fast lookup)\n",
    "├── 00000000000000000000.timeindex\n",
    "│\n",
    "├── 00000000000000001000.log    (offsets 1000-1999)\n",
    "├── 00000000000000001000.index\n",
    "├── 00000000000000001000.timeindex\n",
    "│\n",
    "└── 00000000000000002000.log    (active segment)\n",
    "    └── 00000000000000002000.index\n",
    "\n",
    "Benefits:\n",
    "- Easy to delete old data (delete old segments)\n",
    "- Fast seeking with indexes\n",
    "- Parallel reads from different segments\n",
    "```\n",
    "\n",
    "### Storage Mechanics\n",
    "\n",
    "**Write Path:**\n",
    "```\n",
    "Producer → Network → Socket Buffer → Page Cache → Disk\n",
    "             ↓\n",
    "         Batching\n",
    "         Compression\n",
    "         \n",
    "- Writes to page cache (RAM)\n",
    "- OS flushes to disk asynchronously\n",
    "- No explicit fsync (configurable)\n",
    "- Fast because: sequential I/O + page cache\n",
    "```\n",
    "\n",
    "**Read Path:**\n",
    "```\n",
    "Consumer → Request → Broker → Page Cache → Network\n",
    "                                  ↓\n",
    "                             Zero-copy transfer\n",
    "                             \n",
    "- Reads from page cache (fast!)\n",
    "- sendfile() system call (zero-copy)\n",
    "- No application-level copying\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Import libraries and connect to Kafka\n",
    "from confluent_kafka import Producer, Consumer, KafkaException\n",
    "from confluent_kafka.admin import AdminClient, NewTopic, ConfigResource\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "admin_client = AdminClient({\"bootstrap.servers\": \"localhost:9092\"})\n",
    "\n",
    "print(\"[OK] Connected to Kafka cluster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine partition log configuration\n",
    "TOPIC_NAME = \"deep-dive-topic\"\n",
    "\n",
    "# Create topic with specific log settings\n",
    "new_topic = NewTopic(\n",
    "    topic=TOPIC_NAME,\n",
    "    num_partitions=3,\n",
    "    replication_factor=1,\n",
    "    config={\n",
    "        \"segment.bytes\": \"10485760\",  # 10 MB per segment\n",
    "        \"segment.ms\": \"3600000\",  # 1 hour\n",
    "        \"retention.bytes\": \"104857600\",  # 100 MB total\n",
    "        \"retention.ms\": \"604800000\",  # 7 days\n",
    "        \"cleanup.policy\": \"delete\",  # vs 'compact'\n",
    "        \"compression.type\": \"gzip\",\n",
    "        \"min.insync.replicas\": \"1\",\n",
    "    },\n",
    ")\n",
    "\n",
    "try:\n",
    "    futures = admin_client.create_topics([new_topic])\n",
    "    for topic, future in futures.items():\n",
    "        try:\n",
    "            future.result()\n",
    "            print(f\"[OK] Created topic '{topic}' with custom log settings\")\n",
    "        except KafkaException as e:\n",
    "            if \"TOPIC_ALREADY_EXISTS\" in str(e):\n",
    "                print(f\"[OK] Topic '{topic}' already exists\")\n",
    "            else:\n",
    "                raise\n",
    "except Exception as e:\n",
    "    print(f\"[FAIL] Error: {e}\")\n",
    "\n",
    "# Show configuration\n",
    "print(\"\\n[DATA] Log Configuration:\")\n",
    "print(\"  Segment size: 10 MB (creates new segment after 10 MB)\")\n",
    "print(\"  Segment time: 1 hour (creates new segment after 1 hour)\")\n",
    "print(\"  Retention: 100 MB or 7 days (whichever comes first)\")\n",
    "print(\"  Cleanup: Delete old segments (vs compaction)\")\n",
    "print(\"  Compression: gzip (saves disk and network)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Replication and Fault Tolerance\n",
    "\n",
    "### How Replication Works\n",
    "\n",
    "**Replication Factor = 3:**\n",
    "```\n",
    "Topic: payments, Partition 0\n",
    "\n",
    "┌─────────────┐       ┌─────────────┐       ┌─────────────┐\n",
    "│  Broker 1   │       │  Broker 2   │       │  Broker 3   │\n",
    "│             │       │             │       │             │\n",
    "│  LEADER     │──────→│  FOLLOWER   │──────→│  FOLLOWER   │\n",
    "│  [0][1][2]  │       │  [0][1][2]  │       │  [0][1][2]  │\n",
    "└─────────────┘       └─────────────┘       └─────────────┘\n",
    "      ↑                     ↑                     ↑\n",
    "   Producers            Replication           Replication\n",
    "   Consumers\n",
    "\n",
    "- Leader: Handles all reads and writes\n",
    "- Followers: Replicate data from leader\n",
    "- ISR (In-Sync Replicas): Followers that are caught up\n",
    "```\n",
    "\n",
    "### Leader Election\n",
    "\n",
    "**What happens when a leader fails?**\n",
    "```\n",
    "Before:                      After Leader Fails:\n",
    "Broker 1: LEADER             Broker 1: [DOWN]\n",
    "Broker 2: FOLLOWER (ISR)     Broker 2: NEW LEADER ←\n",
    "Broker 3: FOLLOWER (ISR)     Broker 3: FOLLOWER (ISR)\n",
    "\n",
    "Process:\n",
    "1. Controller detects leader failure\n",
    "2. Selects new leader from ISR\n",
    "3. Updates metadata\n",
    "4. Clients reconnect to new leader\n",
    "5. Total downtime: < 1 second\n",
    "```\n",
    "\n",
    "### Acknowledgment Modes (acks)\n",
    "\n",
    "**Producer acks configuration:**\n",
    "\n",
    "| acks | Behavior | Durability | Latency | Use Case |\n",
    "|------|----------|------------|---------|----------|\n",
    "| 0 | Fire and forget | Lowest | Fastest | Metrics, logs |\n",
    "| 1 | Leader confirms | Medium | Medium | Most apps |\n",
    "| all | All ISR confirm | Highest | Slowest | Financial, critical |\n",
    "\n",
    "**Visual:**\n",
    "```\n",
    "acks = 0:\n",
    "Producer → Leader\n",
    "           (no wait)\n",
    "\n",
    "acks = 1:\n",
    "Producer → Leader → [writes to log] → ACK to producer\n",
    "\n",
    "acks = all:\n",
    "Producer → Leader → [writes] → Follower 1 → [replicates]\n",
    "                             → Follower 2 → [replicates]\n",
    "                    ← ACK (after all ISR confirm)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate different acks configurations\n",
    "import time\n",
    "\n",
    "\n",
    "def test_acks_performance(acks_config, num_messages=100):\n",
    "    \"\"\"Test producer performance with different acks settings\"\"\"\n",
    "    config = {\n",
    "        \"bootstrap.servers\": \"localhost:9092\",\n",
    "        \"acks\": acks_config,\n",
    "        \"linger.ms\": 0,  # No batching for fair comparison\n",
    "    }\n",
    "\n",
    "    producer = Producer(config)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in range(num_messages):\n",
    "        event = {\"id\": i, \"timestamp\": datetime.now().isoformat()}\n",
    "        producer.produce(topic=TOPIC_NAME, value=json.dumps(event))\n",
    "        producer.poll(0)\n",
    "\n",
    "    producer.flush()\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    throughput = num_messages / elapsed\n",
    "\n",
    "    return elapsed, throughput\n",
    "\n",
    "\n",
    "print(\"[DATA] Testing different acks configurations...\\n\")\n",
    "\n",
    "# Test acks=1\n",
    "elapsed_1, throughput_1 = test_acks_performance(\"1\", 100)\n",
    "print(f\"acks=1:   {elapsed_1:.3f}s, {throughput_1:.0f} msg/s\")\n",
    "\n",
    "# Test acks=all\n",
    "elapsed_all, throughput_all = test_acks_performance(\"all\", 100)\n",
    "print(f\"acks=all: {elapsed_all:.3f}s, {throughput_all:.0f} msg/s\")\n",
    "\n",
    "print(\"\\n[OK] acks='all' is slower but more durable\")\n",
    "print(\"     Use acks='all' + idempotence for critical data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Producer Performance Tuning\n",
    "\n",
    "### Key Producer Settings\n",
    "\n",
    "**Batching:**\n",
    "```\n",
    "Without Batching:              With Batching:\n",
    "[msg1] → Network               [msg1, msg2, msg3, msg4, msg5]\n",
    "[msg2] → Network                         ↓\n",
    "[msg3] → Network                      Network\n",
    "[msg4] → Network               (one network call)\n",
    "[msg5] → Network\n",
    "(5 network calls)\n",
    "```\n",
    "\n",
    "**Important Settings:**\n",
    "\n",
    "| Setting | Default | Description | Tuning |\n",
    "|---------|---------|-------------|--------|\n",
    "| `batch.size` | 16384 | Max batch size (bytes) | Increase for throughput |\n",
    "| `linger.ms` | 0 | Wait time before sending | Increase for batching |\n",
    "| `compression.type` | none | Compression algorithm | Use gzip or lz4 |\n",
    "| `buffer.memory` | 33554432 | Total buffer size | Increase for high volume |\n",
    "| `max.in.flight.requests.per.connection` | 5 | Parallel requests | Reduce for ordering |\n",
    "| `enable.idempotence` | false | Prevent duplicates | Set true for reliability |\n",
    "\n",
    "### Batching Strategy\n",
    "\n",
    "```\n",
    "Producer Buffer:\n",
    "┌─────────────────────────────────────┐\n",
    "│ Batch for Partition 0               │\n",
    "│ [msg1][msg2][msg3]...               │\n",
    "│                                     │\n",
    "│ Batch for Partition 1               │\n",
    "│ [msg10][msg11][msg12]...            │\n",
    "└─────────────────────────────────────┘\n",
    "\n",
    "Sends when:\n",
    "1. Batch reaches batch.size, OR\n",
    "2. linger.ms time expires\n",
    "\n",
    "Trade-off:\n",
    "- Higher linger.ms = Better batching, Higher latency\n",
    "- Lower linger.ms = Lower latency, Less batching\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare batching configurations\n",
    "def test_batching(batch_size, linger_ms, num_messages=1000):\n",
    "    \"\"\"Test different batching configurations\"\"\"\n",
    "    config = {\n",
    "        \"bootstrap.servers\": \"localhost:9092\",\n",
    "        \"acks\": \"1\",\n",
    "        \"batch.size\": batch_size,\n",
    "        \"linger.ms\": linger_ms,\n",
    "        \"compression.type\": \"gzip\",\n",
    "    }\n",
    "\n",
    "    producer = Producer(config)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in range(num_messages):\n",
    "        event = {\n",
    "            \"id\": i,\n",
    "            \"data\": \"x\" * 100,  # Some payload\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "        }\n",
    "        producer.produce(topic=TOPIC_NAME, value=json.dumps(event))\n",
    "        producer.poll(0)\n",
    "\n",
    "    producer.flush()\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    return elapsed, num_messages / elapsed\n",
    "\n",
    "\n",
    "print(\"[DATA] Testing batching configurations (1000 messages)...\\n\")\n",
    "\n",
    "# No batching\n",
    "elapsed1, throughput1 = test_batching(batch_size=1, linger_ms=0)\n",
    "print(f\"No batching (batch.size=1, linger.ms=0):\")\n",
    "print(f\"  Time: {elapsed1:.3f}s, Throughput: {throughput1:.0f} msg/s\")\n",
    "\n",
    "# Small batch, no wait\n",
    "elapsed2, throughput2 = test_batching(batch_size=16384, linger_ms=0)\n",
    "print(f\"\\nDefault batch (batch.size=16KB, linger.ms=0):\")\n",
    "print(f\"  Time: {elapsed2:.3f}s, Throughput: {throughput2:.0f} msg/s\")\n",
    "\n",
    "# Large batch with wait\n",
    "elapsed3, throughput3 = test_batching(batch_size=65536, linger_ms=10)\n",
    "print(f\"\\nOptimized (batch.size=64KB, linger.ms=10):\")\n",
    "print(f\"  Time: {elapsed3:.3f}s, Throughput: {throughput3:.0f} msg/s\")\n",
    "\n",
    "print(f\"\\n[OK] Batching improves throughput by {(throughput3/throughput1):.1f}x!\")\n",
    "print(\"     Trade-off: Adds ~10ms latency per message\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compression\n",
    "\n",
    "**Compression Algorithms:**\n",
    "\n",
    "| Algorithm | Compression Ratio | CPU Usage | Speed | Use Case |\n",
    "|-----------|------------------|-----------|-------|----------|\n",
    "| none | 1x | Lowest | Fastest | Low-latency, small messages |\n",
    "| gzip | 3-5x | High | Slow | Maximum compression |\n",
    "| snappy | 2-3x | Medium | Fast | Balanced |\n",
    "| lz4 | 2-3x | Low | Very Fast | High throughput |\n",
    "| zstd | 3-4x | Medium | Fast | Modern choice |\n",
    "\n",
    "**Benefits:**\n",
    "- Reduces network bandwidth\n",
    "- Reduces disk storage\n",
    "- Can improve throughput (less network I/O)\n",
    "\n",
    "**Trade-offs:**\n",
    "- CPU overhead on producer and consumer\n",
    "- Latency increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare compression types\n",
    "def test_compression(compression_type, num_messages=500):\n",
    "    \"\"\"Test different compression algorithms\"\"\"\n",
    "    config = {\n",
    "        \"bootstrap.servers\": \"localhost:9092\",\n",
    "        \"acks\": \"1\",\n",
    "        \"compression.type\": compression_type,\n",
    "        \"batch.size\": 65536,\n",
    "        \"linger.ms\": 10,\n",
    "    }\n",
    "\n",
    "    producer = Producer(config)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in range(num_messages):\n",
    "        # Create compressible data\n",
    "        event = {\n",
    "            \"id\": i,\n",
    "            \"data\": \"This is some repetitive text. \" * 20,  # Compressible\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "        }\n",
    "        producer.produce(topic=TOPIC_NAME, value=json.dumps(event))\n",
    "        producer.poll(0)\n",
    "\n",
    "    producer.flush()\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    return elapsed\n",
    "\n",
    "\n",
    "print(\"[DATA] Testing compression algorithms (500 messages)...\\n\")\n",
    "\n",
    "results = {}\n",
    "for compression in [\"none\", \"gzip\", \"snappy\", \"lz4\"]:\n",
    "    try:\n",
    "        elapsed = test_compression(compression)\n",
    "        results[compression] = elapsed\n",
    "        print(f\"{compression:10s}: {elapsed:.3f}s\")\n",
    "    except Exception as e:\n",
    "        print(f\"{compression:10s}: Not available - {e}\")\n",
    "\n",
    "if results:\n",
    "    best = min(results, key=results.get)\n",
    "    print(f\"\\n[OK] Best performance: {best}\")\n",
    "    print(\"     Recommendation: Use 'lz4' for best speed/compression balance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Idempotence and Transactions\n",
    "\n",
    "### The Duplicate Problem\n",
    "\n",
    "**Without Idempotence:**\n",
    "```\n",
    "Producer sends message → Network timeout\n",
    "Producer retries → Message written AGAIN\n",
    "\n",
    "Result: Duplicate messages!\n",
    "[msg1] [msg1] [msg2] [msg3] [msg3]\n",
    "       ↑dup        ↑dup\n",
    "```\n",
    "\n",
    "**With Idempotence:**\n",
    "```\n",
    "Producer sends message (seq=0) → Network timeout\n",
    "Producer retries (seq=0) → Broker detects duplicate, ignores\n",
    "\n",
    "Result: No duplicates!\n",
    "[msg1] [msg2] [msg3]\n",
    "```\n",
    "\n",
    "### Enabling Idempotence\n",
    "\n",
    "**Configuration:**\n",
    "```python\n",
    "config = {\n",
    "    'enable.idempotence': True,\n",
    "    'acks': 'all',  # Required for idempotence\n",
    "    'retries': 2147483647,  # Max retries\n",
    "    'max.in.flight.requests.per.connection': 5\n",
    "}\n",
    "```\n",
    "\n",
    "**How It Works:**\n",
    "- Producer assigns sequence numbers to messages\n",
    "- Broker tracks sequence numbers per producer\n",
    "- Duplicates are detected and discarded\n",
    "- Exactly-once semantics within a partition\n",
    "\n",
    "### Transactions\n",
    "\n",
    "**Use Case: Exactly-once across multiple partitions**\n",
    "```\n",
    "Transaction:\n",
    "  BEGIN\n",
    "    Write to topic A, partition 0\n",
    "    Write to topic B, partition 1\n",
    "    Write to topic C, partition 2\n",
    "  COMMIT\n",
    "\n",
    "Result: All writes succeed or all fail (atomic)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate idempotent producer\n",
    "print(\"[DATA] Comparing non-idempotent vs idempotent producers\\n\")\n",
    "\n",
    "# Non-idempotent producer\n",
    "non_idempotent_config = {\n",
    "    \"bootstrap.servers\": \"localhost:9092\",\n",
    "    \"acks\": \"1\",\n",
    "    \"enable.idempotence\": False,\n",
    "    \"retries\": 3,\n",
    "}\n",
    "\n",
    "# Idempotent producer\n",
    "idempotent_config = {\n",
    "    \"bootstrap.servers\": \"localhost:9092\",\n",
    "    \"enable.idempotence\": True,  # Automatically sets acks='all'\n",
    "    \"retries\": 10,\n",
    "    \"max.in.flight.requests.per.connection\": 5,\n",
    "}\n",
    "\n",
    "producer = Producer(idempotent_config)\n",
    "\n",
    "print(\"[OK] Created idempotent producer\")\n",
    "print(\"     Guarantees: No duplicates, ordering preserved\")\n",
    "print(\"     Use case: Financial transactions, order processing\\n\")\n",
    "\n",
    "# Send some events\n",
    "for i in range(10):\n",
    "    event = {\n",
    "        \"transaction_id\": f\"txn_{i}\",\n",
    "        \"amount\": random.randint(100, 1000),\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "    }\n",
    "    producer.produce(topic=TOPIC_NAME, key=f\"txn_{i}\", value=json.dumps(event))\n",
    "\n",
    "producer.flush()\n",
    "print(\"[SUCCESS] Sent 10 transactions with exactly-once guarantees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Consumer Performance Tuning\n",
    "\n",
    "### Key Consumer Settings\n",
    "\n",
    "| Setting | Default | Description | Tuning |\n",
    "|---------|---------|-------------|--------|\n",
    "| `fetch.min.bytes` | 1 | Min data to fetch | Increase for batching |\n",
    "| `fetch.max.wait.ms` | 500 | Max wait time | Balance latency/throughput |\n",
    "| `max.partition.fetch.bytes` | 1048576 | Max per partition | Increase for large messages |\n",
    "| `session.timeout.ms` | 10000 | Consumer heartbeat timeout | Increase for slow processing |\n",
    "| `max.poll.interval.ms` | 300000 | Max time between polls | Increase for long processing |\n",
    "| `enable.auto.commit` | true | Auto commit offsets | Disable for manual control |\n",
    "\n",
    "### Fetch Behavior\n",
    "\n",
    "```\n",
    "Consumer Fetch Request:\n",
    "┌────────────────────────────────────┐\n",
    "│ fetch.min.bytes = 1 KB             │\n",
    "│ fetch.max.wait.ms = 500 ms         │\n",
    "└────────────────────────────────────┘\n",
    "         ↓\n",
    "Returns when:\n",
    "1. Has 1 KB of data, OR\n",
    "2. 500 ms timeout expires\n",
    "\n",
    "Optimization:\n",
    "- Increase fetch.min.bytes for better batching\n",
    "- Increase fetch.max.wait.ms for higher throughput\n",
    "- Decrease for lower latency\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test consumer fetch configurations\n",
    "def test_consumer_fetch(fetch_min_bytes, fetch_max_wait_ms, num_messages=100):\n",
    "    \"\"\"Test different fetch configurations\"\"\"\n",
    "    config = {\n",
    "        \"bootstrap.servers\": \"localhost:9092\",\n",
    "        \"group.id\": f\"test-fetch-{fetch_min_bytes}-{fetch_max_wait_ms}\",\n",
    "        \"auto.offset.reset\": \"earliest\",\n",
    "        \"fetch.min.bytes\": fetch_min_bytes,\n",
    "        \"fetch.max.wait.ms\": fetch_max_wait_ms,\n",
    "    }\n",
    "\n",
    "    consumer = Consumer(config)\n",
    "    consumer.subscribe([TOPIC_NAME])\n",
    "\n",
    "    messages_read = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        while messages_read < num_messages:\n",
    "            msg = consumer.poll(timeout=2.0)\n",
    "            if msg is None:\n",
    "                break\n",
    "            if msg.error():\n",
    "                continue\n",
    "            messages_read += 1\n",
    "    finally:\n",
    "        consumer.close()\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    return elapsed, messages_read / elapsed if elapsed > 0 else 0\n",
    "\n",
    "\n",
    "print(\"[DATA] Testing consumer fetch configurations...\\n\")\n",
    "\n",
    "# Low latency\n",
    "elapsed1, throughput1 = test_consumer_fetch(1, 100, 100)\n",
    "print(f\"Low latency (min=1B, wait=100ms):\")\n",
    "print(f\"  Time: {elapsed1:.3f}s, Throughput: {throughput1:.0f} msg/s\")\n",
    "\n",
    "# High throughput\n",
    "elapsed2, throughput2 = test_consumer_fetch(10240, 500, 100)\n",
    "print(f\"\\nHigh throughput (min=10KB, wait=500ms):\")\n",
    "print(f\"  Time: {elapsed2:.3f}s, Throughput: {throughput2:.0f} msg/s\")\n",
    "\n",
    "print(\"\\n[OK] Higher fetch.min.bytes reduces number of fetch requests\")\n",
    "print(\"     Trade-off: Slightly higher latency for better throughput\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consumer Rebalancing\n",
    "\n",
    "**What is Rebalancing?**\n",
    "```\n",
    "Before (2 consumers, 4 partitions):\n",
    "Consumer 1: [P0, P1]\n",
    "Consumer 2: [P2, P3]\n",
    "\n",
    "Consumer 3 joins →  REBALANCE\n",
    "\n",
    "After (3 consumers, 4 partitions):\n",
    "Consumer 1: [P0]\n",
    "Consumer 2: [P1, P2]\n",
    "Consumer 3: [P3]\n",
    "```\n",
    "\n",
    "**Rebalancing Process:**\n",
    "1. Consumer joins/leaves group\n",
    "2. All consumers **STOP** processing\n",
    "3. Partition assignment recalculated\n",
    "4. Consumers resume with new assignments\n",
    "\n",
    "**Minimizing Rebalance Impact:**\n",
    "- Increase `session.timeout.ms` (slow networks)\n",
    "- Increase `max.poll.interval.ms` (slow processing)\n",
    "- Use incremental cooperative rebalancing (Kafka 2.4+)\n",
    "- Keep consumer group stable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Monitoring Kafka\n",
    "\n",
    "### Key Metrics to Monitor\n",
    "\n",
    "**Producer Metrics:**\n",
    "- `record-send-rate`: Messages produced per second\n",
    "- `record-error-rate`: Failed sends\n",
    "- `request-latency-avg`: Average request latency\n",
    "- `batch-size-avg`: Average batch size\n",
    "- `compression-rate-avg`: Compression efficiency\n",
    "\n",
    "**Consumer Metrics:**\n",
    "- `records-consumed-rate`: Messages consumed per second\n",
    "- `records-lag`: How far behind (critical!)\n",
    "- `fetch-latency-avg`: Average fetch latency\n",
    "- `commit-latency-avg`: Offset commit latency\n",
    "\n",
    "**Broker Metrics:**\n",
    "- `UnderReplicatedPartitions`: Partitions not fully replicated\n",
    "- `OfflinePartitionsCount`: Partitions without leader\n",
    "- `RequestsPerSecond`: Total request rate\n",
    "- `NetworkProcessorAvgIdlePercent`: Network thread idle %\n",
    "\n",
    "### Consumer Lag\n",
    "\n",
    "**Most Important Metric:**\n",
    "```\n",
    "Consumer Lag = Latest Offset - Consumer Offset\n",
    "\n",
    "Partition: [0][1][2][3][4][5][6][7][8][9]\n",
    "                         ↑              ↑\n",
    "                    Consumer      Latest Offset\n",
    "                    (offset 4)      (offset 9)\n",
    "                    \n",
    "Lag = 9 - 4 = 5 messages behind\n",
    "\n",
    "Good: Lag = 0-1000 (keeping up)\n",
    "Warning: Lag growing over time\n",
    "Critical: Lag > millions (falling behind)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor consumer lag\n",
    "from confluent_kafka.admin import AdminClient\n",
    "\n",
    "\n",
    "def get_consumer_lag(group_id, topic):\n",
    "    \"\"\"Calculate consumer lag for a group\"\"\"\n",
    "    admin = AdminClient({\"bootstrap.servers\": \"localhost:9092\"})\n",
    "\n",
    "    # Get committed offsets for group\n",
    "    consumer = Consumer({\"bootstrap.servers\": \"localhost:9092\", \"group.id\": group_id})\n",
    "\n",
    "    # Get topic metadata\n",
    "    metadata = admin.list_topics(timeout=5)\n",
    "\n",
    "    if topic not in metadata.topics:\n",
    "        print(f\"[WARNING] Topic '{topic}' not found\")\n",
    "        return\n",
    "\n",
    "    topic_metadata = metadata.topics[topic]\n",
    "\n",
    "    print(f\"\\n[DATA] Consumer Lag Analysis for group '{group_id}':\\n\")\n",
    "\n",
    "    for partition_id in topic_metadata.partitions:\n",
    "        # Get high water mark (latest offset)\n",
    "        low, high = consumer.get_watermark_offsets(topic=topic, partition=partition_id, timeout=5)\n",
    "\n",
    "        print(f\"Partition {partition_id}:\")\n",
    "        print(f\"  Low offset: {low}\")\n",
    "        print(f\"  High offset: {high}\")\n",
    "        print(f\"  Total messages: {high - low}\")\n",
    "\n",
    "    consumer.close()\n",
    "\n",
    "\n",
    "# Check lag for our test consumers\n",
    "get_consumer_lag(\"user-events-consumer-group\", TOPIC_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Kafka UI for Monitoring\n",
    "\n",
    "**Access Kafka UI:**\n",
    "- URL: http://localhost:8080\n",
    "- View topics, partitions, messages\n",
    "- Monitor consumer groups and lag\n",
    "- Inspect message contents\n",
    "\n",
    "**Key Screens:**\n",
    "1. **Topics**: See all topics, partition count, size\n",
    "2. **Consumers**: View consumer groups, lag, members\n",
    "3. **Brokers**: Monitor broker health, disk usage\n",
    "4. **Messages**: Browse message contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Troubleshooting Common Issues\n",
    "\n",
    "### Issue 1: High Consumer Lag\n",
    "\n",
    "**Symptoms:**\n",
    "- Consumer lag growing over time\n",
    "- Processing falling behind production\n",
    "\n",
    "**Causes & Solutions:**\n",
    "```\n",
    "Cause 1: Slow Processing\n",
    "  → Add more consumers (up to partition count)\n",
    "  → Optimize processing logic\n",
    "  → Increase processing parallelism\n",
    "\n",
    "Cause 2: Not Enough Partitions\n",
    "  → Increase partition count\n",
    "  → Add more consumers\n",
    "\n",
    "Cause 3: Network Issues\n",
    "  → Increase fetch.min.bytes\n",
    "  → Increase max.partition.fetch.bytes\n",
    "  → Check network bandwidth\n",
    "```\n",
    "\n",
    "### Issue 2: Rebalancing Too Frequently\n",
    "\n",
    "**Symptoms:**\n",
    "- Consumers constantly rebalancing\n",
    "- Processing pauses\n",
    "\n",
    "**Solutions:**\n",
    "```python\n",
    "config = {\n",
    "    'session.timeout.ms': 30000,  # Increase from 10s\n",
    "    'max.poll.interval.ms': 600000,  # Increase from 5m\n",
    "    'heartbeat.interval.ms': 3000  # 1/3 of session timeout\n",
    "}\n",
    "```\n",
    "\n",
    "### Issue 3: Message Loss\n",
    "\n",
    "**Prevention:**\n",
    "```python\n",
    "# Producer settings\n",
    "producer_config = {\n",
    "    'acks': 'all',  # Wait for all replicas\n",
    "    'enable.idempotence': True,  # Prevent duplicates\n",
    "    'retries': 10,  # Retry on failure\n",
    "    'max.in.flight.requests.per.connection': 5\n",
    "}\n",
    "\n",
    "# Topic settings\n",
    "topic_config = {\n",
    "    'replication.factor': 3,  # 3 copies\n",
    "    'min.insync.replicas': 2  # Require 2 acks\n",
    "}\n",
    "```\n",
    "\n",
    "### Issue 4: Slow Producers\n",
    "\n",
    "**Optimizations:**\n",
    "```python\n",
    "config = {\n",
    "    'batch.size': 65536,  # 64 KB batches\n",
    "    'linger.ms': 10,  # Wait 10ms for batching\n",
    "    'compression.type': 'lz4',  # Fast compression\n",
    "    'buffer.memory': 67108864,  # 64 MB buffer\n",
    "    'acks': '1'  # Only leader ack (if acceptable)\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate optimized producer configuration\n",
    "print(\"[DATA] Production-Ready Configuration Examples\\n\")\n",
    "\n",
    "# High-throughput producer\n",
    "high_throughput_config = {\n",
    "    \"bootstrap.servers\": \"localhost:9092\",\n",
    "    # Batching\n",
    "    \"batch.size\": 65536,  # 64 KB\n",
    "    \"linger.ms\": 10,\n",
    "    # Compression\n",
    "    \"compression.type\": \"lz4\",\n",
    "    # Memory\n",
    "    \"buffer.memory\": 67108864,  # 64 MB\n",
    "    # Reliability (medium)\n",
    "    \"acks\": \"1\",\n",
    "    \"retries\": 3,\n",
    "}\n",
    "\n",
    "print(\"High-Throughput Producer:\")\n",
    "for key, value in high_throughput_config.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# High-reliability producer\n",
    "high_reliability_config = {\n",
    "    \"bootstrap.servers\": \"localhost:9092\",\n",
    "    # Reliability (maximum)\n",
    "    \"acks\": \"all\",\n",
    "    \"enable.idempotence\": True,\n",
    "    \"retries\": 10,\n",
    "    \"max.in.flight.requests.per.connection\": 5,\n",
    "    # Batching (moderate)\n",
    "    \"batch.size\": 16384,\n",
    "    \"linger.ms\": 5,\n",
    "    # Compression\n",
    "    \"compression.type\": \"gzip\",\n",
    "}\n",
    "\n",
    "print(\"\\nHigh-Reliability Producer:\")\n",
    "for key, value in high_reliability_config.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Low-latency producer\n",
    "low_latency_config = {\n",
    "    \"bootstrap.servers\": \"localhost:9092\",\n",
    "    # Latency (minimize)\n",
    "    \"linger.ms\": 0,\n",
    "    \"batch.size\": 1,\n",
    "    \"compression.type\": \"none\",\n",
    "    # Reliability (basic)\n",
    "    \"acks\": \"1\",\n",
    "    \"retries\": 0,\n",
    "}\n",
    "\n",
    "print(\"\\nLow-Latency Producer:\")\n",
    "for key, value in low_latency_config.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\n[OK] Choose configuration based on your requirements:\")\n",
    "print(\"     - High throughput: Batching + compression\")\n",
    "print(\"     - High reliability: acks=all + idempotence\")\n",
    "print(\"     - Low latency: No batching, no compression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Mini-Project: Performance Benchmarking\n",
    "\n",
    "Let's build a benchmarking tool to test different configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive benchmarking tool\n",
    "import statistics\n",
    "\n",
    "\n",
    "def benchmark_producer(config_name, config, num_messages=1000):\n",
    "    \"\"\"Benchmark producer with given configuration\"\"\"\n",
    "    producer = Producer(config)\n",
    "\n",
    "    latencies = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in range(num_messages):\n",
    "        msg_start = time.time()\n",
    "\n",
    "        event = {\"id\": i, \"data\": \"x\" * 100, \"timestamp\": datetime.now().isoformat()}\n",
    "\n",
    "        producer.produce(topic=TOPIC_NAME, value=json.dumps(event))\n",
    "        producer.poll(0)\n",
    "\n",
    "        msg_latency = (time.time() - msg_start) * 1000  # ms\n",
    "        latencies.append(msg_latency)\n",
    "\n",
    "    producer.flush()\n",
    "    total_time = time.time() - start_time\n",
    "\n",
    "    return {\n",
    "        \"config\": config_name,\n",
    "        \"total_time\": total_time,\n",
    "        \"throughput\": num_messages / total_time,\n",
    "        \"avg_latency\": statistics.mean(latencies),\n",
    "        \"p50_latency\": statistics.median(latencies),\n",
    "        \"p99_latency\": sorted(latencies)[int(len(latencies) * 0.99)],\n",
    "    }\n",
    "\n",
    "\n",
    "# Test configurations\n",
    "configs = {\n",
    "    \"Default\": {\"bootstrap.servers\": \"localhost:9092\", \"acks\": \"1\"},\n",
    "    \"Optimized\": {\n",
    "        \"bootstrap.servers\": \"localhost:9092\",\n",
    "        \"acks\": \"1\",\n",
    "        \"batch.size\": 65536,\n",
    "        \"linger.ms\": 10,\n",
    "        \"compression.type\": \"lz4\",\n",
    "    },\n",
    "    \"Reliable\": {\n",
    "        \"bootstrap.servers\": \"localhost:9092\",\n",
    "        \"acks\": \"all\",\n",
    "        \"enable.idempotence\": True,\n",
    "        \"compression.type\": \"gzip\",\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"[DATA] Running performance benchmarks (1000 messages each)...\\n\")\n",
    "\n",
    "results = []\n",
    "for config_name, config in configs.items():\n",
    "    print(f\"Testing {config_name}...\")\n",
    "    result = benchmark_producer(config_name, config)\n",
    "    results.append(result)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n[DATA] Benchmark Results:\\n\")\n",
    "print(f\"{'Config':<12} {'Time (s)':<10} {'Throughput':<12} {'Avg Lat':<10} {'P99 Lat':<10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for r in results:\n",
    "    print(\n",
    "        f\"{r['config']:<12} {r['total_time']:<10.2f} {r['throughput']:<12.0f} \"\n",
    "        f\"{r['avg_latency']:<10.2f} {r['p99_latency']:<10.2f}\"\n",
    "    )\n",
    "\n",
    "print(\"\\n[SUCCESS] Benchmark complete!\")\n",
    "print(\"\\n[OK] Key Insights:\")\n",
    "print(\"     - Optimized config has highest throughput (batching + compression)\")\n",
    "print(\"     - Reliable config has lower throughput but no data loss\")\n",
    "print(\"     - Choose based on your requirements (speed vs reliability)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Key Takeaways\n",
    "\n",
    "[OK] **Log Structure**: Kafka uses append-only logs with segments for efficient storage\n",
    "\n",
    "[OK] **Replication**: Leader-follower model with ISR for fault tolerance\n",
    "\n",
    "[OK] **Producer Tuning**: Batching, compression, and acks for optimal performance\n",
    "\n",
    "[OK] **Idempotence**: Prevents duplicates with sequence numbers\n",
    "\n",
    "[OK] **Consumer Tuning**: Fetch size, poll interval, and rebalancing\n",
    "\n",
    "[OK] **Monitoring**: Consumer lag is the most critical metric\n",
    "\n",
    "### Configuration Cheat Sheet\n",
    "\n",
    "**For Maximum Throughput:**\n",
    "```python\n",
    "{\n",
    "    'batch.size': 65536,\n",
    "    'linger.ms': 10-100,\n",
    "    'compression.type': 'lz4',\n",
    "    'acks': '1'\n",
    "}\n",
    "```\n",
    "\n",
    "**For Maximum Reliability:**\n",
    "```python\n",
    "{\n",
    "    'acks': 'all',\n",
    "    'enable.idempotence': True,\n",
    "    'retries': 10,\n",
    "    'min.insync.replicas': 2\n",
    "}\n",
    "```\n",
    "\n",
    "**For Minimum Latency:**\n",
    "```python\n",
    "{\n",
    "    'linger.ms': 0,\n",
    "    'batch.size': 1,\n",
    "    'compression.type': 'none',\n",
    "    'acks': '1'\n",
    "}\n",
    "```\n",
    "\n",
    "### Production Best Practices\n",
    "\n",
    "1. **Always use `enable.idempotence=True`** for reliability\n",
    "2. **Monitor consumer lag** continuously\n",
    "3. **Set replication factor ≥ 3** in production\n",
    "4. **Use compression** (lz4 or gzip) to save bandwidth\n",
    "5. **Tune batch.size and linger.ms** based on workload\n",
    "6. **Plan partition count** for future growth\n",
    "7. **Test configurations** with your actual workload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Practice Exercises\n",
    "\n",
    "1. **Create a topic** with replication factor 1 and test producer with different acks settings\n",
    "2. **Benchmark** producer performance with different batch sizes (1KB, 16KB, 64KB)\n",
    "3. **Test compression** algorithms with your actual message payload\n",
    "4. **Monitor consumer lag** and observe what happens when you slow down processing\n",
    "5. **Implement** an idempotent producer with error handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your practice code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Next Steps\n",
    "\n",
    "Congratulations on completing Module 02!\n",
    "\n",
    "### What You've Learned\n",
    "\n",
    "- [OK] Kafka's internal log structure and storage\n",
    "- [OK] Replication and fault tolerance mechanisms\n",
    "- [OK] Producer and consumer performance tuning\n",
    "- [OK] Idempotence and exactly-once semantics\n",
    "- [OK] Monitoring and troubleshooting\n",
    "\n",
    "### Coming Up in Module 03: Stream Processing Fundamentals\n",
    "\n",
    "You'll learn:\n",
    "- What is stream processing?\n",
    "- Stateless vs stateful operations\n",
    "- Windowing concepts (tumbling, sliding, session)\n",
    "- Time semantics (event time, processing time)\n",
    "- Building your first stream processor\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Kafka Performance Tuning](https://kafka.apache.org/documentation/#producerconfigs)\n",
    "- [Kafka Internals](https://kafka.apache.org/documentation/#design)\n",
    "- [Monitoring Kafka](https://docs.confluent.io/platform/current/kafka/monitoring.html)\n",
    "- [Kafka Operations Guide](https://kafka.apache.org/documentation/#operations)\n",
    "\n",
    "---\n",
    "\n",
    "**Ready for stream processing?** Open `03_stream_processing_fundamentals.ipynb` to continue!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
