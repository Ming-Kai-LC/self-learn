{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 06: State Management and Checkpointing\n",
    "\n",
    "**Estimated Time:** 75 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this module, you will:\n",
    "- Understand Flink's state backends and storage options\n",
    "- Configure and tune checkpointing for fault tolerance\n",
    "- Use savepoints for job migration and upgrades\n",
    "- Handle state schema evolution\n",
    "- Monitor and optimize state size\n",
    "- Implement queryable state patterns\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding State in Flink\n",
    "\n",
    "### What is State?\n",
    "\n",
    "**State** = Data maintained across events for stateful operations\n",
    "\n",
    "```\n",
    "Example: Running Count\n",
    "\n",
    "Event 1: value=5  → state=5   (count: 1)\n",
    "Event 2: value=3  → state=8   (count: 2)\n",
    "Event 3: value=7  → state=15  (count: 3)\n",
    "         ↑                ↑\n",
    "      Input           State (remembered)\n",
    "```\n",
    "\n",
    "### Types of State\n",
    "\n",
    "**1. Keyed State** (most common):\n",
    "```\n",
    "After key_by(), each key has its own state\n",
    "\n",
    "key='user1' → state_1\n",
    "key='user2' → state_2\n",
    "key='user3' → state_3\n",
    "\n",
    "Isolated: Changes to one key don't affect others\n",
    "```\n",
    "\n",
    "**2. Operator State**:\n",
    "```\n",
    "State scoped to operator instance (not keyed)\n",
    "\n",
    "Use cases:\n",
    "- Kafka source offsets\n",
    "- Buffering elements\n",
    "```\n",
    "\n",
    "### State Primitives\n",
    "\n",
    "| Type | Description | Use Case |\n",
    "|------|-------------|----------|\n",
    "| ValueState | Single value | Last seen value |\n",
    "| ListState | List of values | Event history |\n",
    "| MapState | Key-value map | Counts by category |\n",
    "| ReducingState | Reduced values | Running sum |\n",
    "| AggregatingState | Aggregated result | Custom aggregation |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "print(\"[OK] Ready for state management examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate different state types\n",
    "class StatefulProcessor:\n",
    "    \"\"\"Demonstrates different types of state\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # ValueState: single value per key\n",
    "        self.value_state = {}  # key -> value\n",
    "\n",
    "        # ListState: list of values per key\n",
    "        self.list_state = defaultdict(list)  # key -> [values]\n",
    "\n",
    "        # MapState: nested map per key\n",
    "        self.map_state = defaultdict(dict)  # key -> {subkey: value}\n",
    "\n",
    "        # ReducingState: accumulated value\n",
    "        self.reducing_state = defaultdict(int)  # key -> sum\n",
    "\n",
    "    def update_value_state(self, key, value):\n",
    "        \"\"\"Update single value\"\"\"\n",
    "        self.value_state[key] = value\n",
    "\n",
    "    def append_list_state(self, key, value):\n",
    "        \"\"\"Append to list (keep last 5)\"\"\"\n",
    "        self.list_state[key].append(value)\n",
    "        # Keep only last 5\n",
    "        if len(self.list_state[key]) > 5:\n",
    "            self.list_state[key] = self.list_state[key][-5:]\n",
    "\n",
    "    def update_map_state(self, key, subkey, value):\n",
    "        \"\"\"Update nested map\"\"\"\n",
    "        self.map_state[key][subkey] = value\n",
    "\n",
    "    def add_reducing_state(self, key, value):\n",
    "        \"\"\"Add to running sum\"\"\"\n",
    "        self.reducing_state[key] += value\n",
    "\n",
    "    def get_state_size(self):\n",
    "        \"\"\"Estimate state size in bytes\"\"\"\n",
    "        total_size = 0\n",
    "        total_size += len(pickle.dumps(self.value_state))\n",
    "        total_size += len(pickle.dumps(self.list_state))\n",
    "        total_size += len(pickle.dumps(self.map_state))\n",
    "        total_size += len(pickle.dumps(self.reducing_state))\n",
    "        return total_size\n",
    "\n",
    "    def print_state(self):\n",
    "        \"\"\"Print current state\"\"\"\n",
    "        print(\"\\n[DATA] Current State:\\n\")\n",
    "        print(f\"ValueState (last value):\")\n",
    "        for key, value in list(self.value_state.items())[:3]:\n",
    "            print(f\"  {key}: {value}\")\n",
    "\n",
    "        print(f\"\\nListState (recent values):\")\n",
    "        for key, values in list(self.list_state.items())[:2]:\n",
    "            print(f\"  {key}: {values}\")\n",
    "\n",
    "        print(f\"\\nMapState (nested):\")\n",
    "        for key, nested in list(self.map_state.items())[:2]:\n",
    "            print(f\"  {key}: {nested}\")\n",
    "\n",
    "        print(f\"\\nReducingState (sum):\")\n",
    "        for key, total in list(self.reducing_state.items())[:3]:\n",
    "            print(f\"  {key}: {total}\")\n",
    "\n",
    "        print(f\"\\nState size: {self.get_state_size()} bytes\")\n",
    "\n",
    "\n",
    "# Test state types\n",
    "processor = StatefulProcessor()\n",
    "\n",
    "print(\"[OK] Processing events with different state types...\\n\")\n",
    "\n",
    "for i in range(20):\n",
    "    user_id = f\"user_{random.randint(1, 3)}\"\n",
    "    value = random.randint(1, 10)\n",
    "    action = random.choice([\"view\", \"click\", \"purchase\"])\n",
    "\n",
    "    # Update different state types\n",
    "    processor.update_value_state(user_id, value)\n",
    "    processor.append_list_state(user_id, value)\n",
    "    processor.update_map_state(user_id, action, value)\n",
    "    processor.add_reducing_state(user_id, value)\n",
    "\n",
    "processor.print_state()\n",
    "print(\"\\n[SUCCESS] State management demonstration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. State Backends\n",
    "\n",
    "### Types of State Backends\n",
    "\n",
    "**1. MemoryStateBackend** (Development only):\n",
    "```\n",
    "Storage: JVM heap\n",
    "Checkpoints: JobManager memory\n",
    "\n",
    "Pros:\n",
    "- Fast (in-memory)\n",
    "- Simple setup\n",
    "\n",
    "Cons:\n",
    "- Limited by heap size\n",
    "- Lost on failure\n",
    "- Not for production!\n",
    "```\n",
    "\n",
    "**2. FsStateBackend**:\n",
    "```\n",
    "Storage: JVM heap\n",
    "Checkpoints: Distributed file system (HDFS, S3)\n",
    "\n",
    "Pros:\n",
    "- Fast access\n",
    "- Durable checkpoints\n",
    "\n",
    "Cons:\n",
    "- Limited by heap size\n",
    "- State < few GB per operator\n",
    "```\n",
    "\n",
    "**3. RocksDBStateBackend** (Production recommended):\n",
    "```\n",
    "Storage: RocksDB (embedded KV store)\n",
    "Checkpoints: Distributed file system\n",
    "\n",
    "Pros:\n",
    "- Scales to TB of state\n",
    "- Incremental checkpoints\n",
    "- Spills to disk\n",
    "\n",
    "Cons:\n",
    "- Slower than heap\n",
    "- Serialization overhead\n",
    "```\n",
    "\n",
    "### Configuration\n",
    "\n",
    "```python\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.datastream.state_backend import RocksDBStateBackend\n",
    "\n",
    "env = StreamExecutionEnvironment.get_execution_environment()\n",
    "\n",
    "# Configure RocksDB backend\n",
    "env.set_state_backend(\n",
    "    RocksDBStateBackend(\n",
    "        checkpoint_path='file:///tmp/checkpoints',\n",
    "        enable_incremental_checkpointing=True\n",
    "    )\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Checkpointing\n",
    "\n",
    "### What is Checkpointing?\n",
    "\n",
    "**Periodic snapshots** of all operator state for fault tolerance:\n",
    "\n",
    "```\n",
    "Processing Timeline:\n",
    "\n",
    "10:00 ── Events ── 10:01 ── Events ── 10:02 ── Events ── 10:03\n",
    "          ↓                   ↓                   ↓\n",
    "      Checkpoint 1       Checkpoint 2       Checkpoint 3\n",
    "      (state saved)      (state saved)      (state saved)\n",
    "\n",
    "Failure at 10:02:30:\n",
    "  ↓\n",
    "Restore from Checkpoint 2 (10:02)\n",
    "Replay events from 10:02 to 10:02:30\n",
    "  ↓\n",
    "Resume normal processing\n",
    "```\n",
    "\n",
    "### Checkpoint Process\n",
    "\n",
    "```\n",
    "1. JobManager triggers checkpoint\n",
    "2. Barrier injected into stream\n",
    "3. Operators align barriers\n",
    "4. Take state snapshot\n",
    "5. Acknowledge to JobManager\n",
    "6. Checkpoint complete!\n",
    "```\n",
    "\n",
    "### Configuration\n",
    "\n",
    "```python\n",
    "# Enable checkpointing\n",
    "env.enable_checkpointing(10000)  # Every 10 seconds\n",
    "\n",
    "# Configure checkpoint behavior\n",
    "checkpoint_config = env.get_checkpoint_config()\n",
    "checkpoint_config.set_min_pause_between_checkpoints(5000)\n",
    "checkpoint_config.set_checkpoint_timeout(60000)\n",
    "checkpoint_config.set_max_concurrent_checkpoints(1)\n",
    "\n",
    "# Cleanup policy\n",
    "from pyflink.datastream import ExternalizedCheckpointCleanup\n",
    "checkpoint_config.enable_externalized_checkpoints(\n",
    "    ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate checkpointing\n",
    "class CheckpointManager:\n",
    "    \"\"\"Manages checkpoints for fault tolerance\"\"\"\n",
    "\n",
    "    def __init__(self, checkpoint_interval_seconds=5):\n",
    "        self.interval = checkpoint_interval_seconds\n",
    "        self.checkpoints = []  # List of checkpoints\n",
    "        self.last_checkpoint_time = None\n",
    "\n",
    "    def should_checkpoint(self, current_time):\n",
    "        \"\"\"Check if it's time to checkpoint\"\"\"\n",
    "        if self.last_checkpoint_time is None:\n",
    "            return True\n",
    "\n",
    "        elapsed = (current_time - self.last_checkpoint_time).total_seconds()\n",
    "        return elapsed >= self.interval\n",
    "\n",
    "    def create_checkpoint(self, state, timestamp):\n",
    "        \"\"\"Create checkpoint snapshot\"\"\"\n",
    "        checkpoint = {\n",
    "            \"id\": len(self.checkpoints) + 1,\n",
    "            \"timestamp\": timestamp,\n",
    "            \"state\": pickle.dumps(state),  # Serialize state\n",
    "            \"size_bytes\": len(pickle.dumps(state)),\n",
    "        }\n",
    "\n",
    "        self.checkpoints.append(checkpoint)\n",
    "        self.last_checkpoint_time = timestamp\n",
    "\n",
    "        return checkpoint[\"id\"]\n",
    "\n",
    "    def restore_from_checkpoint(self, checkpoint_id):\n",
    "        \"\"\"Restore state from checkpoint\"\"\"\n",
    "        for checkpoint in self.checkpoints:\n",
    "            if checkpoint[\"id\"] == checkpoint_id:\n",
    "                return pickle.loads(checkpoint[\"state\"])\n",
    "        return None\n",
    "\n",
    "    def get_latest_checkpoint(self):\n",
    "        \"\"\"Get most recent checkpoint\"\"\"\n",
    "        if self.checkpoints:\n",
    "            return self.checkpoints[-1]\n",
    "        return None\n",
    "\n",
    "    def print_checkpoints(self):\n",
    "        \"\"\"Print checkpoint history\"\"\"\n",
    "        print(\"\\n[DATA] Checkpoint History:\\n\")\n",
    "        for cp in self.checkpoints:\n",
    "            print(f\"Checkpoint {cp['id']}:\")\n",
    "            print(f\"  Time: {cp['timestamp'].strftime('%H:%M:%S')}\")\n",
    "            print(f\"  Size: {cp['size_bytes']} bytes\")\n",
    "\n",
    "\n",
    "# Simulate processing with checkpoints\n",
    "class ProcessorWithCheckpoints:\n",
    "    \"\"\"Processor that creates periodic checkpoints\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.state = defaultdict(int)  # user_id -> count\n",
    "        self.checkpoint_manager = CheckpointManager(checkpoint_interval_seconds=3)\n",
    "        self.events_processed = 0\n",
    "\n",
    "    def process_event(self, event, timestamp):\n",
    "        \"\"\"Process event and checkpoint if needed\"\"\"\n",
    "        # Update state\n",
    "        self.state[event[\"user_id\"]] += 1\n",
    "        self.events_processed += 1\n",
    "\n",
    "        # Check if checkpoint needed\n",
    "        if self.checkpoint_manager.should_checkpoint(timestamp):\n",
    "            cp_id = self.checkpoint_manager.create_checkpoint(dict(self.state), timestamp)\n",
    "            print(f\"  [Checkpoint {cp_id} created at {timestamp.strftime('%H:%M:%S')}]\")\n",
    "\n",
    "    def simulate_failure_and_recovery(self):\n",
    "        \"\"\"Simulate failure and restore from checkpoint\"\"\"\n",
    "        print(\"\\n[WARNING] Simulating failure...\")\n",
    "        print(f\"State before failure: {dict(list(self.state.items())[:3])}\")\n",
    "\n",
    "        # Get latest checkpoint\n",
    "        latest_cp = self.checkpoint_manager.get_latest_checkpoint()\n",
    "\n",
    "        if latest_cp:\n",
    "            print(f\"\\n[OK] Restoring from checkpoint {latest_cp['id']}...\")\n",
    "            restored_state = self.checkpoint_manager.restore_from_checkpoint(latest_cp[\"id\"])\n",
    "            self.state = defaultdict(int, restored_state)\n",
    "            print(f\"State after recovery: {dict(list(self.state.items())[:3])}\")\n",
    "\n",
    "\n",
    "# Test checkpointing\n",
    "processor = ProcessorWithCheckpoints()\n",
    "\n",
    "print(\"[OK] Processing events with periodic checkpoints...\\n\")\n",
    "\n",
    "base_time = datetime.now()\n",
    "for i in range(20):\n",
    "    event = {\"user_id\": f\"user_{random.randint(1, 5)}\", \"action\": \"click\"}\n",
    "    timestamp = base_time + timedelta(seconds=i * 0.5)\n",
    "\n",
    "    processor.process_event(event, timestamp)\n",
    "\n",
    "    if i % 5 == 0:\n",
    "        print(f\"Processed {i} events...\")\n",
    "\n",
    "processor.checkpoint_manager.print_checkpoints()\n",
    "processor.simulate_failure_and_recovery()\n",
    "\n",
    "print(\"\\n[SUCCESS] Checkpointing enables fault tolerance!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Savepoints\n",
    "\n",
    "### Savepoints vs Checkpoints\n",
    "\n",
    "| Feature | Checkpoint | Savepoint |\n",
    "|---------|-----------|------------|\n",
    "| Purpose | Auto recovery | Manual snapshot |\n",
    "| Triggered by | Flink | User |\n",
    "| Lifetime | Temporary | Permanent |\n",
    "| Use case | Fault tolerance | Upgrade, migration |\n",
    "\n",
    "### Savepoint Use Cases\n",
    "\n",
    "**1. Job Upgrades:**\n",
    "```\n",
    "1. Take savepoint of running job\n",
    "2. Cancel job\n",
    "3. Deploy new version\n",
    "4. Start from savepoint\n",
    "   → State preserved!\n",
    "```\n",
    "\n",
    "**2. Cluster Migration:**\n",
    "```\n",
    "1. Savepoint on Cluster A\n",
    "2. Copy savepoint to Cluster B\n",
    "3. Start job on Cluster B\n",
    "   → Seamless migration!\n",
    "```\n",
    "\n",
    "**3. A/B Testing:**\n",
    "```\n",
    "1. Savepoint from production\n",
    "2. Start test job from savepoint\n",
    "3. Compare results\n",
    "```\n",
    "\n",
    "### Creating Savepoints\n",
    "\n",
    "```bash\n",
    "# Trigger savepoint\n",
    "flink savepoint <jobId> [targetDirectory]\n",
    "\n",
    "# Start from savepoint\n",
    "flink run -s <savepointPath> <jobJar>\n",
    "\n",
    "# Dispose savepoint\n",
    "flink savepoint -d <savepointPath>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. State Schema Evolution\n",
    "\n",
    "### The Problem\n",
    "\n",
    "```\n",
    "Version 1:              Version 2:\n",
    "class UserState {       class UserState {\n",
    "  String name;            String name;\n",
    "  int age;                int age;\n",
    "}                         String email;  ← New field!\n",
    "                        }\n",
    "\n",
    "How to upgrade without losing state?\n",
    "```\n",
    "\n",
    "### Solutions\n",
    "\n",
    "**1. POJO Evolution:**\n",
    "```java\n",
    "// Adding fields: OK (default values)\n",
    "// Removing fields: OK (ignored)\n",
    "// Changing types: NOT OK!\n",
    "```\n",
    "\n",
    "**2. Avro Schema Evolution:**\n",
    "```\n",
    "Version 1: {name: string, age: int}\n",
    "Version 2: {name: string, age: int, email: string (default=\"\")}\n",
    "\n",
    "Avro handles:\n",
    "- New fields with defaults\n",
    "- Removed fields\n",
    "- Type promotions\n",
    "```\n",
    "\n",
    "**3. Custom Serializers:**\n",
    "```python\n",
    "# Implement TypeSerializer with version handling\n",
    "class VersionedSerializer:\n",
    "    def serialize(self, obj, version):\n",
    "        # Write version + data\n",
    "        pass\n",
    "    \n",
    "    def deserialize(self, data):\n",
    "        # Read version, migrate if needed\n",
    "        pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Monitoring State\n",
    "\n",
    "### Key Metrics\n",
    "\n",
    "**State Size:**\n",
    "```\n",
    "Track: Total state size per operator\n",
    "Alert if: Growing unboundedly\n",
    "Action: Add TTL, compact state\n",
    "```\n",
    "\n",
    "**Checkpoint Duration:**\n",
    "```\n",
    "Track: Time to complete checkpoint\n",
    "Alert if: > 1 minute\n",
    "Action: Tune checkpoint interval, use incremental\n",
    "```\n",
    "\n",
    "**Checkpoint Alignment:**\n",
    "```\n",
    "Track: Time spent aligning barriers\n",
    "Alert if: High alignment time\n",
    "Action: Check for slow operators\n",
    "```\n",
    "\n",
    "### State TTL (Time-To-Live)\n",
    "\n",
    "```python\n",
    "from pyflink.datastream.state import StateTtlConfig\n",
    "\n",
    "ttl_config = StateTtlConfig \\\n",
    "    .new_builder(Time.hours(1)) \\\n",
    "    .set_update_type(StateTtlConfig.UpdateType.OnCreateAndWrite) \\\n",
    "    .set_state_visibility(StateTtlConfig.StateVisibility.NeverReturnExpired) \\\n",
    "    .build()\n",
    "\n",
    "# Apply to state descriptor\n",
    "state_descriptor.enable_time_to_live(ttl_config)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate State TTL\n",
    "class StateWithTTL:\n",
    "    \"\"\"State that automatically expires old entries\"\"\"\n",
    "\n",
    "    def __init__(self, ttl_seconds=10):\n",
    "        self.ttl = timedelta(seconds=ttl_seconds)\n",
    "        self.state = {}  # key -> (value, timestamp)\n",
    "\n",
    "    def put(self, key, value, timestamp):\n",
    "        \"\"\"Store value with timestamp\"\"\"\n",
    "        self.state[key] = (value, timestamp)\n",
    "\n",
    "    def get(self, key, current_time):\n",
    "        \"\"\"Get value if not expired\"\"\"\n",
    "        if key not in self.state:\n",
    "            return None\n",
    "\n",
    "        value, timestamp = self.state[key]\n",
    "        age = current_time - timestamp\n",
    "\n",
    "        if age > self.ttl:\n",
    "            # Expired!\n",
    "            del self.state[key]\n",
    "            return None\n",
    "\n",
    "        return value\n",
    "\n",
    "    def cleanup_expired(self, current_time):\n",
    "        \"\"\"Remove all expired entries\"\"\"\n",
    "        expired_keys = [\n",
    "            key\n",
    "            for key, (value, timestamp) in self.state.items()\n",
    "            if current_time - timestamp > self.ttl\n",
    "        ]\n",
    "\n",
    "        for key in expired_keys:\n",
    "            del self.state[key]\n",
    "\n",
    "        return len(expired_keys)\n",
    "\n",
    "    def size(self):\n",
    "        \"\"\"Current state size\"\"\"\n",
    "        return len(self.state)\n",
    "\n",
    "\n",
    "# Test TTL\n",
    "state = StateWithTTL(ttl_seconds=5)\n",
    "\n",
    "base_time = datetime.now()\n",
    "\n",
    "print(\"[OK] Testing state with TTL (5 seconds)...\\n\")\n",
    "\n",
    "# Add entries\n",
    "for i in range(5):\n",
    "    key = f\"key_{i}\"\n",
    "    timestamp = base_time + timedelta(seconds=i)\n",
    "    state.put(key, f\"value_{i}\", timestamp)\n",
    "    print(f\"Added {key} at {timestamp.strftime('%H:%M:%S')}\")\n",
    "\n",
    "print(f\"\\nState size: {state.size()}\")\n",
    "\n",
    "# Check at different times\n",
    "for seconds_elapsed in [3, 6, 10]:\n",
    "    current_time = base_time + timedelta(seconds=seconds_elapsed)\n",
    "    expired = state.cleanup_expired(current_time)\n",
    "\n",
    "    print(f\"\\nAt +{seconds_elapsed}s: Expired {expired} entries, {state.size()} remaining\")\n",
    "\n",
    "print(\"\\n[OK] TTL prevents unbounded state growth!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Key Takeaways\n",
    "\n",
    "[OK] **State Types**: ValueState, ListState, MapState for different use cases\n",
    "\n",
    "[OK] **State Backends**: Use RocksDB for production (scales to TB)\n",
    "\n",
    "[OK] **Checkpointing**: Automatic fault tolerance via periodic snapshots\n",
    "\n",
    "[OK] **Savepoints**: Manual snapshots for upgrades and migrations\n",
    "\n",
    "[OK] **State Evolution**: Plan for schema changes\n",
    "\n",
    "[OK] **State TTL**: Prevent unbounded growth with expiration\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Use RocksDB backend** for production\n",
    "2. **Enable incremental checkpointing** for large state\n",
    "3. **Set appropriate checkpoint interval** (10-60 seconds)\n",
    "4. **Monitor state size** and checkpoint duration\n",
    "5. **Use TTL** for session-based state\n",
    "6. **Test savepoint compatibility** before upgrades\n",
    "7. **Externalize checkpoints** for recovery after cancellation\n",
    "\n",
    "### Configuration Checklist\n",
    "\n",
    "```python\n",
    "# Recommended production settings\n",
    "env.set_state_backend(RocksDBStateBackend(...))\n",
    "env.enable_checkpointing(30000)  # 30s\n",
    "\n",
    "config = env.get_checkpoint_config()\n",
    "config.set_checkpoint_timeout(600000)  # 10min\n",
    "config.set_min_pause_between_checkpoints(15000)  # 15s\n",
    "config.set_max_concurrent_checkpoints(1)\n",
    "config.enable_externalized_checkpoints(\n",
    "    ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Practice Exercises\n",
    "\n",
    "1. **Implement checkpointing** with configurable interval\n",
    "2. **Add state TTL** to prevent memory leaks\n",
    "3. **Monitor state size** over time\n",
    "4. **Simulate failure recovery** from checkpoint\n",
    "5. **Test state migration** with schema changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your practice code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Next Steps\n",
    "\n",
    "Congratulations on completing Module 06!\n",
    "\n",
    "### What You've Learned\n",
    "\n",
    "- [OK] State types and state backends\n",
    "- [OK] Checkpointing configuration and tuning\n",
    "- [OK] Savepoints for job migration\n",
    "- [OK] State schema evolution strategies\n",
    "- [OK] State monitoring and TTL\n",
    "\n",
    "### Coming Up in Module 07: Stream Processing Patterns\n",
    "\n",
    "You'll learn:\n",
    "- Common stream processing patterns\n",
    "- Event deduplication\n",
    "- CDC (Change Data Capture)\n",
    "- Exactly-once end-to-end\n",
    "- Best practices and anti-patterns\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Flink State](https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/fault-tolerance/state/)\n",
    "- [Checkpointing](https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/fault-tolerance/checkpointing/)\n",
    "- [Savepoints](https://nightlies.apache.org/flink/flink-docs-master/docs/ops/state/savepoints/)\n",
    "- [State Backends](https://nightlies.apache.org/flink/flink-docs-master/docs/ops/state/state_backends/)\n",
    "\n",
    "---\n",
    "\n",
    "**Ready for patterns?** Open `07_stream_processing_patterns.ipynb` to continue!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
