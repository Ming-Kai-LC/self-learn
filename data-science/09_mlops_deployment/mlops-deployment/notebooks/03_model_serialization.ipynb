{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 03: Model Serialization\n",
    "\n",
    "**Difficulty**: ⭐⭐ Intermediate  \n",
    "**Estimated Time**: 45 minutes  \n",
    "**Prerequisites**: \n",
    "- Module 01: Experiment Tracking with MLflow\n",
    "- Module 02: Model Versioning and Registry\n",
    "- Basic understanding of file I/O\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Understand different model serialization formats (pickle, joblib, ONNX)\n",
    "2. Choose the appropriate serialization method for your use case\n",
    "3. Save and load models using various formats\n",
    "4. Compare file sizes and loading times across formats\n",
    "5. Handle cross-platform and cross-language deployment scenarios\n",
    "6. Implement best practices for model persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Model Serialization Matters\n",
    "\n",
    "Model serialization is the process of converting trained models into a format that can be stored and later reconstructed.\n",
    "\n",
    "**Without proper serialization:**\n",
    "- ❌ Cannot deploy models to production\n",
    "- ❌ Must retrain models every time\n",
    "- ❌ Cannot share models with other systems\n",
    "- ❌ Limited to single programming language\n",
    "\n",
    "**With proper serialization:**\n",
    "- ✅ Save trained models for reuse\n",
    "- ✅ Deploy across different environments\n",
    "- ✅ Share models between teams and systems\n",
    "- ✅ Enable cross-platform deployment\n",
    "- ✅ Optimize model size and loading speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Import required libraries\n",
    "import pickle\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create directory for saved models\n",
    "models_dir = Path('saved_models')\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")\n",
    "print(f\"✓ Models will be saved to: {models_dir.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Overview of Serialization Formats\n",
    "\n",
    "### Main Serialization Methods:\n",
    "\n",
    "| Format | Use Case | Pros | Cons |\n",
    "|--------|----------|------|------|\n",
    "| **Pickle** | Python-only, simple models | Built-in, easy to use | Python-specific, security risks |\n",
    "| **Joblib** | Large NumPy arrays, sklearn | Efficient for numerical data | Python-specific |\n",
    "| **ONNX** | Cross-platform deployment | Language-agnostic, optimized | Complex setup, limited model support |\n",
    "| **SavedModel** | TensorFlow/Keras | Production-ready, versioning | TensorFlow-specific |\n",
    "| **JSON** | Simple models, configs | Human-readable | Not for complex models |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preparing Sample Data and Models\n",
    "\n",
    "Let's create a dataset and train several models to demonstrate different serialization approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=5000,\n",
    "    n_features=30,\n",
    "    n_informative=25,\n",
    "    n_redundant=5,\n",
    "    n_classes=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multiple models for comparison\n",
    "models = {}\n",
    "\n",
    "# Logistic Regression (simple, small model)\n",
    "lr_model = LogisticRegression(max_iter=200, random_state=42)\n",
    "lr_model.fit(X_train, y_train)\n",
    "models['logistic_regression'] = lr_model\n",
    "\n",
    "# Decision Tree (medium complexity)\n",
    "dt_model = DecisionTreeClassifier(max_depth=10, random_state=42)\n",
    "dt_model.fit(X_train, y_train)\n",
    "models['decision_tree'] = dt_model\n",
    "\n",
    "# Random Forest (large, complex model)\n",
    "rf_model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "models['random_forest'] = rf_model\n",
    "\n",
    "# Print model accuracies\n",
    "print(\"Model Training Complete:\")\n",
    "print(\"=\"*60)\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{name:20s}: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pickle: Python's Built-in Serialization\n",
    "\n",
    "**Pickle** is Python's standard serialization format. It's simple but has limitations.\n",
    "\n",
    "### When to use Pickle:\n",
    "- ✅ Simple, small models\n",
    "- ✅ Python-only deployment\n",
    "- ✅ Quick prototyping\n",
    "\n",
    "### When NOT to use Pickle:\n",
    "- ❌ Production systems (security concerns)\n",
    "- ❌ Cross-language deployment\n",
    "- ❌ Large models with NumPy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model using pickle\n",
    "pickle_file = models_dir / 'logistic_regression.pkl'\n",
    "\n",
    "# Save model\n",
    "with open(pickle_file, 'wb') as f:\n",
    "    pickle.dump(models['logistic_regression'], f)\n",
    "\n",
    "print(f\"✓ Model saved to: {pickle_file}\")\n",
    "print(f\"✓ File size: {pickle_file.stat().st_size / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model using pickle\n",
    "start_time = time.time()\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    loaded_model = pickle.load(f)\n",
    "\n",
    "load_time = time.time() - start_time\n",
    "\n",
    "# Verify model works\n",
    "y_pred = loaded_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"✓ Model loaded successfully\")\n",
    "print(f\"✓ Load time: {load_time*1000:.2f} ms\")\n",
    "print(f\"✓ Accuracy: {accuracy:.4f}\")\n",
    "print(f\"\\nModel type: {type(loaded_model).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Joblib: Optimized for Scikit-learn\n",
    "\n",
    "**Joblib** is more efficient than pickle for large NumPy arrays, making it ideal for scikit-learn models.\n",
    "\n",
    "### When to use Joblib:\n",
    "- ✅ Scikit-learn models\n",
    "- ✅ Large models with NumPy arrays\n",
    "- ✅ Models with many estimators (Random Forest, etc.)\n",
    "\n",
    "### Advantages over Pickle:\n",
    "- Faster for large NumPy arrays\n",
    "- More efficient compression\n",
    "- Better for parallel processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Random Forest using joblib (better for large models)\n",
    "joblib_file = models_dir / 'random_forest.joblib'\n",
    "\n",
    "# Save with compression\n",
    "joblib.dump(models['random_forest'], joblib_file, compress=3)\n",
    "\n",
    "print(f\"✓ Model saved to: {joblib_file}\")\n",
    "print(f\"✓ File size: {joblib_file.stat().st_size / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with pickle for the same model\n",
    "pickle_rf_file = models_dir / 'random_forest.pkl'\n",
    "\n",
    "with open(pickle_rf_file, 'wb') as f:\n",
    "    pickle.dump(models['random_forest'], f)\n",
    "\n",
    "# Compare file sizes\n",
    "joblib_size = joblib_file.stat().st_size\n",
    "pickle_size = pickle_rf_file.stat().st_size\n",
    "\n",
    "print(\"File Size Comparison (Random Forest):\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Joblib (compressed): {joblib_size / 1024:.2f} KB\")\n",
    "print(f\"Pickle: {pickle_size / 1024:.2f} KB\")\n",
    "print(f\"Savings: {(1 - joblib_size/pickle_size)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model using joblib\n",
    "start_time = time.time()\n",
    "loaded_rf = joblib.load(joblib_file)\n",
    "load_time = time.time() - start_time\n",
    "\n",
    "# Verify model\n",
    "y_pred = loaded_rf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"✓ Model loaded successfully\")\n",
    "print(f\"✓ Load time: {load_time*1000:.2f} ms\")\n",
    "print(f\"✓ Accuracy: {accuracy:.4f}\")\n",
    "print(f\"✓ Number of estimators: {len(loaded_rf.estimators_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparing Loading Times\n",
    "\n",
    "Let's benchmark different serialization methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark loading times\n",
    "def benchmark_loading(file_path, load_function, n_iterations=10):\n",
    "    \"\"\"\n",
    "    Measure average loading time for a serialized model.\n",
    "    \"\"\"\n",
    "    times = []\n",
    "    \n",
    "    for _ in range(n_iterations):\n",
    "        start = time.time()\n",
    "        _ = load_function(file_path)\n",
    "        times.append(time.time() - start)\n",
    "    \n",
    "    return np.mean(times), np.std(times)\n",
    "\n",
    "# Benchmark results\n",
    "results = []\n",
    "\n",
    "# Pickle loading\n",
    "pickle_mean, pickle_std = benchmark_loading(\n",
    "    pickle_rf_file,\n",
    "    lambda f: pickle.load(open(f, 'rb'))\n",
    ")\n",
    "results.append({\n",
    "    'Method': 'Pickle',\n",
    "    'Mean Time (ms)': pickle_mean * 1000,\n",
    "    'Std (ms)': pickle_std * 1000,\n",
    "    'File Size (KB)': pickle_rf_file.stat().st_size / 1024\n",
    "})\n",
    "\n",
    "# Joblib loading\n",
    "joblib_mean, joblib_std = benchmark_loading(\n",
    "    joblib_file,\n",
    "    joblib.load\n",
    ")\n",
    "results.append({\n",
    "    'Method': 'Joblib',\n",
    "    'Mean Time (ms)': joblib_mean * 1000,\n",
    "    'Std (ms)': joblib_std * 1000,\n",
    "    'File Size (KB)': joblib_file.stat().st_size / 1024\n",
    "})\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"Loading Time Benchmark (Random Forest):\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Determine winner\n",
    "fastest = results_df.loc[results_df['Mean Time (ms)'].idxmin()]\n",
    "print(f\"\\n✓ Fastest method: {fastest['Method']} ({fastest['Mean Time (ms)']:.2f} ms)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Loading time\n",
    "ax1 = axes[0]\n",
    "bars1 = ax1.bar(results_df['Method'], results_df['Mean Time (ms)'], \n",
    "                color=['steelblue', 'seagreen'], alpha=0.7)\n",
    "ax1.errorbar(results_df['Method'], results_df['Mean Time (ms)'],\n",
    "             yerr=results_df['Std (ms)'], fmt='none', color='black', capsize=5)\n",
    "ax1.set_ylabel('Loading Time (ms)', fontweight='bold')\n",
    "ax1.set_title('Model Loading Time Comparison', fontweight='bold')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.2f}',\n",
    "             ha='center', va='bottom')\n",
    "\n",
    "# Plot 2: File size\n",
    "ax2 = axes[1]\n",
    "bars2 = ax2.bar(results_df['Method'], results_df['File Size (KB)'],\n",
    "                color=['steelblue', 'seagreen'], alpha=0.7)\n",
    "ax2.set_ylabel('File Size (KB)', fontweight='bold')\n",
    "ax2.set_title('Serialized Model File Size', fontweight='bold')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.1f}',\n",
    "             ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Joblib is typically faster and produces smaller files for scikit-learn models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ONNX: Cross-Platform Model Format\n",
    "\n",
    "**ONNX** (Open Neural Network Exchange) enables cross-platform and cross-language deployment.\n",
    "\n",
    "### When to use ONNX:\n",
    "- ✅ Deploy to non-Python environments (C++, Java, JavaScript)\n",
    "- ✅ Mobile or edge deployment\n",
    "- ✅ Performance optimization\n",
    "- ✅ Framework interoperability\n",
    "\n",
    "### Supported Models:\n",
    "- Many scikit-learn models\n",
    "- PyTorch, TensorFlow, Keras\n",
    "- XGBoost, LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ONNX libraries if needed\n",
    "# !pip install skl2onnx onnxruntime\n",
    "\n",
    "try:\n",
    "    from skl2onnx import convert_sklearn\n",
    "    from skl2onnx.common.data_types import FloatTensorType\n",
    "    import onnxruntime as rt\n",
    "    onnx_available = True\n",
    "    print(\"✓ ONNX libraries available\")\nexcept ImportError:\n",
    "    onnx_available = False\n",
    "    print(\"⚠ ONNX libraries not installed\")\n",
    "    print(\"  To use ONNX, install: pip install skl2onnx onnxruntime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if onnx_available:\n",
    "    # Convert scikit-learn model to ONNX\n",
    "    onnx_file = models_dir / 'random_forest.onnx'\n",
    "    \n",
    "    # Define input type (crucial for ONNX conversion)\n",
    "    initial_type = [('float_input', FloatTensorType([None, X_train.shape[1]]))]\n",
    "    \n",
    "    # Convert model\n",
    "    onnx_model = convert_sklearn(\n",
    "        models['random_forest'],\n",
    "        initial_types=initial_type,\n",
    "        target_opset=12\n",
    "    )\n",
    "    \n",
    "    # Save ONNX model\n",
    "    with open(onnx_file, 'wb') as f:\n",
    "        f.write(onnx_model.SerializeToString())\n",
    "    \n",
    "    print(f\"✓ Model converted to ONNX\")\n",
    "    print(f\"✓ Saved to: {onnx_file}\")\n",
    "    print(f\"✓ File size: {onnx_file.stat().st_size / 1024:.2f} KB\")\nelse:\n",
    "    print(\"Skipping ONNX conversion (libraries not installed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if onnx_available:\n",
    "    # Load and run ONNX model\n",
    "    sess = rt.InferenceSession(str(onnx_file))\n",
    "    \n",
    "    # Get input name\n",
    "    input_name = sess.get_inputs()[0].name\n",
    "    \n",
    "    # Make predictions\n",
    "    # ONNX requires float32 input\n",
    "    X_test_float32 = X_test.astype(np.float32)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    onnx_pred = sess.run(None, {input_name: X_test_float32})\n",
    "    onnx_time = time.time() - start_time\n",
    "    \n",
    "    # ONNX returns a list, first element is predictions\n",
    "    y_pred_onnx = onnx_pred[0]\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred_onnx)\n",
    "    \n",
    "    print(f\"✓ ONNX model loaded and executed\")\n",
    "    print(f\"✓ Inference time: {onnx_time*1000:.2f} ms\")\n",
    "    print(f\"✓ Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Compare with original model\n",
    "    start_time = time.time()\n",
    "    y_pred_sklearn = models['random_forest'].predict(X_test)\n",
    "    sklearn_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\nComparison:\")\n",
    "    print(f\"  Scikit-learn inference: {sklearn_time*1000:.2f} ms\")\n",
    "    print(f\"  ONNX inference: {onnx_time*1000:.2f} ms\")\n",
    "    print(f\"  Speedup: {sklearn_time/onnx_time:.2f}x\")\nelse:\n",
    "    print(\"Skipping ONNX inference (libraries not installed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Metadata and Versioning\n",
    "\n",
    "Always save metadata alongside your models for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Create comprehensive metadata\n",
    "model_metadata = {\n",
    "    'model_name': 'random_forest_classifier',\n",
    "    'model_type': 'RandomForestClassifier',\n",
    "    'version': '1.0.0',\n",
    "    'created_date': datetime.now().isoformat(),\n",
    "    'framework': 'scikit-learn',\n",
    "    'framework_version': '1.0.2',\n",
    "    'parameters': {\n",
    "        'n_estimators': 100,\n",
    "        'max_depth': 10,\n",
    "        'random_state': 42\n",
    "    },\n",
    "    'training_data': {\n",
    "        'n_samples': len(X_train),\n",
    "        'n_features': X_train.shape[1],\n",
    "        'n_classes': 2\n",
    "    },\n",
    "    'performance': {\n",
    "        'accuracy': accuracy_score(y_test, models['random_forest'].predict(X_test)),\n",
    "        'test_size': len(X_test)\n",
    "    },\n",
    "    'serialization': {\n",
    "        'format': 'joblib',\n",
    "        'compression': 3,\n",
    "        'file_size_kb': joblib_file.stat().st_size / 1024\n",
    "    },\n",
    "    'author': 'MLOps Team',\n",
    "    'description': 'Random Forest model for binary classification'\n",
    "}\n",
    "\n",
    "# Save metadata\n",
    "metadata_file = models_dir / 'random_forest_metadata.json'\n",
    "with open(metadata_file, 'w') as f:\n",
    "    json.dump(model_metadata, f, indent=2)\n",
    "\n",
    "print(\"✓ Model metadata saved\")\n",
    "print(f\"✓ Location: {metadata_file}\")\n",
    "print(\"\\nMetadata content:\")\n",
    "print(json.dumps(model_metadata, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load model with metadata validation\n",
    "def load_model_with_validation(model_path, metadata_path):\n",
    "    \"\"\"\n",
    "    Load a model and validate against its metadata.\n",
    "    \"\"\"\n",
    "    # Load metadata\n",
    "    with open(metadata_path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    # Load model\n",
    "    model = joblib.load(model_path)\n",
    "    \n",
    "    # Validate model type\n",
    "    expected_type = metadata['model_type']\n",
    "    actual_type = type(model).__name__\n",
    "    \n",
    "    if actual_type != expected_type:\n",
    "        raise ValueError(\n",
    "            f\"Model type mismatch! Expected {expected_type}, got {actual_type}\"\n",
    "        )\n",
    "    \n",
    "    print(f\"✓ Model loaded and validated\")\n",
    "    print(f\"  Name: {metadata['model_name']}\")\n",
    "    print(f\"  Version: {metadata['version']}\")\n",
    "    print(f\"  Created: {metadata['created_date']}\")\n",
    "    print(f\"  Performance: {metadata['performance']}\")\n",
    "    \n",
    "    return model, metadata\n",
    "\n",
    "# Test the function\n",
    "loaded_model, metadata = load_model_with_validation(joblib_file, metadata_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Best Practices for Model Serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Practice 1: Version your models\n",
    "def save_versioned_model(model, model_name, version, base_dir='saved_models'):\n",
    "    \"\"\"\n",
    "    Save model with version number in filename.\n",
    "    \"\"\"\n",
    "    base_path = Path(base_dir) / model_name\n",
    "    base_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Create versioned filename\n",
    "    filename = f\"{model_name}_v{version}.joblib\"\n",
    "    filepath = base_path / filename\n",
    "    \n",
    "    # Save model\n",
    "    joblib.dump(model, filepath, compress=3)\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        'model_name': model_name,\n",
    "        'version': version,\n",
    "        'saved_date': datetime.now().isoformat(),\n",
    "        'file_path': str(filepath)\n",
    "    }\n",
    "    \n",
    "    metadata_path = base_path / f\"{model_name}_v{version}_metadata.json\"\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"✓ Saved {model_name} version {version}\")\n",
    "    print(f\"  Model: {filepath}\")\n",
    "    print(f\"  Metadata: {metadata_path}\")\n",
    "    \n",
    "    return filepath, metadata_path\n",
    "\n",
    "# Example usage\n",
    "model_path, meta_path = save_versioned_model(\n",
    "    models['random_forest'],\n",
    "    'fraud_detector',\n",
    "    '1.2.0'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Practice 2: Include data preprocessing in serialization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create a pipeline that includes preprocessing\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=50, random_state=42))\n",
    "])\n",
    "\n",
    "# Train pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Save entire pipeline\n",
    "pipeline_file = models_dir / 'complete_pipeline.joblib'\n",
    "joblib.dump(pipeline, pipeline_file)\n",
    "\n",
    "print(\"✓ Complete pipeline saved (preprocessing + model)\")\n",
    "print(f\"✓ File: {pipeline_file}\")\n",
    "print(f\"\\nPipeline steps:\")\n",
    "for name, step in pipeline.steps:\n",
    "    print(f\"  - {name}: {type(step).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and use pipeline\n",
    "loaded_pipeline = joblib.load(pipeline_file)\n",
    "\n",
    "# Make predictions (preprocessing is automatic)\n",
    "y_pred = loaded_pipeline.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"✓ Pipeline loaded and tested\")\n",
    "print(f\"✓ Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\n✓ No need to manually apply preprocessing - it's built into the pipeline!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Exercises\n",
    "\n",
    "### Exercise 1: Serialization Format Comparison\n",
    "\n",
    "Compare different serialization formats for various model types.\n",
    "\n",
    "**Requirements:**\n",
    "1. Train 3 different model types (e.g., Logistic Regression, SVM, Gradient Boosting)\n",
    "2. Save each using both pickle and joblib\n",
    "3. Compare:\n",
    "   - File sizes\n",
    "   - Loading times\n",
    "   - Memory usage\n",
    "4. Create a visualization showing the results\n",
    "\n",
    "**Bonus**: Include compression levels in your comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "\n",
    "# TODO: Implement serialization comparison\n",
    "# 1. Train multiple models\n",
    "# 2. Save with different formats\n",
    "# 3. Measure metrics\n",
    "# 4. Visualize results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Create a Model Versioning System\n",
    "\n",
    "Build a simple model versioning and management system.\n",
    "\n",
    "**Requirements:**\n",
    "1. Create functions to:\n",
    "   - Save models with automatic version incrementing\n",
    "   - List all saved model versions\n",
    "   - Load a specific version\n",
    "   - Compare performance across versions\n",
    "2. Include metadata for each version (timestamp, parameters, performance)\n",
    "3. Implement a function to rollback to a previous version\n",
    "\n",
    "**Bonus**: Add a function to automatically save the best model based on a metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "\n",
    "class ModelVersionManager:\n",
    "    \"\"\"Manage model versions with automatic serialization.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_dir='model_versions'):\n",
    "        self.base_dir = Path(base_dir)\n",
    "        self.base_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    def save_model(self, model, model_name, metrics=None, metadata=None):\n",
    "        \"\"\"Save model with automatic version increment.\"\"\"\n",
    "        # TODO: Implement\n",
    "        pass\n",
    "    \n",
    "    def list_versions(self, model_name):\n",
    "        \"\"\"List all versions of a model.\"\"\"\n",
    "        # TODO: Implement\n",
    "        pass\n",
    "    \n",
    "    def load_model(self, model_name, version=None):\n",
    "        \"\"\"Load a specific version (latest if not specified).\"\"\"\n",
    "        # TODO: Implement\n",
    "        pass\n",
    "    \n",
    "    def compare_versions(self, model_name):\n",
    "        \"\"\"Compare all versions of a model.\"\"\"\n",
    "        # TODO: Implement\n",
    "        pass\n",
    "\n",
    "# Test your implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Cross-Platform Deployment\n",
    "\n",
    "Prepare a model for deployment in a different environment.\n",
    "\n",
    "**Requirements:**\n",
    "1. Train a model and save it in ONNX format\n",
    "2. Create a simple inference script that:\n",
    "   - Loads the ONNX model\n",
    "   - Accepts input data\n",
    "   - Returns predictions\n",
    "   - Handles errors gracefully\n",
    "3. Document the model's input/output schema\n",
    "4. Create a README explaining how to use the model in production\n",
    "\n",
    "**Bonus**: Test inference speed and compare with the original scikit-learn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "\n",
    "# TODO: Implement cross-platform deployment\n",
    "# 1. Convert model to ONNX\n",
    "# 2. Create inference script\n",
    "# 3. Document schema\n",
    "# 4. Benchmark performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary\n",
    "\n",
    "### Key Concepts Covered\n",
    "\n",
    "1. **Serialization Formats**: Pickle, Joblib, ONNX, and their use cases\n",
    "2. **Performance Comparison**: File size and loading time trade-offs\n",
    "3. **Best Practices**: Versioning, metadata, and validation\n",
    "4. **Pipeline Serialization**: Saving complete preprocessing + model pipelines\n",
    "5. **Cross-Platform Deployment**: Using ONNX for language-agnostic deployment\n",
    "\n",
    "### Decision Guide: Which Format to Use?\n",
    "\n",
    "```\n",
    "Choose Pickle when:\n",
    "  ✓ Simple, small models\n",
    "  ✓ Python-only deployment\n",
    "  ✓ Quick prototyping\n",
    "\n",
    "Choose Joblib when:\n",
    "  ✓ Scikit-learn models\n",
    "  ✓ Large models with NumPy arrays\n",
    "  ✓ Production Python deployment\n",
    "\n",
    "Choose ONNX when:\n",
    "  ✓ Cross-language deployment\n",
    "  ✓ Mobile/edge deployment\n",
    "  ✓ Performance optimization needed\n",
    "  ✓ Framework interoperability\n",
    "```\n",
    "\n",
    "### Best Practices Summary\n",
    "\n",
    "- ✅ **Always version your models** with semantic versioning\n",
    "- ✅ **Save metadata** alongside models (parameters, metrics, dates)\n",
    "- ✅ **Use pipelines** to include preprocessing in serialization\n",
    "- ✅ **Validate on load** to catch deserialization errors early\n",
    "- ✅ **Document input/output schemas** for production deployment\n",
    "- ✅ **Test deserialization** in target environment before deploying\n",
    "- ✅ **Use compression** for large models (joblib compress parameter)\n",
    "\n",
    "### Common Pitfalls to Avoid\n",
    "\n",
    "- ❌ Using pickle in production (security risks)\n",
    "- ❌ Not versioning serialized models\n",
    "- ❌ Forgetting to save preprocessing steps\n",
    "- ❌ Not testing cross-platform compatibility\n",
    "- ❌ Hardcoding file paths\n",
    "- ❌ Not validating model integrity after loading\n",
    "\n",
    "### What's Next\n",
    "\n",
    "In **Module 04: Creating ML APIs with FastAPI**, we'll learn:\n",
    "- Building REST APIs for model serving\n",
    "- Request validation and error handling\n",
    "- API documentation with OpenAPI\n",
    "- Authentication and rate limiting\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- **ONNX Documentation**: https://onnx.ai/\n",
    "- **Joblib Documentation**: https://joblib.readthedocs.io/\n",
    "- **Model Serialization Best Practices**: https://neptune.ai/blog/how-to-save-and-load-ml-models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Proceed to **Module 04: Creating ML APIs with FastAPI** to learn how to serve your serialized models via REST APIs.\n",
    "\n",
    "**Before moving on, ensure you can:**\n",
    "- ✅ Save and load models using pickle and joblib\n",
    "- ✅ Choose the appropriate serialization format for your use case\n",
    "- ✅ Convert models to ONNX for cross-platform deployment\n",
    "- ✅ Include metadata with serialized models\n",
    "- ✅ Serialize complete pipelines (preprocessing + model)\n",
    "- ✅ Implement model versioning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
