{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 03: Model Serialization\n",
    "\n",
    "**Difficulty**: ⭐⭐ Intermediate\n",
    "**Estimated Time**: 50 minutes\n",
    "**Prerequisites**: \n",
    "- [Module 01: Experiment Tracking with MLflow](01_experiment_tracking_mlflow.ipynb)\n",
    "- [Module 02: Model Versioning and Registry](02_model_versioning_registry.ipynb)\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Understand different model serialization formats and their trade-offs\n",
    "2. Serialize models using pickle and joblib\n",
    "3. Export models to ONNX for cross-platform compatibility\n",
    "4. Optimize model size for deployment\n",
    "5. Handle version compatibility issues\n",
    "6. Choose the right serialization format for your use case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Model Serialization Matters\n",
    "\n",
    "### The Challenge\n",
    "\n",
    "You've trained a model in a Jupyter notebook:\n",
    "- **Problem**: How do you save it for later use?\n",
    "- **Problem**: How do you deploy it to a web server?\n",
    "- **Problem**: How do you share it with teammates?\n",
    "- **Problem**: What if production uses a different Python version?\n",
    "\n",
    "### The Solution: Serialization\n",
    "\n",
    "**Serialization** converts a Python object (your model) into a format that can be:\n",
    "- Saved to disk\n",
    "- Transmitted over a network\n",
    "- Loaded in different environments\n",
    "- Used across programming languages\n",
    "\n",
    "### Common Serialization Formats\n",
    "\n",
    "| Format | Best For | Pros | Cons |\n",
    "|--------|----------|------|------|\n",
    "| **pickle** | Python-only deployment | Native Python support | Python version sensitive |\n",
    "| **joblib** | Large numpy arrays | Efficient with sklearn | Python-only |\n",
    "| **ONNX** | Cross-platform deployment | Language-agnostic | Requires conversion |\n",
    "| **TensorFlow SavedModel** | TensorFlow models | Production-ready | Framework-specific |\n",
    "| **PyTorch TorchScript** | PyTorch models | Optimized inference | Framework-specific |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Import all required libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.datasets import make_classification, load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Create models directory\n",
    "models_dir = Path(\"saved_models\")\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "print(f\"Models will be saved to: {models_dir.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparing Data and Training Models\n",
    "\n",
    "Let's train some models to demonstrate different serialization approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=20,\n",
    "    n_informative=15,\n",
    "    n_redundant=5,\n",
    "    n_classes=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a simple Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = rf_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"✓ Random Forest trained\")\n",
    "print(f\"  Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Serialization with Pickle\n",
    "\n",
    "**pickle** is Python's built-in serialization library. It's simple but has limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model using pickle\n",
    "pickle_path = models_dir / \"rf_model.pkl\"\n",
    "\n",
    "with open(pickle_path, 'wb') as f:\n",
    "    pickle.dump(rf_model, f)\n",
    "\n",
    "print(f\"✓ Model saved with pickle\")\n",
    "print(f\"  File: {pickle_path}\")\n",
    "print(f\"  Size: {pickle_path.stat().st_size / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model using pickle\n",
    "with open(pickle_path, 'rb') as f:\n",
    "    loaded_model = pickle.load(f)\n",
    "\n",
    "# Test loaded model\n",
    "loaded_predictions = loaded_model.predict(X_test)\n",
    "loaded_accuracy = accuracy_score(y_test, loaded_predictions)\n",
    "\n",
    "print(f\"✓ Model loaded from pickle\")\n",
    "print(f\"  Accuracy: {loaded_accuracy:.4f}\")\n",
    "print(f\"  Original == Loaded: {accuracy == loaded_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle Pros and Cons\n",
    "\n",
    "**Pros**:\n",
    "- Simple and built into Python\n",
    "- Works with any Python object\n",
    "- Fast for small models\n",
    "\n",
    "**Cons**:\n",
    "- **Security risk**: Never unpickle untrusted data (code execution vulnerability)\n",
    "- **Python version sensitivity**: May not work across different Python versions\n",
    "- **Framework version sensitivity**: Requires same scikit-learn version\n",
    "- Inefficient for large numpy arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Serialization with Joblib\n",
    "\n",
    "**joblib** is optimized for objects containing large numpy arrays, making it ideal for sklearn models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model using joblib\n",
    "joblib_path = models_dir / \"rf_model.joblib\"\n",
    "\n",
    "joblib.dump(rf_model, joblib_path)\n",
    "\n",
    "print(f\"✓ Model saved with joblib\")\n",
    "print(f\"  File: {joblib_path}\")\n",
    "print(f\"  Size: {joblib_path.stat().st_size / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model using joblib\n",
    "loaded_model_joblib = joblib.load(joblib_path)\n",
    "\n",
    "# Test loaded model\n",
    "joblib_predictions = loaded_model_joblib.predict(X_test)\n",
    "joblib_accuracy = accuracy_score(y_test, joblib_predictions)\n",
    "\n",
    "print(f\"✓ Model loaded from joblib\")\n",
    "print(f\"  Accuracy: {joblib_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare file sizes: pickle vs joblib\n",
    "pickle_size = pickle_path.stat().st_size / 1024\n",
    "joblib_size = joblib_path.stat().st_size / 1024\n",
    "\n",
    "print(\"File Size Comparison:\")\n",
    "print(f\"  Pickle: {pickle_size:.2f} KB\")\n",
    "print(f\"  Joblib: {joblib_size:.2f} KB\")\n",
    "print(f\"  Difference: {abs(pickle_size - joblib_size):.2f} KB\")\n",
    "\n",
    "# Visualize comparison\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "formats = ['Pickle', 'Joblib']\n",
    "sizes = [pickle_size, joblib_size]\n",
    "colors = ['skyblue', 'lightcoral']\n",
    "\n",
    "ax.bar(formats, sizes, color=colors, edgecolor='black', alpha=0.7)\n",
    "ax.set_ylabel('File Size (KB)', fontweight='bold', fontsize=12)\n",
    "ax.set_title('Model Serialization Format Comparison', fontweight='bold', fontsize=14)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for i, v in enumerate(sizes):\n",
    "    ax.text(i, v + 1, f'{v:.2f} KB', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joblib Pros and Cons\n",
    "\n",
    "**Pros**:\n",
    "- Efficient for large numpy arrays\n",
    "- Better compression than pickle\n",
    "- Faster for sklearn models\n",
    "- Recommended by scikit-learn documentation\n",
    "\n",
    "**Cons**:\n",
    "- Still Python-only\n",
    "- Same version sensitivity as pickle\n",
    "- Same security concerns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Serialization with Compression\n",
    "\n",
    "Both pickle and joblib support compression to reduce file size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save with different compression levels\n",
    "compression_levels = [0, 3, 6, 9]  # 0 = no compression, 9 = max compression\n",
    "results = []\n",
    "\n",
    "for level in compression_levels:\n",
    "    compressed_path = models_dir / f\"rf_model_compress_{level}.joblib\"\n",
    "    joblib.dump(rf_model, compressed_path, compress=level)\n",
    "    \n",
    "    file_size = compressed_path.stat().st_size / 1024\n",
    "    results.append({\n",
    "        'compression_level': level,\n",
    "        'size_kb': file_size\n",
    "    })\n",
    "    \n",
    "    print(f\"Compression level {level}: {file_size:.2f} KB\")\n",
    "\n",
    "# Visualize compression impact\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(results_df['compression_level'], results_df['size_kb'], \n",
    "        marker='o', linewidth=2, markersize=10, color='darkblue')\n",
    "ax.set_xlabel('Compression Level', fontweight='bold', fontsize=12)\n",
    "ax.set_ylabel('File Size (KB)', fontweight='bold', fontsize=12)\n",
    "ax.set_title('Impact of Compression on Model Size', fontweight='bold', fontsize=14)\n",
    "ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate savings\n",
    "original_size = results_df.loc[results_df['compression_level'] == 0, 'size_kb'].values[0]\n",
    "max_compressed = results_df.loc[results_df['compression_level'] == 9, 'size_kb'].values[0]\n",
    "savings_percent = ((original_size - max_compressed) / original_size) * 100\n",
    "\n",
    "print(f\"\\n✓ Maximum compression savings: {savings_percent:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Serializing Pipelines\n",
    "\n",
    "In production, you often need to save preprocessing steps along with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline with preprocessing and model\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# Train pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "pipeline_accuracy = pipeline.score(X_test, y_test)\n",
    "print(f\"✓ Pipeline trained\")\n",
    "print(f\"  Accuracy: {pipeline_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save entire pipeline\n",
    "pipeline_path = models_dir / \"pipeline.joblib\"\n",
    "joblib.dump(pipeline, pipeline_path)\n",
    "\n",
    "print(f\"✓ Pipeline saved\")\n",
    "print(f\"  Size: {pipeline_path.stat().st_size / 1024:.2f} KB\")\n",
    "\n",
    "# Load and test\n",
    "loaded_pipeline = joblib.load(pipeline_path)\n",
    "loaded_pipeline_accuracy = loaded_pipeline.score(X_test, y_test)\n",
    "\n",
    "print(f\"\\n✓ Pipeline loaded\")\n",
    "print(f\"  Accuracy: {loaded_pipeline_accuracy:.4f}\")\n",
    "print(f\"  Includes: {[step[0] for step in loaded_pipeline.steps]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ONNX: Cross-Platform Model Format\n",
    "\n",
    "**ONNX (Open Neural Network Exchange)** allows models to be used across different frameworks and languages.\n",
    "\n",
    "**Use cases**:\n",
    "- Deploy Python-trained models to C++/Java applications\n",
    "- Use GPU acceleration with ONNX Runtime\n",
    "- Ensure model compatibility across environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install skl2onnx onnxruntime\n",
    "\n",
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "import onnxruntime as rt\n",
    "\n",
    "# Define input type for ONNX conversion\n",
    "# We need to specify the shape: (None, 20) means any number of samples with 20 features\n",
    "initial_type = [('float_input', FloatTensorType([None, X_train.shape[1]]))]\n",
    "\n",
    "# Convert sklearn model to ONNX\n",
    "onnx_model = convert_sklearn(rf_model, initial_types=initial_type)\n",
    "\n",
    "# Save ONNX model\n",
    "onnx_path = models_dir / \"rf_model.onnx\"\n",
    "with open(onnx_path, \"wb\") as f:\n",
    "    f.write(onnx_model.SerializeToString())\n",
    "\n",
    "print(f\"✓ Model converted to ONNX\")\n",
    "print(f\"  File: {onnx_path}\")\n",
    "print(f\"  Size: {onnx_path.stat().st_size / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and use ONNX model\n",
    "session = rt.InferenceSession(str(onnx_path))\n",
    "\n",
    "# Get input name (required for ONNX Runtime)\n",
    "input_name = session.get_inputs()[0].name\n",
    "\n",
    "# Make predictions\n",
    "# ONNX requires float32 format\n",
    "onnx_predictions = session.run(None, {input_name: X_test.astype(np.float32)})[0]\n",
    "\n",
    "# Calculate accuracy\n",
    "onnx_accuracy = accuracy_score(y_test, onnx_predictions)\n",
    "\n",
    "print(f\"✓ ONNX model predictions\")\n",
    "print(f\"  Accuracy: {onnx_accuracy:.4f}\")\n",
    "print(f\"  Same as original: {onnx_accuracy == accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONNX Pros and Cons\n",
    "\n",
    "**Pros**:\n",
    "- Cross-platform and cross-language\n",
    "- Optimized for inference (faster predictions)\n",
    "- Reduces version compatibility issues\n",
    "- Supports GPU acceleration\n",
    "\n",
    "**Cons**:\n",
    "- Not all sklearn models supported\n",
    "- Conversion adds complexity\n",
    "- Larger file size than joblib\n",
    "- Requires ONNX Runtime for inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Size Optimization\n",
    "\n",
    "Smaller models mean faster loading and less storage/bandwidth cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models with different complexity\n",
    "model_configs = [\n",
    "    {\"name\": \"small\", \"n_estimators\": 10, \"max_depth\": 5},\n",
    "    {\"name\": \"medium\", \"n_estimators\": 50, \"max_depth\": 10},\n",
    "    {\"name\": \"large\", \"n_estimators\": 200, \"max_depth\": 20},\n",
    "]\n",
    "\n",
    "size_results = []\n",
    "\n",
    "for config in model_configs:\n",
    "    # Train model\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=config[\"n_estimators\"],\n",
    "        max_depth=config[\"max_depth\"],\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    accuracy = model.score(X_test, y_test)\n",
    "    \n",
    "    # Save and measure size\n",
    "    model_path = models_dir / f\"rf_{config['name']}.joblib\"\n",
    "    joblib.dump(model, model_path, compress=3)\n",
    "    file_size = model_path.stat().st_size / 1024\n",
    "    \n",
    "    size_results.append({\n",
    "        'model': config['name'],\n",
    "        'n_estimators': config['n_estimators'],\n",
    "        'max_depth': config['max_depth'],\n",
    "        'accuracy': accuracy,\n",
    "        'size_kb': file_size\n",
    "    })\n",
    "\n",
    "size_df = pd.DataFrame(size_results)\n",
    "print(\"Model Size vs Accuracy Trade-off:\")\n",
    "print(size_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize size vs accuracy trade-off\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "scatter = ax.scatter(\n",
    "    size_df['size_kb'], \n",
    "    size_df['accuracy'],\n",
    "    s=200,\n",
    "    c=range(len(size_df)),\n",
    "    cmap='viridis',\n",
    "    edgecolors='black',\n",
    "    linewidth=2,\n",
    "    alpha=0.7\n",
    ")\n",
    "\n",
    "# Add labels for each point\n",
    "for idx, row in size_df.iterrows():\n",
    "    ax.annotate(\n",
    "        row['model'],\n",
    "        (row['size_kb'], row['accuracy']),\n",
    "        xytext=(10, 5),\n",
    "        textcoords='offset points',\n",
    "        fontweight='bold'\n",
    "    )\n",
    "\n",
    "ax.set_xlabel('Model Size (KB)', fontweight='bold', fontsize=12)\n",
    "ax.set_ylabel('Accuracy', fontweight='bold', fontsize=12)\n",
    "ax.set_title('Model Size vs Accuracy Trade-off', fontweight='bold', fontsize=14)\n",
    "ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Version Compatibility Best Practices\n",
    "\n",
    "Avoid the \"it works on my machine\" problem by tracking versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import sklearn\n",
    "\n",
    "# Create metadata to save with model\n",
    "model_metadata = {\n",
    "    \"model_type\": \"RandomForestClassifier\",\n",
    "    \"model_params\": rf_model.get_params(),\n",
    "    \"training_accuracy\": accuracy,\n",
    "    \"python_version\": sys.version,\n",
    "    \"sklearn_version\": sklearn.__version__,\n",
    "    \"numpy_version\": np.__version__,\n",
    "    \"training_date\": pd.Timestamp.now().isoformat(),\n",
    "    \"n_features\": X_train.shape[1],\n",
    "    \"n_classes\": len(np.unique(y_train))\n",
    "}\n",
    "\n",
    "# Save metadata alongside model\n",
    "metadata_path = models_dir / \"rf_model_metadata.json\"\n",
    "import json\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(model_metadata, f, indent=2)\n",
    "\n",
    "print(\"✓ Model metadata saved\")\n",
    "print(f\"\\nMetadata:\")\n",
    "for key, value in model_metadata.items():\n",
    "    if key != 'model_params':  # Skip params for brevity\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Exercises\n",
    "\n",
    "Practice different serialization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Save and Load with Different Formats\n",
    "\n",
    "Train a LogisticRegression model and save it using pickle, joblib, and ONNX.\n",
    "\n",
    "**Requirements**:\n",
    "1. Train a LogisticRegression model\n",
    "2. Save it in all three formats\n",
    "3. Compare file sizes\n",
    "4. Verify all loaded models produce same predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1 Solution\n",
    "\n",
    "# Train model\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_model.fit(X_train, y_train)\n",
    "original_accuracy = lr_model.score(X_test, y_test)\n",
    "\n",
    "print(f\"✓ Logistic Regression trained (Accuracy: {original_accuracy:.4f})\\n\")\n",
    "\n",
    "# Save with pickle\n",
    "pickle_lr_path = models_dir / \"lr_model.pkl\"\n",
    "with open(pickle_lr_path, 'wb') as f:\n",
    "    pickle.dump(lr_model, f)\n",
    "pickle_size = pickle_lr_path.stat().st_size / 1024\n",
    "\n",
    "# Save with joblib\n",
    "joblib_lr_path = models_dir / \"lr_model.joblib\"\n",
    "joblib.dump(lr_model, joblib_lr_path)\n",
    "joblib_size = joblib_lr_path.stat().st_size / 1024\n",
    "\n",
    "# Save with ONNX\n",
    "onnx_lr = convert_sklearn(lr_model, initial_types=initial_type)\n",
    "onnx_lr_path = models_dir / \"lr_model.onnx\"\n",
    "with open(onnx_lr_path, \"wb\") as f:\n",
    "    f.write(onnx_lr.SerializeToString())\n",
    "onnx_size = onnx_lr_path.stat().st_size / 1024\n",
    "\n",
    "# Compare sizes\n",
    "print(\"File Size Comparison:\")\n",
    "print(f\"  Pickle: {pickle_size:.2f} KB\")\n",
    "print(f\"  Joblib: {joblib_size:.2f} KB\")\n",
    "print(f\"  ONNX: {onnx_size:.2f} KB\")\n",
    "\n",
    "# Verify predictions\n",
    "pickle_model = pickle.load(open(pickle_lr_path, 'rb'))\n",
    "joblib_model = joblib.load(joblib_lr_path)\n",
    "onnx_session = rt.InferenceSession(str(onnx_lr_path))\n",
    "onnx_pred = onnx_session.run(None, {input_name: X_test.astype(np.float32)})[0]\n",
    "\n",
    "print(f\"\\n✓ All models produce same predictions: \"\n",
    "      f\"{np.array_equal(pickle_model.predict(X_test), joblib_model.predict(X_test)) and np.array_equal(pickle_model.predict(X_test), onnx_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Pipeline Serialization\n",
    "\n",
    "Create a pipeline with multiple preprocessing steps and serialize it.\n",
    "\n",
    "**Requirements**:\n",
    "1. Create pipeline: StandardScaler → LogisticRegression\n",
    "2. Train on the dataset\n",
    "3. Save the entire pipeline\n",
    "4. Load and verify it works correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2 Solution\n",
    "\n",
    "# Create pipeline\n",
    "lr_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression(max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "# Train\n",
    "lr_pipeline.fit(X_train, y_train)\n",
    "pipeline_acc = lr_pipeline.score(X_test, y_test)\n",
    "\n",
    "print(f\"✓ Pipeline trained (Accuracy: {pipeline_acc:.4f})\")\n",
    "\n",
    "# Save\n",
    "pipeline_lr_path = models_dir / \"lr_pipeline.joblib\"\n",
    "joblib.dump(lr_pipeline, pipeline_lr_path)\n",
    "print(f\"✓ Pipeline saved ({pipeline_lr_path.stat().st_size / 1024:.2f} KB)\")\n",
    "\n",
    "# Load and verify\n",
    "loaded_lr_pipeline = joblib.load(pipeline_lr_path)\n",
    "loaded_acc = loaded_lr_pipeline.score(X_test, y_test)\n",
    "\n",
    "print(f\"✓ Pipeline loaded (Accuracy: {loaded_acc:.4f})\")\n",
    "print(f\"  Pipeline steps: {[step[0] for step in loaded_lr_pipeline.steps]}\")\n",
    "print(f\"  Accuracy preserved: {pipeline_acc == loaded_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Compression Comparison\n",
    "\n",
    "Experiment with different compression levels and measure the impact.\n",
    "\n",
    "**Requirements**:\n",
    "1. Save the same model with compression levels 0, 3, 6, 9\n",
    "2. Measure file sizes\n",
    "3. Time how long it takes to load each\n",
    "4. Create a visualization comparing size and load time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3 Solution\n",
    "\n",
    "import time\n",
    "\n",
    "compression_results = []\n",
    "\n",
    "for level in [0, 3, 6, 9]:\n",
    "    # Save with compression\n",
    "    comp_path = models_dir / f\"model_compress_{level}.joblib\"\n",
    "    joblib.dump(rf_model, comp_path, compress=level)\n",
    "    \n",
    "    # Measure size\n",
    "    file_size = comp_path.stat().st_size / 1024\n",
    "    \n",
    "    # Measure load time (average of 10 runs)\n",
    "    load_times = []\n",
    "    for _ in range(10):\n",
    "        start = time.time()\n",
    "        _ = joblib.load(comp_path)\n",
    "        load_times.append(time.time() - start)\n",
    "    \n",
    "    avg_load_time = np.mean(load_times) * 1000  # Convert to ms\n",
    "    \n",
    "    compression_results.append({\n",
    "        'level': level,\n",
    "        'size_kb': file_size,\n",
    "        'load_time_ms': avg_load_time\n",
    "    })\n",
    "\n",
    "comp_df = pd.DataFrame(compression_results)\n",
    "print(\"Compression Analysis:\")\n",
    "print(comp_df.to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Size comparison\n",
    "ax1.plot(comp_df['level'], comp_df['size_kb'], marker='o', linewidth=2, color='darkblue')\n",
    "ax1.set_xlabel('Compression Level', fontweight='bold')\n",
    "ax1.set_ylabel('File Size (KB)', fontweight='bold')\n",
    "ax1.set_title('File Size vs Compression', fontweight='bold')\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Load time comparison\n",
    "ax2.plot(comp_df['level'], comp_df['load_time_ms'], marker='o', linewidth=2, color='darkred')\n",
    "ax2.set_xlabel('Compression Level', fontweight='bold')\n",
    "ax2.set_ylabel('Load Time (ms)', fontweight='bold')\n",
    "ax2.set_title('Load Time vs Compression', fontweight='bold')\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Choose the right format**:\n",
    "   - **pickle**: Quick prototyping (Python-only)\n",
    "   - **joblib**: sklearn models (recommended)\n",
    "   - **ONNX**: Cross-platform deployment\n",
    "\n",
    "2. **Compression reduces file size** with minimal impact on load time\n",
    "\n",
    "3. **Always serialize the entire pipeline** (preprocessing + model) for production\n",
    "\n",
    "4. **Track versions** (Python, libraries, training date) to avoid compatibility issues\n",
    "\n",
    "5. **Model size matters** for deployment costs and latency\n",
    "\n",
    "6. **Security**: Never unpickle untrusted data\n",
    "\n",
    "### Decision Guide\n",
    "\n",
    "**Use pickle/joblib when**:\n",
    "- Deploying to Python environments only\n",
    "- You control both training and inference environments\n",
    "- Quick iteration is important\n",
    "\n",
    "**Use ONNX when**:\n",
    "- Deploying to non-Python environments\n",
    "- Need cross-framework compatibility\n",
    "- Performance optimization is critical\n",
    "- Working with edge devices\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In **Module 04**, we'll learn about:\n",
    "- **Creating ML APIs** with FastAPI\n",
    "- **Request/response models** with Pydantic\n",
    "- **Input validation** and error handling\n",
    "- **Testing API endpoints**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Additional Resources\n",
    "\n",
    "### Documentation\n",
    "- **Joblib Persistence**: https://joblib.readthedocs.io/en/latest/persistence.html\n",
    "- **sklearn Model Persistence**: https://scikit-learn.org/stable/model_persistence.html\n",
    "- **ONNX**: https://onnx.ai/\n",
    "- **skl2onnx**: https://onnx.ai/sklearn-onnx/\n",
    "\n",
    "### Tutorials\n",
    "- **ONNX Runtime**: https://onnxruntime.ai/\n",
    "- **Model Deployment Best Practices**: https://ml-ops.org/\n",
    "\n",
    "### Advanced Topics\n",
    "- TensorFlow SavedModel format\n",
    "- PyTorch TorchScript\n",
    "- Model quantization for edge deployment\n",
    "- Model serving with TensorFlow Serving"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
