{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 01: Experiment Tracking with MLflow\n",
    "\n",
    "**Difficulty**: â­â­ Intermediate\n",
    "**Estimated Time**: 60 minutes\n",
    "**Prerequisites**: \n",
    "- [Module 00: Introduction to MLOps](00_introduction_to_mlops.ipynb)\n",
    "- Basic understanding of scikit-learn\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Set up MLflow for experiment tracking\n",
    "2. Log parameters, metrics, and artifacts to MLflow\n",
    "3. Compare multiple experiments using MLflow UI\n",
    "4. Use MLflow autologging for popular ML frameworks\n",
    "5. Organize experiments with tags and nested runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Experiment Tracking?\n",
    "\n",
    "### The Problem Without Tracking\n",
    "\n",
    "Imagine you've trained 50 models over 2 weeks:\n",
    "- **Model #32 had the best validation accuracy**, but which hyperparameters did you use?\n",
    "- **Model #47 worked great on production-like data**, but what was the random seed?\n",
    "- **Your manager asks**: \"Can you reproduce the model from last Tuesday?\"\n",
    "- **Result**: ðŸ˜° You have no idea!\n",
    "\n",
    "### The Solution: MLflow Tracking\n",
    "\n",
    "**MLflow** is an open-source platform for managing the ML lifecycle. It has four main components:\n",
    "\n",
    "1. **MLflow Tracking**: Log and query experiments (parameters, metrics, artifacts)\n",
    "2. **MLflow Projects**: Package code in reproducible format\n",
    "3. **MLflow Models**: Deploy models to various platforms\n",
    "4. **MLflow Registry**: Central model store for versioning and staging\n",
    "\n",
    "In this module, we focus on **MLflow Tracking**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Import all required libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.datasets import make_classification, load_diabetes\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    mean_squared_error, mean_absolute_error, r2_score\n",
    ")\n",
    "\n",
    "# MLflow\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(f\"MLflow version: {mlflow.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MLflow Basics: Your First Tracked Experiment\n",
    "\n",
    "Let's start with a simple example to understand MLflow's core concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up MLflow tracking URI (local directory for this tutorial)\n",
    "# In production, this would point to a remote tracking server\n",
    "mlflow.set_tracking_uri(\"file:./mlruns\")\n",
    "\n",
    "# Set experiment name\n",
    "# All runs will be grouped under this experiment\n",
    "mlflow.set_experiment(\"classification_experiments\")\n",
    "\n",
    "print(\"MLflow tracking URI:\", mlflow.get_tracking_uri())\n",
    "print(\"Current experiment:\", mlflow.get_experiment_by_name(\"classification_experiments\").name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic dataset for classification\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=20,\n",
    "    n_informative=15,\n",
    "    n_redundant=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "print(f\"Test set size: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic MLflow Run Structure\n",
    "\n",
    "Every tracked experiment follows this pattern:\n",
    "\n",
    "```python\n",
    "with mlflow.start_run():\n",
    "    # 1. Log parameters\n",
    "    mlflow.log_param(\"param_name\", value)\n",
    "    \n",
    "    # 2. Train model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # 3. Log metrics\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    \n",
    "    # 4. Log model\n",
    "    mlflow.sklearn.log_model(model, \"model\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Track a simple logistic regression model\n",
    "\n",
    "with mlflow.start_run(run_name=\"logistic_regression_baseline\"):\n",
    "    \n",
    "    # Define hyperparameters\n",
    "    max_iter = 1000\n",
    "    C = 1.0\n",
    "    \n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"model_type\", \"LogisticRegression\")\n",
    "    mlflow.log_param(\"max_iter\", max_iter)\n",
    "    mlflow.log_param(\"C\", C)\n",
    "    mlflow.log_param(\"solver\", \"lbfgs\")\n",
    "    \n",
    "    # Train model\n",
    "    model = LogisticRegression(max_iter=max_iter, C=C, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "    mlflow.log_metric(\"f1_score\", f1)\n",
    "    \n",
    "    # Log the trained model\n",
    "    mlflow.sklearn.log_model(model, \"model\")\n",
    "    \n",
    "    # Add tags for better organization\n",
    "    mlflow.set_tag(\"stage\", \"baseline\")\n",
    "    mlflow.set_tag(\"team\", \"data-science\")\n",
    "    \n",
    "    print(f\"âœ“ Logged run with accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  Run ID: {mlflow.active_run().info.run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comparing Multiple Experiments\n",
    "\n",
    "The power of MLflow comes from comparing different approaches systematically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with different Random Forest configurations\n",
    "\n",
    "# Define hyperparameter grid\n",
    "rf_configs = [\n",
    "    {\"n_estimators\": 50, \"max_depth\": 5, \"min_samples_split\": 2},\n",
    "    {\"n_estimators\": 100, \"max_depth\": 10, \"min_samples_split\": 2},\n",
    "    {\"n_estimators\": 100, \"max_depth\": 10, \"min_samples_split\": 5},\n",
    "    {\"n_estimators\": 200, \"max_depth\": 15, \"min_samples_split\": 2},\n",
    "    {\"n_estimators\": 200, \"max_depth\": None, \"min_samples_split\": 5},\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, config in enumerate(rf_configs, 1):\n",
    "    with mlflow.start_run(run_name=f\"random_forest_config_{i}\"):\n",
    "        \n",
    "        # Log all hyperparameters\n",
    "        mlflow.log_param(\"model_type\", \"RandomForest\")\n",
    "        mlflow.log_params(config)  # Log dictionary of params at once\n",
    "        \n",
    "        # Train model\n",
    "        model = RandomForestClassifier(**config, random_state=42, n_jobs=-1)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metrics({\n",
    "            \"accuracy\": accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1_score\": f1\n",
    "        })\n",
    "        \n",
    "        # Log model\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "        \n",
    "        # Tags\n",
    "        mlflow.set_tag(\"stage\", \"experimentation\")\n",
    "        mlflow.set_tag(\"model_family\", \"tree_based\")\n",
    "        \n",
    "        # Store results for visualization\n",
    "        results.append({\n",
    "            \"config_id\": i,\n",
    "            \"n_estimators\": config[\"n_estimators\"],\n",
    "            \"max_depth\": config[\"max_depth\"] or \"None\",\n",
    "            \"accuracy\": accuracy,\n",
    "            \"f1_score\": f1\n",
    "        })\n",
    "        \n",
    "        print(f\"âœ“ Config {i}: Accuracy={accuracy:.4f}, F1={f1:.4f}\")\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nAll experiments completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize experiment results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Accuracy comparison\n",
    "axes[0].bar(results_df['config_id'], results_df['accuracy'], \n",
    "            color='skyblue', edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('Configuration ID', fontweight='bold')\n",
    "axes[0].set_ylabel('Accuracy', fontweight='bold')\n",
    "axes[0].set_title('Model Accuracy Across Configurations', fontweight='bold')\n",
    "axes[0].set_ylim(0.85, 0.95)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 2: F1 Score comparison\n",
    "axes[1].bar(results_df['config_id'], results_df['f1_score'], \n",
    "            color='lightcoral', edgecolor='black', alpha=0.7)\n",
    "axes[1].set_xlabel('Configuration ID', fontweight='bold')\n",
    "axes[1].set_ylabel('F1 Score', fontweight='bold')\n",
    "axes[1].set_title('F1 Score Across Configurations', fontweight='bold')\n",
    "axes[1].set_ylim(0.85, 0.95)\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show best configuration\n",
    "best_config = results_df.loc[results_df['f1_score'].idxmax()]\n",
    "print(f\"\\nBest Configuration: #{int(best_config['config_id'])}\")\n",
    "print(f\"  n_estimators: {int(best_config['n_estimators'])}\")\n",
    "print(f\"  max_depth: {best_config['max_depth']}\")\n",
    "print(f\"  F1 Score: {best_config['f1_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Logging Artifacts (Plots, Files, Models)\n",
    "\n",
    "Beyond metrics, you can log:\n",
    "- **Plots**: Confusion matrices, ROC curves, feature importance\n",
    "- **Files**: Datasets, configuration files, text reports\n",
    "- **Models**: Serialized models for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "with mlflow.start_run(run_name=\"random_forest_with_artifacts\"):\n",
    "    \n",
    "    # Train model\n",
    "    config = {\"n_estimators\": 100, \"max_depth\": 10, \"random_state\": 42}\n",
    "    mlflow.log_params(config)\n",
    "    \n",
    "    model = RandomForestClassifier(**config)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate and log metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    \n",
    "    # Create and log confusion matrix plot\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    disp.plot(ax=ax, cmap='Blues', values_format='d')\n",
    "    ax.set_title('Confusion Matrix', fontweight='bold', fontsize=14)\n",
    "    \n",
    "    # Save plot temporarily and log to MLflow\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        plot_path = os.path.join(tmpdir, \"confusion_matrix.png\")\n",
    "        plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "        mlflow.log_artifact(plot_path, artifact_path=\"plots\")\n",
    "    \n",
    "    plt.close()\n",
    "    \n",
    "    # Create and log feature importance plot\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': [f'feature_{i}' for i in range(X.shape[1])],\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False).head(10)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.barh(feature_importance['feature'], feature_importance['importance'], color='teal')\n",
    "    ax.set_xlabel('Importance', fontweight='bold')\n",
    "    ax.set_title('Top 10 Feature Importances', fontweight='bold', fontsize=14)\n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        plot_path = os.path.join(tmpdir, \"feature_importance.png\")\n",
    "        plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "        mlflow.log_artifact(plot_path, artifact_path=\"plots\")\n",
    "    \n",
    "    plt.close()\n",
    "    \n",
    "    # Log text report as artifact\n",
    "    report = f\"\"\"\n",
    "    Model Performance Report\n",
    "    ========================\n",
    "    Model Type: Random Forest\n",
    "    Accuracy: {accuracy:.4f}\n",
    "    \n",
    "    Hyperparameters:\n",
    "    - n_estimators: {config['n_estimators']}\n",
    "    - max_depth: {config['max_depth']}\n",
    "    \n",
    "    Training set size: {len(X_train)}\n",
    "    Test set size: {len(X_test)}\n",
    "    \"\"\"\n",
    "    \n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        report_path = os.path.join(tmpdir, \"report.txt\")\n",
    "        with open(report_path, 'w') as f:\n",
    "            f.write(report)\n",
    "        mlflow.log_artifact(report_path, artifact_path=\"reports\")\n",
    "    \n",
    "    # Log model\n",
    "    mlflow.sklearn.log_model(model, \"model\")\n",
    "    \n",
    "    print(\"âœ“ Logged model with plots and reports!\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  Artifacts: confusion_matrix.png, feature_importance.png, report.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. MLflow Autologging\n",
    "\n",
    "MLflow provides **autologging** for popular frameworks:\n",
    "- scikit-learn\n",
    "- TensorFlow/Keras\n",
    "- PyTorch\n",
    "- XGBoost\n",
    "- LightGBM\n",
    "\n",
    "**Autologging automatically captures**:\n",
    "- Model hyperparameters\n",
    "- Training metrics\n",
    "- Model artifacts\n",
    "- Plots (for some frameworks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable autologging for scikit-learn\n",
    "mlflow.sklearn.autolog()\n",
    "\n",
    "with mlflow.start_run(run_name=\"autolog_random_forest\"):\n",
    "    # Just train the model - MLflow logs everything automatically!\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=150,\n",
    "        max_depth=12,\n",
    "        min_samples_split=3,\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate (autolog captures this too)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # You can still log custom metrics/tags\n",
    "    mlflow.log_metric(\"custom_metric\", accuracy * 100)\n",
    "    mlflow.set_tag(\"note\", \"Using autologging\")\n",
    "    \n",
    "    print(f\"âœ“ Autologging captured everything automatically!\")\n",
    "    print(f\"  Check MLflow UI to see all logged parameters and metrics\")\n",
    "\n",
    "# Disable autologging when done\n",
    "mlflow.sklearn.autolog(disable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Nested Runs for Complex Experiments\n",
    "\n",
    "For experiments with multiple stages (e.g., hyperparameter tuning with cross-validation), use **nested runs**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nested runs example: Parent run with child runs for each fold\n",
    "\n",
    "with mlflow.start_run(run_name=\"cross_validation_experiment\") as parent_run:\n",
    "    \n",
    "    # Log parent-level info\n",
    "    mlflow.log_param(\"cv_folds\", 5)\n",
    "    mlflow.log_param(\"model_type\", \"RandomForest\")\n",
    "    \n",
    "    # Configuration\n",
    "    config = {\"n_estimators\": 100, \"max_depth\": 10, \"random_state\": 42}\n",
    "    mlflow.log_params(config)\n",
    "    \n",
    "    # Perform cross-validation with nested runs\n",
    "    model = RandomForestClassifier(**config)\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    \n",
    "    # Log each fold as a nested run\n",
    "    for fold, score in enumerate(cv_scores, 1):\n",
    "        with mlflow.start_run(run_name=f\"fold_{fold}\", nested=True):\n",
    "            mlflow.log_metric(\"accuracy\", score)\n",
    "            mlflow.log_param(\"fold\", fold)\n",
    "    \n",
    "    # Log aggregate metrics in parent run\n",
    "    mlflow.log_metric(\"mean_cv_accuracy\", cv_scores.mean())\n",
    "    mlflow.log_metric(\"std_cv_accuracy\", cv_scores.std())\n",
    "    \n",
    "    print(f\"âœ“ Cross-validation completed\")\n",
    "    print(f\"  Mean CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
    "    print(f\"  Parent Run ID: {parent_run.info.run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Querying and Searching Experiments\n",
    "\n",
    "MLflow provides a powerful API to search and filter runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "# Get current experiment\n",
    "experiment = client.get_experiment_by_name(\"classification_experiments\")\n",
    "experiment_id = experiment.experiment_id\n",
    "\n",
    "# Search for runs with accuracy > 0.90\n",
    "runs = client.search_runs(\n",
    "    experiment_ids=[experiment_id],\n",
    "    filter_string=\"metrics.accuracy > 0.90\",\n",
    "    order_by=[\"metrics.accuracy DESC\"],\n",
    "    max_results=10\n",
    ")\n",
    "\n",
    "print(f\"Found {len(runs)} runs with accuracy > 0.90\\n\")\n",
    "\n",
    "# Display top runs\n",
    "results = []\n",
    "for run in runs:\n",
    "    results.append({\n",
    "        \"run_name\": run.data.tags.get(\"mlflow.runName\", \"N/A\"),\n",
    "        \"accuracy\": run.data.metrics.get(\"accuracy\", 0),\n",
    "        \"model_type\": run.data.params.get(\"model_type\", \"N/A\"),\n",
    "        \"run_id\": run.info.run_id[:8]  # Shortened for display\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Accessing the MLflow UI\n",
    "\n",
    "To view all your experiments in a nice web interface:\n",
    "\n",
    "```bash\n",
    "# Run this command in your terminal (not in notebook)\n",
    "mlflow ui --backend-store-uri file:./mlruns\n",
    "```\n",
    "\n",
    "Then open your browser to: **http://localhost:5000**\n",
    "\n",
    "### What You Can Do in the UI:\n",
    "- **Compare runs** side-by-side\n",
    "- **Visualize metrics** over time\n",
    "- **Download artifacts** (models, plots, reports)\n",
    "- **Search and filter** runs\n",
    "- **Add notes and tags**\n",
    "- **Promote models** to registry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Exercises\n",
    "\n",
    "Test your understanding with these exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Track a Regression Model\n",
    "\n",
    "Use the diabetes dataset and track a regression experiment with MLflow.\n",
    "\n",
    "**Requirements**:\n",
    "1. Load diabetes dataset\n",
    "2. Split into train/test\n",
    "3. Train a GradientBoostingRegressor\n",
    "4. Log parameters: n_estimators, learning_rate, max_depth\n",
    "5. Log metrics: MSE, MAE, RÂ²\n",
    "6. Log the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n",
    "\n",
    "# Load dataset\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Split data\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Train and log with MLflow\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1 Solution\n",
    "\n",
    "# Load diabetes dataset\n",
    "diabetes = load_diabetes()\n",
    "X, y = diabetes.data, diabetes.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Set experiment\n",
    "mlflow.set_experiment(\"regression_experiments\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"gradient_boosting_regression\"):\n",
    "    \n",
    "    # Hyperparameters\n",
    "    params = {\n",
    "        \"n_estimators\": 100,\n",
    "        \"learning_rate\": 0.1,\n",
    "        \"max_depth\": 3,\n",
    "        \"random_state\": 42\n",
    "    }\n",
    "    \n",
    "    # Log parameters\n",
    "    mlflow.log_params(params)\n",
    "    \n",
    "    # Train model\n",
    "    model = GradientBoostingRegressor(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metrics({\n",
    "        \"mse\": mse,\n",
    "        \"mae\": mae,\n",
    "        \"r2_score\": r2\n",
    "    })\n",
    "    \n",
    "    # Log model\n",
    "    mlflow.sklearn.log_model(model, \"model\")\n",
    "    \n",
    "    print(f\"âœ“ Regression model tracked successfully!\")\n",
    "    print(f\"  MSE: {mse:.2f}\")\n",
    "    print(f\"  MAE: {mae:.2f}\")\n",
    "    print(f\"  RÂ²: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Compare Multiple Learning Rates\n",
    "\n",
    "Run experiments with different learning rates and find the best one.\n",
    "\n",
    "**Requirements**:\n",
    "1. Test learning rates: [0.01, 0.05, 0.1, 0.2, 0.5]\n",
    "2. Keep other hyperparameters constant\n",
    "3. Log all experiments\n",
    "4. Print which learning rate gave the best RÂ² score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2 Solution\n",
    "\n",
    "learning_rates = [0.01, 0.05, 0.1, 0.2, 0.5]\n",
    "results = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    with mlflow.start_run(run_name=f\"gbr_lr_{lr}\"):\n",
    "        \n",
    "        # Log parameters\n",
    "        params = {\n",
    "            \"n_estimators\": 100,\n",
    "            \"learning_rate\": lr,\n",
    "            \"max_depth\": 3,\n",
    "            \"random_state\": 42\n",
    "        }\n",
    "        mlflow.log_params(params)\n",
    "        \n",
    "        # Train model\n",
    "        model = GradientBoostingRegressor(**params)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate\n",
    "        y_pred = model.predict(X_test)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metric(\"r2_score\", r2)\n",
    "        \n",
    "        results.append({\"learning_rate\": lr, \"r2_score\": r2})\n",
    "        print(f\"Learning Rate {lr}: RÂ² = {r2:.4f}\")\n",
    "\n",
    "# Find best\n",
    "results_df = pd.DataFrame(results)\n",
    "best = results_df.loc[results_df['r2_score'].idxmax()]\n",
    "print(f\"\\nâœ“ Best learning rate: {best['learning_rate']} (RÂ² = {best['r2_score']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Log Prediction vs Actual Plot\n",
    "\n",
    "Create a scatter plot comparing predictions vs actual values and log it as an artifact.\n",
    "\n",
    "**Requirements**:\n",
    "1. Train a model\n",
    "2. Create scatter plot (actual vs predicted)\n",
    "3. Add reference line (perfect predictions)\n",
    "4. Save and log plot to MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3 Solution\n",
    "\n",
    "with mlflow.start_run(run_name=\"regression_with_plot\"):\n",
    "    \n",
    "    # Train model\n",
    "    params = {\"n_estimators\": 100, \"learning_rate\": 0.1, \"random_state\": 42}\n",
    "    mlflow.log_params(params)\n",
    "    \n",
    "    model = GradientBoostingRegressor(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Metrics\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mlflow.log_metric(\"r2_score\", r2)\n",
    "    \n",
    "    # Create plot\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.scatter(y_test, y_pred, alpha=0.6, edgecolors='k', s=50)\n",
    "    \n",
    "    # Add perfect prediction line\n",
    "    min_val = min(y_test.min(), y_pred.min())\n",
    "    max_val = max(y_test.max(), y_pred.max())\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')\n",
    "    \n",
    "    ax.set_xlabel('Actual Values', fontweight='bold', fontsize=12)\n",
    "    ax.set_ylabel('Predicted Values', fontweight='bold', fontsize=12)\n",
    "    ax.set_title(f'Predictions vs Actual (RÂ² = {r2:.4f})', fontweight='bold', fontsize=14)\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    # Save and log plot\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        plot_path = os.path.join(tmpdir, \"predictions_vs_actual.png\")\n",
    "        plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "        mlflow.log_artifact(plot_path, artifact_path=\"plots\")\n",
    "    \n",
    "    plt.show()\n",
    "    print(\"âœ“ Plot logged to MLflow!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **MLflow Tracking** solves the \"which model was that?\" problem by systematically logging experiments\n",
    "\n",
    "2. **Core tracking components**:\n",
    "   - **Parameters**: Hyperparameters and configuration\n",
    "   - **Metrics**: Performance measures (accuracy, loss, etc.)\n",
    "   - **Artifacts**: Models, plots, reports, data files\n",
    "   - **Tags**: Metadata for organization\n",
    "\n",
    "3. **Autologging** saves time by automatically capturing model info for popular frameworks\n",
    "\n",
    "4. **MLflow UI** provides powerful visualization and comparison tools\n",
    "\n",
    "5. **Nested runs** help organize complex experiments like cross-validation\n",
    "\n",
    "6. **Search API** enables programmatic filtering and analysis of experiments\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "- Always use descriptive run names\n",
    "- Log enough context to reproduce the experiment\n",
    "- Use tags to organize experiments by team, stage, or project\n",
    "- Log plots and reports, not just numbers\n",
    "- Review experiments regularly in the UI\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In **Module 02**, we'll learn about:\n",
    "- **Model versioning** with MLflow Model Registry\n",
    "- **Staging models** (dev â†’ staging â†’ production)\n",
    "- **Model lineage** tracking\n",
    "- **Transitioning models** between stages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Additional Resources\n",
    "\n",
    "### Documentation\n",
    "- **MLflow Tracking**: https://mlflow.org/docs/latest/tracking.html\n",
    "- **MLflow Python API**: https://mlflow.org/docs/latest/python_api/index.html\n",
    "- **Autologging**: https://mlflow.org/docs/latest/tracking.html#automatic-logging\n",
    "\n",
    "### Tutorials\n",
    "- **MLflow Quickstart**: https://mlflow.org/docs/latest/quickstart.html\n",
    "- **MLflow Examples**: https://github.com/mlflow/mlflow/tree/master/examples\n",
    "\n",
    "### Advanced Topics\n",
    "- Setting up remote tracking servers\n",
    "- Integrating with cloud storage (S3, Azure Blob)\n",
    "- Using MLflow with distributed training\n",
    "- MLflow Projects for reproducible workflows"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
