{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Module 05: Containerization with Docker\n\n**Difficulty**: \u2b50\u2b50 Intermediate\n**Estimated Time**: 65 minutes\n**Prerequisites**:\n- [Module 04: ML APIs with FastAPI](04_ml_apis_fastapi.ipynb)\n- Basic command line knowledge\n\n## Learning Objectives\nBy the end of this notebook, you will be able to:\n1. Understand why Docker is essential for ML deployment\n2. Create Dockerfiles for ML applications\n3. Build Docker images for ML models\n4. Run containers with proper configuration\n5. Optimize Docker images for production\n6. Use Docker best practices for ML workloads"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Why Docker for ML?\n\n### The Problem: \"It Works on My Machine\"\n\nYou've trained a perfect model on your laptop:\n- Python 3.9, scikit-learn 1.0, CUDA 11.4\n- Works flawlessly!\n\nProduction server:\n- Python 3.7, scikit-learn 0.23, no CUDA\n- Model crashes... \ud83d\ude30\n\n### The Solution: Docker Containers\n\n**Docker** packages your application with **ALL** dependencies:\n- Exact Python version\n- All libraries with correct versions\n- System dependencies\n- Environment variables\n- Configuration files\n\n**Result**: If it works in Docker on your laptop, it works **anywhere**.\n\n### Benefits for ML\n\n1. **Reproducibility**: Exact same environment everywhere\n2. **Portability**: Run on any machine with Docker\n3. **Isolation**: Each model in its own container\n4. **Scalability**: Easy to replicate containers\n5. **Version Control**: Docker images are versioned\n\n### Docker vs VM\n\n| Feature | Docker Container | Virtual Machine |\n|---------|------------------|----------------|\n| Size | 100 MB | 10 GB |\n| Startup | < 1 second | Minutes |\n| Performance | Native | Overhead |\n| Isolation | Process | OS |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup: Import required libraries\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nimport json\nfrom pathlib import Path\nimport subprocess\n\nprint(\"Setup complete!\")\nprint(\"\\nNote: This notebook demonstrates Docker concepts.\")\nprint(\"Actual Docker commands should be run in a terminal.\")\nprint(\"\\nDocker must be installed on your system to run the examples.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Docker Fundamentals\n\n### Key Concepts\n\n**Image**: Blueprint for containers (like a class)\n- Contains OS, code, dependencies\n- Immutable\n- Can be shared via registries\n\n**Container**: Running instance of an image (like an object)\n- Isolated process\n- Has its own filesystem\n- Can be started/stopped\n\n**Dockerfile**: Recipe to build an image\n- Text file with instructions\n- Each instruction creates a layer\n\n**Registry**: Storage for images\n- Docker Hub (public)\n- AWS ECR, Google GCR (private)\n\n### Essential Docker Commands\n\n```bash\n# Build image from Dockerfile\ndocker build -t my-ml-model:v1 .\n\n# Run container\ndocker run -p 8000:8000 my-ml-model:v1\n\n# List images\ndocker images\n\n# List running containers\ndocker ps\n\n# Stop container\ndocker stop <container-id>\n\n# View logs\ndocker logs <container-id>\n\n# Remove container\ndocker rm <container-id>\n\n# Remove image\ndocker rmi my-ml-model:v1\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Creating a Dockerfile\n\nLet's create a Dockerfile for our FastAPI ML application."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create project directory\nproject_dir = Path(\"docker_ml_app\")\nproject_dir.mkdir(exist_ok=True)\n\n# Create app subdirectory\napp_dir = project_dir / \"app\"\napp_dir.mkdir(exist_ok=True)\n\nprint(f\"\u2713 Project structure created\")\nprint(f\"  Directory: {project_dir.absolute()}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create requirements.txt\nrequirements_content = \"\"\"fastapi==0.104.1\nuvicorn[standard]==0.24.0\nscikit-learn==1.3.2\nnumpy==1.24.3\npandas==2.0.3\npydantic==2.5.0\njoblib==1.3.2\n\"\"\"\n\nwith open(project_dir / \"requirements.txt\", \"w\") as f:\n    f.write(requirements_content)\n\nprint(\"\u2713 requirements.txt created\")\nprint(\"\\nContents:\")\nprint(requirements_content)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create Dockerfile\ndockerfile_content = \"\"\"# Start from official Python image\nFROM python:3.9-slim\n\n# Set working directory\nWORKDIR /app\n\n# Copy requirements first (for layer caching)\nCOPY requirements.txt .\n\n# Install dependencies\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy application code\nCOPY ./app ./app\n\n# Copy model files\nCOPY ./models ./models\n\n# Expose port\nEXPOSE 8000\n\n# Run the application\nCMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n\"\"\"\n\nwith open(project_dir / \"Dockerfile\", \"w\") as f:\n    f.write(dockerfile_content)\n\nprint(\"\u2713 Dockerfile created\")\nprint(\"\\nContents:\")\nprint(dockerfile_content)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Understanding Dockerfile Instructions\n\nLet's break down each instruction:\n\n### FROM python:3.9-slim\n- **Purpose**: Choose base image\n- **Why slim?**: Smaller size (100MB vs 900MB for full image)\n- **Trade-off**: May need to install system dependencies\n\n### WORKDIR /app\n- **Purpose**: Set working directory inside container\n- **Why?**: All subsequent commands run from here\n\n### COPY requirements.txt .\n- **Purpose**: Copy file from host to container\n- **Why first?**: Docker layer caching - if requirements don't change, this layer is reused\n\n### RUN pip install...\n- **Purpose**: Execute command during build\n- **--no-cache-dir**: Don't store pip cache (saves space)\n\n### COPY ./app ./app\n- **Purpose**: Copy application code\n- **Why after requirements?**: Code changes more often than dependencies\n\n### EXPOSE 8000\n- **Purpose**: Document which port the app uses\n- **Note**: Doesn't actually publish the port (done with `docker run -p`)\n\n### CMD [...]\n- **Purpose**: Command to run when container starts\n- **Format**: JSON array for better handling"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Optimizing Docker Images for ML\n\n### Multi-Stage Builds\n\nReduce final image size by using separate build and runtime stages:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create optimized Dockerfile with multi-stage build\noptimized_dockerfile = \"\"\"# Build stage\nFROM python:3.9 as builder\n\nWORKDIR /app\n\n# Install dependencies\nCOPY requirements.txt .\nRUN pip install --user --no-cache-dir -r requirements.txt\n\n# Runtime stage\nFROM python:3.9-slim\n\nWORKDIR /app\n\n# Copy only installed packages from builder\nCOPY --from=builder /root/.local /root/.local\n\n# Copy application\nCOPY ./app ./app\nCOPY ./models ./models\n\n# Make sure scripts in .local are usable\nENV PATH=/root/.local/bin:$PATH\n\nEXPOSE 8000\n\nCMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n\"\"\"\n\nwith open(project_dir / \"Dockerfile.optimized\", \"w\") as f:\n    f.write(optimized_dockerfile)\n\nprint(\"\u2713 Optimized Dockerfile created\")\nprint(\"\\nKey optimizations:\")\nprint(\"  1. Multi-stage build (smaller final image)\")\nprint(\"  2. Only runtime dependencies in final image\")\nprint(\"  3. Build artifacts discarded\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. .dockerignore File\n\nExclude unnecessary files from the build context:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create .dockerignore\ndockerignore_content = \"\"\"# Python\n__pycache__\n*.pyc\n*.pyo\n*.pyd\n.Python\n*.so\n\n# Jupyter\n.ipynb_checkpoints\n*.ipynb\n\n# Virtual environments\nvenv/\nenv/\nENV/\n\n# IDEs\n.vscode/\n.idea/\n*.swp\n\n# Git\n.git/\n.gitignore\n\n# Data (use separate volume)\ndata/raw/\ndata/processed/\n*.csv\n*.parquet\n\n# Documentation\nREADME.md\ndocs/\n\n# Testing\ntests/\n.pytest_cache/\n.coverage\n\"\"\"\n\nwith open(project_dir / \".dockerignore\", \"w\") as f:\n    f.write(dockerignore_content)\n\nprint(\"\u2713 .dockerignore created\")\nprint(\"\\nBenefits:\")\nprint(\"  - Faster builds (smaller context)\")\nprint(\"  - Smaller images\")\nprint(\"  - Prevents copying sensitive files\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Docker Best Practices for ML\n\n### 1. Use Specific Base Images\n\n```dockerfile\n# \u274c Bad: Latest tag changes over time\nFROM python:latest\n\n# \u2705 Good: Specific version\nFROM python:3.9.18-slim\n```\n\n### 2. Minimize Layers\n\n```dockerfile\n# \u274c Bad: Multiple RUN commands (more layers)\nRUN pip install numpy\nRUN pip install pandas\nRUN pip install scikit-learn\n\n# \u2705 Good: Single RUN command\nRUN pip install numpy pandas scikit-learn\n```\n\n### 3. Order Instructions by Change Frequency\n\n```dockerfile\n# \u2705 Good order:\n# 1. Base image (rarely changes)\n# 2. System dependencies (rarely changes)\n# 3. Python dependencies (changes occasionally)\n# 4. Application code (changes frequently)\n```\n\n### 4. Use .dockerignore\n\nExclude unnecessary files (shown above)\n\n### 5. Run as Non-Root User\n\n```dockerfile\n# Create non-root user\nRUN useradd -m -u 1000 mluser\nUSER mluser\n```\n\n### 6. Set Environment Variables\n\n```dockerfile\n# Prevent Python buffering (see logs immediately)\nENV PYTHONUNBUFFERED=1\n\n# Disable pip version check\nENV PIP_DISABLE_PIP_VERSION_CHECK=1\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. Building and Running\n\nHere's how you would build and run the Docker image:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create example build and run commands\nbuild_run_guide = \"\"\"# Step 1: Build the Docker image\n# Run this in the directory containing the Dockerfile\ndocker build -t ml-api:v1.0 .\n\n# Step 2: Run the container\n# -p 8000:8000 maps port 8000 from container to host\n# -d runs in detached mode (background)\n# --name gives the container a friendly name\ndocker run -d -p 8000:8000 --name ml-api-container ml-api:v1.0\n\n# Step 3: Check if running\ndocker ps\n\n# Step 4: View logs\ndocker logs ml-api-container\n\n# Step 5: Test the API\ncurl http://localhost:8000/\n\n# Step 6: Stop the container\ndocker stop ml-api-container\n\n# Step 7: Remove the container\ndocker rm ml-api-container\n\n# Advanced: Run with environment variables\ndocker run -d -p 8000:8000 \\\\\n  -e MODEL_VERSION=1.0 \\\\\n  -e LOG_LEVEL=INFO \\\\\n  --name ml-api-container \\\\\n  ml-api:v1.0\n\n# Advanced: Mount model directory as volume\ndocker run -d -p 8000:8000 \\\\\n  -v $(pwd)/models:/app/models \\\\\n  --name ml-api-container \\\\\n  ml-api:v1.0\n\"\"\"\n\nprint(\"Docker Build & Run Guide:\")\nprint(build_run_guide)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 9. Exercises\n\nPractice Dockerfile creation and optimization."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercise 1: Create Complete Project Structure\n\nCreate a complete Docker project with a simple FastAPI app.\n\n**Requirements**:\n1. Create app/main.py with a simple FastAPI app\n2. Create requirements.txt\n3. Create Dockerfile\n4. Add .dockerignore"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Exercise 1: Your code here\n\n# YOUR CODE HERE"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Exercise 1 Solution\n\n# Create complete project structure\nexercise_dir = Path(\"exercise_docker_app\")\nexercise_dir.mkdir(exist_ok=True)\n(exercise_dir / \"app\").mkdir(exist_ok=True)\n(exercise_dir / \"models\").mkdir(exist_ok=True)\n\n# Create simple FastAPI app\napp_code = '''\"\"\"Simple ML API for Docker demo\"\"\"\nfrom fastapi import FastAPI\nimport joblib\nfrom pathlib import Path\n\napp = FastAPI(title=\"Simple ML API\")\n\n@app.get(\"/\")\ndef root():\n    return {\"message\": \"ML API is running in Docker!\", \"status\": \"healthy\"}\n\n@app.get(\"/health\")\ndef health():\n    return {\"status\": \"OK\"}\n'''\n\nwith open(exercise_dir / \"app\" / \"main.py\", \"w\") as f:\n    f.write(app_code)\n\n# Requirements\nreqs = \"fastapi==0.104.1\\nuvicorn[standard]==0.24.0\\njoblib==1.3.2\\n\"\nwith open(exercise_dir / \"requirements.txt\", \"w\") as f:\n    f.write(reqs)\n\n# Dockerfile\ndockerfile = \"\"\"FROM python:3.9-slim\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\nCOPY ./app ./app\nEXPOSE 8000\nCMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n\"\"\"\nwith open(exercise_dir / \"Dockerfile\", \"w\") as f:\n    f.write(dockerfile)\n\n# .dockerignore\ndockerignore = \"__pycache__/\\n*.pyc\\n.git/\\n.vscode/\\n\"\nwith open(exercise_dir / \".dockerignore\", \"w\") as f:\n    f.write(dockerignore)\n\nprint(\"\u2713 Complete Docker project created!\")\nprint(f\"  Location: {exercise_dir.absolute()}\")\nprint(f\"  Files: app/main.py, requirements.txt, Dockerfile, .dockerignore\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercise 2: Optimize Dockerfile\n\nImprove the Dockerfile with best practices.\n\n**Requirements**:\n1. Use multi-stage build\n2. Add non-root user\n3. Set Python environment variables\n4. Minimize layers"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Exercise 2 Solution\n\noptimized_ex_dockerfile = \"\"\"# Build stage\nFROM python:3.9 as builder\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install --user --no-cache-dir -r requirements.txt\n\n# Runtime stage\nFROM python:3.9-slim\n\n# Set environment variables\nENV PYTHONUNBUFFERED=1 \\\\\n    PIP_DISABLE_PIP_VERSION_CHECK=1\n\n# Create non-root user\nRUN useradd -m -u 1000 mluser\n\nWORKDIR /app\n\n# Copy dependencies from builder\nCOPY --from=builder /root/.local /home/mluser/.local\n\n# Copy application\nCOPY --chown=mluser:mluser ./app ./app\n\n# Switch to non-root user\nUSER mluser\n\n# Update PATH\nENV PATH=/home/mluser/.local/bin:$PATH\n\nEXPOSE 8000\n\nCMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n\"\"\"\n\nwith open(exercise_dir / \"Dockerfile.optimized\", \"w\") as f:\n    f.write(optimized_ex_dockerfile)\n\nprint(\"\u2713 Optimized Dockerfile created!\")\nprint(\"\\nOptimizations applied:\")\nprint(\"  \u2713 Multi-stage build (smaller image)\")\nprint(\"  \u2713 Non-root user (security)\")\nprint(\"  \u2713 Environment variables (best practices)\")\nprint(\"  \u2713 Minimal layers\")\nprint(\"\\nEstimated size reduction: ~40%\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercise 3: Docker Compose for ML Stack\n\nCreate a docker-compose.yml for a complete ML serving stack.\n\n**Requirements**:\n1. ML API service\n2. Redis for caching\n3. Proper networking\n4. Volume mounts"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Exercise 3 Solution\n\ndocker_compose = \"\"\"version: '3.8'\n\nservices:\n  # ML API Service\n  ml-api:\n    build:\n      context: .\n      dockerfile: Dockerfile.optimized\n    ports:\n      - \"8000:8000\"\n    environment:\n      - MODEL_VERSION=1.0\n      - REDIS_HOST=redis\n      - REDIS_PORT=6379\n    volumes:\n      - ./models:/app/models:ro\n    depends_on:\n      - redis\n    restart: unless-stopped\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/health\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n\n  # Redis for caching predictions\n  redis:\n    image: redis:7-alpine\n    ports:\n      - \"6379:6379\"\n    volumes:\n      - redis-data:/data\n    restart: unless-stopped\n\nvolumes:\n  redis-data:\n\nnetworks:\n  default:\n    name: ml-network\n\"\"\"\n\nwith open(exercise_dir / \"docker-compose.yml\", \"w\") as f:\n    f.write(docker_compose)\n\nprint(\"\u2713 Docker Compose configuration created!\")\nprint(\"\\nStack includes:\")\nprint(\"  - ML API (port 8000)\")\nprint(\"  - Redis cache (port 6379)\")\nprint(\"  - Health checks\")\nprint(\"  - Persistent volumes\")\nprint(\"\\nTo run: docker-compose up -d\")\nprint(\"To stop: docker-compose down\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 10. Summary\n\n### Key Takeaways\n\n1. **Docker ensures consistency** across development and production environments\n\n2. **Dockerfiles are recipes** for building reproducible images\n\n3. **Layer caching speeds up builds** - order instructions by change frequency\n\n4. **Multi-stage builds reduce image size** significantly\n\n5. **Security matters** - use non-root users and specific base images\n\n6. **.dockerignore prevents bloat** by excluding unnecessary files\n\n### Docker Best Practices Checklist\n\n- \u2705 Use specific base image versions (not `latest`)\n- \u2705 Leverage layer caching (requirements before code)\n- \u2705 Use multi-stage builds\n- \u2705 Run as non-root user\n- \u2705 Set Python environment variables\n- \u2705 Create .dockerignore file\n- \u2705 Minimize number of layers\n- \u2705 Use COPY instead of ADD\n- \u2705 Don't store secrets in images\n- \u2705 Tag images properly (version numbers)\n\n### What's Next?\n\nIn **Module 06**, we'll explore:\n- **Different model serving patterns** (batch, real-time, streaming)\n- **Trade-offs** between serving approaches\n- **When to use** each pattern\n- **Implementation examples** for each"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 11. Additional Resources\n\n### Documentation\n- **Docker Docs**: https://docs.docker.com/\n- **Dockerfile Reference**: https://docs.docker.com/engine/reference/builder/\n- **Docker Compose**: https://docs.docker.com/compose/\n- **Best Practices**: https://docs.docker.com/develop/dev-best-practices/\n\n### Tutorials\n- **Docker for Data Science**: https://www.docker.com/blog/tag/data-science/\n- **ML in Production**: https://madewithml.com/courses/mlops/docker/\n\n### Advanced Topics\n- Multi-container applications with Docker Compose\n- Docker networking and volumes\n- GPU support in Docker (nvidia-docker)\n- Kubernetes for container orchestration\n- Docker security scanning"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}