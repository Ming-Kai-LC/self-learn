{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 01: Experiment Tracking with MLflow\n",
    "\n",
    "**Difficulty**: ‚≠ê‚≠ê Intermediate  \n",
    "**Estimated Time**: 60 minutes  \n",
    "**Prerequisites**: \n",
    "- Module 00: Introduction to MLOps\n",
    "- Basic machine learning model training\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Set up and configure MLflow for experiment tracking\n",
    "2. Log parameters, metrics, and artifacts during model training\n",
    "3. Compare multiple experiment runs to identify the best model\n",
    "4. Organize experiments with tags and nested runs\n",
    "5. Retrieve and load models from previous experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Experiment Tracking Matters\n",
    "\n",
    "Imagine you've trained 50 different versions of a model with varying:\n",
    "- Hyperparameters (learning rate, number of layers, etc.)\n",
    "- Feature engineering approaches\n",
    "- Training data subsets\n",
    "- Preprocessing techniques\n",
    "\n",
    "**Without experiment tracking:**\n",
    "- ‚ùå \"Which parameters gave the best accuracy?\"\n",
    "- ‚ùå \"Can't remember which data preprocessing we used for model v23\"\n",
    "- ‚ùå \"The model worked last week, but I changed something...\"\n",
    "- ‚ùå \"Let me manually copy metrics into a spreadsheet\"\n",
    "\n",
    "**With experiment tracking:**\n",
    "- ‚úÖ All experiments automatically logged\n",
    "- ‚úÖ Easy comparison across runs\n",
    "- ‚úÖ Reproducible results\n",
    "- ‚úÖ Collaboration enabled (team can see all experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Setup: Import required libraries\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úì Libraries imported successfully\")\n",
    "print(f\"‚úì MLflow version: {mlflow.__version__}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting Up MLflow\n",
    "\n",
    "MLflow has four main components:\n",
    "1. **MLflow Tracking**: Log parameters, metrics, and artifacts\n",
    "2. **MLflow Projects**: Package code in a reproducible format\n",
    "3. **MLflow Models**: Manage and deploy models\n",
    "4. **MLflow Registry**: Centralized model store (covered in Module 02)\n",
    "\n",
    "In this notebook, we'll focus on **MLflow Tracking**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Set up MLflow tracking URI\n",
    "# By default, MLflow logs to ./mlruns directory\n",
    "# For production, you'd use a remote tracking server\n",
    "\n",
    "mlflow.set_tracking_uri(\"file:./mlruns\")\n",
    "\n",
    "# Create or set an experiment\n",
    "# Experiments group related runs together\n",
    "experiment_name = \"credit_risk_classification\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "print(f\"‚úì MLflow tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "print(f\"‚úì Active experiment: {experiment_name}\")\n",
    "print(f\"\\nYou can view the MLflow UI by running:\")\n",
    "print(f\"  mlflow ui\")\n",
    "print(f\"Then navigate to http://localhost:5000\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating Sample Data for Experiments\n",
    "\n",
    "Let's create a binary classification dataset to simulate a credit risk prediction problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate synthetic credit risk dataset\n",
    "# Features: income, debt, credit_history, employment_length, etc.\n",
    "X, y = make_classification(\n",
    "    n_samples=2000,\n",
    "    n_features=20,\n",
    "    n_informative=15,\n",
    "    n_redundant=5,\n",
    "    n_classes=2,\n",
    "    weights=[0.7, 0.3],  # Imbalanced classes (70% good credit, 30% bad)\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create feature names for better interpretability\n",
    "feature_names = [f'feature_{i}' for i in range(20)]\n",
    "X_df = pd.DataFrame(X, columns=feature_names)\n",
    "X_df['target'] = y\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(f\"  Class 0 (Good Credit): {(y == 0).sum()} ({(y == 0).mean()*100:.1f}%)\")\n",
    "print(f\"  Class 1 (Bad Credit): {(y == 1).sum()} ({(y == 1).mean()*100:.1f}%)\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nSample data:\")\n",
    "X_df.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Basic MLflow Tracking: Single Experiment Run\n",
    "\n",
    "Let's start with a simple example of tracking one model training run.\n",
    "\n",
    "### Key Concepts:\n",
    "- **Parameters**: Input values that configure the model (e.g., max_depth, learning_rate)\n",
    "- **Metrics**: Output values that measure performance (e.g., accuracy, F1-score)\n",
    "- **Artifacts**: Files produced during the run (e.g., plots, models, datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Start an MLflow run\n",
    "with mlflow.start_run(run_name=\"baseline_logistic_regression\") as run:\n",
    "    \n",
    "    # Define model parameters\n",
    "    params = {\n",
    "        'C': 1.0,\n",
    "        'max_iter': 100,\n",
    "        'solver': 'lbfgs'\n",
    "    }\n",
    "    \n",
    "    # Log parameters\n",
    "    mlflow.log_params(params)\n",
    "    \n",
    "    # Train model\n",
    "    model = LogisticRegression(**params, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred),\n",
    "        'f1_score': f1_score(y_test, y_pred),\n",
    "        'roc_auc': roc_auc_score(y_test, y_pred_proba)\n",
    "    }\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metrics(metrics)\n",
    "    \n",
    "    # Log model\n",
    "    mlflow.sklearn.log_model(model, \"model\")\n",
    "    \n",
    "    # Log additional metadata as tags\n",
    "    mlflow.set_tag(\"model_type\", \"LogisticRegression\")\n",
    "    mlflow.set_tag(\"dataset\", \"synthetic_credit_risk\")\n",
    "    \n",
    "    print(\"‚úì Run completed and logged to MLflow\")\n",
    "    print(f\"‚úì Run ID: {run.info.run_id}\")\n",
    "    print(f\"\\nLogged Metrics:\")\n",
    "    for metric_name, metric_value in metrics.items():\n",
    "        print(f\"  {metric_name}: {metric_value:.4f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tracking Multiple Experiments\n",
    "\n",
    "In practice, you'll want to compare multiple models and hyperparameter configurations. Let's train several models and track them all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define different model configurations to experiment with\n",
    "experiments_config = [\n",
    "    {\n",
    "        'name': 'logistic_regression_c0.1',\n",
    "        'model': LogisticRegression,\n",
    "        'params': {'C': 0.1, 'max_iter': 100, 'solver': 'lbfgs', 'random_state': 42}\n",
    "    },\n",
    "    {\n",
    "        'name': 'logistic_regression_c10',\n",
    "        'model': LogisticRegression,\n",
    "        'params': {'C': 10.0, 'max_iter': 100, 'solver': 'lbfgs', 'random_state': 42}\n",
    "    },\n",
    "    {\n",
    "        'name': 'random_forest_depth5',\n",
    "        'model': RandomForestClassifier,\n",
    "        'params': {'n_estimators': 100, 'max_depth': 5, 'random_state': 42}\n",
    "    },\n",
    "    {\n",
    "        'name': 'random_forest_depth10',\n",
    "        'model': RandomForestClassifier,\n",
    "        'params': {'n_estimators': 100, 'max_depth': 10, 'random_state': 42}\n",
    "    },\n",
    "    {\n",
    "        'name': 'random_forest_depth20',\n",
    "        'model': RandomForestClassifier,\n",
    "        'params': {'n_estimators': 100, 'max_depth': 20, 'random_state': 42}\n",
    "    }\n",
    "]\n",
    "\n",
    "# Store results for comparison\n",
    "results = []\n",
    "\n",
    "print(\"Running experiments...\\n\")\n",
    "\n",
    "for config in experiments_config:\n",
    "    with mlflow.start_run(run_name=config['name']):\n",
    "        # Log parameters\n",
    "        mlflow.log_params(config['params'])\n",
    "        \n",
    "        # Train model\n",
    "        model = config['model'](**config['params'])\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred),\n",
    "            'recall': recall_score(y_test, y_pred),\n",
    "            'f1_score': f1_score(y_test, y_pred),\n",
    "            'roc_auc': roc_auc_score(y_test, y_pred_proba)\n",
    "        }\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metrics(metrics)\n",
    "        \n",
    "        # Log model\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "        \n",
    "        # Tag with model type\n",
    "        mlflow.set_tag(\"model_type\", config['model'].__name__)\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'run_name': config['name'],\n",
    "            **metrics\n",
    "        })\n",
    "        \n",
    "        print(f\"‚úì Completed: {config['name']}\")\n",
    "\n",
    "print(\"\\n‚úì All experiments completed!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparing Experiment Results\n",
    "\n",
    "Now let's visualize and compare the results of all our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create comparison DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"Experiment Results Comparison:\")\n",
    "print(\"=\" * 80)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Find best model for each metric\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Best Models by Metric:\")\n",
    "print(\"=\" * 80)\n",
    "for metric in ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc']:\n",
    "    best_idx = results_df[metric].idxmax()\n",
    "    best_model = results_df.loc[best_idx, 'run_name']\n",
    "    best_score = results_df.loc[best_idx, metric]\n",
    "    print(f\"{metric.upper()}: {best_model} ({best_score:.4f})\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('Model Performance Comparison Across Experiments', \n",
    "             fontsize=16, fontweight='bold')\n",
    "\n",
    "metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc']\n",
    "\n",
    "for idx, metric in enumerate(metrics_to_plot):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    \n",
    "    # Sort by metric value for better visualization\n",
    "    sorted_df = results_df.sort_values(metric, ascending=True)\n",
    "    \n",
    "    # Create horizontal bar chart\n",
    "    bars = ax.barh(range(len(sorted_df)), sorted_df[metric], \n",
    "                   color='steelblue', alpha=0.7)\n",
    "    \n",
    "    # Highlight best performer\n",
    "    best_idx = sorted_df[metric].idxmax()\n",
    "    bars[list(sorted_df.index).index(best_idx)].set_color('seagreen')\n",
    "    \n",
    "    ax.set_yticks(range(len(sorted_df)))\n",
    "    ax.set_yticklabels(sorted_df['run_name'], fontsize=9)\n",
    "    ax.set_xlabel(metric.replace('_', ' ').title(), fontweight='bold')\n",
    "    ax.set_xlim(sorted_df[metric].min() - 0.02, sorted_df[metric].max() + 0.02)\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (bar, value) in enumerate(zip(bars, sorted_df[metric])):\n",
    "        ax.text(value + 0.002, bar.get_y() + bar.get_height()/2,\n",
    "                f'{value:.3f}',\n",
    "                va='center', fontsize=8)\n",
    "\n",
    "# Remove extra subplot\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisualization shows Random Forest with depth=10 or 20 generally performs best.\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Logging Artifacts: Saving Plots and Files\n",
    "\n",
    "Beyond metrics, we often want to save visualizations, datasets, or other files associated with an experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Train a model and log comprehensive artifacts\n",
    "with mlflow.start_run(run_name=\"best_model_with_artifacts\"):\n",
    "    \n",
    "    # Train best performing model (based on previous experiments)\n",
    "    params = {'n_estimators': 100, 'max_depth': 10, 'random_state': 42}\n",
    "    model = RandomForestClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Log parameters and metrics\n",
    "    mlflow.log_params(params)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred),\n",
    "        'f1_score': f1_score(y_test, y_pred),\n",
    "        'roc_auc': roc_auc_score(y_test, y_pred_proba)\n",
    "    }\n",
    "    mlflow.log_metrics(metrics)\n",
    "    \n",
    "    # Create and log confusion matrix plot\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
    "    ax.set_xlabel('Predicted', fontweight='bold')\n",
    "    ax.set_ylabel('Actual', fontweight='bold')\n",
    "    ax.set_title('Confusion Matrix', fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "    mlflow.log_artifact('confusion_matrix.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Create and log ROC curve\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    ax.plot(fpr, tpr, linewidth=2, label=f'ROC (AUC = {metrics[\"roc_auc\"]:.3f})')\n",
    "    ax.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random Classifier')\n",
    "    ax.set_xlabel('False Positive Rate', fontweight='bold')\n",
    "    ax.set_ylabel('True Positive Rate', fontweight='bold')\n",
    "    ax.set_title('ROC Curve', fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('roc_curve.png', dpi=150, bbox_inches='tight')\n",
    "    mlflow.log_artifact('roc_curve.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Log feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    feature_importance.to_csv('feature_importance.csv', index=False)\n",
    "    mlflow.log_artifact('feature_importance.csv')\n",
    "    \n",
    "    # Log the model\n",
    "    mlflow.sklearn.log_model(model, \"model\")\n",
    "    \n",
    "    print(\"‚úì Model, metrics, and artifacts logged successfully!\")\n",
    "    print(\"\\nLogged artifacts:\")\n",
    "    print(\"  - confusion_matrix.png\")\n",
    "    print(\"  - roc_curve.png\")\n",
    "    print(\"  - feature_importance.csv\")\n",
    "    print(\"  - model/\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Retrieving and Loading Previous Runs\n",
    "\n",
    "One of the key benefits of MLflow is the ability to retrieve past experiments and load models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Search for runs in the current experiment\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "experiment = client.get_experiment_by_name(experiment_name)\n",
    "\n",
    "# Get all runs from this experiment\n",
    "runs = mlflow.search_runs(\n",
    "    experiment_ids=[experiment.experiment_id],\n",
    "    order_by=[\"metrics.f1_score DESC\"]\n",
    ")\n",
    "\n",
    "print(\"All Runs in Experiment (sorted by F1 score):\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Display key information\n",
    "display_cols = ['run_id', 'tags.mlflow.runName', 'metrics.accuracy', \n",
    "                'metrics.f1_score', 'metrics.roc_auc']\n",
    "available_cols = [col for col in display_cols if col in runs.columns]\n",
    "print(runs[available_cols].head(10).to_string(index=False))\n",
    "\n",
    "# Get the best run by F1 score\n",
    "best_run_id = runs.iloc[0]['run_id']\n",
    "print(f\"\\n‚úì Best run ID: {best_run_id}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load the best model from MLflow\n",
    "best_model_uri = f\"runs:/{best_run_id}/model\"\n",
    "loaded_model = mlflow.sklearn.load_model(best_model_uri)\n",
    "\n",
    "print(f\"‚úì Model loaded from run: {best_run_id}\")\n",
    "print(f\"‚úì Model type: {type(loaded_model).__name__}\")\n",
    "\n",
    "# Verify the model works\n",
    "test_predictions = loaded_model.predict(X_test[:5])\n",
    "print(f\"\\nTest predictions on first 5 samples: {test_predictions}\")\n",
    "print(f\"Actual values: {y_test[:5]}\")\n",
    "\n",
    "print(\"\\n‚úì Successfully loaded and tested model from MLflow!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Advanced: Nested Runs for Hyperparameter Tuning\n",
    "\n",
    "For complex experiments like grid search or cross-validation, you can organize runs hierarchically using nested runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Hyperparameter tuning with nested runs\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, 15]\n",
    "}\n",
    "\n",
    "# Parent run for the entire hyperparameter search\n",
    "with mlflow.start_run(run_name=\"hyperparameter_tuning\") as parent_run:\n",
    "    \n",
    "    best_score = 0\n",
    "    best_params = {}\n",
    "    \n",
    "    # Track total number of combinations\n",
    "    total_combinations = len(param_grid['n_estimators']) * len(param_grid['max_depth'])\n",
    "    mlflow.log_param(\"total_combinations\", total_combinations)\n",
    "    \n",
    "    combination_num = 0\n",
    "    \n",
    "    # Grid search\n",
    "    for n_est in param_grid['n_estimators']:\n",
    "        for max_d in param_grid['max_depth']:\n",
    "            combination_num += 1\n",
    "            \n",
    "            # Nested run for each hyperparameter combination\n",
    "            with mlflow.start_run(\n",
    "                run_name=f\"n{n_est}_d{max_d}\", \n",
    "                nested=True\n",
    "            ) as child_run:\n",
    "                \n",
    "                # Define and train model\n",
    "                params = {\n",
    "                    'n_estimators': n_est,\n",
    "                    'max_depth': max_d,\n",
    "                    'random_state': 42\n",
    "                }\n",
    "                mlflow.log_params(params)\n",
    "                \n",
    "                model = RandomForestClassifier(**params)\n",
    "                \n",
    "                # Use cross-validation for more robust evaluation\n",
    "                cv_scores = cross_val_score(\n",
    "                    model, X_train, y_train, \n",
    "                    cv=5, scoring='f1'\n",
    "                )\n",
    "                \n",
    "                mean_cv_score = cv_scores.mean()\n",
    "                std_cv_score = cv_scores.std()\n",
    "                \n",
    "                # Log metrics\n",
    "                mlflow.log_metric(\"cv_f1_mean\", mean_cv_score)\n",
    "                mlflow.log_metric(\"cv_f1_std\", std_cv_score)\n",
    "                \n",
    "                # Train on full training set and evaluate on test\n",
    "                model.fit(X_train, y_train)\n",
    "                y_pred = model.predict(X_test)\n",
    "                test_f1 = f1_score(y_test, y_pred)\n",
    "                mlflow.log_metric(\"test_f1\", test_f1)\n",
    "                \n",
    "                # Update best parameters if this is better\n",
    "                if mean_cv_score > best_score:\n",
    "                    best_score = mean_cv_score\n",
    "                    best_params = params\n",
    "                \n",
    "                print(f\"[{combination_num}/{total_combinations}] \"\n",
    "                      f\"n_estimators={n_est}, max_depth={max_d}: \"\n",
    "                      f\"CV F1={mean_cv_score:.4f} (¬±{std_cv_score:.4f})\")\n",
    "    \n",
    "    # Log best parameters to parent run\n",
    "    mlflow.log_params({f\"best_{k}\": v for k, v in best_params.items()})\n",
    "    mlflow.log_metric(\"best_cv_f1\", best_score)\n",
    "    \n",
    "    print(f\"\\n‚úì Hyperparameter tuning complete!\")\n",
    "    print(f\"‚úì Best parameters: {best_params}\")\n",
    "    print(f\"‚úì Best CV F1 score: {best_score:.4f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Exercises\n",
    "\n",
    "### üéØ Exercise 1: Track a New Experiment\n",
    "\n",
    "Create a new experiment to compare different classifiers on the same dataset.\n",
    "\n",
    "**Requirements:**\n",
    "1. Create an experiment named \"classifier_comparison\"\n",
    "2. Train and log at least 3 different classifier types (e.g., SVM, KNN, Gradient Boosting)\n",
    "3. Log parameters, metrics, and a confusion matrix for each\n",
    "4. Identify which classifier performs best\n",
    "\n",
    "**Bonus**: Log the training time for each model as a metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Your solution here\n",
    "import time\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# TODO: Implement your solution\n",
    "# 1. Set experiment\n",
    "# 2. Define classifiers\n",
    "# 3. Train and log each one\n",
    "# 4. Compare results"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Exercise 2: Log Custom Artifacts\n",
    "\n",
    "Enhance the experiment tracking by logging additional useful artifacts.\n",
    "\n",
    "**Requirements:**\n",
    "1. Train a Random Forest model\n",
    "2. Create and log a feature importance bar plot\n",
    "3. Create and log a precision-recall curve\n",
    "4. Save and log a text file with a model summary (parameters, metrics, insights)\n",
    "\n",
    "**Hint**: Use `plt.savefig()` for plots and standard file I/O for text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Your solution here\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# TODO: Implement your solution"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Exercise 3: Query and Compare Past Runs\n",
    "\n",
    "Practice retrieving and analyzing past experiments.\n",
    "\n",
    "**Requirements:**\n",
    "1. Search for all runs where accuracy > 0.85\n",
    "2. Find the run with the best precision score\n",
    "3. Load that model and make predictions on new data\n",
    "4. Create a visualization comparing the top 5 runs across all metrics\n",
    "\n",
    "**Hint**: Use `mlflow.search_runs()` with filter strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Your solution here\n",
    "\n",
    "# TODO: Implement your solution\n",
    "# 1. Search runs with filter\n",
    "# 2. Find best precision\n",
    "# 3. Load model\n",
    "# 4. Visualize comparison"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary\n",
    "\n",
    "### Key Concepts Covered\n",
    "\n",
    "1. **MLflow Setup**: Configured tracking URI and experiments\n",
    "2. **Logging**: Tracked parameters, metrics, and artifacts\n",
    "3. **Comparison**: Compared multiple experiment runs\n",
    "4. **Artifacts**: Saved plots, models, and files\n",
    "5. **Retrieval**: Loaded past experiments and models\n",
    "6. **Nested Runs**: Organized complex experiments hierarchically\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "- ‚úÖ **Always log parameters**: Even if you think they won't matter\n",
    "- ‚úÖ **Use descriptive run names**: Makes finding experiments easier\n",
    "- ‚úÖ **Log artifacts liberally**: Plots and files help future you understand results\n",
    "- ‚úÖ **Use tags**: Organize experiments by team, project, or model type\n",
    "- ‚úÖ **Version your data**: Track which dataset version was used\n",
    "- ‚úÖ **Document insights**: Add notes about why certain experiments were run\n",
    "\n",
    "### Common Pitfalls to Avoid\n",
    "\n",
    "- ‚ùå Not logging random seeds (makes reproduction impossible)\n",
    "- ‚ùå Overwriting runs (each experiment should be a new run)\n",
    "- ‚ùå Logging too few metrics (log more than you think you need)\n",
    "- ‚ùå Not cleaning up artifacts (can consume significant disk space)\n",
    "\n",
    "### What's Next\n",
    "\n",
    "In **Module 02: Model Versioning and Registry**, we'll learn:\n",
    "- How to use MLflow Model Registry\n",
    "- Model lifecycle management (staging, production, archived)\n",
    "- Model lineage and governance\n",
    "- Transitioning models between stages\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- **MLflow Documentation**: https://mlflow.org/docs/latest/tracking.html\n",
    "- **MLflow Tutorial**: https://mlflow.org/docs/latest/tutorials-and-examples/tutorial.html\n",
    "- **Experiment Tracking Best Practices**: https://neptune.ai/blog/ml-experiment-tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Proceed to **Module 02: Model Versioning and Registry** to learn how to manage model versions and promote models through different lifecycle stages.\n",
    "\n",
    "**Before moving on, ensure you can:**\n",
    "- ‚úÖ Set up MLflow tracking and create experiments\n",
    "- ‚úÖ Log parameters, metrics, and artifacts\n",
    "- ‚úÖ Compare multiple experiment runs\n",
    "- ‚úÖ Retrieve and load past models\n",
    "- ‚úÖ Organize experiments with nested runs and tags"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
