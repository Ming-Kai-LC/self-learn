{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 05: Google Cloud AI Platform Basics\n",
    "\n",
    "**Difficulty**: â­â­\n",
    "**Estimated Time**: 55 minutes\n",
    "**Prerequisites**: \n",
    "- [Module 03: Azure ML Studio Introduction](03_azure_ml_studio_introduction.ipynb)\n",
    "- Basic understanding of cloud computing concepts\n",
    "- Familiarity with Python and ML workflows\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Set up GCP project and understand billing concepts\n",
    "2. Use Cloud Storage for ML data management\n",
    "3. Query and analyze data with BigQuery\n",
    "4. Work with AI Platform Notebooks for development\n",
    "5. Submit training jobs to AI Platform\n",
    "6. Leverage pre-built ML APIs (Vision, Natural Language, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Introduction\n",
    "\n",
    "### Google Cloud Platform (GCP) Overview\n",
    "\n",
    "GCP provides comprehensive AI/ML services integrated with data infrastructure.\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚         Google Cloud AI/ML Ecosystem                     â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                         â”‚\n",
    "â”‚  Data Storage & Processing                              â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚\n",
    "â”‚  â”‚ Cloud Storage (objects, files)       â”‚              â”‚\n",
    "â”‚  â”‚ BigQuery (data warehouse)            â”‚              â”‚\n",
    "â”‚  â”‚ Cloud SQL (relational DB)            â”‚              â”‚\n",
    "â”‚  â”‚ Dataflow (stream/batch processing)   â”‚              â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚\n",
    "â”‚                    â”‚                                    â”‚\n",
    "â”‚                    â–¼                                    â”‚\n",
    "â”‚  ML Development                                         â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚\n",
    "â”‚  â”‚ Vertex AI (unified platform)         â”‚              â”‚\n",
    "â”‚  â”‚ AI Platform Notebooks (JupyterLab)   â”‚              â”‚\n",
    "â”‚  â”‚ Colab Enterprise                     â”‚              â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚\n",
    "â”‚                    â”‚                                    â”‚\n",
    "â”‚                    â–¼                                    â”‚\n",
    "â”‚  ML Training & Deployment                               â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚\n",
    "â”‚  â”‚ Vertex AI Training                   â”‚              â”‚\n",
    "â”‚  â”‚ Vertex AI Predictions                â”‚              â”‚\n",
    "â”‚  â”‚ AutoML                               â”‚              â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚\n",
    "â”‚                    â”‚                                    â”‚\n",
    "â”‚                    â–¼                                    â”‚\n",
    "â”‚  Pre-built ML APIs                                      â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚\n",
    "â”‚  â”‚ Vision API, Natural Language API     â”‚              â”‚\n",
    "â”‚  â”‚ Translation, Speech-to-Text          â”‚              â”‚\n",
    "â”‚  â”‚ Recommendations AI                   â”‚              â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Free Tier**: GCP offers $300 credit for new users (90 days) + always-free tier for many services!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock GCP SDK for demonstration\n",
    "# In production, use: from google.cloud import storage, bigquery, aiplatform\n",
    "\n",
    "class MockGCPProject:\n",
    "    \"\"\"Simulates GCP project configuration\"\"\"\n",
    "    \n",
    "    def __init__(self, project_id, region='us-central1'):\n",
    "        self.project_id = project_id\n",
    "        self.region = region\n",
    "        self.services_enabled = []\n",
    "    \n",
    "    def enable_service(self, service_name):\n",
    "        \"\"\"Enable GCP service API\"\"\"\n",
    "        self.services_enabled.append(service_name)\n",
    "        print(f\"âœ“ Enabled {service_name}\")\n",
    "    \n",
    "    def get_info(self):\n",
    "        \"\"\"Get project information\"\"\"\n",
    "        return {\n",
    "            'project_id': self.project_id,\n",
    "            'region': self.region,\n",
    "            'services_enabled': self.services_enabled\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GCP Project and Billing Setup\n",
    "\n",
    "Every GCP resource belongs to a project. Projects organize resources and manage billing.\n",
    "\n",
    "### Project Hierarchy\n",
    "\n",
    "```\n",
    "Organization (optional)\n",
    "    â”‚\n",
    "    â”œâ”€â”€ Folder: Development\n",
    "    â”‚   â”œâ”€â”€ Project: ml-dev-001\n",
    "    â”‚   â””â”€â”€ Project: ml-dev-002\n",
    "    â”‚\n",
    "    â””â”€â”€ Folder: Production\n",
    "        â”œâ”€â”€ Project: ml-prod-001\n",
    "        â””â”€â”€ Project: ml-prod-002\n",
    "\n",
    "Each Project contains:\n",
    "â”œâ”€â”€ Resources (VMs, Storage, etc.)\n",
    "â”œâ”€â”€ Billing Account (linked)\n",
    "â”œâ”€â”€ IAM Permissions\n",
    "â””â”€â”€ Enabled APIs\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and configure GCP project\n",
    "\n",
    "def create_project_config():\n",
    "    \"\"\"Define GCP project configuration\"\"\"\n",
    "    config = {\n",
    "        'project_id': 'my-ml-project-123456',  # Must be globally unique\n",
    "        'project_name': 'ML Development Project',\n",
    "        'region': 'us-central1',  # Iowa (cheaper, good for ML)\n",
    "        'zone': 'us-central1-a',\n",
    "        'billing_account_id': 'XXXXXX-XXXXXX-XXXXXX',\n",
    "        'labels': {\n",
    "            'environment': 'development',\n",
    "            'team': 'ml-team',\n",
    "            'cost-center': 'ml-research'\n",
    "        }\n",
    "    }\n",
    "    return config\n",
    "\n",
    "project_config = create_project_config()\n",
    "print(\"GCP Project Configuration:\")\n",
    "print(json.dumps(project_config, indent=2))\n",
    "\n",
    "# Create mock project\n",
    "project = MockGCPProject(\n",
    "    project_id=project_config['project_id'],\n",
    "    region=project_config['region']\n",
    ")\n",
    "print(f\"\\nâœ“ Project created: {project.project_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable required APIs\n",
    "\n",
    "required_apis = [\n",
    "    'compute.googleapis.com',           # Compute Engine\n",
    "    'storage-api.googleapis.com',       # Cloud Storage\n",
    "    'bigquery.googleapis.com',          # BigQuery\n",
    "    'aiplatform.googleapis.com',        # Vertex AI\n",
    "    'notebooks.googleapis.com',         # AI Platform Notebooks\n",
    "    'vision.googleapis.com',            # Vision API\n",
    "    'language.googleapis.com',          # Natural Language API\n",
    "]\n",
    "\n",
    "print(\"Enabling GCP APIs:\\n\")\n",
    "for api in required_apis:\n",
    "    project.enable_service(api)\n",
    "\n",
    "print(\"\\nðŸ’¡ Tip: Most APIs have free tier quotas before billing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Budget and billing alerts\n",
    "\n",
    "def create_budget_alert():\n",
    "    \"\"\"Configure budget alerts to control costs\"\"\"\n",
    "    budget_config = {\n",
    "        'display_name': 'ML Project Monthly Budget',\n",
    "        'amount': {\n",
    "            'specified_amount': {\n",
    "                'currency_code': 'USD',\n",
    "                'units': 100  # $100/month\n",
    "            }\n",
    "        },\n",
    "        'threshold_rules': [\n",
    "            {'threshold_percent': 0.50, 'spend_basis': 'CURRENT_SPEND'},  # 50%\n",
    "            {'threshold_percent': 0.75, 'spend_basis': 'CURRENT_SPEND'},  # 75%\n",
    "            {'threshold_percent': 0.90, 'spend_basis': 'CURRENT_SPEND'},  # 90%\n",
    "            {'threshold_percent': 1.00, 'spend_basis': 'CURRENT_SPEND'},  # 100%\n",
    "        ],\n",
    "        'notifications': {\n",
    "            'pubsub_topic': 'projects/my-project/topics/budget-alerts',\n",
    "            'monitoring_notification_channels': [],\n",
    "            'disable_default_iam_recipients': False\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"âš ï¸  Budget Alert Configuration:\")\n",
    "    print(f\"Monthly budget: ${budget_config['amount']['specified_amount']['units']}\")\n",
    "    print(\"Alerts at: 50%, 75%, 90%, 100% of budget\")\n",
    "    print(\"\\nðŸ’° Always set budget alerts to avoid unexpected charges!\")\n",
    "    \n",
    "    return budget_config\n",
    "\n",
    "budget = create_budget_alert()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cloud Storage for ML Data\n",
    "\n",
    "Cloud Storage is GCP's object storage service, ideal for ML datasets.\n",
    "\n",
    "### Storage Architecture\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚           Cloud Storage Structure                        â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                         â”‚\n",
    "â”‚  Bucket: gs://my-ml-bucket/                             â”‚\n",
    "â”‚  â”œâ”€â”€ data/                                              â”‚\n",
    "â”‚  â”‚   â”œâ”€â”€ raw/                                           â”‚\n",
    "â”‚  â”‚   â”‚   â””â”€â”€ sales_2024.csv                             â”‚\n",
    "â”‚  â”‚   â”œâ”€â”€ processed/                                     â”‚\n",
    "â”‚  â”‚   â”‚   â””â”€â”€ sales_clean.parquet                        â”‚\n",
    "â”‚  â”‚   â””â”€â”€ features/                                      â”‚\n",
    "â”‚  â”‚       â””â”€â”€ feature_store.tfrecord                     â”‚\n",
    "â”‚  â”œâ”€â”€ models/                                            â”‚\n",
    "â”‚  â”‚   â”œâ”€â”€ classifier_v1/                                 â”‚\n",
    "â”‚  â”‚   â””â”€â”€ classifier_v2/                                 â”‚\n",
    "â”‚  â””â”€â”€ artifacts/                                         â”‚\n",
    "â”‚      â”œâ”€â”€ plots/                                         â”‚\n",
    "â”‚      â””â”€â”€ reports/                                       â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  Storage Classes:                                       â”‚\n",
    "â”‚  - Standard: Frequent access ($0.020/GB/month)          â”‚\n",
    "â”‚  - Nearline: Monthly access ($0.010/GB/month)           â”‚\n",
    "â”‚  - Coldline: Quarterly access ($0.004/GB/month)         â”‚\n",
    "â”‚  - Archive: Yearly access ($0.0012/GB/month)            â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloud Storage bucket configuration\n",
    "\n",
    "def create_storage_bucket_config():\n",
    "    \"\"\"Configure Cloud Storage bucket for ML data\"\"\"\n",
    "    bucket_config = {\n",
    "        'name': 'my-ml-data-bucket',  # Globally unique\n",
    "        'location': 'US-CENTRAL1',  # Same region as compute\n",
    "        'storage_class': 'STANDARD',  # or NEARLINE, COLDLINE\n",
    "        'uniform_bucket_level_access': True,  # Recommended\n",
    "        'versioning': {\n",
    "            'enabled': True  # Keep file versions\n",
    "        },\n",
    "        'lifecycle_rules': [\n",
    "            {\n",
    "                'action': {'type': 'SetStorageClass', 'storage_class': 'NEARLINE'},\n",
    "                'condition': {'age': 30}  # Move to Nearline after 30 days\n",
    "            },\n",
    "            {\n",
    "                'action': {'type': 'Delete'},\n",
    "                'condition': {'age': 365}  # Delete after 1 year\n",
    "            }\n",
    "        ],\n",
    "        'labels': {\n",
    "            'purpose': 'ml-data',\n",
    "            'team': 'ml-team'\n",
    "        }\n",
    "    }\n",
    "    return bucket_config\n",
    "\n",
    "bucket_config = create_storage_bucket_config()\n",
    "print(\"Cloud Storage Bucket Configuration:\")\n",
    "print(json.dumps(bucket_config, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate Cloud Storage operations\n",
    "\n",
    "class CloudStorageBucket:\n",
    "    \"\"\"Simulates GCP Cloud Storage bucket operations\"\"\"\n",
    "    \n",
    "    def __init__(self, bucket_name):\n",
    "        self.bucket_name = bucket_name\n",
    "        self.blobs = {}  # filename -> metadata\n",
    "    \n",
    "    def upload_file(self, local_path, gcs_path):\n",
    "        \"\"\"Upload file to Cloud Storage\"\"\"\n",
    "        blob_info = {\n",
    "            'name': gcs_path,\n",
    "            'size': np.random.randint(1000, 1000000),  # bytes\n",
    "            'uploaded_at': datetime.now(),\n",
    "            'content_type': 'application/octet-stream'\n",
    "        }\n",
    "        self.blobs[gcs_path] = blob_info\n",
    "        print(f\"âœ“ Uploaded {local_path} â†’ gs://{self.bucket_name}/{gcs_path}\")\n",
    "        return gcs_path\n",
    "    \n",
    "    def download_file(self, gcs_path, local_path):\n",
    "        \"\"\"Download file from Cloud Storage\"\"\"\n",
    "        if gcs_path in self.blobs:\n",
    "            print(f\"âœ“ Downloaded gs://{self.bucket_name}/{gcs_path} â†’ {local_path}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"âœ— File not found: {gcs_path}\")\n",
    "            return False\n",
    "    \n",
    "    def list_files(self, prefix=''):\n",
    "        \"\"\"List files with optional prefix filter\"\"\"\n",
    "        matching_blobs = [\n",
    "            name for name in self.blobs.keys() \n",
    "            if name.startswith(prefix)\n",
    "        ]\n",
    "        return matching_blobs\n",
    "    \n",
    "    def get_uri(self, blob_name):\n",
    "        \"\"\"Get GCS URI for file\"\"\"\n",
    "        return f\"gs://{self.bucket_name}/{blob_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Cloud Storage operations\n",
    "bucket = CloudStorageBucket('my-ml-data-bucket')\n",
    "\n",
    "# Upload files\n",
    "print(\"Uploading files to Cloud Storage:\\n\")\n",
    "bucket.upload_file('data/train.csv', 'data/raw/train.csv')\n",
    "bucket.upload_file('data/test.csv', 'data/raw/test.csv')\n",
    "bucket.upload_file('models/model.pkl', 'models/v1/model.pkl')\n",
    "\n",
    "# List files\n",
    "print(\"\\nFiles in data/raw/:\")\n",
    "for file in bucket.list_files('data/raw/'):\n",
    "    print(f\"  - {bucket.get_uri(file)}\")\n",
    "\n",
    "# Download file\n",
    "print(\"\\nDownloading model:\")\n",
    "bucket.download_file('models/v1/model.pkl', 'downloaded_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. BigQuery for Data Analysis\n",
    "\n",
    "BigQuery is a serverless data warehouse for analytics at scale.\n",
    "\n",
    "### BigQuery Architecture\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚              BigQuery Architecture                       â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                         â”‚\n",
    "â”‚  Data Sources                                           â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚\n",
    "â”‚  â”‚ Cloud Storage (CSV, Parquet)     â”‚                  â”‚\n",
    "â”‚  â”‚ Cloud SQL                        â”‚                  â”‚\n",
    "â”‚  â”‚ Streaming (Dataflow)             â”‚                  â”‚\n",
    "â”‚  â”‚ External tables                  â”‚                  â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚\n",
    "â”‚                â”‚                                        â”‚\n",
    "â”‚                â–¼                                        â”‚\n",
    "â”‚  BigQuery Dataset                                       â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚\n",
    "â”‚  â”‚ Tables (structured data)         â”‚                  â”‚\n",
    "â”‚  â”‚ - Partitioned (by date/time)     â”‚                  â”‚\n",
    "â”‚  â”‚ - Clustered (by columns)         â”‚                  â”‚\n",
    "â”‚  â”‚ - Nested/repeated fields         â”‚                  â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚\n",
    "â”‚                â”‚                                        â”‚\n",
    "â”‚                â–¼                                        â”‚\n",
    "â”‚  SQL Queries                                            â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚\n",
    "â”‚  â”‚ Standard SQL                     â”‚                  â”‚\n",
    "â”‚  â”‚ ML functions (BQML)              â”‚                  â”‚\n",
    "â”‚  â”‚ Federated queries                â”‚                  â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  Pricing:                                               â”‚\n",
    "â”‚  - Storage: $0.020/GB/month (first 10GB free)          â”‚\n",
    "â”‚  - Queries: $5/TB processed (first 1TB/month free)      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BigQuery dataset and table configuration\n",
    "\n",
    "def create_bigquery_dataset():\n",
    "    \"\"\"Configure BigQuery dataset\"\"\"\n",
    "    dataset_config = {\n",
    "        'dataset_id': 'ml_analytics',\n",
    "        'project_id': 'my-ml-project-123456',\n",
    "        'location': 'US',  # Multi-region for better availability\n",
    "        'default_table_expiration_ms': 90 * 24 * 60 * 60 * 1000,  # 90 days\n",
    "        'description': 'ML analytics and feature engineering',\n",
    "        'labels': {\n",
    "            'team': 'ml-team',\n",
    "            'purpose': 'analytics'\n",
    "        }\n",
    "    }\n",
    "    return dataset_config\n",
    "\n",
    "def create_table_schema():\n",
    "    \"\"\"Define table schema for customer data\"\"\"\n",
    "    schema = [\n",
    "        {'name': 'customer_id', 'type': 'STRING', 'mode': 'REQUIRED'},\n",
    "        {'name': 'purchase_date', 'type': 'DATE', 'mode': 'REQUIRED'},\n",
    "        {'name': 'amount', 'type': 'FLOAT64', 'mode': 'REQUIRED'},\n",
    "        {'name': 'product_category', 'type': 'STRING', 'mode': 'NULLABLE'},\n",
    "        {'name': 'metadata', 'type': 'JSON', 'mode': 'NULLABLE'}\n",
    "    ]\n",
    "    \n",
    "    table_config = {\n",
    "        'table_id': 'customer_transactions',\n",
    "        'schema': schema,\n",
    "        'time_partitioning': {\n",
    "            'type': 'DAY',\n",
    "            'field': 'purchase_date'  # Partition by date\n",
    "        },\n",
    "        'clustering_fields': ['product_category', 'customer_id']\n",
    "    }\n",
    "    return table_config\n",
    "\n",
    "dataset = create_bigquery_dataset()\n",
    "table = create_table_schema()\n",
    "\n",
    "print(\"BigQuery Dataset:\")\n",
    "print(json.dumps(dataset, indent=2))\n",
    "print(\"\\nTable Schema:\")\n",
    "print(json.dumps(table, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate BigQuery operations\n",
    "\n",
    "class BigQueryClient:\n",
    "    \"\"\"Simulates BigQuery client operations\"\"\"\n",
    "    \n",
    "    def __init__(self, project_id):\n",
    "        self.project_id = project_id\n",
    "        self.query_history = []\n",
    "    \n",
    "    def query(self, sql):\n",
    "        \"\"\"\n",
    "        Execute SQL query and return results\n",
    "        Simulates query execution with random data\n",
    "        \"\"\"\n",
    "        # Record query\n",
    "        query_info = {\n",
    "            'sql': sql,\n",
    "            'executed_at': datetime.now(),\n",
    "            'bytes_processed': np.random.randint(1000000, 10000000)\n",
    "        }\n",
    "        self.query_history.append(query_info)\n",
    "        \n",
    "        # Simulate results\n",
    "        if 'COUNT' in sql.upper():\n",
    "            results = pd.DataFrame({'count': [np.random.randint(1000, 100000)]})\n",
    "        elif 'AVG' in sql.upper() or 'SUM' in sql.upper():\n",
    "            results = pd.DataFrame({\n",
    "                'metric': [np.random.rand() * 1000]\n",
    "            })\n",
    "        else:\n",
    "            # Return sample rows\n",
    "            results = pd.DataFrame({\n",
    "                'customer_id': [f'C{i:04d}' for i in range(5)],\n",
    "                'amount': np.random.rand(5) * 100,\n",
    "                'category': np.random.choice(['A', 'B', 'C'], 5)\n",
    "            })\n",
    "        \n",
    "        # Calculate cost (simplified)\n",
    "        bytes_billed = query_info['bytes_processed']\n",
    "        tb_processed = bytes_billed / (1024**4)\n",
    "        cost = tb_processed * 5  # $5 per TB\n",
    "        \n",
    "        print(f\"Query executed:\")\n",
    "        print(f\"  Bytes processed: {bytes_billed:,}\")\n",
    "        print(f\"  Estimated cost: ${cost:.4f}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def load_from_gcs(self, gcs_uri, table_ref):\n",
    "        \"\"\"Load data from Cloud Storage into BigQuery\"\"\"\n",
    "        print(f\"Loading data from {gcs_uri} into {table_ref}\")\n",
    "        rows_loaded = np.random.randint(1000, 100000)\n",
    "        print(f\"âœ“ Loaded {rows_loaded:,} rows\")\n",
    "        return rows_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test BigQuery operations\n",
    "bq = BigQueryClient('my-ml-project-123456')\n",
    "\n",
    "# Example queries\n",
    "print(\"Running BigQuery analysis:\\n\")\n",
    "\n",
    "# Count query\n",
    "sql1 = \"\"\"\n",
    "SELECT COUNT(*) as count\n",
    "FROM `my-ml-project-123456.ml_analytics.customer_transactions`\n",
    "WHERE purchase_date >= '2024-01-01'\n",
    "\"\"\"\n",
    "result1 = bq.query(sql1)\n",
    "print(result1)\n",
    "print()\n",
    "\n",
    "# Aggregation query\n",
    "sql2 = \"\"\"\n",
    "SELECT \n",
    "  customer_id,\n",
    "  SUM(amount) as total_amount,\n",
    "  COUNT(*) as num_purchases\n",
    "FROM `my-ml-project-123456.ml_analytics.customer_transactions`\n",
    "GROUP BY customer_id\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "result2 = bq.query(sql2)\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. AI Platform Notebooks\n",
    "\n",
    "Managed JupyterLab environment for ML development.\n",
    "\n",
    "### Notebook Instance Types\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚         AI Platform Notebooks Options                    â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                         â”‚\n",
    "â”‚  1. Pre-configured Environments                         â”‚\n",
    "â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚\n",
    "â”‚     â”‚ Python 3 (basic)             â”‚                   â”‚\n",
    "â”‚     â”‚ TensorFlow Enterprise        â”‚                   â”‚\n",
    "â”‚     â”‚ PyTorch                      â”‚                   â”‚\n",
    "â”‚     â”‚ R                            â”‚                   â”‚\n",
    "â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  2. Machine Types                                       â”‚\n",
    "â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚\n",
    "â”‚     â”‚ n1-standard-4 (4 vCPU)       â”‚  ~$0.19/hour      â”‚\n",
    "â”‚     â”‚ n1-highmem-8 (8 vCPU)        â”‚  ~$0.47/hour      â”‚\n",
    "â”‚     â”‚ With GPU (T4, V100)          â”‚  +$0.35-1.00/hr   â”‚\n",
    "â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  3. Features                                            â”‚\n",
    "â”‚     - JupyterLab interface                              â”‚\n",
    "â”‚     - Git integration                                   â”‚\n",
    "â”‚     - Terminal access                                   â”‚\n",
    "â”‚     - Auto-shutdown (cost saving)                       â”‚\n",
    "â”‚     - Direct GCS/BigQuery access                        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI Platform Notebook instance configuration\n",
    "\n",
    "def create_notebook_instance_config():\n",
    "    \"\"\"Configure AI Platform Notebook instance\"\"\"\n",
    "    instance_config = {\n",
    "        'name': 'ml-development-notebook',\n",
    "        'machine_type': 'n1-standard-4',  # 4 vCPU, 15 GB RAM\n",
    "        'accelerator': {\n",
    "            'type': 'NVIDIA_TESLA_T4',  # Optional GPU\n",
    "            'core_count': 1\n",
    "        },\n",
    "        'environment': {\n",
    "            'vm_image': {\n",
    "                'project': 'deeplearning-platform-release',\n",
    "                'image_family': 'tf-latest-gpu'  # TensorFlow with GPU\n",
    "            }\n",
    "        },\n",
    "        'disk': {\n",
    "            'boot_disk_type': 'PD_SSD',\n",
    "            'boot_disk_size_gb': 100\n",
    "        },\n",
    "        'idle_shutdown': {\n",
    "            'enabled': True,\n",
    "            'idle_timeout': '60m'  # Auto-shutdown after 1 hour\n",
    "        },\n",
    "        'labels': {\n",
    "            'team': 'ml-team',\n",
    "            'purpose': 'development'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"ðŸ’° Cost Optimization Tips:\")\n",
    "    print(\"- Enable idle shutdown\")\n",
    "    print(\"- Use standard (not highmem) VMs when possible\")\n",
    "    print(\"- Start without GPU, add only when needed\")\n",
    "    print(\"- Stop instance when not in use\\n\")\n",
    "    \n",
    "    return instance_config\n",
    "\n",
    "notebook_config = create_notebook_instance_config()\n",
    "print(\"AI Platform Notebook Configuration:\")\n",
    "print(json.dumps(notebook_config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Jobs on AI Platform\n",
    "\n",
    "Submit custom training jobs to managed infrastructure.\n",
    "\n",
    "### Training Job Flow\n",
    "\n",
    "```\n",
    "1. Package training code â†’ 2. Upload to GCS â†’ 3. Submit job\n",
    "                              â†“\n",
    "4. AI Platform provisions VMs â†’ 5. Downloads code & data\n",
    "                              â†“\n",
    "6. Runs training â†’ 7. Saves model to GCS â†’ 8. Releases VMs\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training job configuration\n",
    "\n",
    "def create_training_job_config():\n",
    "    \"\"\"Configure custom training job on AI Platform\"\"\"\n",
    "    job_config = {\n",
    "        'job_id': 'classifier_training_001',\n",
    "        'training_input': {\n",
    "            'scaleTier': 'CUSTOM',  # or BASIC, STANDARD_1, PREMIUM_1\n",
    "            'masterType': 'n1-standard-4',\n",
    "            'workerCount': 0,  # No workers for single-machine\n",
    "            'packageUris': [\n",
    "                'gs://my-ml-bucket/training/trainer-0.1.tar.gz'\n",
    "            ],\n",
    "            'pythonModule': 'trainer.task',  # Entry point\n",
    "            'args': [\n",
    "                '--train-data', 'gs://my-ml-bucket/data/train.csv',\n",
    "                '--eval-data', 'gs://my-ml-bucket/data/eval.csv',\n",
    "                '--job-dir', 'gs://my-ml-bucket/jobs/classifier_001',\n",
    "                '--epochs', '10',\n",
    "                '--learning-rate', '0.001'\n",
    "            ],\n",
    "            'region': 'us-central1',\n",
    "            'runtimeVersion': '2.11',  # TensorFlow version\n",
    "            'pythonVersion': '3.9'\n",
    "        }\n",
    "    }\n",
    "    return job_config\n",
    "\n",
    "job_config = create_training_job_config()\n",
    "print(\"Training Job Configuration:\")\n",
    "print(json.dumps(job_config, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate training job execution\n",
    "\n",
    "class TrainingJob:\n",
    "    \"\"\"Simulates AI Platform training job\"\"\"\n",
    "    \n",
    "    def __init__(self, job_id):\n",
    "        self.job_id = job_id\n",
    "        self.state = 'PREPARING'\n",
    "        self.metrics = {}\n",
    "    \n",
    "    def submit(self):\n",
    "        \"\"\"Submit training job\"\"\"\n",
    "        print(f\"Submitting job: {self.job_id}\")\n",
    "        print(\"State: PREPARING\")\n",
    "        print(\"Provisioning compute resources...\\n\")\n",
    "        self.state = 'RUNNING'\n",
    "    \n",
    "    def run_training(self, epochs=5):\n",
    "        \"\"\"Simulate training execution\"\"\"\n",
    "        print(\"Training started\\n\")\n",
    "        \n",
    "        for epoch in range(1, epochs + 1):\n",
    "            # Simulate metrics\n",
    "            loss = 1.0 / epoch + np.random.rand() * 0.1\n",
    "            accuracy = 0.6 + (epoch * 0.05) + np.random.rand() * 0.02\n",
    "            \n",
    "            self.metrics[epoch] = {\n",
    "                'loss': loss,\n",
    "                'accuracy': accuracy\n",
    "            }\n",
    "            \n",
    "            print(f\"Epoch {epoch}/{epochs}: loss={loss:.4f}, accuracy={accuracy:.4f}\")\n",
    "        \n",
    "        self.state = 'SUCCEEDED'\n",
    "        print(f\"\\nâœ“ Training completed successfully\")\n",
    "        print(f\"Model saved to: gs://my-ml-bucket/jobs/{self.job_id}/model/\")\n",
    "    \n",
    "    def get_status(self):\n",
    "        \"\"\"Get job status\"\"\"\n",
    "        return {\n",
    "            'job_id': self.job_id,\n",
    "            'state': self.state,\n",
    "            'final_metrics': self.metrics.get(max(self.metrics.keys())) if self.metrics else None\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training job simulation\n",
    "job = TrainingJob('classifier_training_001')\n",
    "job.submit()\n",
    "job.run_training(epochs=5)\n",
    "\n",
    "# Check final status\n",
    "status = job.get_status()\n",
    "print(\"\\nFinal Status:\")\n",
    "print(json.dumps(status, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Pre-built ML APIs\n",
    "\n",
    "GCP offers ready-to-use ML APIs for common tasks.\n",
    "\n",
    "### Available APIs\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚           Pre-built ML APIs                              â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                         â”‚\n",
    "â”‚  Vision API                                             â”‚\n",
    "â”‚  - Label detection                                      â”‚\n",
    "â”‚  - Face detection                                       â”‚\n",
    "â”‚  - OCR (text extraction)                                â”‚\n",
    "â”‚  - Logo detection                                       â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  Natural Language API                                   â”‚\n",
    "â”‚  - Sentiment analysis                                   â”‚\n",
    "â”‚  - Entity extraction                                    â”‚\n",
    "â”‚  - Syntax analysis                                      â”‚\n",
    "â”‚  - Content classification                               â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  Translation API                                        â”‚\n",
    "â”‚  - 100+ languages                                       â”‚\n",
    "â”‚  - Auto language detection                              â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  Speech-to-Text API                                     â”‚\n",
    "â”‚  - Real-time transcription                              â”‚\n",
    "â”‚  - Speaker diarization                                  â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  Pricing: Pay per request (free tier available)         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate Vision API\n",
    "\n",
    "class VisionAPIClient:\n",
    "    \"\"\"Simulates Google Cloud Vision API\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.api_calls = 0\n",
    "    \n",
    "    def detect_labels(self, image_uri):\n",
    "        \"\"\"\n",
    "        Detect labels in image\n",
    "        Returns: List of labels with confidence scores\n",
    "        \"\"\"\n",
    "        self.api_calls += 1\n",
    "        \n",
    "        # Simulated labels\n",
    "        labels = [\n",
    "            {'description': 'Cat', 'score': 0.98},\n",
    "            {'description': 'Pet', 'score': 0.95},\n",
    "            {'description': 'Mammal', 'score': 0.93},\n",
    "            {'description': 'Whiskers', 'score': 0.87},\n",
    "        ]\n",
    "        \n",
    "        print(f\"Analyzing image: {image_uri}\")\n",
    "        print(\"\\nDetected labels:\")\n",
    "        for label in labels:\n",
    "            print(f\"  {label['description']}: {label['score']:.2%}\")\n",
    "        \n",
    "        return labels\n",
    "    \n",
    "    def detect_text(self, image_uri):\n",
    "        \"\"\"OCR text extraction from image\"\"\"\n",
    "        self.api_calls += 1\n",
    "        \n",
    "        # Simulated OCR result\n",
    "        text = \"Sample text extracted from image\\nMultiple lines supported\"\n",
    "        \n",
    "        print(f\"Extracting text from: {image_uri}\")\n",
    "        print(f\"\\nExtracted text:\\n{text}\")\n",
    "        \n",
    "        return text\n",
    "\n",
    "# Test Vision API\n",
    "vision = VisionAPIClient()\n",
    "labels = vision.detect_labels('gs://my-bucket/images/cat.jpg')\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "text = vision.detect_text('gs://my-bucket/images/receipt.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate Natural Language API\n",
    "\n",
    "class NaturalLanguageAPIClient:\n",
    "    \"\"\"Simulates Google Cloud Natural Language API\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.api_calls = 0\n",
    "    \n",
    "    def analyze_sentiment(self, text):\n",
    "        \"\"\"\n",
    "        Analyze sentiment of text\n",
    "        Returns: Sentiment score and magnitude\n",
    "        \"\"\"\n",
    "        self.api_calls += 1\n",
    "        \n",
    "        # Simulated sentiment\n",
    "        sentiment = {\n",
    "            'score': np.random.randn() * 0.3,  # -1 to 1\n",
    "            'magnitude': np.random.rand() * 2  # 0 to inf\n",
    "        }\n",
    "        \n",
    "        print(f\"Analyzing text: '{text[:50]}...'\")\n",
    "        print(f\"\\nSentiment:\")\n",
    "        print(f\"  Score: {sentiment['score']:.2f} (negative to positive)\")\n",
    "        print(f\"  Magnitude: {sentiment['magnitude']:.2f} (strength)\")\n",
    "        \n",
    "        if sentiment['score'] > 0.25:\n",
    "            print(f\"  Overall: Positive\")\n",
    "        elif sentiment['score'] < -0.25:\n",
    "            print(f\"  Overall: Negative\")\n",
    "        else:\n",
    "            print(f\"  Overall: Neutral\")\n",
    "        \n",
    "        return sentiment\n",
    "    \n",
    "    def extract_entities(self, text):\n",
    "        \"\"\"Extract named entities from text\"\"\"\n",
    "        self.api_calls += 1\n",
    "        \n",
    "        # Simulated entities\n",
    "        entities = [\n",
    "            {'name': 'Google Cloud', 'type': 'ORGANIZATION', 'salience': 0.8},\n",
    "            {'name': 'machine learning', 'type': 'OTHER', 'salience': 0.6},\n",
    "            {'name': 'United States', 'type': 'LOCATION', 'salience': 0.3},\n",
    "        ]\n",
    "        \n",
    "        print(f\"Extracting entities from: '{text[:50]}...'\")\n",
    "        print(f\"\\nEntities found:\")\n",
    "        for entity in entities:\n",
    "            print(f\"  {entity['name']} ({entity['type']}) - salience: {entity['salience']:.2f}\")\n",
    "        \n",
    "        return entities\n",
    "\n",
    "# Test Natural Language API\n",
    "nlp = NaturalLanguageAPIClient()\n",
    "sentiment = nlp.analyze_sentiment(\n",
    "    \"Google Cloud Platform offers excellent machine learning tools!\"\n",
    ")\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "entities = nlp.extract_entities(\n",
    "    \"Google Cloud provides AI services in data centers across the United States.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Data Pipeline Design\n",
    "\n",
    "Design a data pipeline that:\n",
    "1. Ingests daily CSV files from Cloud Storage\n",
    "2. Loads into BigQuery with partitioning\n",
    "3. Runs SQL transformations for features\n",
    "4. Exports results back to GCS for training\n",
    "\n",
    "Consider cost optimization and scheduling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "def design_data_pipeline():\n",
    "    \"\"\"\n",
    "    Design complete data pipeline for ML\n",
    "    \n",
    "    Include:\n",
    "    - Data ingestion from GCS\n",
    "    - BigQuery loading and transformations\n",
    "    - Feature engineering SQL\n",
    "    - Export for training\n",
    "    - Scheduling strategy\n",
    "    \n",
    "    Returns:\n",
    "        dict with pipeline specification\n",
    "    \"\"\"\n",
    "    # TODO: Design pipeline\n",
    "    pass\n",
    "\n",
    "# Test your pipeline\n",
    "# pipeline = design_data_pipeline()\n",
    "# print(json.dumps(pipeline, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Cost Optimization\n",
    "\n",
    "You have:\n",
    "- 100 GB of training data in Cloud Storage\n",
    "- BigQuery queries processing 5 TB/month\n",
    "- Notebook instance running 40 hours/week\n",
    "- 10 training jobs/month (4 hours each, n1-standard-8)\n",
    "\n",
    "Calculate monthly cost and propose optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "def calculate_gcp_costs(usage):\n",
    "    \"\"\"\n",
    "    Calculate GCP costs and suggest optimizations\n",
    "    \n",
    "    Args:\n",
    "        usage: dict with resource usage details\n",
    "    \n",
    "    Returns:\n",
    "        dict with cost breakdown and optimization suggestions\n",
    "    \"\"\"\n",
    "    # TODO: Calculate costs\n",
    "    # Cloud Storage: $0.020/GB/month\n",
    "    # BigQuery: $5/TB processed\n",
    "    # Compute: varies by machine type\n",
    "    pass\n",
    "\n",
    "# Test your calculator\n",
    "# usage = {\n",
    "#     'storage_gb': 100,\n",
    "#     'bigquery_tb_per_month': 5,\n",
    "#     'notebook_hours_per_week': 40,\n",
    "#     'training_jobs_per_month': 10,\n",
    "#     'training_hours_per_job': 4\n",
    "# }\n",
    "# costs = calculate_gcp_costs(usage)\n",
    "# print(json.dumps(costs, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: API Integration\n",
    "\n",
    "Build a content moderation system using:\n",
    "- Vision API: Detect inappropriate images\n",
    "- Natural Language API: Analyze text sentiment/toxicity\n",
    "- Combine results for final decision\n",
    "\n",
    "Handle rate limits and error cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "class ContentModerationSystem:\n",
    "    \"\"\"Content moderation using GCP ML APIs\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # TODO: Initialize API clients\n",
    "        pass\n",
    "    \n",
    "    def moderate_content(self, image_uri=None, text=None):\n",
    "        \"\"\"\n",
    "        Moderate content using Vision and NLP APIs\n",
    "        \n",
    "        Returns:\n",
    "            dict with moderation decision and reasoning\n",
    "        \"\"\"\n",
    "        # TODO: Implement moderation logic\n",
    "        pass\n",
    "\n",
    "# Test your system\n",
    "# moderator = ContentModerationSystem()\n",
    "# result = moderator.moderate_content(\n",
    "#     image_uri='gs://my-bucket/user_upload.jpg',\n",
    "#     text='User comment text here'\n",
    "# )\n",
    "# print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. **GCP Setup**: Project configuration, billing, and API enablement\n",
    "2. **Cloud Storage**: Object storage for ML datasets and models\n",
    "3. **BigQuery**: Serverless data warehouse for analytics at scale\n",
    "4. **AI Platform Notebooks**: Managed JupyterLab for development\n",
    "5. **Training Jobs**: Custom training on managed infrastructure\n",
    "6. **Pre-built APIs**: Vision, Natural Language for common ML tasks\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- Always set budget alerts to control costs\n",
    "- Use Cloud Storage lifecycle policies to optimize storage costs\n",
    "- Partition and cluster BigQuery tables for query performance\n",
    "- Enable idle shutdown for notebook instances\n",
    "- Start with pre-built APIs before building custom models\n",
    "- Use the free tier and credits for learning\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "- [Module 06: GCP Vertex AI](06_gcp_vertex_ai.ipynb)\n",
    "- Explore Dataflow for real-time data processing\n",
    "- Practice with BigQuery ML for in-database modeling\n",
    "- Build end-to-end ML pipeline with GCP services\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [GCP Free Tier](https://cloud.google.com/free)\n",
    "- [BigQuery Best Practices](https://cloud.google.com/bigquery/docs/best-practices)\n",
    "- [AI Platform Documentation](https://cloud.google.com/ai-platform/docs)\n",
    "- [ML APIs Pricing](https://cloud.google.com/products/ai)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
