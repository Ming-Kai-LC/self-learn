{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 08: Cloud Cost Optimization Strategies\n",
    "\n",
    "**Difficulty**: ‚≠ê‚≠ê\n",
    "**Estimated Time**: 75 minutes\n",
    "**Prerequisites**: \n",
    "- [Module 00: Introduction to Cloud ML Services](00_introduction_to_cloud_ml_services.ipynb)\n",
    "- [Module 01: AWS SageMaker Basics](01_aws_sagemaker_basics.ipynb)\n",
    "- [Module 07: Serverless ML](07_serverless_ml.ipynb)\n",
    "- Basic understanding of cloud pricing models\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Understand cloud ML pricing models across AWS, Azure, and GCP\n",
    "2. Maximize free tier benefits for learning and small projects\n",
    "3. Use Spot/Preemptible instances for training to reduce costs by 70-90%\n",
    "4. Implement auto-scaling and right-sizing strategies\n",
    "5. Optimize storage costs with lifecycle policies\n",
    "6. Set up cost monitoring, alerts, and budgets\n",
    "7. Use cost allocation tags for tracking expenses\n",
    "8. Make informed decisions between on-demand, reserved, and spot instances\n",
    "\n",
    "## Why Cost Optimization Matters\n",
    "\n",
    "Cloud costs can spiral out of control quickly:\n",
    "- üö® **Forgotten resources**: Leaving an ml.p3.2xlarge instance running overnight = $90\n",
    "- üö® **Over-provisioning**: Using ml.m5.xlarge when ml.t3.medium suffices = 2x unnecessary cost\n",
    "- üö® **Inefficient storage**: Storing processed data in standard S3 vs glacier = 25x cost\n",
    "- üö® **Missed free tier**: Not using 1M free Lambda requests = $200/year wasted\n",
    "\n",
    "**Good news**: With proper strategies, you can run ML workloads for < $10/month or even FREE for learning!\n",
    "\n",
    "### Free Tier Summary (2024)\n",
    "\n",
    "| Service | AWS Free Tier | Azure Free Tier | GCP Free Tier |\n",
    "|---------|---------------|-----------------|---------------|\n",
    "| **Compute** | 750 hrs t2.micro/month | 750 hrs B1S VM/month | f1-micro 24/7 |\n",
    "| **Storage** | 5GB S3 Standard | 5GB Blob Storage | 5GB Standard |\n",
    "| **Serverless** | 1M Lambda requests | 1M Functions executions | 2M Functions invocations |\n",
    "| **Database** | 750 hrs RDS db.t2.micro | 250GB SQL Database | 1GB Cloud SQL |\n",
    "| **Data Transfer** | 1GB out/month | 15GB out/month | 1GB out/month |\n",
    "| **Duration** | 12 months (some always free) | 12 months + $200 credit | $300 credit (90 days) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Standard library imports\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import calendar\n",
    "\n",
    "# Data science libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Cloud cost estimation (simulated)\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"Set2\")\n",
    "np.random.seed(42)\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 2)\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "print(f\"Notebook executed on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Cloud ML Pricing Models\n",
    "\n",
    "### 1.1: Core Pricing Components\n",
    "\n",
    "Cloud ML costs come from 4 main areas:\n",
    "1. **Compute**: Instance hours for training and inference\n",
    "2. **Storage**: Data storage (S3, Blob, Cloud Storage)\n",
    "3. **Data Transfer**: Moving data between regions or to internet\n",
    "4. **Additional Services**: Managed services (SageMaker, Vertex AI, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "@dataclass\n",
    "class InstancePricing:\n",
    "    \"\"\"Represents cloud instance pricing information\"\"\"\n",
    "    name: str\n",
    "    vcpus: int\n",
    "    memory_gb: float\n",
    "    gpu: Optional[str]\n",
    "    on_demand_hourly: float\n",
    "    spot_hourly: Optional[float]  # Spot/Preemptible pricing\n",
    "    reserved_1yr: Optional[float]  # 1-year reserved pricing\n",
    "    reserved_3yr: Optional[float]  # 3-year reserved pricing\n",
    "\n",
    "# AWS SageMaker Instance Pricing (us-east-1, 2024 approximate)\n",
    "aws_instances = [\n",
    "    InstancePricing('ml.t3.medium', 2, 4, None, 0.065, None, 0.042, 0.027),\n",
    "    InstancePricing('ml.m5.large', 2, 8, None, 0.134, 0.040, 0.087, 0.056),\n",
    "    InstancePricing('ml.m5.xlarge', 4, 16, None, 0.269, 0.081, 0.175, 0.112),\n",
    "    InstancePricing('ml.c5.2xlarge', 8, 16, None, 0.476, 0.143, 0.310, 0.198),\n",
    "    InstancePricing('ml.p3.2xlarge', 8, 61, 'V100', 4.284, 1.285, 2.785, 1.784),\n",
    "    InstancePricing('ml.g4dn.xlarge', 4, 16, 'T4', 0.736, 0.221, 0.479, 0.307),\n",
    "]\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "pricing_df = pd.DataFrame([\n",
    "    {\n",
    "        'Instance': inst.name,\n",
    "        'vCPUs': inst.vcpus,\n",
    "        'Memory (GB)': inst.memory_gb,\n",
    "        'GPU': inst.gpu or '-',\n",
    "        'On-Demand ($/hr)': inst.on_demand_hourly,\n",
    "        'Spot ($/hr)': inst.spot_hourly or '-',\n",
    "        'Reserved 1yr ($/hr)': inst.reserved_1yr or '-',\n",
    "        'Reserved 3yr ($/hr)': inst.reserved_3yr or '-',\n",
    "    }\n",
    "    for inst in aws_instances\n",
    "])\n",
    "\n",
    "print(\"AWS SageMaker Instance Pricing (us-east-1)\\n\")\n",
    "print(pricing_df.to_string(index=False))\n",
    "print(\"\\nüí° Key Observations:\")\n",
    "print(\"   - Spot instances: 60-70% cheaper than on-demand\")\n",
    "print(\"   - Reserved 3yr: 50-60% cheaper than on-demand\")\n",
    "print(\"   - GPU instances: 10-65x more expensive than CPU\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2: Cost Breakdown Analysis\n",
    "\n",
    "Let's analyze how costs accumulate over time for different usage patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def calculate_monthly_cost(instance: InstancePricing, hours_per_day: float, \n",
    "                          days_per_month: int = 30, pricing_model: str = 'on-demand'):\n",
    "    \"\"\"\n",
    "    Calculate monthly cost for an instance based on usage pattern\n",
    "    \n",
    "    pricing_model: 'on-demand', 'spot', 'reserved-1yr', 'reserved-3yr'\n",
    "    \"\"\"\n",
    "    total_hours = hours_per_day * days_per_month\n",
    "    \n",
    "    pricing_map = {\n",
    "        'on-demand': instance.on_demand_hourly,\n",
    "        'spot': instance.spot_hourly or instance.on_demand_hourly,\n",
    "        'reserved-1yr': instance.reserved_1yr or instance.on_demand_hourly,\n",
    "        'reserved-3yr': instance.reserved_3yr or instance.on_demand_hourly\n",
    "    }\n",
    "    \n",
    "    hourly_rate = pricing_map[pricing_model]\n",
    "    monthly_cost = total_hours * hourly_rate\n",
    "    \n",
    "    return monthly_cost\n",
    "\n",
    "# Compare different usage scenarios\n",
    "usage_scenarios = [\n",
    "    {'name': 'Light Development', 'hours_per_day': 2},\n",
    "    {'name': 'Active Development', 'hours_per_day': 8},\n",
    "    {'name': 'Production 24/7', 'hours_per_day': 24},\n",
    "]\n",
    "\n",
    "# Analyze ml.m5.large costs\n",
    "instance = aws_instances[1]  # ml.m5.large\n",
    "\n",
    "cost_analysis = []\n",
    "for scenario in usage_scenarios:\n",
    "    for model in ['on-demand', 'spot', 'reserved-1yr', 'reserved-3yr']:\n",
    "        cost = calculate_monthly_cost(instance, scenario['hours_per_day'], pricing_model=model)\n",
    "        cost_analysis.append({\n",
    "            'Usage Pattern': scenario['name'],\n",
    "            'Hours/Day': scenario['hours_per_day'],\n",
    "            'Pricing Model': model,\n",
    "            'Monthly Cost ($)': cost\n",
    "        })\n",
    "\n",
    "cost_df = pd.DataFrame(cost_analysis)\n",
    "\n",
    "# Pivot for better visualization\n",
    "pivot_df = cost_df.pivot(index='Usage Pattern', columns='Pricing Model', values='Monthly Cost ($)')\n",
    "print(f\"\\nMonthly Cost Analysis: {instance.name}\\n\")\n",
    "print(pivot_df.to_string())\n",
    "print(\"\\nüí∞ Cost Savings Opportunities:\")\n",
    "print(f\"   - Using Spot for development: Save ${pivot_df['on-demand'][0] - pivot_df['spot'][0]:.2f}/month\")\n",
    "print(f\"   - Reserved 3yr for production: Save ${(pivot_df['on-demand'][2] - pivot_df['reserved-3yr'][2]):.2f}/month\")\n",
    "print(f\"   - Turning off 24/7 when unused: Save ${pivot_df['on-demand'][2]:.2f}/month\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Maximizing Free Tier Benefits\n",
    "\n",
    "### 2.1: Free Tier Strategy\n",
    "\n",
    "The free tier is perfect for:\n",
    "- Learning and experimentation\n",
    "- Small personal projects\n",
    "- Proof-of-concept development\n",
    "- Side projects with low traffic\n",
    "\n",
    "**How to Stay Within Free Tier:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class FreeTierTracker:\n",
    "    \"\"\"Track usage against AWS free tier limits\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Free tier limits (monthly)\n",
    "        self.limits = {\n",
    "            'ec2_hours': 750,  # t2.micro hours\n",
    "            's3_storage_gb': 5,\n",
    "            's3_get_requests': 20000,\n",
    "            's3_put_requests': 2000,\n",
    "            'lambda_requests': 1_000_000,\n",
    "            'lambda_gb_seconds': 400_000,\n",
    "            'data_transfer_gb': 1,  # outbound\n",
    "        }\n",
    "        \n",
    "        # Current usage\n",
    "        self.usage = {key: 0 for key in self.limits.keys()}\n",
    "    \n",
    "    def add_usage(self, resource: str, amount: float):\n",
    "        \"\"\"Record resource usage\"\"\"\n",
    "        if resource in self.usage:\n",
    "            self.usage[resource] += amount\n",
    "    \n",
    "    def get_utilization(self) -> pd.DataFrame:\n",
    "        \"\"\"Get utilization report\"\"\"\n",
    "        data = []\n",
    "        for resource, limit in self.limits.items():\n",
    "            used = self.usage[resource]\n",
    "            utilization_pct = (used / limit) * 100\n",
    "            remaining = max(0, limit - used)\n",
    "            \n",
    "            status = '‚úÖ Safe'\n",
    "            if utilization_pct > 90:\n",
    "                status = 'üö® Over Limit'\n",
    "            elif utilization_pct > 75:\n",
    "                status = '‚ö†Ô∏è Warning'\n",
    "            \n",
    "            data.append({\n",
    "                'Resource': resource,\n",
    "                'Used': f\"{used:,.0f}\",\n",
    "                'Limit': f\"{limit:,.0f}\",\n",
    "                'Remaining': f\"{remaining:,.0f}\",\n",
    "                'Utilization': f\"{utilization_pct:.1f}%\",\n",
    "                'Status': status\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "\n",
    "# Example: Track a month of usage\n",
    "tracker = FreeTierTracker()\n",
    "\n",
    "# Simulate usage\n",
    "tracker.add_usage('ec2_hours', 200)  # Running t2.micro for ~6.5 hours/day\n",
    "tracker.add_usage('s3_storage_gb', 2.5)  # 2.5GB stored\n",
    "tracker.add_usage('s3_get_requests', 5000)  # 5k GET requests\n",
    "tracker.add_usage('lambda_requests', 450_000)  # 450k Lambda invocations\n",
    "tracker.add_usage('lambda_gb_seconds', 180_000)  # Lambda compute\n",
    "tracker.add_usage('data_transfer_gb', 0.3)  # 300MB data transfer\n",
    "\n",
    "# Display utilization\n",
    "print(\"AWS Free Tier Utilization Report\\n\")\n",
    "print(tracker.get_utilization().to_string(index=False))\n",
    "print(\"\\nüí° Tips to Stay in Free Tier:\")\n",
    "print(\"   1. Stop instances when not in use (don't just pause)\")\n",
    "print(\"   2. Use S3 Intelligent-Tiering for automatic cost optimization\")\n",
    "print(\"   3. Set up billing alerts at 50%, 75%, and 90% of free tier limits\")\n",
    "print(\"   4. Delete old snapshots and unused volumes\")\n",
    "print(\"   5. Use CloudWatch to monitor usage in real-time\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2: Free Tier ML Project Blueprint\n",
    "\n",
    "Here's how to run a complete ML project on AWS free tier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "free_tier_ml_blueprint = {\n",
    "    'architecture': {\n",
    "        'data_storage': 'S3 (5GB free)',\n",
    "        'training': 'Local laptop or Colab/Kaggle (free GPU)',\n",
    "        'model_storage': 'S3 (< 100MB model)',\n",
    "        'inference': 'AWS Lambda (1M free requests)',\n",
    "        'api': 'API Gateway (1M free calls for 12 months)',\n",
    "        'monitoring': 'CloudWatch (basic metrics free)'\n",
    "    },\n",
    "    'estimated_costs': {\n",
    "        'storage': '$0 (< 5GB)',\n",
    "        'lambda': '$0 (< 1M requests)',\n",
    "        'api_gateway': '$0 (first 12 months)',\n",
    "        'data_transfer': '$0 (< 1GB)',\n",
    "        'total_monthly': '$0'\n",
    "    },\n",
    "    'limitations': [\n",
    "        'No GPU training (use Colab instead)',\n",
    "        'Small datasets only (< 5GB)',\n",
    "        'Inference only (no real-time endpoints)',\n",
    "        'Low traffic (< 1M predictions/month)'\n",
    "    ],\n",
    "    'best_practices': [\n",
    "        'Train models locally or on Colab',\n",
    "        'Use lightweight models (< 50MB)',\n",
    "        'Implement caching to reduce Lambda cold starts',\n",
    "        'Compress data before uploading to S3',\n",
    "        'Set up lifecycle policies to auto-delete old data'\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"Free Tier ML Project Blueprint\\n\")\n",
    "print(\"Architecture:\")\n",
    "for component, detail in free_tier_ml_blueprint['architecture'].items():\n",
    "    print(f\"  - {component}: {detail}\")\n",
    "\n",
    "print(\"\\nEstimated Monthly Costs:\")\n",
    "for item, cost in free_tier_ml_blueprint['estimated_costs'].items():\n",
    "    print(f\"  - {item}: {cost}\")\n",
    "\n",
    "print(\"\\n‚úÖ This architecture supports:\")\n",
    "print(\"   - 1M predictions/month\")\n",
    "print(\"   - ~100 API calls/hour average\")\n",
    "print(\"   - 5GB of training data\")\n",
    "print(\"   - Multiple model versions\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Managed Spot Training\n",
    "\n",
    "Spot instances can reduce training costs by **70-90%** but may be interrupted.\n",
    "\n",
    "### 3.1: Understanding Spot Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def compare_spot_vs_ondemand(instance: InstancePricing, training_hours: float,\n",
    "                             spot_interruption_rate: float = 0.1):\n",
    "    \"\"\"\n",
    "    Compare cost and reliability of spot vs on-demand training\n",
    "    \n",
    "    spot_interruption_rate: Probability of spot interruption (typical: 5-10%)\n",
    "    \"\"\"\n",
    "    # On-demand costs\n",
    "    ondemand_cost = training_hours * instance.on_demand_hourly\n",
    "    ondemand_reliability = 1.0  # 100% reliable\n",
    "    \n",
    "    # Spot costs (assuming interruption adds 10% overhead)\n",
    "    spot_effective_hours = training_hours * (1 + spot_interruption_rate)\n",
    "    spot_cost = spot_effective_hours * (instance.spot_hourly or instance.on_demand_hourly * 0.3)\n",
    "    spot_reliability = 1.0 - spot_interruption_rate\n",
    "    \n",
    "    savings = ondemand_cost - spot_cost\n",
    "    savings_pct = (savings / ondemand_cost) * 100\n",
    "    \n",
    "    return {\n",
    "        'instance': instance.name,\n",
    "        'training_hours': training_hours,\n",
    "        'ondemand_cost': ondemand_cost,\n",
    "        'spot_cost': spot_cost,\n",
    "        'savings': savings,\n",
    "        'savings_pct': savings_pct,\n",
    "        'spot_reliability': spot_reliability * 100\n",
    "    }\n",
    "\n",
    "# Compare different training scenarios\n",
    "training_scenarios = [\n",
    "    {'instance': aws_instances[1], 'hours': 10, 'name': 'Quick experiment'},\n",
    "    {'instance': aws_instances[3], 'hours': 24, 'name': 'Medium training job'},\n",
    "    {'instance': aws_instances[4], 'hours': 100, 'name': 'Large GPU training'},\n",
    "]\n",
    "\n",
    "comparisons = []\n",
    "for scenario in training_scenarios:\n",
    "    result = compare_spot_vs_ondemand(scenario['instance'], scenario['hours'])\n",
    "    result['scenario'] = scenario['name']\n",
    "    comparisons.append(result)\n",
    "\n",
    "comparison_df = pd.DataFrame(comparisons)\n",
    "\n",
    "print(\"Spot vs On-Demand Training Cost Comparison\\n\")\n",
    "print(comparison_df[[\n",
    "    'scenario', 'instance', 'training_hours', \n",
    "    'ondemand_cost', 'spot_cost', 'savings', 'savings_pct'\n",
    "]].to_string(index=False))\n",
    "\n",
    "print(\"\\nüí∞ Key Insights:\")\n",
    "print(f\"   - Average savings with Spot: {comparison_df['savings_pct'].mean():.1f}%\")\n",
    "print(f\"   - Total potential savings: ${comparison_df['savings'].sum():.2f}\")\n",
    "print(f\"   - Spot reliability: ~{comparison_df['spot_reliability'].mean():.0f}% (with checkpointing)\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2: Spot Training Best Practices\n",
    "\n",
    "To use spot instances effectively for ML training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# SageMaker Spot Training configuration example\n",
    "spot_training_config = {\n",
    "    'estimator_config': {\n",
    "        'instance_type': 'ml.p3.2xlarge',\n",
    "        'instance_count': 1,\n",
    "        'use_spot_instances': True,\n",
    "        'max_run': 86400,  # 24 hours max\n",
    "        'max_wait': 86400,  # How long to wait for spot capacity\n",
    "        # Key: Enable checkpointing!\n",
    "        'checkpoint_s3_uri': 's3://my-bucket/checkpoints/',\n",
    "        'checkpoint_local_path': '/opt/ml/checkpoints'\n",
    "    },\n",
    "    'training_script_requirements': [\n",
    "        'Save checkpoints every N epochs',\n",
    "        'Resume from checkpoint on restart',\n",
    "        'Handle SIGTERM gracefully (save state before shutdown)'\n",
    "    ],\n",
    "    'terraform_example': '''\n",
    "resource \"aws_sagemaker_training_job\" \"spot_training\" {\n",
    "  training_job_name = \"my-spot-training-job\"\n",
    "  role_arn          = aws_iam_role.sagemaker.arn\n",
    "\n",
    "  algorithm_specification {\n",
    "    training_image = \"your-training-image\"\n",
    "  }\n",
    "\n",
    "  resource_config {\n",
    "    instance_type   = \"ml.p3.2xlarge\"\n",
    "    instance_count  = 1\n",
    "    volume_size_in_gb = 50\n",
    "  }\n",
    "\n",
    "  # Enable Spot Training\n",
    "  enable_managed_spot_training = true\n",
    "  \n",
    "  stopping_condition {\n",
    "    max_runtime_in_seconds = 86400\n",
    "    max_wait_time_in_seconds = 86400\n",
    "  }\n",
    "\n",
    "  # Critical: Configure checkpointing\n",
    "  checkpoint_config {\n",
    "    s3_uri = \"s3://my-bucket/checkpoints/\"\n",
    "    local_path = \"/opt/ml/checkpoints\"\n",
    "  }\n",
    "}\n",
    "'''\n",
    "}\n",
    "\n",
    "print(\"Spot Training Best Practices\\n\")\n",
    "print(\"Configuration:\")\n",
    "print(json.dumps(spot_training_config['estimator_config'], indent=2))\n",
    "print(\"\\n‚úÖ Requirements for Spot Training Success:\")\n",
    "for idx, req in enumerate(spot_training_config['training_script_requirements'], 1):\n",
    "    print(f\"   {idx}. {req}\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è When NOT to use Spot:\")\n",
    "print(\"   - Time-critical training (deadline-driven)\")\n",
    "print(\"   - Training jobs < 1 hour (interruption overhead too high)\")\n",
    "print(\"   - Cannot implement checkpointing (legacy code)\")\n",
    "print(\"   - Need guaranteed completion time\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Storage Optimization\n",
    "\n",
    "Storage costs can accumulate quickly if not managed properly.\n",
    "\n",
    "### 4.1: S3 Storage Tiers and Lifecycle Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# S3 Storage Classes and Pricing (per GB/month)\n",
    "s3_storage_classes = pd.DataFrame([\n",
    "    {\n",
    "        'Storage Class': 'S3 Standard',\n",
    "        'Cost ($/GB/mo)': 0.023,\n",
    "        'Retrieval Cost': '$0',\n",
    "        'Retrieval Time': 'Milliseconds',\n",
    "        'Use Case': 'Frequently accessed data',\n",
    "        'Durability': '99.999999999%'\n",
    "    },\n",
    "    {\n",
    "        'Storage Class': 'S3 Intelligent-Tiering',\n",
    "        'Cost ($/GB/mo)': 0.023,  # Same as Standard for frequent access\n",
    "        'Retrieval Cost': '$0',\n",
    "        'Retrieval Time': 'Milliseconds',\n",
    "        'Use Case': 'Unknown or changing access patterns',\n",
    "        'Durability': '99.999999999%'\n",
    "    },\n",
    "    {\n",
    "        'Storage Class': 'S3 Standard-IA',\n",
    "        'Cost ($/GB/mo)': 0.0125,\n",
    "        'Retrieval Cost': '$0.01/GB',\n",
    "        'Retrieval Time': 'Milliseconds',\n",
    "        'Use Case': 'Infrequently accessed (< 1/month)',\n",
    "        'Durability': '99.999999999%'\n",
    "    },\n",
    "    {\n",
    "        'Storage Class': 'S3 Glacier Instant',\n",
    "        'Cost ($/GB/mo)': 0.004,\n",
    "        'Retrieval Cost': '$0.03/GB',\n",
    "        'Retrieval Time': 'Milliseconds',\n",
    "        'Use Case': 'Archive with instant access',\n",
    "        'Durability': '99.999999999%'\n",
    "    },\n",
    "    {\n",
    "        'Storage Class': 'S3 Glacier Flexible',\n",
    "        'Cost ($/GB/mo)': 0.0036,\n",
    "        'Retrieval Cost': '$0.01-0.03/GB',\n",
    "        'Retrieval Time': 'Minutes to hours',\n",
    "        'Use Case': 'Long-term archive',\n",
    "        'Durability': '99.999999999%'\n",
    "    },\n",
    "    {\n",
    "        'Storage Class': 'S3 Glacier Deep Archive',\n",
    "        'Cost ($/GB/mo)': 0.00099,\n",
    "        'Retrieval Cost': '$0.02/GB',\n",
    "        'Retrieval Time': '12-48 hours',\n",
    "        'Use Case': 'Compliance/regulatory archives',\n",
    "        'Durability': '99.999999999%'\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"AWS S3 Storage Classes Comparison\\n\")\n",
    "print(s3_storage_classes.to_string(index=False))\n",
    "print(\"\\nüí° Storage Class Selection Guide:\")\n",
    "print(\"   - Active training data: S3 Standard\")\n",
    "print(\"   - Processed datasets: S3 Intelligent-Tiering (auto-optimizes)\")\n",
    "print(\"   - Model artifacts: S3 Standard-IA (if not frequently deployed)\")\n",
    "print(\"   - Historical data: S3 Glacier Instant or Flexible\")\n",
    "print(\"   - Compliance archives: S3 Glacier Deep Archive\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2: S3 Lifecycle Policy Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# S3 Lifecycle Policy for ML data management\n",
    "s3_lifecycle_policy = {\n",
    "    'Rules': [\n",
    "        {\n",
    "            'Id': 'TransitionRawData',\n",
    "            'Status': 'Enabled',\n",
    "            'Prefix': 'data/raw/',\n",
    "            'Transitions': [\n",
    "                {\n",
    "                    'Days': 30,\n",
    "                    'StorageClass': 'INTELLIGENT_TIERING',\n",
    "                    'Reason': 'Auto-optimize based on access patterns'\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            'Id': 'TransitionProcessedData',\n",
    "            'Status': 'Enabled',\n",
    "            'Prefix': 'data/processed/',\n",
    "            'Transitions': [\n",
    "                {\n",
    "                    'Days': 90,\n",
    "                    'StorageClass': 'GLACIER_IR',\n",
    "                    'Reason': 'Old processed data rarely accessed'\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            'Id': 'DeleteTempFiles',\n",
    "            'Status': 'Enabled',\n",
    "            'Prefix': 'temp/',\n",
    "            'Expiration': {\n",
    "                'Days': 7,\n",
    "                'Reason': 'Temporary files no longer needed'\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'Id': 'ArchiveOldModels',\n",
    "            'Status': 'Enabled',\n",
    "            'Prefix': 'models/archive/',\n",
    "            'Transitions': [\n",
    "                {\n",
    "                    'Days': 180,\n",
    "                    'StorageClass': 'GLACIER_FLEXIBLE',\n",
    "                    'Reason': 'Keep for compliance but rarely used'\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Terraform version of lifecycle policy\n",
    "terraform_lifecycle = '''\n",
    "resource \"aws_s3_bucket_lifecycle_configuration\" \"ml_data\" {\n",
    "  bucket = aws_s3_bucket.ml_data.id\n",
    "\n",
    "  rule {\n",
    "    id     = \"transition-raw-data\"\n",
    "    status = \"Enabled\"\n",
    "\n",
    "    filter {\n",
    "      prefix = \"data/raw/\"\n",
    "    }\n",
    "\n",
    "    transition {\n",
    "      days          = 30\n",
    "      storage_class = \"INTELLIGENT_TIERING\"\n",
    "    }\n",
    "  }\n",
    "\n",
    "  rule {\n",
    "    id     = \"delete-temp-files\"\n",
    "    status = \"Enabled\"\n",
    "\n",
    "    filter {\n",
    "      prefix = \"temp/\"\n",
    "    }\n",
    "\n",
    "    expiration {\n",
    "      days = 7\n",
    "    }\n",
    "  }\n",
    "}\n",
    "'''\n",
    "\n",
    "print(\"S3 Lifecycle Policy for ML Data Management\\n\")\n",
    "print(json.dumps(s3_lifecycle_policy, indent=2))\n",
    "print(\"\\nüìä Expected Cost Savings:\")\n",
    "print(\"   - 30-day transition to Intelligent-Tiering: 0-50% savings\")\n",
    "print(\"   - 90-day transition to Glacier: 80%+ savings\")\n",
    "print(\"   - Auto-delete temp files: Prevents waste accumulation\")\n",
    "print(\"\\nTerraform implementation saved to: s3_lifecycle_policy.tf\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3: Storage Cost Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def calculate_storage_costs(data_gb: float, storage_class: str, \n",
    "                           retrievals_per_month: int = 0) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate monthly S3 storage costs including retrieval\n",
    "    \"\"\"\n",
    "    # Storage costs per GB/month\n",
    "    storage_prices = {\n",
    "        'standard': 0.023,\n",
    "        'intelligent_tiering': 0.023,  # Frequent tier\n",
    "        'standard_ia': 0.0125,\n",
    "        'glacier_ir': 0.004,\n",
    "        'glacier': 0.0036,\n",
    "        'deep_archive': 0.00099\n",
    "    }\n",
    "    \n",
    "    # Retrieval costs per GB\n",
    "    retrieval_prices = {\n",
    "        'standard': 0,\n",
    "        'intelligent_tiering': 0,\n",
    "        'standard_ia': 0.01,\n",
    "        'glacier_ir': 0.03,\n",
    "        'glacier': 0.02,\n",
    "        'deep_archive': 0.02\n",
    "    }\n",
    "    \n",
    "    storage_cost = data_gb * storage_prices[storage_class]\n",
    "    retrieval_cost = (data_gb * retrievals_per_month * \n",
    "                     retrieval_prices[storage_class])\n",
    "    total_cost = storage_cost + retrieval_cost\n",
    "    \n",
    "    return {\n",
    "        'storage_class': storage_class,\n",
    "        'data_gb': data_gb,\n",
    "        'storage_cost': storage_cost,\n",
    "        'retrieval_cost': retrieval_cost,\n",
    "        'total_monthly_cost': total_cost\n",
    "    }\n",
    "\n",
    "# Example: 100GB dataset with different storage strategies\n",
    "dataset_size = 100  # GB\n",
    "\n",
    "storage_scenarios = [\n",
    "    {'class': 'standard', 'retrievals': 10, 'name': 'Active Development'},\n",
    "    {'class': 'intelligent_tiering', 'retrievals': 5, 'name': 'Smart Auto-Tiering'},\n",
    "    {'class': 'standard_ia', 'retrievals': 2, 'name': 'Infrequent Access'},\n",
    "    {'class': 'glacier_ir', 'retrievals': 1, 'name': 'Archived (Instant)'},\n",
    "    {'class': 'glacier', 'retrievals': 0, 'name': 'Deep Archive'},\n",
    "]\n",
    "\n",
    "storage_costs = []\n",
    "for scenario in storage_scenarios:\n",
    "    cost = calculate_storage_costs(\n",
    "        dataset_size, \n",
    "        scenario['class'], \n",
    "        scenario['retrievals']\n",
    "    )\n",
    "    cost['scenario'] = scenario['name']\n",
    "    storage_costs.append(cost)\n",
    "\n",
    "storage_cost_df = pd.DataFrame(storage_costs)\n",
    "\n",
    "print(f\"Storage Cost Analysis for {dataset_size}GB Dataset\\n\")\n",
    "print(storage_cost_df[[\n",
    "    'scenario', 'storage_class', 'storage_cost', \n",
    "    'retrieval_cost', 'total_monthly_cost'\n",
    "]].to_string(index=False))\n",
    "\n",
    "# Calculate savings\n",
    "baseline_cost = storage_cost_df.iloc[0]['total_monthly_cost']\n",
    "best_cost = storage_cost_df['total_monthly_cost'].min()\n",
    "max_savings = baseline_cost - best_cost\n",
    "max_savings_pct = (max_savings / baseline_cost) * 100\n",
    "\n",
    "print(f\"\\nüí∞ Maximum potential savings: ${max_savings:.2f}/month ({max_savings_pct:.1f}%)\")\n",
    "print(f\"   Annual savings: ${max_savings * 12:.2f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Cost Monitoring and Alerts\n",
    "\n",
    "### 5.1: Setting Up Billing Alerts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Terraform configuration for AWS Budgets and Alerts\n",
    "terraform_budget = '''\n",
    "# Budget with multiple alert thresholds\n",
    "resource \"aws_budgets_budget\" \"ml_monthly_budget\" {\n",
    "  name              = \"ml-monthly-budget\"\n",
    "  budget_type       = \"COST\"\n",
    "  limit_amount      = \"50\"  # $50/month budget\n",
    "  limit_unit        = \"USD\"\n",
    "  time_unit         = \"MONTHLY\"\n",
    "\n",
    "  # Alert at 50% of budget\n",
    "  notification {\n",
    "    comparison_operator        = \"GREATER_THAN\"\n",
    "    threshold                  = 50\n",
    "    threshold_type            = \"PERCENTAGE\"\n",
    "    notification_type         = \"FORECASTED\"\n",
    "    subscriber_email_addresses = [\"your-email@example.com\"]\n",
    "  }\n",
    "\n",
    "  # Alert at 75% of budget\n",
    "  notification {\n",
    "    comparison_operator        = \"GREATER_THAN\"\n",
    "    threshold                  = 75\n",
    "    threshold_type            = \"PERCENTAGE\"\n",
    "    notification_type         = \"ACTUAL\"\n",
    "    subscriber_email_addresses = [\"your-email@example.com\"]\n",
    "  }\n",
    "\n",
    "  # Alert at 90% of budget\n",
    "  notification {\n",
    "    comparison_operator        = \"GREATER_THAN\"\n",
    "    threshold                  = 90\n",
    "    threshold_type            = \"PERCENTAGE\"\n",
    "    notification_type         = \"ACTUAL\"\n",
    "    subscriber_email_addresses = [\"your-email@example.com\"]\n",
    "  }\n",
    "\n",
    "  # Alert when exceeded\n",
    "  notification {\n",
    "    comparison_operator        = \"GREATER_THAN\"\n",
    "    threshold                  = 100\n",
    "    threshold_type            = \"PERCENTAGE\"\n",
    "    notification_type         = \"ACTUAL\"\n",
    "    subscriber_email_addresses = [\"your-email@example.com\"]\n",
    "  }\n",
    "\n",
    "  # Filter to specific services (optional)\n",
    "  cost_filter {\n",
    "    name   = \"Service\"\n",
    "    values = [\"Amazon SageMaker\", \"Amazon S3\", \"AWS Lambda\"]\n",
    "  }\n",
    "}\n",
    "\n",
    "# CloudWatch Alarm for specific resource costs\n",
    "resource \"aws_cloudwatch_metric_alarm\" \"sagemaker_endpoint_cost\" {\n",
    "  alarm_name          = \"sagemaker-endpoint-high-cost\"\n",
    "  comparison_operator = \"GreaterThanThreshold\"\n",
    "  evaluation_periods  = \"1\"\n",
    "  metric_name         = \"EstimatedCharges\"\n",
    "  namespace           = \"AWS/Billing\"\n",
    "  period              = \"21600\"  # 6 hours\n",
    "  statistic           = \"Maximum\"\n",
    "  threshold           = \"10\"  # $10 threshold\n",
    "\n",
    "  alarm_description = \"Alert when SageMaker costs exceed $10\"\n",
    "  alarm_actions     = [aws_sns_topic.billing_alerts.arn]\n",
    "\n",
    "  dimensions = {\n",
    "    ServiceName = \"AmazonSageMaker\"\n",
    "  }\n",
    "}\n",
    "'''\n",
    "\n",
    "print(\"Cost Monitoring & Alert Configuration\\n\")\n",
    "print(\"Terraform configuration saved to: aws_budgets.tf\")\n",
    "print(\"\\nüìä Recommended Alert Thresholds:\")\n",
    "print(\"   - 50% (Forecasted): Early warning, time to review\")\n",
    "print(\"   - 75% (Actual): Concerning, investigate immediately\")\n",
    "print(\"   - 90% (Actual): Critical, take action\")\n",
    "print(\"   - 100% (Actual): Over budget, shutdown non-essential resources\")\n",
    "print(\"\\n‚öôÔ∏è Setup Steps:\")\n",
    "print(\"   1. Enable billing alerts in AWS Console\")\n",
    "print(\"   2. Create SNS topic for notifications\")\n",
    "print(\"   3. Apply Terraform configuration\")\n",
    "print(\"   4. Verify email subscription\")\n",
    "print(\"   5. Test with small budget ($1) first\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2: Cost Allocation Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Example cost allocation tagging strategy\n",
    "cost_allocation_tags = {\n",
    "    'Project': 'customer-churn-prediction',\n",
    "    'Environment': 'development',  # development, staging, production\n",
    "    'Team': 'data-science',\n",
    "    'CostCenter': 'ML-R&D',\n",
    "    'Owner': 'john.doe@company.com',\n",
    "    'Purpose': 'training',  # training, inference, storage, experimentation\n",
    "    'AutoShutdown': 'true',  # For automated cleanup\n",
    "    'ExpirationDate': '2024-12-31'  # When to archive/delete\n",
    "}\n",
    "\n",
    "# Terraform example with tags\n",
    "terraform_tags_example = '''\n",
    "# Consistent tagging across all resources\n",
    "locals {\n",
    "  common_tags = {\n",
    "    Project     = \"customer-churn-prediction\"\n",
    "    Environment = \"development\"\n",
    "    Team        = \"data-science\"\n",
    "    CostCenter  = \"ML-R&D\"\n",
    "    ManagedBy   = \"terraform\"\n",
    "  }\n",
    "}\n",
    "\n",
    "# S3 bucket with tags\n",
    "resource \"aws_s3_bucket\" \"ml_data\" {\n",
    "  bucket = \"my-ml-data-bucket\"\n",
    "  tags   = merge(local.common_tags, {\n",
    "    Purpose = \"data-storage\"\n",
    "  })\n",
    "}\n",
    "\n",
    "# SageMaker endpoint with tags\n",
    "resource \"aws_sagemaker_endpoint\" \"model\" {\n",
    "  name = \"customer-churn-endpoint\"\n",
    "  tags = merge(local.common_tags, {\n",
    "    Purpose        = \"inference\"\n",
    "    AutoShutdown   = \"true\"\n",
    "    ShutdownTime   = \"20:00\"  # 8 PM daily\n",
    "  })\n",
    "}\n",
    "'''\n",
    "\n",
    "print(\"Cost Allocation Tag Strategy\\n\")\n",
    "print(\"Recommended Tags:\")\n",
    "for key, value in cost_allocation_tags.items():\n",
    "    print(f\"  - {key}: {value}\")\n",
    "\n",
    "print(\"\\nüìä Benefits of Cost Allocation Tags:\")\n",
    "print(\"   1. Track costs by project, team, or environment\")\n",
    "print(\"   2. Identify cost drivers and optimization opportunities\")\n",
    "print(\"   3. Automate resource cleanup (based on tags)\")\n",
    "print(\"   4. Generate accurate billing reports\")\n",
    "print(\"   5. Enforce governance policies\")\n",
    "\n",
    "print(\"\\nüè∑Ô∏è Tag Best Practices:\")\n",
    "print(\"   - Use consistent naming conventions\")\n",
    "print(\"   - Tag all billable resources\")\n",
    "print(\"   - Activate cost allocation tags in billing console\")\n",
    "print(\"   - Review and update tags regularly\")\n",
    "print(\"   - Use automation to enforce tagging policies\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Right-Sizing and Auto-Scaling\n",
    "\n",
    "### 6.1: Instance Right-Sizing Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def recommend_instance_size(cpu_usage_pct: float, memory_usage_pct: float,\n",
    "                           current_instance: InstancePricing) -> dict:\n",
    "    \"\"\"\n",
    "    Recommend instance right-sizing based on utilization metrics\n",
    "    \n",
    "    Rules:\n",
    "    - If both CPU and memory < 30%: Downsize\n",
    "    - If either > 80%: Upsize\n",
    "    - Otherwise: Keep current size\n",
    "    \"\"\"\n",
    "    recommendation = 'keep'\n",
    "    reason = 'Utilization is within optimal range (30-80%)'\n",
    "    potential_savings = 0\n",
    "    \n",
    "    if cpu_usage_pct < 30 and memory_usage_pct < 30:\n",
    "        recommendation = 'downsize'\n",
    "        reason = f'Low utilization (CPU: {cpu_usage_pct}%, Memory: {memory_usage_pct}%)'\n",
    "        # Simulate ~50% cost reduction by downsizing\n",
    "        potential_savings = current_instance.on_demand_hourly * 0.5 * 730  # Monthly\n",
    "    elif cpu_usage_pct > 80 or memory_usage_pct > 80:\n",
    "        recommendation = 'upsize'\n",
    "        reason = f'High utilization (CPU: {cpu_usage_pct}%, Memory: {memory_usage_pct}%)'\n",
    "        potential_savings = 0  # Cost increases, but improves performance\n",
    "    \n",
    "    return {\n",
    "        'current_instance': current_instance.name,\n",
    "        'cpu_usage': cpu_usage_pct,\n",
    "        'memory_usage': memory_usage_pct,\n",
    "        'recommendation': recommendation,\n",
    "        'reason': reason,\n",
    "        'monthly_savings': potential_savings\n",
    "    }\n",
    "\n",
    "# Simulate utilization for different instances\n",
    "utilization_data = [\n",
    "    {'instance': aws_instances[2], 'cpu': 25, 'memory': 20},  # Under-utilized\n",
    "    {'instance': aws_instances[1], 'cpu': 55, 'memory': 60},  # Well-sized\n",
    "    {'instance': aws_instances[0], 'cpu': 85, 'memory': 90},  # Over-utilized\n",
    "]\n",
    "\n",
    "rightsizing_recommendations = []\n",
    "for data in utilization_data:\n",
    "    rec = recommend_instance_size(data['cpu'], data['memory'], data['instance'])\n",
    "    rightsizing_recommendations.append(rec)\n",
    "\n",
    "rightsizing_df = pd.DataFrame(rightsizing_recommendations)\n",
    "\n",
    "print(\"Instance Right-Sizing Recommendations\\n\")\n",
    "print(rightsizing_df.to_string(index=False))\n",
    "\n",
    "total_savings = rightsizing_df['monthly_savings'].sum()\n",
    "print(f\"\\nüí∞ Total potential monthly savings: ${total_savings:.2f}\")\n",
    "print(f\"   Annual savings: ${total_savings * 12:.2f}\")\n",
    "\n",
    "print(\"\\nüìä Right-Sizing Best Practices:\")\n",
    "print(\"   - Monitor utilization for at least 2 weeks\")\n",
    "print(\"   - Look at peak usage, not just averages\")\n",
    "print(\"   - Consider workload patterns (batch vs real-time)\")\n",
    "print(\"   - Test smaller instances before committing\")\n",
    "print(\"   - Use CloudWatch for automated recommendations\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned comprehensive cloud cost optimization strategies:\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Pricing Models**\n",
    "   - On-demand: Pay-per-use, no commitment\n",
    "   - Spot: 70-90% savings, can be interrupted\n",
    "   - Reserved: 30-60% savings, 1-3 year commitment\n",
    "   - Free tier: $0 for learning and small projects\n",
    "\n",
    "2. **Free Tier Maximization**\n",
    "   - Use free services: Lambda, S3 (5GB), serverless\n",
    "   - Train on Colab/Kaggle (free GPU)\n",
    "   - Deploy with serverless for < 1M requests/month\n",
    "   - Set up billing alerts at 50%, 75%, 90%\n",
    "\n",
    "3. **Spot Training**\n",
    "   - 70-90% cost reduction for training\n",
    "   - Requires checkpointing\n",
    "   - Best for non-urgent, long-running jobs\n",
    "   - Use managed spot training in SageMaker\n",
    "\n",
    "4. **Storage Optimization**\n",
    "   - S3 Standard: Frequent access ($0.023/GB/mo)\n",
    "   - Intelligent-Tiering: Auto-optimize\n",
    "   - Glacier: Archive ($0.004/GB/mo)\n",
    "   - Lifecycle policies: Auto-transition & delete\n",
    "\n",
    "5. **Cost Monitoring**\n",
    "   - AWS Budgets: Set spending limits\n",
    "   - CloudWatch Alarms: Real-time alerts\n",
    "   - Cost allocation tags: Track by project/team\n",
    "   - Regular cost reviews: Weekly or monthly\n",
    "\n",
    "6. **Right-Sizing**\n",
    "   - Monitor utilization (CPU, memory)\n",
    "   - Downsize if < 30% utilized\n",
    "   - Upsize if > 80% utilized\n",
    "   - Use auto-scaling for variable workloads\n",
    "\n",
    "### Cost Optimization Checklist:\n",
    "\n",
    "‚úÖ Use free tier for learning  \n",
    "‚úÖ Spot instances for training  \n",
    "‚úÖ Serverless for low-traffic inference  \n",
    "‚úÖ S3 lifecycle policies  \n",
    "‚úÖ Billing alerts at 50%, 75%, 90%  \n",
    "‚úÖ Cost allocation tags  \n",
    "‚úÖ Right-size instances  \n",
    "‚úÖ Delete unused resources  \n",
    "‚úÖ Stop instances when not in use  \n",
    "‚úÖ Review costs weekly  \n",
    "\n",
    "### Realistic ML Project Costs:\n",
    "\n",
    "| Scale | Traffic | Strategy | Monthly Cost |\n",
    "|-------|---------|----------|-------------|\n",
    "| Learning | N/A | Free tier + Colab | **$0** |\n",
    "| Small Project | <1M requests | Free tier + Lambda | **$0-5** |\n",
    "| Medium Project | 1-10M requests | Spot training + Lambda | **$20-50** |\n",
    "| Production | 10M+ requests | Spot + Reserved + Auto-scale | **$200-500** |\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **[Module 09: Multi-Cloud ML Considerations](09_multi_cloud_ml_considerations.ipynb)**: Cross-platform strategies\n",
    "- **[Module 10: Cloud Storage for ML](10_cloud_storage_for_ml.ipynb)**: Deep dive into cloud storage\n",
    "- **Practice**: Set up billing alerts for your AWS account\n",
    "- **Explore**: AWS Cost Explorer for detailed cost analysis\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [AWS Cost Optimization](https://aws.amazon.com/pricing/cost-optimization/)\n",
    "- [Azure Cost Management](https://azure.microsoft.com/en-us/products/cost-management/)\n",
    "- [GCP Cost Optimization](https://cloud.google.com/cost-management)\n",
    "- [AWS Budgets Documentation](https://docs.aws.amazon.com/cost-management/latest/userguide/budgets-managing-costs.html)\n",
    "- [SageMaker Managed Spot Training](https://docs.aws.amazon.com/sagemaker/latest/dg/model-managed-spot-training.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Free Tier Budget Planner ‚≠ê\n",
    "\n",
    "Create a free tier budget planner that:\n",
    "1. Takes your planned usage as input (storage, compute hours, Lambda requests)\n",
    "2. Calculates if you'll stay within free tier limits\n",
    "3. Suggests optimizations if you exceed limits\n",
    "4. Provides cost estimates if you go over\n",
    "\n",
    "Test with at least 3 different project scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Your code here\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Spot vs On-Demand Decision Tool ‚≠ê‚≠ê\n",
    "\n",
    "Build a tool that recommends whether to use spot or on-demand instances based on:\n",
    "1. Training time requirements\n",
    "2. Deadline urgency\n",
    "3. Interruption tolerance\n",
    "4. Cost sensitivity\n",
    "5. Checkpointing capability\n",
    "\n",
    "Create a decision matrix and visualize the recommendation logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Your code here\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Storage Lifecycle Optimizer ‚≠ê‚≠ê\n",
    "\n",
    "Design and implement a storage lifecycle policy for an ML project with:\n",
    "- 500GB raw data\n",
    "- 200GB processed data\n",
    "- 50GB model artifacts\n",
    "- 100GB temporary files\n",
    "\n",
    "Calculate:\n",
    "1. Costs with no lifecycle policy\n",
    "2. Costs with optimized lifecycle policy\n",
    "3. Monthly and annual savings\n",
    "4. Optimal transition timelines\n",
    "\n",
    "Visualize cost savings over 12 months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Your code here\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Cost Monitoring Dashboard ‚≠ê‚≠ê‚≠ê\n",
    "\n",
    "Create a comprehensive cost monitoring system:\n",
    "\n",
    "1. **Simulate monthly costs** for:\n",
    "   - Compute (training & inference)\n",
    "   - Storage (S3)\n",
    "   - Data transfer\n",
    "   - Serverless (Lambda)\n",
    "\n",
    "2. **Implement alerts** at:\n",
    "   - 50%, 75%, 90% of budget\n",
    "   - Anomaly detection (sudden cost spikes)\n",
    "\n",
    "3. **Generate reports**:\n",
    "   - Cost breakdown by service\n",
    "   - Trends over time\n",
    "   - Recommendations for optimization\n",
    "\n",
    "4. **Visualize**:\n",
    "   - Cost trends (line chart)\n",
    "   - Service breakdown (pie chart)\n",
    "   - Budget utilization (gauge chart)\n",
    "\n",
    "**Bonus**: Add forecasting for next month's costs based on trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Your code here\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Multi-Cloud Cost Comparison ‚≠ê‚≠ê‚≠ê\n",
    "\n",
    "Compare costs for the same ML workload across AWS, Azure, and GCP:\n",
    "\n",
    "**Workload specification:**\n",
    "- Training: 100 hours/month on GPU\n",
    "- Inference: 1M predictions/month\n",
    "- Storage: 200GB data\n",
    "- Data transfer: 50GB/month\n",
    "\n",
    "Calculate and compare:\n",
    "1. Total monthly costs per platform\n",
    "2. Free tier benefits\n",
    "3. Spot/preemptible savings\n",
    "4. Reserved instance pricing\n",
    "5. Hidden costs (data egress, API calls)\n",
    "\n",
    "Present findings in:\n",
    "- Comparison table\n",
    "- Cost breakdown charts\n",
    "- Recommendation report\n",
    "\n",
    "**Bonus**: Include pricing for different regions and show geographic optimization opportunities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Your code here\n"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
