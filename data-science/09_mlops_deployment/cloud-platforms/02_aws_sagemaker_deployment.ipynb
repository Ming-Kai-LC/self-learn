{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 02: AWS SageMaker Model Deployment\n",
    "\n",
    "**Difficulty**: ⭐⭐⭐\n",
    "**Estimated Time**: 60 minutes\n",
    "**Prerequisites**: \n",
    "- [Module 01: AWS SageMaker Basics](01_aws_sagemaker_basics.ipynb)\n",
    "- Understanding of ML model deployment concepts\n",
    "- Basic knowledge of AWS services\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Deploy ML models to SageMaker real-time endpoints with auto-scaling\n",
    "2. Use Batch Transform for large-scale offline inference\n",
    "3. Configure multi-model endpoints to reduce costs\n",
    "4. Implement A/B testing and traffic splitting strategies\n",
    "5. Set up model monitoring and CloudWatch logging\n",
    "6. Apply cost optimization techniques for SageMaker endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Introduction\n",
    "\n",
    "### SageMaker Deployment Options\n",
    "\n",
    "AWS SageMaker provides several deployment patterns:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│           SageMaker Deployment Options                  │\n",
    "├─────────────────────────────────────────────────────────┤\n",
    "│                                                         │\n",
    "│  1. Real-time Endpoints                                 │\n",
    "│     ├─ Low latency (ms)                                 │\n",
    "│     ├─ Auto-scaling                                     │\n",
    "│     └─ 24/7 availability                                │\n",
    "│                                                         │\n",
    "│  2. Batch Transform                                     │\n",
    "│     ├─ Large datasets                                   │\n",
    "│     ├─ No real-time requirement                         │\n",
    "│     └─ Cost-effective                                   │\n",
    "│                                                         │\n",
    "│  3. Serverless Inference                                │\n",
    "│     ├─ Auto-scales to zero                              │\n",
    "│     ├─ Intermittent traffic                             │\n",
    "│     └─ Pay per request                                  │\n",
    "│                                                         │\n",
    "│  4. Asynchronous Inference                              │\n",
    "│     ├─ Long processing times                            │\n",
    "│     ├─ Queue-based                                      │\n",
    "│     └─ S3 input/output                                  │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Cost Warning**: Real-time endpoints run continuously and incur hourly charges. Always delete endpoints after testing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock SageMaker SDK classes for demonstration\n",
    "# In production, use: import boto3, sagemaker\n",
    "\n",
    "class MockSageMakerClient:\n",
    "    \"\"\"Simulates AWS SageMaker client for educational purposes\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.endpoints = {}\n",
    "        self.models = {}\n",
    "        self.endpoint_configs = {}\n",
    "    \n",
    "    def create_model(self, **kwargs):\n",
    "        model_name = kwargs['ModelName']\n",
    "        self.models[model_name] = kwargs\n",
    "        return {'ModelArn': f'arn:aws:sagemaker:model/{model_name}'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Real-time Endpoint Deployment\n",
    "\n",
    "Real-time endpoints provide low-latency predictions for online applications.\n",
    "\n",
    "### Deployment Architecture\n",
    "\n",
    "```\n",
    "┌──────────────┐     ┌──────────────────────────────────┐\n",
    "│   Client     │────▶│   Load Balancer                  │\n",
    "│ Application  │     └──────────────────────────────────┘\n",
    "└──────────────┘                    │\n",
    "                                    ▼\n",
    "                    ┌───────────────────────────────────┐\n",
    "                    │      SageMaker Endpoint           │\n",
    "                    ├───────────────────────────────────┤\n",
    "                    │  ┌─────────┐  ┌─────────┐        │\n",
    "                    │  │Instance │  │Instance │        │\n",
    "                    │  │  (min)  │  │ (scaled)│        │\n",
    "                    │  └─────────┘  └─────────┘        │\n",
    "                    │                                   │\n",
    "                    │  Auto-scaling based on:           │\n",
    "                    │  - InvocationsPerInstance         │\n",
    "                    │  - CPUUtilization                 │\n",
    "                    │  - Custom metrics                 │\n",
    "                    └───────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Real-time endpoint configuration\n",
    "\n",
    "def create_endpoint_config_dict():\n",
    "    \"\"\"Define endpoint configuration with production variants\"\"\"\n",
    "    config = {\n",
    "        'EndpointConfigName': 'my-model-config-v1',\n",
    "        'ProductionVariants': [\n",
    "            {\n",
    "                'VariantName': 'AllTraffic',\n",
    "                'ModelName': 'my-trained-model',\n",
    "                'InstanceType': 'ml.t2.medium',  # Free tier eligible\n",
    "                'InitialInstanceCount': 1,\n",
    "                'InitialVariantWeight': 1.0\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    return config\n",
    "\n",
    "endpoint_config = create_endpoint_config_dict()\n",
    "print(\"Endpoint Configuration:\")\n",
    "print(json.dumps(endpoint_config, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Auto-scaling configuration\n",
    "\n",
    "def create_autoscaling_config():\n",
    "    \"\"\"Configure auto-scaling for endpoint based on traffic\"\"\"\n",
    "    autoscaling_config = {\n",
    "        'ResourceId': 'endpoint/my-endpoint/variant/AllTraffic',\n",
    "        'ScalableDimension': 'sagemaker:variant:DesiredInstanceCount',\n",
    "        'MinCapacity': 1,  # Minimum instances\n",
    "        'MaxCapacity': 5,  # Maximum instances\n",
    "        'TargetTrackingScalingPolicyConfiguration': {\n",
    "            'TargetValue': 70.0,  # Target 70 invocations/instance\n",
    "            'PredefinedMetricSpecification': {\n",
    "                'PredefinedMetricType': 'SageMakerVariantInvocationsPerInstance'\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    return autoscaling_config\n",
    "\n",
    "autoscaling = create_autoscaling_config()\n",
    "print(\"Auto-scaling Configuration:\")\n",
    "print(json.dumps(autoscaling, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions\n",
    "\n",
    "Once deployed, invoke the endpoint with your data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated endpoint invocation\n",
    "\n",
    "class MockEndpoint:\n",
    "    \"\"\"Simulates a SageMaker endpoint for predictions\"\"\"\n",
    "    \n",
    "    def __init__(self, model_type='classifier'):\n",
    "        self.model_type = model_type\n",
    "        self.invocation_count = 0\n",
    "        self.latencies = []\n",
    "    \n",
    "    def invoke(self, data):\n",
    "        \"\"\"Simulate model prediction with latency tracking\"\"\"\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Simulate prediction\n",
    "        if self.model_type == 'classifier':\n",
    "            prediction = np.random.choice([0, 1], p=[0.7, 0.3])\n",
    "        else:\n",
    "            prediction = np.random.randn() * 10 + 50\n",
    "        \n",
    "        latency = (time.time() - start_time) * 1000  # ms\n",
    "        self.latencies.append(latency)\n",
    "        self.invocation_count += 1\n",
    "        \n",
    "        return {'prediction': prediction, 'latency_ms': latency}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test endpoint invocation\n",
    "endpoint = MockEndpoint(model_type='classifier')\n",
    "\n",
    "# Make sample predictions\n",
    "test_data = np.random.randn(5, 10)\n",
    "results = []\n",
    "\n",
    "for i, sample in enumerate(test_data):\n",
    "    result = endpoint.invoke(sample)\n",
    "    results.append(result)\n",
    "    print(f\"Request {i+1}: Prediction={result['prediction']}, \"\n",
    "          f\"Latency={result['latency_ms']:.2f}ms\")\n",
    "\n",
    "print(f\"\\nAverage latency: {np.mean(endpoint.latencies):.2f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Batch Transform for Large-Scale Inference\n",
    "\n",
    "Batch Transform is ideal for:\n",
    "- Processing large datasets offline\n",
    "- No real-time latency requirements\n",
    "- Cost optimization (pay only during processing)\n",
    "\n",
    "### Batch Transform Architecture\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│                  Batch Transform                         │\n",
    "├─────────────────────────────────────────────────────────┤\n",
    "│                                                         │\n",
    "│  S3 Input Bucket        SageMaker           S3 Output  │\n",
    "│  ┌──────────┐         ┌──────────┐         ┌────────┐ │\n",
    "│  │ data.csv │───────▶ │ Transform│────────▶│results │ │\n",
    "│  │ (large)  │         │   Job    │         │  .csv  │ │\n",
    "│  └──────────┘         └──────────┘         └────────┘ │\n",
    "│                            │                           │\n",
    "│                       Auto-scales                      │\n",
    "│                       instances                        │\n",
    "│                       based on                         │\n",
    "│                       data size                        │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Batch transform configuration\n",
    "\n",
    "def create_batch_transform_job():\n",
    "    \"\"\"Configure batch transform for large-scale inference\"\"\"\n",
    "    job_config = {\n",
    "        'TransformJobName': 'batch-inference-job-2024-01',\n",
    "        'ModelName': 'my-trained-model',\n",
    "        'TransformInput': {\n",
    "            'DataSource': {\n",
    "                'S3DataSource': {\n",
    "                    'S3DataType': 'S3Prefix',\n",
    "                    'S3Uri': 's3://my-bucket/input-data/'\n",
    "                }\n",
    "            },\n",
    "            'ContentType': 'text/csv',\n",
    "            'SplitType': 'Line'  # Process line-by-line\n",
    "        },\n",
    "        'TransformOutput': {\n",
    "            'S3OutputPath': 's3://my-bucket/predictions/',\n",
    "            'AssembleWith': 'Line'\n",
    "        },\n",
    "        'TransformResources': {\n",
    "            'InstanceType': 'ml.m5.xlarge',\n",
    "            'InstanceCount': 2  # Parallel processing\n",
    "        }\n",
    "    }\n",
    "    return job_config\n",
    "\n",
    "batch_config = create_batch_transform_job()\n",
    "print(\"Batch Transform Configuration:\")\n",
    "print(json.dumps(batch_config, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate batch transform processing\n",
    "\n",
    "class BatchTransformSimulator:\n",
    "    \"\"\"Simulates batch transform job execution\"\"\"\n",
    "    \n",
    "    def __init__(self, instance_count=2):\n",
    "        self.instance_count = instance_count\n",
    "        self.processed_records = 0\n",
    "    \n",
    "    def process_batch(self, data_size):\n",
    "        \"\"\"Simulate processing large dataset in batches\"\"\"\n",
    "        batch_size = data_size // self.instance_count\n",
    "        \n",
    "        print(f\"Processing {data_size} records...\")\n",
    "        print(f\"Using {self.instance_count} instances\")\n",
    "        print(f\"Batch size per instance: {batch_size}\\n\")\n",
    "        \n",
    "        for i in range(self.instance_count):\n",
    "            start = i * batch_size\n",
    "            end = start + batch_size\n",
    "            print(f\"Instance {i+1}: Processing records {start}-{end}\")\n",
    "            self.processed_records += batch_size\n",
    "        \n",
    "        return self.processed_records\n",
    "\n",
    "# Simulate batch processing\n",
    "batch_job = BatchTransformSimulator(instance_count=2)\n",
    "total_records = 100000\n",
    "processed = batch_job.process_batch(total_records)\n",
    "print(f\"\\nTotal processed: {processed:,} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multi-Model Endpoints\n",
    "\n",
    "Multi-model endpoints allow hosting multiple models on a single endpoint, significantly reducing costs.\n",
    "\n",
    "### Multi-Model Architecture\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│          Multi-Model Endpoint                            │\n",
    "├─────────────────────────────────────────────────────────┤\n",
    "│                                                         │\n",
    "│  S3 Model Store         Endpoint Container             │\n",
    "│  ┌──────────────┐      ┌─────────────────────┐        │\n",
    "│  │ model-a.tar  │      │  Memory Cache       │        │\n",
    "│  │ model-b.tar  │◀────▶│  ┌──────┐ ┌──────┐ │        │\n",
    "│  │ model-c.tar  │      │  │Model │ │Model │ │        │\n",
    "│  │     ...      │      │  │  A   │ │  B   │ │        │\n",
    "│  └──────────────┘      │  └──────┘ └──────┘ │        │\n",
    "│                        │                     │        │\n",
    "│                        │  Lazy loading       │        │\n",
    "│                        │  LRU eviction       │        │\n",
    "│                        └─────────────────────┘        │\n",
    "│                                                         │\n",
    "│  Benefits:                                              │\n",
    "│  - Share compute across models                         │\n",
    "│  - Cost-effective for many models                      │\n",
    "│  - Dynamic loading based on demand                     │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-model endpoint configuration\n",
    "\n",
    "def create_multi_model_config():\n",
    "    \"\"\"Configure endpoint to host multiple models\"\"\"\n",
    "    config = {\n",
    "        'EndpointConfigName': 'multi-model-endpoint-config',\n",
    "        'ProductionVariants': [\n",
    "            {\n",
    "                'VariantName': 'AllModels',\n",
    "                'ModelName': 'multi-model-container',\n",
    "                'InstanceType': 'ml.m5.xlarge',\n",
    "                'InitialInstanceCount': 1,\n",
    "                # Key setting for multi-model endpoint\n",
    "                'ModelDataUrl': 's3://my-bucket/models/',\n",
    "                'Mode': 'MultiModel'\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    return config\n",
    "\n",
    "multi_model_config = create_multi_model_config()\n",
    "print(\"Multi-Model Endpoint Configuration:\")\n",
    "print(json.dumps(multi_model_config, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate multi-model endpoint invocation\n",
    "\n",
    "class MultiModelEndpoint:\n",
    "    \"\"\"Simulates multi-model endpoint with caching\"\"\"\n",
    "    \n",
    "    def __init__(self, cache_size=2):\n",
    "        self.cache = {}  # LRU cache for loaded models\n",
    "        self.cache_size = cache_size\n",
    "        self.load_count = {}\n",
    "    \n",
    "    def invoke(self, model_name, data):\n",
    "        \"\"\"Invoke specific model from multi-model endpoint\"\"\"\n",
    "        if model_name not in self.cache:\n",
    "            # Simulate loading model from S3\n",
    "            print(f\"Loading {model_name} from S3...\")\n",
    "            if len(self.cache) >= self.cache_size:\n",
    "                # Evict least recently used model\n",
    "                evicted = list(self.cache.keys())[0]\n",
    "                print(f\"Evicting {evicted} from cache\")\n",
    "                del self.cache[evicted]\n",
    "            \n",
    "            self.cache[model_name] = True\n",
    "            self.load_count[model_name] = self.load_count.get(model_name, 0) + 1\n",
    "        \n",
    "        # Make prediction\n",
    "        prediction = np.random.randn()\n",
    "        return {'model': model_name, 'prediction': prediction}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multi-model endpoint\n",
    "mme = MultiModelEndpoint(cache_size=2)\n",
    "\n",
    "# Invoke different models\n",
    "models = ['model-a', 'model-b', 'model-c', 'model-a']\n",
    "test_data = np.random.randn(10)\n",
    "\n",
    "for model in models:\n",
    "    result = mme.invoke(model, test_data)\n",
    "    print(f\"Prediction from {model}: {result['prediction']:.3f}\")\n",
    "\n",
    "print(f\"\\nModel load statistics: {mme.load_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. A/B Testing and Traffic Splitting\n",
    "\n",
    "Production variants allow testing multiple model versions with controlled traffic distribution.\n",
    "\n",
    "### A/B Testing Architecture\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│             A/B Testing Endpoint                         │\n",
    "├─────────────────────────────────────────────────────────┤\n",
    "│                                                         │\n",
    "│         Incoming Traffic (100%)                         │\n",
    "│                   │                                     │\n",
    "│                   ▼                                     │\n",
    "│         ┌─────────────────────┐                        │\n",
    "│         │  Load Balancer      │                        │\n",
    "│         └─────────────────────┘                        │\n",
    "│              │           │                              │\n",
    "│         70%  │           │  30%                         │\n",
    "│              ▼           ▼                              │\n",
    "│    ┌──────────────┐  ┌──────────────┐                 │\n",
    "│    │  Variant A   │  │  Variant B   │                 │\n",
    "│    │ (Champion)   │  │ (Challenger) │                 │\n",
    "│    │  Model v1.0  │  │  Model v2.0  │                 │\n",
    "│    └──────────────┘  └──────────────┘                 │\n",
    "│                                                         │\n",
    "│    Compare metrics:                                     │\n",
    "│    - Accuracy                                           │\n",
    "│    - Latency                                            │\n",
    "│    - Cost                                               │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A/B testing endpoint configuration\n",
    "\n",
    "def create_ab_test_config():\n",
    "    \"\"\"Configure endpoint with multiple model variants for A/B testing\"\"\"\n",
    "    config = {\n",
    "        'EndpointConfigName': 'ab-test-config-v1',\n",
    "        'ProductionVariants': [\n",
    "            {\n",
    "                'VariantName': 'VariantA-Champion',\n",
    "                'ModelName': 'model-v1-stable',\n",
    "                'InstanceType': 'ml.t2.medium',\n",
    "                'InitialInstanceCount': 2,\n",
    "                'InitialVariantWeight': 0.7  # 70% traffic\n",
    "            },\n",
    "            {\n",
    "                'VariantName': 'VariantB-Challenger',\n",
    "                'ModelName': 'model-v2-experimental',\n",
    "                'InstanceType': 'ml.t2.medium',\n",
    "                'InitialInstanceCount': 1,\n",
    "                'InitialVariantWeight': 0.3  # 30% traffic\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    return config\n",
    "\n",
    "ab_config = create_ab_test_config()\n",
    "print(\"A/B Testing Configuration:\")\n",
    "print(json.dumps(ab_config, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate A/B testing traffic distribution\n",
    "\n",
    "class ABTestEndpoint:\n",
    "    \"\"\"Simulates A/B testing with traffic splitting\"\"\"\n",
    "    \n",
    "    def __init__(self, variant_weights):\n",
    "        self.variant_weights = variant_weights\n",
    "        self.variant_metrics = {v: {'count': 0, 'latencies': [], 'predictions': []} \n",
    "                               for v in variant_weights.keys()}\n",
    "    \n",
    "    def invoke(self, data):\n",
    "        \"\"\"Route request to variant based on weights\"\"\"\n",
    "        # Randomly select variant based on weights\n",
    "        variants = list(self.variant_weights.keys())\n",
    "        weights = list(self.variant_weights.values())\n",
    "        selected_variant = np.random.choice(variants, p=weights)\n",
    "        \n",
    "        # Simulate prediction with different characteristics\n",
    "        if selected_variant == 'VariantA':\n",
    "            latency = np.random.gamma(2, 50)  # ms\n",
    "            prediction = np.random.randn() * 10 + 50\n",
    "        else:\n",
    "            latency = np.random.gamma(1.5, 40)  # Faster but experimental\n",
    "            prediction = np.random.randn() * 12 + 52\n",
    "        \n",
    "        # Track metrics\n",
    "        self.variant_metrics[selected_variant]['count'] += 1\n",
    "        self.variant_metrics[selected_variant]['latencies'].append(latency)\n",
    "        self.variant_metrics[selected_variant]['predictions'].append(prediction)\n",
    "        \n",
    "        return {'variant': selected_variant, 'prediction': prediction}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run A/B test simulation\n",
    "ab_endpoint = ABTestEndpoint({\n",
    "    'VariantA': 0.7,\n",
    "    'VariantB': 0.3\n",
    "})\n",
    "\n",
    "# Simulate 1000 requests\n",
    "num_requests = 1000\n",
    "for _ in range(num_requests):\n",
    "    test_data = np.random.randn(10)\n",
    "    ab_endpoint.invoke(test_data)\n",
    "\n",
    "# Analyze results\n",
    "print(\"A/B Test Results:\\n\")\n",
    "for variant, metrics in ab_endpoint.variant_metrics.items():\n",
    "    print(f\"{variant}:\")\n",
    "    print(f\"  Requests: {metrics['count']} ({metrics['count']/num_requests*100:.1f}%)\")\n",
    "    print(f\"  Avg Latency: {np.mean(metrics['latencies']):.2f}ms\")\n",
    "    print(f\"  Avg Prediction: {np.mean(metrics['predictions']):.2f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Monitoring and CloudWatch Integration\n",
    "\n",
    "Monitor endpoint performance and model quality in production.\n",
    "\n",
    "### Monitoring Architecture\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│           SageMaker Model Monitoring                     │\n",
    "├─────────────────────────────────────────────────────────┤\n",
    "│                                                         │\n",
    "│  Endpoint         CloudWatch         Alerts             │\n",
    "│  ┌────────┐      ┌──────────┐      ┌────────┐         │\n",
    "│  │Metrics │─────▶│ Dashboards│─────▶│  SNS   │         │\n",
    "│  │- Latency│     │ - Graphs  │      │ Email  │         │\n",
    "│  │- Errors │     │ - Logs    │      │ Lambda │         │\n",
    "│  │- Invocs │     └──────────┘      └────────┘         │\n",
    "│  └────────┘                                             │\n",
    "│                                                         │\n",
    "│  Model Monitor   Data Quality       Data Drift         │\n",
    "│  ┌───────────┐   ┌────────────┐    ┌────────────┐    │\n",
    "│  │Capture    │──▶│ Violations │───▶│  Alerts    │    │\n",
    "│  │Input/Output│   │ Detected   │    │  Triggered │    │\n",
    "│  └───────────┘   └────────────┘    └────────────┘    │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model monitoring configuration\n",
    "\n",
    "def create_monitoring_schedule():\n",
    "    \"\"\"Configure model monitoring for data quality and drift detection\"\"\"\n",
    "    schedule_config = {\n",
    "        'MonitoringScheduleName': 'model-quality-monitor',\n",
    "        'MonitoringScheduleConfig': {\n",
    "            'MonitoringJobDefinition': {\n",
    "                'MonitoringInputs': [\n",
    "                    {\n",
    "                        'EndpointInput': {\n",
    "                            'EndpointName': 'my-endpoint',\n",
    "                            'LocalPath': '/opt/ml/processing/input'\n",
    "                        }\n",
    "                    }\n",
    "                ],\n",
    "                'MonitoringOutputConfig': {\n",
    "                    'MonitoringOutputs': [\n",
    "                        {\n",
    "                            'S3Output': {\n",
    "                                'S3Uri': 's3://my-bucket/monitoring-reports/',\n",
    "                                'LocalPath': '/opt/ml/processing/output'\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "            'ScheduleConfig': {\n",
    "                'ScheduleExpression': 'cron(0 * * * ? *)'  # Hourly\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    return schedule_config\n",
    "\n",
    "monitor_config = create_monitoring_schedule()\n",
    "print(\"Model Monitoring Schedule:\")\n",
    "print(json.dumps(monitor_config, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate CloudWatch metrics collection\n",
    "\n",
    "class EndpointMonitor:\n",
    "    \"\"\"Simulates CloudWatch metrics for endpoint monitoring\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.metrics = []\n",
    "    \n",
    "    def record_invocation(self, latency, success=True):\n",
    "        \"\"\"Record metrics for endpoint invocation\"\"\"\n",
    "        metric = {\n",
    "            'timestamp': datetime.now(),\n",
    "            'latency': latency,\n",
    "            'success': success,\n",
    "            'invocations': 1,\n",
    "            'errors': 0 if success else 1\n",
    "        }\n",
    "        self.metrics.append(metric)\n",
    "    \n",
    "    def get_summary_stats(self):\n",
    "        \"\"\"Calculate summary statistics from metrics\"\"\"\n",
    "        latencies = [m['latency'] for m in self.metrics]\n",
    "        total_invocations = len(self.metrics)\n",
    "        total_errors = sum(m['errors'] for m in self.metrics)\n",
    "        \n",
    "        return {\n",
    "            'total_invocations': total_invocations,\n",
    "            'error_rate': total_errors / total_invocations,\n",
    "            'avg_latency': np.mean(latencies),\n",
    "            'p50_latency': np.percentile(latencies, 50),\n",
    "            'p95_latency': np.percentile(latencies, 95),\n",
    "            'p99_latency': np.percentile(latencies, 99)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate endpoint monitoring\n",
    "monitor = EndpointMonitor()\n",
    "\n",
    "# Simulate 100 requests with varying latencies\n",
    "for _ in range(100):\n",
    "    # Most requests are fast\n",
    "    if np.random.rand() < 0.95:\n",
    "        latency = np.random.gamma(2, 30)  # Fast requests\n",
    "        success = True\n",
    "    else:\n",
    "        # Occasional slow or failed request\n",
    "        latency = np.random.gamma(5, 100)  # Slow request\n",
    "        success = np.random.rand() > 0.1\n",
    "    \n",
    "    monitor.record_invocation(latency, success)\n",
    "\n",
    "# Display monitoring summary\n",
    "stats = monitor.get_summary_stats()\n",
    "print(\"Endpoint Monitoring Summary:\\n\")\n",
    "print(f\"Total Invocations: {stats['total_invocations']}\")\n",
    "print(f\"Error Rate: {stats['error_rate']*100:.2f}%\")\n",
    "print(f\"\\nLatency Statistics (ms):\")\n",
    "print(f\"  Average: {stats['avg_latency']:.2f}\")\n",
    "print(f\"  P50: {stats['p50_latency']:.2f}\")\n",
    "print(f\"  P95: {stats['p95_latency']:.2f}\")\n",
    "print(f\"  P99: {stats['p99_latency']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cost Optimization Strategies\n",
    "\n",
    "SageMaker endpoints can be expensive. Here are strategies to optimize costs:\n",
    "\n",
    "### Cost Optimization Techniques\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│         SageMaker Cost Optimization                      │\n",
    "├─────────────────────────────────────────────────────────┤\n",
    "│                                                         │\n",
    "│  1. Right-sizing Instances                              │\n",
    "│     - Start small (ml.t2.medium)                        │\n",
    "│     - Monitor CPU/Memory usage                          │\n",
    "│     - Upgrade only when needed                          │\n",
    "│                                                         │\n",
    "│  2. Auto-scaling                                        │\n",
    "│     - Scale down during low traffic                     │\n",
    "│     - Set appropriate min/max instances                 │\n",
    "│     - Use target tracking policies                      │\n",
    "│                                                         │\n",
    "│  3. Deployment Patterns                                 │\n",
    "│     - Multi-model endpoints (many models)               │\n",
    "│     - Serverless inference (intermittent)               │\n",
    "│     - Batch transform (no real-time needed)             │\n",
    "│                                                         │\n",
    "│  4. Lifecycle Management                                │\n",
    "│     - Delete unused endpoints                           │\n",
    "│     - Stop non-production endpoints                     │\n",
    "│     - Use scheduled scaling                             │\n",
    "│                                                         │\n",
    "│  5. Data Capture                                        │\n",
    "│     - Sample requests (not 100%)                        │\n",
    "│     - Compress captured data                            │\n",
    "│     - Set S3 lifecycle policies                         │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost estimation calculator\n",
    "\n",
    "def estimate_endpoint_cost(instance_type, instance_count, hours_per_month):\n",
    "    \"\"\"Estimate monthly cost for SageMaker endpoint\"\"\"\n",
    "    \n",
    "    # Simplified pricing (actual prices vary by region)\n",
    "    hourly_rates = {\n",
    "        'ml.t2.medium': 0.065,\n",
    "        'ml.m5.large': 0.134,\n",
    "        'ml.m5.xlarge': 0.269,\n",
    "        'ml.c5.xlarge': 0.238,\n",
    "    }\n",
    "    \n",
    "    hourly_rate = hourly_rates.get(instance_type, 0.10)\n",
    "    monthly_cost = hourly_rate * instance_count * hours_per_month\n",
    "    \n",
    "    return {\n",
    "        'instance_type': instance_type,\n",
    "        'instance_count': instance_count,\n",
    "        'hours_per_month': hours_per_month,\n",
    "        'hourly_rate': hourly_rate,\n",
    "        'monthly_cost': monthly_cost\n",
    "    }\n",
    "\n",
    "# Compare different deployment scenarios\n",
    "scenarios = [\n",
    "    ('Always-on (24/7)', 'ml.t2.medium', 1, 730),\n",
    "    ('Business hours only', 'ml.t2.medium', 1, 200),\n",
    "    ('Auto-scaled (avg)', 'ml.t2.medium', 2, 500),\n",
    "    ('Multi-model endpoint', 'ml.m5.large', 1, 730),\n",
    "]\n",
    "\n",
    "print(\"Cost Comparison for Different Deployment Strategies:\\n\")\n",
    "for name, instance, count, hours in scenarios:\n",
    "    cost = estimate_endpoint_cost(instance, count, hours)\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Configuration: {count}x {instance}\")\n",
    "    print(f\"  Hours/month: {hours}\")\n",
    "    print(f\"  Estimated cost: ${cost['monthly_cost']:.2f}/month\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serverless inference configuration (cost-effective for low traffic)\n",
    "\n",
    "def create_serverless_config():\n",
    "    \"\"\"Configure serverless inference endpoint (scales to zero)\"\"\"\n",
    "    config = {\n",
    "        'EndpointConfigName': 'serverless-endpoint-config',\n",
    "        'ProductionVariants': [\n",
    "            {\n",
    "                'VariantName': 'ServerlessVariant',\n",
    "                'ModelName': 'my-model',\n",
    "                'ServerlessConfig': {\n",
    "                    'MemorySizeInMB': 2048,  # 2GB\n",
    "                    'MaxConcurrency': 10,  # Max concurrent invocations\n",
    "                    'ProvisionedConcurrency': 0  # Scales to zero\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    print(\"Serverless Inference Benefits:\")\n",
    "    print(\"- No charge when not in use\")\n",
    "    print(\"- Automatic scaling\")\n",
    "    print(\"- Pay per request (after free tier)\")\n",
    "    print(\"- Ideal for intermittent traffic\\n\")\n",
    "    \n",
    "    return config\n",
    "\n",
    "serverless_config = create_serverless_config()\n",
    "print(json.dumps(serverless_config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cleanup and Best Practices\n",
    "\n",
    "**CRITICAL**: Always delete endpoints when not in use to avoid charges!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Endpoint cleanup helper\n",
    "\n",
    "def cleanup_endpoint(endpoint_name, delete_config=True, delete_model=True):\n",
    "    \"\"\"\n",
    "    Clean up SageMaker resources to avoid charges\n",
    "    \n",
    "    Steps:\n",
    "    1. Delete endpoint (stops billing immediately)\n",
    "    2. Delete endpoint configuration\n",
    "    3. Delete model\n",
    "    \"\"\"\n",
    "    cleanup_plan = {\n",
    "        'endpoint_name': endpoint_name,\n",
    "        'steps': [\n",
    "            f\"1. Delete endpoint: {endpoint_name}\",\n",
    "            f\"2. Delete endpoint config: {endpoint_name}-config\" if delete_config else \"Skip\",\n",
    "            f\"3. Delete model: {endpoint_name}-model\" if delete_model else \"Skip\"\n",
    "        ],\n",
    "        'warning': 'BILLING STOPS AFTER STEP 1!'\n",
    "    }\n",
    "    return cleanup_plan\n",
    "\n",
    "# Example cleanup\n",
    "cleanup = cleanup_endpoint('my-production-endpoint')\n",
    "print(\"Cleanup Plan:\")\n",
    "for step in cleanup['steps']:\n",
    "    if step != \"Skip\":\n",
    "        print(f\"  {step}\")\n",
    "print(f\"\\n⚠️  {cleanup['warning']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Practices Summary\n",
    "\n",
    "1. **Development**: Use smaller instances (ml.t2.medium) for testing\n",
    "2. **Production**: Right-size based on load testing\n",
    "3. **Monitoring**: Set up CloudWatch alarms for errors and latency\n",
    "4. **Cost**: Use serverless or multi-model endpoints when possible\n",
    "5. **Testing**: Use A/B testing before full rollout\n",
    "6. **Cleanup**: Always delete unused endpoints\n",
    "7. **Security**: Use VPC endpoints and encryption\n",
    "8. **Logging**: Enable data capture for model monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Design Deployment Strategy\n",
    "\n",
    "You have three models to deploy:\n",
    "- Model A: Used by 10,000 users daily (9am-5pm)\n",
    "- Model B: Used by 100 users sporadically\n",
    "- Model C: Batch processing 1M records nightly\n",
    "\n",
    "Design the optimal deployment strategy for each model. Consider:\n",
    "- Endpoint type (real-time, serverless, batch)\n",
    "- Instance type and count\n",
    "- Auto-scaling configuration\n",
    "- Estimated monthly cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "def design_deployment_strategy():\n",
    "    \"\"\"\n",
    "    Design deployment strategy for three different use cases\n",
    "    \n",
    "    Consider:\n",
    "    - Traffic patterns\n",
    "    - Latency requirements\n",
    "    - Cost optimization\n",
    "    - Scalability needs\n",
    "    \"\"\"\n",
    "    strategies = {\n",
    "        'model_a': {\n",
    "            # TODO: Fill in deployment strategy\n",
    "        },\n",
    "        'model_b': {\n",
    "            # TODO: Fill in deployment strategy\n",
    "        },\n",
    "        'model_c': {\n",
    "            # TODO: Fill in deployment strategy\n",
    "        }\n",
    "    }\n",
    "    return strategies\n",
    "\n",
    "# Test your design\n",
    "# strategies = design_deployment_strategy()\n",
    "# print(json.dumps(strategies, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: A/B Test Analysis\n",
    "\n",
    "You're running an A/B test with:\n",
    "- Variant A: Current model (70% traffic)\n",
    "- Variant B: New model (30% traffic)\n",
    "\n",
    "After 1 week:\n",
    "- Variant A: 95% accuracy, 80ms average latency\n",
    "- Variant B: 97% accuracy, 120ms average latency\n",
    "\n",
    "Should you switch to Variant B? Write code to analyze the trade-offs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "def analyze_ab_test(variant_a_metrics, variant_b_metrics, sla_latency=100):\n",
    "    \"\"\"\n",
    "    Analyze A/B test results and make deployment recommendation\n",
    "    \n",
    "    Args:\n",
    "        variant_a_metrics: dict with accuracy, latency\n",
    "        variant_b_metrics: dict with accuracy, latency\n",
    "        sla_latency: maximum acceptable latency in ms\n",
    "    \n",
    "    Returns:\n",
    "        dict with recommendation and reasoning\n",
    "    \"\"\"\n",
    "    # TODO: Implement analysis logic\n",
    "    pass\n",
    "\n",
    "# Test your analysis\n",
    "# variant_a = {'accuracy': 0.95, 'latency': 80}\n",
    "# variant_b = {'accuracy': 0.97, 'latency': 120}\n",
    "# recommendation = analyze_ab_test(variant_a, variant_b)\n",
    "# print(recommendation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Cost Optimization Calculator\n",
    "\n",
    "Create a function that compares costs between:\n",
    "1. Single model endpoint (always-on)\n",
    "2. Multi-model endpoint (10 models)\n",
    "3. Serverless inference\n",
    "\n",
    "Assume:\n",
    "- Each model receives 1000 requests/day\n",
    "- Average processing time: 50ms\n",
    "- SLA: 99.9% availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "def compare_deployment_costs(num_models, requests_per_day, avg_process_time_ms):\n",
    "    \"\"\"\n",
    "    Compare monthly costs for different deployment patterns\n",
    "    \n",
    "    Calculate costs for:\n",
    "    - Individual endpoints (num_models separate endpoints)\n",
    "    - Multi-model endpoint (1 endpoint hosting all models)\n",
    "    - Serverless inference\n",
    "    \n",
    "    Returns:\n",
    "        dict with cost breakdown for each option\n",
    "    \"\"\"\n",
    "    # TODO: Implement cost comparison\n",
    "    pass\n",
    "\n",
    "# Test your calculator\n",
    "# costs = compare_deployment_costs(\n",
    "#     num_models=10,\n",
    "#     requests_per_day=1000,\n",
    "#     avg_process_time_ms=50\n",
    "# )\n",
    "# print(json.dumps(costs, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Monitoring Alert System\n",
    "\n",
    "Design a monitoring system that:\n",
    "1. Tracks endpoint latency, errors, and invocations\n",
    "2. Detects anomalies (latency > 2x normal, error rate > 1%)\n",
    "3. Generates alerts with severity levels\n",
    "4. Suggests remediation actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "class EndpointAlertSystem:\n",
    "    \"\"\"Monitor endpoint metrics and generate alerts\"\"\"\n",
    "    \n",
    "    def __init__(self, baseline_latency, baseline_error_rate):\n",
    "        self.baseline_latency = baseline_latency\n",
    "        self.baseline_error_rate = baseline_error_rate\n",
    "        self.alerts = []\n",
    "    \n",
    "    def check_metrics(self, current_metrics):\n",
    "        \"\"\"\n",
    "        Check current metrics against baselines\n",
    "        Generate alerts for anomalies\n",
    "        \n",
    "        Args:\n",
    "            current_metrics: dict with latency, error_rate, invocations\n",
    "        \n",
    "        Returns:\n",
    "            list of alerts with severity and remediation\n",
    "        \"\"\"\n",
    "        # TODO: Implement anomaly detection\n",
    "        pass\n",
    "\n",
    "# Test your alert system\n",
    "# alert_system = EndpointAlertSystem(baseline_latency=50, baseline_error_rate=0.001)\n",
    "# current = {'latency': 150, 'error_rate': 0.05, 'invocations': 1000}\n",
    "# alerts = alert_system.check_metrics(current)\n",
    "# for alert in alerts:\n",
    "#     print(f\"{alert['severity']}: {alert['message']}\")\n",
    "#     print(f\"Remediation: {alert['remediation']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. **Real-time Endpoints**: Deploy models with auto-scaling for production traffic\n",
    "2. **Batch Transform**: Cost-effective offline inference for large datasets\n",
    "3. **Multi-Model Endpoints**: Host multiple models on single endpoint to reduce costs\n",
    "4. **A/B Testing**: Safely test new model versions with traffic splitting\n",
    "5. **Monitoring**: Track endpoint performance with CloudWatch metrics\n",
    "6. **Cost Optimization**: Strategies to minimize SageMaker deployment costs\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- Choose deployment pattern based on traffic patterns and latency requirements\n",
    "- Always monitor endpoint performance and set up alerts\n",
    "- Use A/B testing to validate new models before full rollout\n",
    "- Implement auto-scaling to handle variable traffic\n",
    "- Delete unused endpoints to avoid unnecessary charges\n",
    "- Consider serverless or multi-model endpoints for cost savings\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "- [Module 03: Azure ML Studio Introduction](03_azure_ml_studio_introduction.ipynb)\n",
    "- Practice deploying models with different instance types\n",
    "- Experiment with CloudWatch dashboards and alarms\n",
    "- Explore SageMaker Pipelines for automated deployments\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [SageMaker Deployment Best Practices](https://docs.aws.amazon.com/sagemaker/latest/dg/best-practices.html)\n",
    "- [SageMaker Pricing Calculator](https://aws.amazon.com/sagemaker/pricing/)\n",
    "- [Auto-scaling Guide](https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling.html)\n",
    "- [Model Monitoring](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
