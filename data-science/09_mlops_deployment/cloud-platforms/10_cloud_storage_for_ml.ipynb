{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 10: Cloud Storage for ML\n",
    "\n",
    "**Difficulty**: ‚≠ê‚≠ê\n",
    "**Estimated Time**: 70 minutes\n",
    "**Prerequisites**: \n",
    "- [Module 00: Introduction to Cloud ML Services](00_introduction_to_cloud_ml_services.ipynb)\n",
    "- [Module 08: Cost Optimization Strategies](08_cost_optimization_strategies.ipynb)\n",
    "- Basic understanding of object storage\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Understand differences between AWS S3, Azure Blob, and Google Cloud Storage\n",
    "2. Implement efficient data lake architectures for ML\n",
    "3. Use versioning and encryption for data security\n",
    "4. Configure access control with IAM and SAS tokens\n",
    "5. Optimize data transfer and costs\n",
    "6. Choose appropriate storage tiers for ML workloads\n",
    "7. Implement lifecycle policies for automated cost optimization\n",
    "8. Design scalable storage strategies for large-scale ML\n",
    "\n",
    "## Why Cloud Storage Matters for ML\n",
    "\n",
    "ML projects involve massive amounts of data:\n",
    "- **Raw data**: Original datasets (TBs to PBs)\n",
    "- **Processed data**: Cleaned, transformed features\n",
    "- **Model artifacts**: Trained models, checkpoints\n",
    "- **Experiment results**: Logs, metrics, visualizations\n",
    "- **Production data**: Inference requests/responses\n",
    "\n",
    "Cloud storage provides:\n",
    "- ‚úÖ **Scalability**: Store petabytes without infrastructure\n",
    "- ‚úÖ **Durability**: 99.999999999% (11 nines) data durability\n",
    "- ‚úÖ **Accessibility**: Access from anywhere, any compute\n",
    "- ‚úÖ **Cost-effective**: Pay only for what you store\n",
    "- ‚úÖ **Integration**: Works seamlessly with ML services\n",
    "\n",
    "### Storage Cost Distribution (Typical ML Project)\n",
    "- 60-70%: Raw and processed datasets\n",
    "- 15-20%: Model artifacts and checkpoints\n",
    "- 10-15%: Logs and experiment tracking\n",
    "- 5-10%: Data transfer costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Standard library imports\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "\n",
    "# Data science libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# File handling\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "# Cloud storage simulation\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Optional\n",
    "from enum import Enum\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"Set2\")\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "print(f\"Notebook executed on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Cloud Storage Services Comparison\n",
    "\n",
    "### 1.1: Service Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Comprehensive comparison of cloud storage services\n",
    "storage_comparison = pd.DataFrame([\n",
    "    {\n",
    "        'Feature': 'Service Name',\n",
    "        'AWS': 'S3 (Simple Storage Service)',\n",
    "        'Azure': 'Blob Storage',\n",
    "        'GCP': 'Cloud Storage',\n",
    "        'Notes': 'All offer object storage'\n",
    "    },\n",
    "    {\n",
    "        'Feature': 'Storage Unit',\n",
    "        'AWS': 'Bucket',\n",
    "        'Azure': 'Container',\n",
    "        'GCP': 'Bucket',\n",
    "        'Notes': 'Top-level organization'\n",
    "    },\n",
    "    {\n",
    "        'Feature': 'Object Naming',\n",
    "        'AWS': 'Key (with prefixes)',\n",
    "        'Azure': 'Blob name',\n",
    "        'GCP': 'Object name',\n",
    "        'Notes': 'Flat namespace, not true folders'\n",
    "    },\n",
    "    {\n",
    "        'Feature': 'Max Object Size',\n",
    "        'AWS': '5TB',\n",
    "        'Azure': '190.7TB (block blob)',\n",
    "        'GCP': '5TB',\n",
    "        'Notes': 'Use multipart upload for large files'\n",
    "    },\n",
    "    {\n",
    "        'Feature': 'Durability',\n",
    "        'AWS': '99.999999999% (11 nines)',\n",
    "        'Azure': '99.999999999% (11 nines)',\n",
    "        'GCP': '99.999999999% (11 nines)',\n",
    "        'Notes': 'Standard across all providers'\n",
    "    },\n",
    "    {\n",
    "        'Feature': 'Availability',\n",
    "        'AWS': '99.99% (Standard)',\n",
    "        'Azure': '99.9% (LRS), 99.99% (GRS)',\n",
    "        'GCP': '99.95% (Standard)',\n",
    "        'Notes': 'Varies by storage class'\n",
    "    },\n",
    "    {\n",
    "        'Feature': 'Storage Classes',\n",
    "        'AWS': '6 tiers',\n",
    "        'Azure': '4 tiers',\n",
    "        'GCP': '4 classes',\n",
    "        'Notes': 'Hot to archive tiers'\n",
    "    },\n",
    "    {\n",
    "        'Feature': 'Standard Pricing',\n",
    "        'AWS': '$0.023/GB/month',\n",
    "        'Azure': '$0.018/GB/month',\n",
    "        'GCP': '$0.020/GB/month',\n",
    "        'Notes': 'First 5GB free (varies by tier)'\n",
    "    },\n",
    "    {\n",
    "        'Feature': 'Data Transfer IN',\n",
    "        'AWS': 'Free',\n",
    "        'Azure': 'Free',\n",
    "        'GCP': 'Free',\n",
    "        'Notes': 'Upload is always free'\n",
    "    },\n",
    "    {\n",
    "        'Feature': 'Data Transfer OUT',\n",
    "        'AWS': '$0.09/GB (>10TB)',\n",
    "        'Azure': '$0.087/GB (>10TB)',\n",
    "        'GCP': '$0.12/GB (>10TB)',\n",
    "        'Notes': 'Download costs (internet egress)'\n",
    "    },\n",
    "    {\n",
    "        'Feature': 'Free Tier',\n",
    "        'AWS': '5GB Standard, 20k GET, 2k PUT',\n",
    "        'Azure': '5GB LRS, 50k ops',\n",
    "        'GCP': '5GB Standard, 5k Class A ops',\n",
    "        'Notes': '12 months for AWS/Azure, always for GCP'\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"Cloud Storage Services Comparison\\n\")\n",
    "print(storage_comparison.to_string(index=False))\n",
    "print(\"\\nüí° Key Insights:\")\n",
    "print(\"   - All three are very similar in capabilities\")\n",
    "print(\"   - Azure Blob is slightly cheaper for standard storage\")\n",
    "print(\"   - GCP has highest egress costs\")\n",
    "print(\"   - S3 has most mature ecosystem and integrations\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2: Storage Tiers Deep Dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class StorageTier:\n",
    "    \"\"\"Represents a storage tier with pricing and characteristics\"\"\"\n",
    "    def __init__(self, name, storage_cost, retrieval_cost, \n",
    "                 retrieval_time, min_days=0, best_for=\"\"):\n",
    "        self.name = name\n",
    "        self.storage_cost = storage_cost  # $/GB/month\n",
    "        self.retrieval_cost = retrieval_cost  # $/GB\n",
    "        self.retrieval_time = retrieval_time\n",
    "        self.min_days = min_days  # Minimum storage duration\n",
    "        self.best_for = best_for\n",
    "\n",
    "# AWS S3 Storage Classes\n",
    "aws_s3_tiers = [\n",
    "    StorageTier(\"S3 Standard\", 0.023, 0, \"Milliseconds\", 0, \"Frequently accessed\"),\n",
    "    StorageTier(\"S3 Intelligent-Tiering\", 0.023, 0, \"Milliseconds\", 0, \"Unknown patterns\"),\n",
    "    StorageTier(\"S3 Standard-IA\", 0.0125, 0.01, \"Milliseconds\", 30, \"Monthly access\"),\n",
    "    StorageTier(\"S3 One Zone-IA\", 0.01, 0.01, \"Milliseconds\", 30, \"Reproducible data\"),\n",
    "    StorageTier(\"S3 Glacier Instant\", 0.004, 0.03, \"Milliseconds\", 90, \"Quarterly access\"),\n",
    "    StorageTier(\"S3 Glacier Flexible\", 0.0036, 0.02, \"Minutes-Hours\", 90, \"Yearly access\"),\n",
    "    StorageTier(\"S3 Glacier Deep\", 0.00099, 0.02, \"12-48 hours\", 180, \"Compliance archives\"),\n",
    "]\n",
    "\n",
    "# Azure Blob Storage Tiers\n",
    "azure_blob_tiers = [\n",
    "    StorageTier(\"Hot (LRS)\", 0.018, 0, \"Milliseconds\", 0, \"Active data\"),\n",
    "    StorageTier(\"Cool (LRS)\", 0.01, 0.01, \"Milliseconds\", 30, \"Monthly access\"),\n",
    "    StorageTier(\"Cold (LRS)\", 0.0045, 0.03, \"Milliseconds\", 90, \"Quarterly access\"),\n",
    "    StorageTier(\"Archive (LRS)\", 0.00099, 0.022, \"Hours\", 180, \"Compliance archives\"),\n",
    "]\n",
    "\n",
    "# GCP Cloud Storage Classes\n",
    "gcp_storage_classes = [\n",
    "    StorageTier(\"Standard\", 0.020, 0, \"Milliseconds\", 0, \"Frequently accessed\"),\n",
    "    StorageTier(\"Nearline\", 0.010, 0.01, \"Milliseconds\", 30, \"Monthly access\"),\n",
    "    StorageTier(\"Coldline\", 0.004, 0.02, \"Milliseconds\", 90, \"Quarterly access\"),\n",
    "    StorageTier(\"Archive\", 0.0012, 0.05, \"Milliseconds\", 365, \"Yearly access\"),\n",
    "]\n",
    "\n",
    "def compare_storage_tiers(data_gb=100, accesses_per_month=10):\n",
    "    \"\"\"\n",
    "    Compare total costs across different storage tiers\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for tier in aws_s3_tiers + azure_blob_tiers + gcp_storage_classes:\n",
    "        storage_cost = data_gb * tier.storage_cost\n",
    "        retrieval_cost = data_gb * accesses_per_month * tier.retrieval_cost\n",
    "        total_cost = storage_cost + retrieval_cost\n",
    "        \n",
    "        provider = 'AWS' if 'S3' in tier.name else ('Azure' if 'LRS' in tier.name else 'GCP')\n",
    "        \n",
    "        results.append({\n",
    "            'Provider': provider,\n",
    "            'Tier': tier.name,\n",
    "            'Storage ($/mo)': storage_cost,\n",
    "            'Retrieval ($/mo)': retrieval_cost,\n",
    "            'Total ($/mo)': total_cost,\n",
    "            'Retrieval Time': tier.retrieval_time,\n",
    "            'Best For': tier.best_for\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Compare costs for 100GB with 10 accesses/month\n",
    "cost_comparison = compare_storage_tiers(100, 10)\n",
    "\n",
    "print(f\"Cost Comparison: 100GB Storage, 10 Full Retrievals/Month\\n\")\n",
    "print(cost_comparison.to_string(index=False))\n",
    "\n",
    "# Find cheapest option\n",
    "cheapest = cost_comparison.loc[cost_comparison['Total ($/mo)'].idxmin()]\n",
    "print(f\"\\nüí∞ Cheapest Option: {cheapest['Provider']} {cheapest['Tier']}\")\n",
    "print(f\"   Total Cost: ${cheapest['Total ($/mo)']:.2f}/month\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Data Lake Architecture for ML\n",
    "\n",
    "A data lake is a centralized repository that stores all structured and unstructured data at any scale.\n",
    "\n",
    "### 2.1: Recommended Folder Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Recommended data lake structure for ML projects\n",
    "data_lake_structure = {\n",
    "    'raw/': {\n",
    "        'description': 'Original, immutable data',\n",
    "        'storage_class': 'Standard ‚Üí Intelligent-Tiering',\n",
    "        'retention': 'Permanent (or 1-2 years)',\n",
    "        'example': 'raw/2024/11/19/data_source_001.parquet',\n",
    "        'characteristics': [\n",
    "            'Never modify after upload',\n",
    "            'Partition by date for efficient querying',\n",
    "            'Use lifecycle policies to transition to cheaper tiers'\n",
    "        ]\n",
    "    },\n",
    "    'processed/': {\n",
    "        'description': 'Cleaned, transformed data',\n",
    "        'storage_class': 'Standard',\n",
    "        'retention': '6-12 months',\n",
    "        'example': 'processed/customer_features_v2.parquet',\n",
    "        'characteristics': [\n",
    "            'Result of ETL pipelines',\n",
    "            'Frequently accessed for training',\n",
    "            'Version controlled (v1, v2, etc.)'\n",
    "        ]\n",
    "    },\n",
    "    'features/': {\n",
    "        'description': 'Feature store data',\n",
    "        'storage_class': 'Standard',\n",
    "        'retention': '3-6 months active + archive',\n",
    "        'example': 'features/user_behavior/2024_11.parquet',\n",
    "        'characteristics': [\n",
    "            'Engineered features ready for ML',\n",
    "            'Organized by feature group',\n",
    "            'Includes metadata and lineage'\n",
    "        ]\n",
    "    },\n",
    "    'models/': {\n",
    "        'description': 'Trained model artifacts',\n",
    "        'storage_class': 'Standard for active, IA for old versions',\n",
    "        'retention': 'Current + last 5 versions',\n",
    "        'example': 'models/customer_churn/v1.2.3/model.pkl',\n",
    "        'characteristics': [\n",
    "            'Semantic versioning',\n",
    "            'Include metadata.json with metrics',\n",
    "            'Archive old versions to Glacier'\n",
    "        ]\n",
    "    },\n",
    "    'experiments/': {\n",
    "        'description': 'Experiment tracking data',\n",
    "        'storage_class': 'Standard ‚Üí IA after 30 days',\n",
    "        'retention': '3 months active + archive',\n",
    "        'example': 'experiments/exp_20241119_001/metrics.json',\n",
    "        'characteristics': [\n",
    "            'Hyperparameters, metrics, plots',\n",
    "            'Organized by experiment ID',\n",
    "            'Use MLflow or similar for tracking'\n",
    "        ]\n",
    "    },\n",
    "    'predictions/': {\n",
    "        'description': 'Batch prediction outputs',\n",
    "        'storage_class': 'Standard ‚Üí IA after 7 days',\n",
    "        'retention': '30-90 days',\n",
    "        'example': 'predictions/2024/11/19/batch_001.parquet',\n",
    "        'characteristics': [\n",
    "            'Input features + predictions',\n",
    "            'For debugging and monitoring',\n",
    "            'Lifecycle policy to auto-delete'\n",
    "        ]\n",
    "    },\n",
    "    'logs/': {\n",
    "        'description': 'Application and training logs',\n",
    "        'storage_class': 'Standard ‚Üí IA after 7 days',\n",
    "        'retention': '30 days',\n",
    "        'example': 'logs/training/2024_11_19.log',\n",
    "        'characteristics': [\n",
    "            'Training logs, error logs',\n",
    "            'Auto-delete after 30 days',\n",
    "            'Consider CloudWatch/Stackdriver instead'\n",
    "        ]\n",
    "    },\n",
    "    'temp/': {\n",
    "        'description': 'Temporary intermediate files',\n",
    "        'storage_class': 'Standard',\n",
    "        'retention': '7 days (auto-delete)',\n",
    "        'example': 'temp/job_12345/intermediate.parquet',\n",
    "        'characteristics': [\n",
    "            'Short-lived processing artifacts',\n",
    "            'Aggressive lifecycle policy',\n",
    "            'Clean up immediately after job'\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Recommended Data Lake Structure for ML\\n\")\n",
    "for folder, details in data_lake_structure.items():\n",
    "    print(f\"üìÅ {folder}\")\n",
    "    print(f\"   {details['description']}\")\n",
    "    print(f\"   Storage: {details['storage_class']}\")\n",
    "    print(f\"   Retention: {details['retention']}\")\n",
    "    print(f\"   Example: {details['example']}\")\n",
    "    print()\n",
    "\n",
    "print(\"üéØ Best Practices:\")\n",
    "print(\"   - Partition by date for time-series data\")\n",
    "print(\"   - Use meaningful prefixes (folders) for organization\")\n",
    "print(\"   - Implement lifecycle policies for automated tiering\")\n",
    "print(\"   - Version all datasets and models\")\n",
    "print(\"   - Never modify raw data (immutable)\")\n",
    "print(\"   - Use metadata files (JSON/YAML) alongside data\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2: Data Lake Cost Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def simulate_data_lake_costs(months=12):\n",
    "    \"\"\"\n",
    "    Simulate data lake growth and costs over time\n",
    "    \"\"\"\n",
    "    # Initial data sizes (GB)\n",
    "    data = {\n",
    "        'raw': 100,\n",
    "        'processed': 50,\n",
    "        'features': 20,\n",
    "        'models': 2,\n",
    "        'experiments': 5,\n",
    "        'predictions': 10,\n",
    "        'logs': 5,\n",
    "        'temp': 3\n",
    "    }\n",
    "    \n",
    "    # Monthly growth rates\n",
    "    growth_rates = {\n",
    "        'raw': 1.10,  # 10% growth/month\n",
    "        'processed': 1.08,\n",
    "        'features': 1.05,\n",
    "        'models': 1.15,  # Accumulates more models\n",
    "        'experiments': 1.20,  # Many experiments\n",
    "        'predictions': 1.05,\n",
    "        'logs': 1.0,  # Deleted monthly\n",
    "        'temp': 1.0  # Deleted weekly\n",
    "    }\n",
    "    \n",
    "    # Storage costs ($/GB/month)\n",
    "    storage_costs = {\n",
    "        'raw': 0.015,  # Intelligent-Tiering average\n",
    "        'processed': 0.023,  # Standard\n",
    "        'features': 0.023,  # Standard\n",
    "        'models': 0.0125,  # Standard-IA for old versions\n",
    "        'experiments': 0.015,  # Mix of Standard and IA\n",
    "        'predictions': 0.0125,  # Standard-IA\n",
    "        'logs': 0.023,  # Standard\n",
    "        'temp': 0.023  # Standard\n",
    "    }\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for month in range(1, months + 1):\n",
    "        month_cost = 0\n",
    "        month_data = {}\n",
    "        \n",
    "        for category in data.keys():\n",
    "            # Apply growth\n",
    "            data[category] *= growth_rates[category]\n",
    "            \n",
    "            # Calculate cost\n",
    "            cost = data[category] * storage_costs[category]\n",
    "            month_cost += cost\n",
    "            month_data[category] = data[category]\n",
    "        \n",
    "        total_storage = sum(data.values())\n",
    "        \n",
    "        results.append({\n",
    "            'Month': month,\n",
    "            'Total Storage (GB)': total_storage,\n",
    "            'Monthly Cost ($)': month_cost,\n",
    "            **{f'{k} (GB)': v for k, v in month_data.items()}\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Simulate 12 months\n",
    "cost_projection = simulate_data_lake_costs(12)\n",
    "\n",
    "print(\"Data Lake Cost Projection (12 months)\\n\")\n",
    "print(cost_projection[['Month', 'Total Storage (GB)', 'Monthly Cost ($)']].to_string(index=False))\n",
    "\n",
    "# Summary statistics\n",
    "total_cost = cost_projection['Monthly Cost ($)'].sum()\n",
    "avg_monthly_cost = cost_projection['Monthly Cost ($)'].mean()\n",
    "final_storage = cost_projection.iloc[-1]['Total Storage (GB)']\n",
    "\n",
    "print(f\"\\nüìä Summary:\")\n",
    "print(f\"   Total 12-month cost: ${total_cost:.2f}\")\n",
    "print(f\"   Average monthly cost: ${avg_monthly_cost:.2f}\")\n",
    "print(f\"   Final storage size: {final_storage:.0f} GB\")\n",
    "print(f\"\\nüí° Cost Optimization Opportunities:\")\n",
    "print(f\"   - Implement lifecycle policies: Save ~30%\")\n",
    "print(f\"   - Delete old experiments: Save ~15%\")\n",
    "print(f\"   - Compress data: Save ~20-40%\")\n",
    "print(f\"   - Archive old models: Save ~10%\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Versioning and Encryption\n",
    "\n",
    "### 3.1: S3 Versioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# S3 Versioning example (simulated)\n",
    "class S3ObjectVersion:\n",
    "    \"\"\"Simulates S3 object versioning\"\"\"\n",
    "    def __init__(self, key, version_id, size_bytes, last_modified):\n",
    "        self.key = key\n",
    "        self.version_id = version_id\n",
    "        self.size_bytes = size_bytes\n",
    "        self.last_modified = last_modified\n",
    "        self.is_latest = False\n",
    "        self.is_delete_marker = False\n",
    "\n",
    "# Simulate versioning for a model file\n",
    "model_versions = [\n",
    "    S3ObjectVersion('models/customer_churn/model.pkl', 'v1', 10_000_000, \n",
    "                    datetime.now() - timedelta(days=30)),\n",
    "    S3ObjectVersion('models/customer_churn/model.pkl', 'v2', 12_000_000, \n",
    "                    datetime.now() - timedelta(days=15)),\n",
    "    S3ObjectVersion('models/customer_churn/model.pkl', 'v3', 11_500_000, \n",
    "                    datetime.now() - timedelta(days=7)),\n",
    "    S3ObjectVersion('models/customer_churn/model.pkl', 'v4', 11_800_000, \n",
    "                    datetime.now()),\n",
    "]\n",
    "model_versions[-1].is_latest = True\n",
    "\n",
    "# Display version history\n",
    "version_data = [{\n",
    "    'Version ID': v.version_id,\n",
    "    'Size (MB)': v.size_bytes / 1_000_000,\n",
    "    'Last Modified': v.last_modified.strftime('%Y-%m-%d'),\n",
    "    'Latest': '‚úÖ' if v.is_latest else '',\n",
    "} for v in model_versions]\n",
    "\n",
    "version_df = pd.DataFrame(version_data)\n",
    "print(\"S3 Object Versioning Example\\n\")\n",
    "print(f\"Key: {model_versions[0].key}\\n\")\n",
    "print(version_df.to_string(index=False))\n",
    "\n",
    "# Calculate storage cost with versioning\n",
    "total_storage_mb = sum(v.size_bytes for v in model_versions) / 1_000_000\n",
    "monthly_cost = (total_storage_mb / 1024) * 0.023  # Standard S3 pricing\n",
    "\n",
    "print(f\"\\nüí∞ Versioning Storage Cost:\")\n",
    "print(f\"   Total storage (all versions): {total_storage_mb:.2f} MB\")\n",
    "print(f\"   Monthly cost: ${monthly_cost:.4f}\")\n",
    "print(f\"\\n‚ö†Ô∏è Versioning Considerations:\")\n",
    "print(\"   - Every overwrite creates a new version\")\n",
    "print(\"   - All versions count toward storage costs\")\n",
    "print(\"   - Use lifecycle policies to delete old versions\")\n",
    "print(\"   - Can recover from accidental deletions\")\n",
    "print(\"\\n‚úÖ Best Practices:\")\n",
    "print(\"   - Enable versioning for critical data (models, features)\")\n",
    "print(\"   - Delete old versions after N days (e.g., 90 days)\")\n",
    "print(\"   - Use version IDs for reproducibility\")\n",
    "print(\"   - Consider versioning in application (not S3) for temp data\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2: Encryption Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Encryption options comparison\n",
    "encryption_options = pd.DataFrame([\n",
    "    {\n",
    "        'Method': 'SSE-S3 (AWS Managed)',\n",
    "        'Provider': 'AWS',\n",
    "        'Key Management': 'AWS manages keys',\n",
    "        'Cost': 'Free',\n",
    "        'Ease': 'Easiest',\n",
    "        'Control': 'Low',\n",
    "        'Use Case': 'Default encryption'\n",
    "    },\n",
    "    {\n",
    "        'Method': 'SSE-KMS (AWS KMS)',\n",
    "        'Provider': 'AWS',\n",
    "        'Key Management': 'AWS KMS (you control)',\n",
    "        'Cost': '$1/month/key + API calls',\n",
    "        'Ease': 'Medium',\n",
    "        'Control': 'High',\n",
    "        'Use Case': 'Compliance, audit trail'\n",
    "    },\n",
    "    {\n",
    "        'Method': 'SSE-C (Customer Provided)',\n",
    "        'Provider': 'AWS',\n",
    "        'Key Management': 'You manage keys',\n",
    "        'Cost': 'Free',\n",
    "        'Ease': 'Hard',\n",
    "        'Control': 'Full',\n",
    "        'Use Case': 'Strict data sovereignty'\n",
    "    },\n",
    "    {\n",
    "        'Method': 'Client-Side Encryption',\n",
    "        'Provider': 'All',\n",
    "        'Key Management': 'You manage everything',\n",
    "        'Cost': 'Free (storage)',\n",
    "        'Ease': 'Hardest',\n",
    "        'Control': 'Full',\n",
    "        'Use Case': 'Zero-trust, maximum security'\n",
    "    },\n",
    "    {\n",
    "        'Method': 'Azure Storage Service Encryption',\n",
    "        'Provider': 'Azure',\n",
    "        'Key Management': 'Microsoft manages',\n",
    "        'Cost': 'Free',\n",
    "        'Ease': 'Easiest',\n",
    "        'Control': 'Low',\n",
    "        'Use Case': 'Default encryption'\n",
    "    },\n",
    "    {\n",
    "        'Method': 'GCP Default Encryption',\n",
    "        'Provider': 'GCP',\n",
    "        'Key Management': 'Google manages',\n",
    "        'Cost': 'Free',\n",
    "        'Ease': 'Easiest',\n",
    "        'Control': 'Low',\n",
    "        'Use Case': 'Default encryption'\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"Cloud Storage Encryption Options\\n\")\n",
    "print(encryption_options.to_string(index=False))\n",
    "print(\"\\nüîí Encryption Best Practices:\")\n",
    "print(\"   - Always use encryption (at minimum SSE-S3/default)\")\n",
    "print(\"   - Use SSE-KMS for compliance requirements\")\n",
    "print(\"   - Enable encryption in transit (HTTPS)\")\n",
    "print(\"   - Rotate keys regularly (automated with KMS)\")\n",
    "print(\"   - Consider client-side encryption for PII data\")\n",
    "print(\"\\nüí° For ML Projects:\")\n",
    "print(\"   - Default encryption (SSE-S3): Sufficient for most cases\")\n",
    "print(\"   - KMS encryption: Healthcare, finance, regulated industries\")\n",
    "print(\"   - Client-side: Sensitive customer data, PII\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Access Control\n",
    "\n",
    "### 4.1: IAM Policies for S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Example IAM policies for ML data access\n",
    "iam_policies = {\n",
    "    'data_scientist_readonly': {\n",
    "        'Description': 'Read-only access to processed data and models',\n",
    "        'Policy': {\n",
    "            'Version': '2012-10-17',\n",
    "            'Statement': [\n",
    "                {\n",
    "                    'Effect': 'Allow',\n",
    "                    'Action': [\n",
    "                        's3:GetObject',\n",
    "                        's3:ListBucket'\n",
    "                    ],\n",
    "                    'Resource': [\n",
    "                        'arn:aws:s3:::ml-data-bucket/processed/*',\n",
    "                        'arn:aws:s3:::ml-data-bucket/models/*',\n",
    "                        'arn:aws:s3:::ml-data-bucket'\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    'ml_engineer_full': {\n",
    "        'Description': 'Full access to all ML data',\n",
    "        'Policy': {\n",
    "            'Version': '2012-10-17',\n",
    "            'Statement': [\n",
    "                {\n",
    "                    'Effect': 'Allow',\n",
    "                    'Action': 's3:*',\n",
    "                    'Resource': [\n",
    "                        'arn:aws:s3:::ml-data-bucket/*',\n",
    "                        'arn:aws:s3:::ml-data-bucket'\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    'training_job_role': {\n",
    "        'Description': 'SageMaker training job access',\n",
    "        'Policy': {\n",
    "            'Version': '2012-10-17',\n",
    "            'Statement': [\n",
    "                {\n",
    "                    'Effect': 'Allow',\n",
    "                    'Action': [\n",
    "                        's3:GetObject',\n",
    "                        's3:PutObject'\n",
    "                    ],\n",
    "                    'Resource': [\n",
    "                        'arn:aws:s3:::ml-data-bucket/processed/*',\n",
    "                        'arn:aws:s3:::ml-data-bucket/models/*',\n",
    "                        'arn:aws:s3:::ml-data-bucket/experiments/*'\n",
    "                    ]\n",
    "                },\n",
    "                {\n",
    "                    'Effect': 'Deny',\n",
    "                    'Action': 's3:*',\n",
    "                    'Resource': 'arn:aws:s3:::ml-data-bucket/raw/*'\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"IAM Policy Examples for ML Data Access\\n\")\n",
    "for policy_name, details in iam_policies.items():\n",
    "    print(f\"üìã {policy_name}\")\n",
    "    print(f\"   {details['Description']}\")\n",
    "    print(f\"   Policy: {json.dumps(details['Policy'], indent=6)}\")\n",
    "    print()\n",
    "\n",
    "print(\"üîê Access Control Best Practices:\")\n",
    "print(\"   - Principle of least privilege\")\n",
    "print(\"   - Use IAM roles, not access keys\")\n",
    "print(\"   - Separate read and write permissions\")\n",
    "print(\"   - Protect raw data (read-only for most users)\")\n",
    "print(\"   - Use bucket policies + IAM policies together\")\n",
    "print(\"   - Enable MFA delete for critical data\")\n",
    "print(\"   - Regular access audits with CloudTrail\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Data Transfer Optimization\n",
    "\n",
    "### 5.1: Data Transfer Costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def calculate_transfer_costs(transfer_gb, source, destination):\n",
    "    \"\"\"\n",
    "    Calculate data transfer costs between different locations\n",
    "    \n",
    "    Pricing (AWS example, approximate):\n",
    "    - Within same region: FREE\n",
    "    - Between regions: $0.02/GB\n",
    "    - To internet: $0.09/GB (first 10TB)\n",
    "    - Between AWS and CloudFront: FREE\n",
    "    \"\"\"\n",
    "    transfer_types = {\n",
    "        ('s3', 'ec2_same_region'): 0,\n",
    "        ('s3', 's3_same_region'): 0,\n",
    "        ('s3', 'ec2_different_region'): 0.02,\n",
    "        ('s3', 's3_different_region'): 0.02,\n",
    "        ('s3', 'internet'): 0.09,\n",
    "        ('s3', 'cloudfront'): 0,\n",
    "        ('s3', 'sagemaker_same_region'): 0,\n",
    "    }\n",
    "    \n",
    "    key = (source, destination)\n",
    "    rate = transfer_types.get(key, 0.09)  # Default to internet pricing\n",
    "    cost = transfer_gb * rate\n",
    "    \n",
    "    return {\n",
    "        'transfer_gb': transfer_gb,\n",
    "        'source': source,\n",
    "        'destination': destination,\n",
    "        'rate_per_gb': rate,\n",
    "        'total_cost': cost\n",
    "    }\n",
    "\n",
    "# Example transfer scenarios\n",
    "transfer_scenarios = [\n",
    "    ('s3', 'sagemaker_same_region', 100, 'Training data to SageMaker'),\n",
    "    ('s3', 'ec2_different_region', 50, 'Inference data cross-region'),\n",
    "    ('s3', 'internet', 10, 'Download model for local testing'),\n",
    "    ('s3', 's3_different_region', 200, 'Backup to different region'),\n",
    "]\n",
    "\n",
    "transfer_results = []\n",
    "for source, dest, gb, description in transfer_scenarios:\n",
    "    result = calculate_transfer_costs(gb, source, dest)\n",
    "    result['description'] = description\n",
    "    transfer_results.append(result)\n",
    "\n",
    "transfer_df = pd.DataFrame(transfer_results)\n",
    "\n",
    "print(\"Data Transfer Cost Analysis\\n\")\n",
    "print(transfer_df[[\n",
    "    'description', 'transfer_gb', 'rate_per_gb', 'total_cost'\n",
    "]].to_string(index=False))\n",
    "\n",
    "total_transfer_cost = transfer_df['total_cost'].sum()\n",
    "print(f\"\\nüí∞ Total transfer cost: ${total_transfer_cost:.2f}\")\n",
    "\n",
    "print(\"\\nüìä Transfer Cost Optimization Tips:\")\n",
    "print(\"   1. Train in same region as data (FREE transfer)\")\n",
    "print(\"   2. Use CloudFront for downloads (FREE from S3)\")\n",
    "print(\"   3. Compress data before transfer (30-80% reduction)\")\n",
    "print(\"   4. Use S3 Transfer Acceleration for large uploads\")\n",
    "print(\"   5. Minimize cross-region transfers\")\n",
    "print(\"   6. Download only what you need (filter, sample)\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned comprehensive cloud storage strategies for ML:\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Storage Services Comparison**\n",
    "   - AWS S3, Azure Blob, GCP Storage are functionally equivalent\n",
    "   - Azure Blob slightly cheaper for standard storage\n",
    "   - All offer 11 nines (99.999999999%) durability\n",
    "   - Free tier: 5GB for 12 months\n",
    "\n",
    "2. **Storage Tiers**\n",
    "   - Standard: Frequent access ($0.02-0.023/GB/month)\n",
    "   - Infrequent Access: Monthly access ($0.01-0.0125/GB/month)\n",
    "   - Archive: Rare access ($0.001-0.004/GB/month)\n",
    "   - Use lifecycle policies for automatic tiering\n",
    "\n",
    "3. **Data Lake Architecture**\n",
    "   - Organized folder structure: raw/, processed/, features/, models/\n",
    "   - Raw data is immutable\n",
    "   - Version control for models and features\n",
    "   - Lifecycle policies for cost optimization\n",
    "\n",
    "4. **Versioning**\n",
    "   - Enable for critical data (models, features)\n",
    "   - All versions count toward storage costs\n",
    "   - Use lifecycle policies to delete old versions\n",
    "   - Enables rollback and reproducibility\n",
    "\n",
    "5. **Encryption**\n",
    "   - Default encryption (SSE-S3): Free and sufficient\n",
    "   - KMS encryption: Compliance, audit trail\n",
    "   - Client-side: Maximum security, full control\n",
    "   - Always use HTTPS for transfer\n",
    "\n",
    "6. **Access Control**\n",
    "   - Principle of least privilege\n",
    "   - Use IAM roles, not access keys\n",
    "   - Protect raw data (read-only)\n",
    "   - Regular audits with CloudTrail\n",
    "\n",
    "7. **Data Transfer**\n",
    "   - Within region: FREE\n",
    "   - Cross-region: $0.02/GB\n",
    "   - To internet: $0.09/GB\n",
    "   - Optimize: compress, filter, same-region training\n",
    "\n",
    "### Cost Optimization Checklist:\n",
    "\n",
    "‚úÖ Implement lifecycle policies  \n",
    "‚úÖ Use appropriate storage tiers  \n",
    "‚úÖ Compress data (Parquet, gzip)  \n",
    "‚úÖ Delete temporary files  \n",
    "‚úÖ Archive old experiments  \n",
    "‚úÖ Minimize cross-region transfers  \n",
    "‚úÖ Delete old model versions  \n",
    "‚úÖ Use Intelligent-Tiering for unknown patterns  \n",
    "‚úÖ Monitor storage usage regularly  \n",
    "‚úÖ Clean up failed jobs  \n",
    "\n",
    "### Typical ML Project Storage Costs:\n",
    "\n",
    "| Project Size | Monthly Data | Storage Cost | Transfer Cost | Total |\n",
    "|--------------|--------------|--------------|---------------|-------|\n",
    "| Small | 50GB | $1 | $0.50 | **$1.50** |\n",
    "| Medium | 500GB | $10 | $2 | **$12** |\n",
    "| Large | 5TB | $100 | $10 | **$110** |\n",
    "| Enterprise | 50TB | $1000 | $50 | **$1050** |\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **[Module 11: Final Project - Deploy Model on Cloud](11_final_project_deploy_model_on_cloud.ipynb)**: Capstone project\n",
    "- **Practice**: Set up S3 bucket with lifecycle policies\n",
    "- **Explore**: AWS DataSync for large-scale migrations\n",
    "- **Implement**: Data lake architecture for your project\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [AWS S3 Documentation](https://docs.aws.amazon.com/s3/)\n",
    "- [Azure Blob Storage](https://learn.microsoft.com/en-us/azure/storage/blobs/)\n",
    "- [GCP Cloud Storage](https://cloud.google.com/storage/docs)\n",
    "- [Data Lake Best Practices](https://aws.amazon.com/big-data/datalakes-and-analytics/)\n",
    "- [S3 Intelligent-Tiering](https://aws.amazon.com/s3/storage-classes/intelligent-tiering/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Storage Tier Optimizer ‚≠ê\n",
    "\n",
    "Create a storage tier recommendation tool:\n",
    "\n",
    "1. Input: Data size, access frequency (daily, weekly, monthly, yearly)\n",
    "2. Calculate costs for all storage tiers\n",
    "3. Include retrieval costs based on access frequency\n",
    "4. Recommend optimal tier\n",
    "5. Show cost savings vs standard tier\n",
    "\n",
    "Test with at least 5 different scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Your code here\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Lifecycle Policy Generator ‚≠ê‚≠ê\n",
    "\n",
    "Build a lifecycle policy generator for ML projects:\n",
    "\n",
    "1. **Define data categories** with retention requirements:\n",
    "   - Raw data: 1 year ‚Üí archive\n",
    "   - Processed: 6 months ‚Üí IA ‚Üí delete\n",
    "   - Models: Keep latest 5 versions\n",
    "   - Experiments: 90 days ‚Üí delete\n",
    "   - Temp: 7 days ‚Üí delete\n",
    "\n",
    "2. **Generate**:\n",
    "   - AWS S3 lifecycle policy (JSON)\n",
    "   - Azure Blob lifecycle policy\n",
    "   - Cost savings estimate\n",
    "\n",
    "3. **Visualize** storage costs over time with and without policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Your code here\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Data Lake Cost Analyzer ‚≠ê‚≠ê\n",
    "\n",
    "Analyze your existing or planned data lake:\n",
    "\n",
    "1. **Inventory**: List all data categories and sizes\n",
    "2. **Current costs**: Calculate with current storage classes\n",
    "3. **Optimized costs**: Apply appropriate tiers and lifecycle policies\n",
    "4. **Savings**: Show monthly and annual savings\n",
    "5. **Growth projection**: Model 12-month growth\n",
    "6. **Visualization**:\n",
    "   - Storage breakdown by category\n",
    "   - Cost trends over time\n",
    "   - Savings opportunities\n",
    "\n",
    "Present findings as a management report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Your code here\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Access Control Policy Designer ‚≠ê‚≠ê‚≠ê\n",
    "\n",
    "Design a complete access control system:\n",
    "\n",
    "**Roles**:\n",
    "- Data Scientists (read-only)\n",
    "- ML Engineers (read-write processed, models)\n",
    "- Data Engineers (full access)\n",
    "- Training Jobs (specific paths only)\n",
    "- Inference Services (models only)\n",
    "\n",
    "**Tasks**:\n",
    "1. Create IAM policies for each role\n",
    "2. Implement bucket policies for defense in depth\n",
    "3. Document permission matrix\n",
    "4. Identify potential security gaps\n",
    "5. Generate Terraform code for policies\n",
    "\n",
    "**Bonus**: Add MFA requirements for sensitive data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Your code here\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Multi-Cloud Storage Migration Plan ‚≠ê‚≠ê‚≠ê\n",
    "\n",
    "Plan a migration from AWS S3 to Azure Blob (or vice versa):\n",
    "\n",
    "**Scenario**: 10TB ML data lake on AWS ‚Üí Azure\n",
    "\n",
    "**Tasks**:\n",
    "1. **Inventory**: List all objects, sizes, access patterns\n",
    "2. **Mapping**: Map S3 buckets to Azure containers\n",
    "3. **Cost analysis**:\n",
    "   - Transfer costs\n",
    "   - Downtime costs\n",
    "   - Tool/service costs\n",
    "4. **Migration strategy**:\n",
    "   - Tools (AWS DataSync, AzCopy, rclone)\n",
    "   - Phased approach vs big bang\n",
    "   - Validation and testing\n",
    "5. **Timeline**: Week-by-week plan\n",
    "6. **Risks**: Identify and mitigation strategies\n",
    "7. **Rollback plan**: How to revert if needed\n",
    "\n",
    "**Deliverable**: Complete migration runbook with costs, timeline, and risks.\n",
    "\n",
    "**Bonus**: Implement sync script using boto3 and Azure SDK (simulated)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Your code here\n"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
