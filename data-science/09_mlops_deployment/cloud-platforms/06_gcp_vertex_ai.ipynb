{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 06: GCP Vertex AI Advanced Features\n",
    "\n",
    "**Difficulty**: â­â­â­\n",
    "**Estimated Time**: 65 minutes\n",
    "**Prerequisites**: \n",
    "- [Module 05: Google Cloud AI Platform Basics](05_google_cloud_ai_platform_basics.ipynb)\n",
    "- Understanding of ML pipelines and deployment\n",
    "- Familiarity with GCP services\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Use Vertex AI Workbench for unified ML development\n",
    "2. Apply AutoML for Tables, Vision, and NLP tasks\n",
    "3. Submit custom training jobs with distributed training\n",
    "4. Build ML pipelines with Vertex AI Pipelines (Kubeflow)\n",
    "5. Deploy models to Vertex AI endpoints with monitoring\n",
    "6. Leverage Feature Store for feature management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Introduction\n",
    "\n",
    "### Vertex AI Overview\n",
    "\n",
    "Vertex AI is GCP's unified ML platform that consolidates all ML services.\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚           Vertex AI Platform Architecture                â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                         â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n",
    "â”‚  â”‚             Vertex AI Workbench                   â”‚ â”‚\n",
    "â”‚  â”‚  - Managed Notebooks (JupyterLab)                 â”‚ â”‚\n",
    "â”‚  â”‚  - User-managed Notebooks                         â”‚ â”‚\n",
    "â”‚  â”‚  - Colab Enterprise Integration                   â”‚ â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\n",
    "â”‚                          â”‚                              â”‚\n",
    "â”‚                          â–¼                              â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n",
    "â”‚  â”‚              Data & Features                      â”‚ â”‚\n",
    "â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚ â”‚\n",
    "â”‚  â”‚  â”‚ Datasets     â”‚    â”‚ Feature Store    â”‚        â”‚ â”‚\n",
    "â”‚  â”‚  â”‚ - Managed    â”‚    â”‚ - Online serving â”‚        â”‚ â”‚\n",
    "â”‚  â”‚  â”‚ - BigQuery   â”‚    â”‚ - Offline batch  â”‚        â”‚ â”‚\n",
    "â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚ â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\n",
    "â”‚                          â”‚                              â”‚\n",
    "â”‚                          â–¼                              â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n",
    "â”‚  â”‚               Training                            â”‚ â”‚\n",
    "â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚ â”‚\n",
    "â”‚  â”‚  â”‚ AutoML       â”‚    â”‚ Custom Training  â”‚        â”‚ â”‚\n",
    "â”‚  â”‚  â”‚ - Tables     â”‚    â”‚ - Distributed    â”‚        â”‚ â”‚\n",
    "â”‚  â”‚  â”‚ - Vision     â”‚    â”‚ - Hyperparameter â”‚        â”‚ â”‚\n",
    "â”‚  â”‚  â”‚ - NLP        â”‚    â”‚ - GPU/TPU        â”‚        â”‚ â”‚\n",
    "â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚ â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\n",
    "â”‚                          â”‚                              â”‚\n",
    "â”‚                          â–¼                              â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n",
    "â”‚  â”‚            Pipelines & Experiments                â”‚ â”‚\n",
    "â”‚  â”‚  - Vertex AI Pipelines (Kubeflow)                 â”‚ â”‚\n",
    "â”‚  â”‚  - Experiment tracking                            â”‚ â”‚\n",
    "â”‚  â”‚  - Model registry                                 â”‚ â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\n",
    "â”‚                          â”‚                              â”‚\n",
    "â”‚                          â–¼                              â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n",
    "â”‚  â”‚              Deployment                           â”‚ â”‚\n",
    "â”‚  â”‚  - Prediction endpoints                           â”‚ â”‚\n",
    "â”‚  â”‚  - Batch prediction                               â”‚ â”‚\n",
    "â”‚  â”‚  - Model monitoring                               â”‚ â”‚\n",
    "â”‚  â”‚  - Explainability                                 â”‚ â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Benefits**:\n",
    "- Unified platform (no switching between services)\n",
    "- MLOps capabilities built-in\n",
    "- Integration with BigQuery, Cloud Storage, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock Vertex AI SDK for demonstration\n",
    "# In production, use: from google.cloud import aiplatform\n",
    "\n",
    "class MockVertexAI:\n",
    "    \"\"\"Simulates Vertex AI client\"\"\"\n",
    "    \n",
    "    def __init__(self, project, location='us-central1'):\n",
    "        self.project = project\n",
    "        self.location = location\n",
    "        self.initialized = True\n",
    "    \n",
    "    def init(self):\n",
    "        \"\"\"Initialize Vertex AI SDK\"\"\"\n",
    "        print(f\"âœ“ Vertex AI initialized\")\n",
    "        print(f\"  Project: {self.project}\")\n",
    "        print(f\"  Location: {self.location}\")\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Vertex AI Workbench\n",
    "\n",
    "Managed notebook environment with integrated ML tools.\n",
    "\n",
    "### Workbench Features\n",
    "\n",
    "```\n",
    "Vertex AI Workbench\n",
    "â”œâ”€â”€ Managed Notebooks\n",
    "â”‚   â”œâ”€â”€ Fully managed (no infrastructure)\n",
    "â”‚   â”œâ”€â”€ Auto-shutdown\n",
    "â”‚   â”œâ”€â”€ Git integration\n",
    "â”‚   â””â”€â”€ Built-in Vertex AI access\n",
    "â”‚\n",
    "â”œâ”€â”€ User-Managed Notebooks\n",
    "â”‚   â”œâ”€â”€ More control over environment\n",
    "â”‚   â”œâ”€â”€ Custom docker images\n",
    "â”‚   â”œâ”€â”€ Root access\n",
    "â”‚   â””â”€â”€ Persistent VMs\n",
    "â”‚\n",
    "â””â”€â”€ Integrated Tools\n",
    "    â”œâ”€â”€ TensorBoard\n",
    "    â”œâ”€â”€ What-If Tool\n",
    "    â”œâ”€â”€ Data labeling\n",
    "    â””â”€â”€ Model deployment\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workbench instance configuration\n",
    "\n",
    "def create_workbench_instance():\n",
    "    \"\"\"Configure Vertex AI Workbench instance\"\"\"\n",
    "    config = {\n",
    "        'name': 'ml-workbench-001',\n",
    "        'type': 'managed',  # or 'user-managed'\n",
    "        'machine_type': 'n1-standard-4',\n",
    "        'accelerator': {\n",
    "            'type': 'NVIDIA_TESLA_T4',\n",
    "            'count': 1\n",
    "        },\n",
    "        'boot_disk': {\n",
    "            'type': 'PD_SSD',\n",
    "            'size_gb': 100\n",
    "        },\n",
    "        'data_disk': {\n",
    "            'type': 'PD_STANDARD',\n",
    "            'size_gb': 500\n",
    "        },\n",
    "        'runtime': {\n",
    "            'python_version': '3.10',\n",
    "            'frameworks': ['tensorflow', 'pytorch', 'sklearn']\n",
    "        },\n",
    "        'idle_shutdown_timeout': 60,  # minutes\n",
    "        'labels': {\n",
    "            'team': 'ml-team',\n",
    "            'environment': 'development'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"ðŸ’° Workbench Cost Tips:\")\n",
    "    print(\"- Managed notebooks auto-shutdown when idle\")\n",
    "    print(\"- Use standard VMs (not highmem) initially\")\n",
    "    print(\"- Add GPU only when training large models\")\n",
    "    print(\"- Set aggressive idle shutdown timeout\\n\")\n",
    "    \n",
    "    return config\n",
    "\n",
    "workbench = create_workbench_instance()\n",
    "print(\"Vertex AI Workbench Configuration:\")\n",
    "print(json.dumps(workbench, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. AutoML on Vertex AI\n",
    "\n",
    "Automated ML for common tasks without writing training code.\n",
    "\n",
    "### AutoML Task Types\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚           Vertex AI AutoML Options                       â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                         â”‚\n",
    "â”‚  AutoML Tables (Tabular Data)                           â”‚\n",
    "â”‚  â”œâ”€ Classification                                      â”‚\n",
    "â”‚  â”œâ”€ Regression                                          â”‚\n",
    "â”‚  â”œâ”€ Forecasting                                         â”‚\n",
    "â”‚  â””â”€ Auto feature engineering                            â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  AutoML Vision                                          â”‚\n",
    "â”‚  â”œâ”€ Image classification                                â”‚\n",
    "â”‚  â”œâ”€ Object detection                                    â”‚\n",
    "â”‚  â”œâ”€ Image segmentation                                  â”‚\n",
    "â”‚  â””â”€ Transfer learning                                   â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  AutoML Natural Language                                â”‚\n",
    "â”‚  â”œâ”€ Text classification                                 â”‚\n",
    "â”‚  â”œâ”€ Entity extraction                                   â”‚\n",
    "â”‚  â”œâ”€ Sentiment analysis                                  â”‚\n",
    "â”‚  â””â”€ Custom models                                       â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  AutoML Video                                           â”‚\n",
    "â”‚  â”œâ”€ Video classification                                â”‚\n",
    "â”‚  â”œâ”€ Object tracking                                     â”‚\n",
    "â”‚  â””â”€ Action recognition                                  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoML Tables configuration\n",
    "\n",
    "def create_automl_tabular_training():\n",
    "    \"\"\"Configure AutoML for tabular data\"\"\"\n",
    "    training_config = {\n",
    "        'display_name': 'churn_prediction_automl',\n",
    "        'dataset': 'projects/my-project/locations/us-central1/datasets/123456',\n",
    "        'target_column': 'churn',\n",
    "        'prediction_type': 'classification',\n",
    "        'optimization_objective': 'maximize-au-roc',\n",
    "        'transformations': {\n",
    "            'auto': True,  # Automatic feature engineering\n",
    "            'column_transformations': [\n",
    "                {'column': 'age', 'transformation': 'numeric'},\n",
    "                {'column': 'tenure', 'transformation': 'numeric'},\n",
    "                {'column': 'contract_type', 'transformation': 'categorical'},\n",
    "                {'column': 'monthly_charges', 'transformation': 'numeric'}\n",
    "            ]\n",
    "        },\n",
    "        'budget_milli_node_hours': 1000,  # ~1 node hour = $3\n",
    "        'split': {\n",
    "            'training_fraction': 0.8,\n",
    "            'validation_fraction': 0.1,\n",
    "            'test_fraction': 0.1\n",
    "        },\n",
    "        'export_evaluated_data_items': True\n",
    "    }\n",
    "    \n",
    "    print(\"AutoML Tables Training:\")\n",
    "    print(f\"Budget: {training_config['budget_milli_node_hours']/1000} node-hours\")\n",
    "    print(f\"Estimated cost: ~${training_config['budget_milli_node_hours']/1000 * 3:.2f}\")\n",
    "    print(\"\\nâ±ï¸  Training typically takes 2-6 hours\\n\")\n",
    "    \n",
    "    return training_config\n",
    "\n",
    "automl_config = create_automl_tabular_training()\n",
    "print(json.dumps(automl_config, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate AutoML training\n",
    "\n",
    "class AutoMLTrainer:\n",
    "    \"\"\"Simulates Vertex AI AutoML training\"\"\"\n",
    "    \n",
    "    def __init__(self, display_name, objective):\n",
    "        self.display_name = display_name\n",
    "        self.objective = objective\n",
    "        self.state = 'PENDING'\n",
    "        self.models_tried = []\n",
    "    \n",
    "    def train(self, max_trials=20):\n",
    "        \"\"\"Run AutoML training with multiple trials\"\"\"\n",
    "        print(f\"Starting AutoML training: {self.display_name}\")\n",
    "        print(f\"Objective: {self.objective}\\n\")\n",
    "        \n",
    "        self.state = 'RUNNING'\n",
    "        \n",
    "        algorithms = [\n",
    "            'Gradient Boosted Trees',\n",
    "            'Random Forest',\n",
    "            'Neural Network',\n",
    "            'Linear Model',\n",
    "            'Ensemble'\n",
    "        ]\n",
    "        \n",
    "        for i in range(max_trials):\n",
    "            algo = np.random.choice(algorithms)\n",
    "            score = 0.65 + np.random.rand() * 0.3\n",
    "            \n",
    "            model_trial = {\n",
    "                'trial': i + 1,\n",
    "                'algorithm': algo,\n",
    "                'score': score\n",
    "            }\n",
    "            self.models_tried.append(model_trial)\n",
    "            \n",
    "            if (i + 1) % 5 == 0:\n",
    "                print(f\"Trial {i+1}/{max_trials}: {algo} - Score: {score:.4f}\")\n",
    "        \n",
    "        # Find best model\n",
    "        best_model = max(self.models_tried, key=lambda x: x['score'])\n",
    "        self.state = 'SUCCEEDED'\n",
    "        \n",
    "        print(f\"\\nâœ“ Training completed!\")\n",
    "        print(f\"Best model: {best_model['algorithm']}\")\n",
    "        print(f\"Best score: {best_model['score']:.4f}\")\n",
    "        \n",
    "        return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run AutoML simulation\n",
    "automl = AutoMLTrainer(\n",
    "    display_name='churn_prediction_automl',\n",
    "    objective='maximize-au-roc'\n",
    ")\n",
    "\n",
    "best_model = automl.train(max_trials=20)\n",
    "\n",
    "print(\"\\nAutoML Benefits:\")\n",
    "print(\"- No manual feature engineering\")\n",
    "print(\"- Tries multiple algorithms automatically\")\n",
    "print(\"- Handles data preprocessing\")\n",
    "print(\"- Provides model explanations\")\n",
    "print(\"- Fast deployment to endpoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Custom Training Jobs\n",
    "\n",
    "Run custom training code with full control over the training process.\n",
    "\n",
    "### Custom Training Architecture\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚          Custom Training on Vertex AI                    â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                         â”‚\n",
    "â”‚  Training Code                                          â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚\n",
    "â”‚  â”‚ Python package or script     â”‚                      â”‚\n",
    "â”‚  â”‚ - train.py (entry point)     â”‚                      â”‚\n",
    "â”‚  â”‚ - model code                 â”‚                      â”‚\n",
    "â”‚  â”‚ - requirements.txt           â”‚                      â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚\n",
    "â”‚           â”‚                                             â”‚\n",
    "â”‚           â–¼                                             â”‚\n",
    "â”‚  Container Image                                        â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚\n",
    "â”‚  â”‚ Pre-built:                   â”‚                      â”‚\n",
    "â”‚  â”‚ - TensorFlow                 â”‚                      â”‚\n",
    "â”‚  â”‚ - PyTorch                    â”‚                      â”‚\n",
    "â”‚  â”‚ - Scikit-learn               â”‚                      â”‚\n",
    "â”‚  â”‚ Custom:                      â”‚                      â”‚\n",
    "â”‚  â”‚ - Your own Dockerfile        â”‚                      â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚\n",
    "â”‚           â”‚                                             â”‚\n",
    "â”‚           â–¼                                             â”‚\n",
    "â”‚  Training Job                                           â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚\n",
    "â”‚  â”‚ Compute:                     â”‚                      â”‚\n",
    "â”‚  â”‚ - Single VM                  â”‚                      â”‚\n",
    "â”‚  â”‚ - Distributed (multi-VM)     â”‚                      â”‚\n",
    "â”‚  â”‚ - GPU/TPU support            â”‚                      â”‚\n",
    "â”‚  â”‚                              â”‚                      â”‚\n",
    "â”‚  â”‚ Features:                    â”‚                      â”‚\n",
    "â”‚  â”‚ - Hyperparameter tuning      â”‚                      â”‚\n",
    "â”‚  â”‚ - Checkpointing              â”‚                      â”‚\n",
    "â”‚  â”‚ - TensorBoard integration    â”‚                      â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom training job configuration\n",
    "\n",
    "def create_custom_training_job():\n",
    "    \"\"\"Configure custom training job on Vertex AI\"\"\"\n",
    "    job_config = {\n",
    "        'display_name': 'custom-classifier-training',\n",
    "        'container_spec': {\n",
    "            'image_uri': 'gcr.io/cloud-aiplatform/training/tf-cpu.2-11:latest',\n",
    "            'command': ['python', 'task.py'],\n",
    "            'args': [\n",
    "                '--train-data', 'gs://my-bucket/data/train.csv',\n",
    "                '--eval-data', 'gs://my-bucket/data/eval.csv',\n",
    "                '--model-dir', 'gs://my-bucket/models/output',\n",
    "                '--epochs', '20',\n",
    "                '--batch-size', '32'\n",
    "            ]\n",
    "        },\n",
    "        'machine_spec': {\n",
    "            'machine_type': 'n1-standard-8',\n",
    "            'accelerator_type': 'NVIDIA_TESLA_T4',\n",
    "            'accelerator_count': 1\n",
    "        },\n",
    "        'replica_count': 1,  # Single-node training\n",
    "        'scheduling': {\n",
    "            'timeout': '7200s',  # 2 hours\n",
    "            'restart_job_on_worker_restart': True\n",
    "        },\n",
    "        'service_account': 'vertex-ai-sa@my-project.iam.gserviceaccount.com',\n",
    "        'tensorboard': 'projects/my-project/locations/us-central1/tensorboards/123'\n",
    "    }\n",
    "    return job_config\n",
    "\n",
    "# Distributed training configuration\n",
    "def create_distributed_training_job():\n",
    "    \"\"\"Configure distributed training (multi-node)\"\"\"\n",
    "    job_config = {\n",
    "        'display_name': 'distributed-training',\n",
    "        'worker_pool_specs': [\n",
    "            {  # Chief worker\n",
    "                'machine_spec': {\n",
    "                    'machine_type': 'n1-standard-16',\n",
    "                    'accelerator_type': 'NVIDIA_TESLA_V100',\n",
    "                    'accelerator_count': 2\n",
    "                },\n",
    "                'replica_count': 1,\n",
    "                'container_spec': {\n",
    "                    'image_uri': 'gcr.io/my-project/trainer:latest',\n",
    "                    'command': ['python', '-m', 'trainer.task']\n",
    "                }\n",
    "            },\n",
    "            {  # Worker pool\n",
    "                'machine_spec': {\n",
    "                    'machine_type': 'n1-standard-16',\n",
    "                    'accelerator_type': 'NVIDIA_TESLA_V100',\n",
    "                    'accelerator_count': 2\n",
    "                },\n",
    "                'replica_count': 3,  # 3 worker nodes\n",
    "                'container_spec': {\n",
    "                    'image_uri': 'gcr.io/my-project/trainer:latest',\n",
    "                    'command': ['python', '-m', 'trainer.task']\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    return job_config\n",
    "\n",
    "single_node = create_custom_training_job()\n",
    "distributed = create_distributed_training_job()\n",
    "\n",
    "print(\"Single-Node Training:\")\n",
    "print(json.dumps(single_node, indent=2))\n",
    "print(\"\\nDistributed Training:\")\n",
    "print(json.dumps(distributed, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Vertex AI Pipelines\n",
    "\n",
    "Build reproducible ML workflows using Kubeflow Pipelines.\n",
    "\n",
    "### Pipeline Architecture\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚         Vertex AI Pipelines (Kubeflow)                   â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                         â”‚\n",
    "â”‚  Pipeline = DAG of Components                           â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚\n",
    "â”‚         â”‚ Data Ingestionâ”‚                               â”‚\n",
    "â”‚         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚\n",
    "â”‚                â”‚                                        â”‚\n",
    "â”‚                â–¼                                        â”‚\n",
    "â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚\n",
    "â”‚         â”‚ Preprocessing â”‚                               â”‚\n",
    "â”‚         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚\n",
    "â”‚                â”‚                                        â”‚\n",
    "â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚\n",
    "â”‚       â”‚                 â”‚                               â”‚\n",
    "â”‚       â–¼                 â–¼                               â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚\n",
    "â”‚  â”‚ Train   â”‚      â”‚ Train   â”‚  (Parallel)              â”‚\n",
    "â”‚  â”‚ Model A â”‚      â”‚ Model B â”‚                          â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                          â”‚\n",
    "â”‚       â”‚                â”‚                                â”‚\n",
    "â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚\n",
    "â”‚                â–¼                                        â”‚\n",
    "â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚\n",
    "â”‚         â”‚  Evaluation  â”‚                                â”‚\n",
    "â”‚         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚\n",
    "â”‚                â”‚                                        â”‚\n",
    "â”‚                â–¼                                        â”‚\n",
    "â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚\n",
    "â”‚         â”‚ Conditional  â”‚                                â”‚\n",
    "â”‚         â”‚ Deployment   â”‚  (if metrics > threshold)     â”‚\n",
    "â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  Features:                                              â”‚\n",
    "â”‚  - Reusable components                                  â”‚\n",
    "â”‚  - Caching for faster runs                              â”‚\n",
    "â”‚  - Experiment tracking                                  â”‚\n",
    "â”‚  - Version control                                      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pipeline components (conceptual)\n",
    "\n",
    "def create_pipeline_definition():\n",
    "    \"\"\"\n",
    "    Define ML pipeline structure\n",
    "    In practice, use Kubeflow Pipelines SDK\n",
    "    \"\"\"\n",
    "    pipeline_def = {\n",
    "        'name': 'training_pipeline',\n",
    "        'description': 'End-to-end ML training pipeline',\n",
    "        'parameters': {\n",
    "            'project_id': {'type': 'String'},\n",
    "            'data_path': {'type': 'String'},\n",
    "            'model_threshold': {'type': 'Float', 'default': 0.85}\n",
    "        },\n",
    "        'components': [\n",
    "            {\n",
    "                'name': 'data_ingestion',\n",
    "                'type': 'ContainerOp',\n",
    "                'image': 'gcr.io/my-project/data-ingestion:v1',\n",
    "                'inputs': ['data_path'],\n",
    "                'outputs': ['raw_data']\n",
    "            },\n",
    "            {\n",
    "                'name': 'preprocessing',\n",
    "                'type': 'ContainerOp',\n",
    "                'image': 'gcr.io/my-project/preprocessing:v1',\n",
    "                'inputs': ['raw_data'],\n",
    "                'outputs': ['train_data', 'eval_data']\n",
    "            },\n",
    "            {\n",
    "                'name': 'training',\n",
    "                'type': 'CustomTrainingJobOp',\n",
    "                'inputs': ['train_data', 'eval_data'],\n",
    "                'outputs': ['model', 'metrics']\n",
    "            },\n",
    "            {\n",
    "                'name': 'evaluation',\n",
    "                'type': 'ContainerOp',\n",
    "                'image': 'gcr.io/my-project/evaluation:v1',\n",
    "                'inputs': ['model', 'eval_data'],\n",
    "                'outputs': ['eval_metrics']\n",
    "            },\n",
    "            {\n",
    "                'name': 'deploy_model',\n",
    "                'type': 'ModelDeployOp',\n",
    "                'condition': 'eval_metrics.auc >= model_threshold',\n",
    "                'inputs': ['model'],\n",
    "                'outputs': ['endpoint']\n",
    "            }\n",
    "        ],\n",
    "        'execution': {\n",
    "            'caching': True,\n",
    "            'schedule': '0 2 * * *',  # Daily at 2 AM\n",
    "        }\n",
    "    }\n",
    "    return pipeline_def\n",
    "\n",
    "pipeline = create_pipeline_definition()\n",
    "print(\"Pipeline Definition:\")\n",
    "print(json.dumps(pipeline, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate pipeline execution\n",
    "\n",
    "class VertexAIPipeline:\n",
    "    \"\"\"Simulates Vertex AI Pipeline execution\"\"\"\n",
    "    \n",
    "    def __init__(self, pipeline_name):\n",
    "        self.pipeline_name = pipeline_name\n",
    "        self.runs = []\n",
    "    \n",
    "    def run(self, parameters):\n",
    "        \"\"\"Execute pipeline with given parameters\"\"\"\n",
    "        run_id = f\"run_{len(self.runs) + 1}\"\n",
    "        \n",
    "        print(f\"Starting pipeline run: {run_id}\")\n",
    "        print(f\"Parameters: {parameters}\\n\")\n",
    "        \n",
    "        # Simulate component execution\n",
    "        components = [\n",
    "            'data_ingestion',\n",
    "            'preprocessing',\n",
    "            'training',\n",
    "            'evaluation',\n",
    "            'deploy_model'\n",
    "        ]\n",
    "        \n",
    "        run_info = {\n",
    "            'run_id': run_id,\n",
    "            'parameters': parameters,\n",
    "            'component_results': {}\n",
    "        }\n",
    "        \n",
    "        for component in components:\n",
    "            print(f\"Executing: {component}...\")\n",
    "            \n",
    "            # Simulate component execution\n",
    "            if component == 'evaluation':\n",
    "                auc = 0.7 + np.random.rand() * 0.25\n",
    "                run_info['component_results'][component] = {'auc': auc}\n",
    "                print(f\"  Metrics: AUC={auc:.4f}\")\n",
    "                \n",
    "                # Conditional deployment\n",
    "                if auc < parameters.get('model_threshold', 0.85):\n",
    "                    print(f\"  âš ï¸  AUC below threshold, skipping deployment\\n\")\n",
    "                    break\n",
    "            elif component == 'deploy_model':\n",
    "                print(f\"  âœ“ Model deployed to endpoint\\n\")\n",
    "        \n",
    "        run_info['status'] = 'SUCCEEDED'\n",
    "        self.runs.append(run_info)\n",
    "        \n",
    "        print(f\"Pipeline run {run_id} completed!\")\n",
    "        return run_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute pipeline\n",
    "pipeline_executor = VertexAIPipeline('training_pipeline')\n",
    "\n",
    "run_result = pipeline_executor.run({\n",
    "    'project_id': 'my-ml-project',\n",
    "    'data_path': 'gs://my-bucket/data/train.csv',\n",
    "    'model_threshold': 0.85\n",
    "})\n",
    "\n",
    "print(\"\\nPipeline Benefits:\")\n",
    "print(\"- Reproducible workflows\")\n",
    "print(\"- Component reusability\")\n",
    "print(\"- Automated orchestration\")\n",
    "print(\"- Experiment tracking\")\n",
    "print(\"- Easy scheduling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Deployment on Vertex AI\n",
    "\n",
    "Deploy models to managed endpoints for predictions.\n",
    "\n",
    "### Deployment Options\n",
    "\n",
    "```\n",
    "1. Online Prediction (real-time)\n",
    "2. Batch Prediction (large datasets)\n",
    "3. Private Endpoints (VPC)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model deployment configuration\n",
    "\n",
    "def deploy_model_to_endpoint():\n",
    "    \"\"\"Configure model deployment to Vertex AI endpoint\"\"\"\n",
    "    deployment_config = {\n",
    "        'endpoint': {\n",
    "            'display_name': 'classifier-endpoint-prod',\n",
    "            'description': 'Production classification endpoint',\n",
    "            'labels': {\n",
    "                'environment': 'production',\n",
    "                'model': 'classifier'\n",
    "            }\n",
    "        },\n",
    "        'deployed_model': {\n",
    "            'display_name': 'classifier-v1',\n",
    "            'model': 'projects/my-project/locations/us-central1/models/12345',\n",
    "            'machine_type': 'n1-standard-4',\n",
    "            'min_replica_count': 1,\n",
    "            'max_replica_count': 5,\n",
    "            'autoscaling': {\n",
    "                'target_cpu_utilization': 60,\n",
    "                'target_accelerator_duty_cycle': 60\n",
    "            },\n",
    "            'enable_access_logging': True,\n",
    "            'enable_container_logging': True\n",
    "        },\n",
    "        'traffic_split': {\n",
    "            'classifier-v1': 100  # 100% traffic to this version\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"ðŸ’° Deployment Cost Optimization:\")\n",
    "    print(\"- Min replica count = 1 (scales down when idle)\")\n",
    "    print(\"- Use standard VMs (not highmem)\")\n",
    "    print(\"- Enable auto-scaling based on load\")\n",
    "    print(\"- Monitor and adjust replicas based on traffic\\n\")\n",
    "    \n",
    "    return deployment_config\n",
    "\n",
    "deployment = deploy_model_to_endpoint()\n",
    "print(\"Deployment Configuration:\")\n",
    "print(json.dumps(deployment, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate predictions\n",
    "\n",
    "class VertexAIEndpoint:\n",
    "    \"\"\"Simulates Vertex AI prediction endpoint\"\"\"\n",
    "    \n",
    "    def __init__(self, endpoint_name):\n",
    "        self.endpoint_name = endpoint_name\n",
    "        self.prediction_count = 0\n",
    "    \n",
    "    def predict(self, instances):\n",
    "        \"\"\"\n",
    "        Make predictions on instances\n",
    "        \n",
    "        Args:\n",
    "            instances: List of input instances\n",
    "        \n",
    "        Returns:\n",
    "            predictions with confidence scores\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        for instance in instances:\n",
    "            # Simulated prediction\n",
    "            pred_class = np.random.choice([0, 1], p=[0.6, 0.4])\n",
    "            confidence = np.random.rand()\n",
    "            \n",
    "            predictions.append({\n",
    "                'class': int(pred_class),\n",
    "                'confidence': float(confidence),\n",
    "                'probabilities': {\n",
    "                    '0': float(1 - confidence) if pred_class == 1 else float(confidence),\n",
    "                    '1': float(confidence) if pred_class == 1 else float(1 - confidence)\n",
    "                }\n",
    "            })\n",
    "        \n",
    "        self.prediction_count += len(instances)\n",
    "        return predictions\n",
    "    \n",
    "    def batch_predict(self, gcs_source, gcs_destination):\n",
    "        \"\"\"Run batch prediction on large dataset\"\"\"\n",
    "        print(f\"Batch prediction job started\")\n",
    "        print(f\"Source: {gcs_source}\")\n",
    "        print(f\"Destination: {gcs_destination}\")\n",
    "        \n",
    "        # Simulate processing\n",
    "        num_instances = np.random.randint(10000, 100000)\n",
    "        print(f\"\\nProcessing {num_instances:,} instances...\")\n",
    "        print(f\"âœ“ Batch prediction completed\")\n",
    "        print(f\"Results saved to: {gcs_destination}\")\n",
    "        \n",
    "        return {'instances_processed': num_instances}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test endpoint predictions\n",
    "endpoint = VertexAIEndpoint('classifier-endpoint-prod')\n",
    "\n",
    "# Online predictions\n",
    "print(\"Online Predictions:\\n\")\n",
    "test_instances = [\n",
    "    {'age': 45, 'tenure': 24, 'monthly_charges': 75.5},\n",
    "    {'age': 32, 'tenure': 12, 'monthly_charges': 45.0}\n",
    "]\n",
    "\n",
    "predictions = endpoint.predict(test_instances)\n",
    "for i, pred in enumerate(predictions):\n",
    "    print(f\"Instance {i+1}:\")\n",
    "    print(f\"  Predicted class: {pred['class']}\")\n",
    "    print(f\"  Confidence: {pred['confidence']:.2%}\")\n",
    "    print()\n",
    "\n",
    "# Batch predictions\n",
    "print(\"=\"*50)\n",
    "print(\"\\nBatch Predictions:\\n\")\n",
    "batch_result = endpoint.batch_predict(\n",
    "    gcs_source='gs://my-bucket/batch-input/*.csv',\n",
    "    gcs_destination='gs://my-bucket/batch-predictions/'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Store\n",
    "\n",
    "Centralized repository for feature management and serving.\n",
    "\n",
    "### Feature Store Architecture\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚           Vertex AI Feature Store                        â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                         â”‚\n",
    "â”‚  Feature Registry                                       â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚\n",
    "â”‚  â”‚ Entity Types:                       â”‚               â”‚\n",
    "â”‚  â”‚ - customer                          â”‚               â”‚\n",
    "â”‚  â”‚ - product                           â”‚               â”‚\n",
    "â”‚  â”‚                                     â”‚               â”‚\n",
    "â”‚  â”‚ Features:                           â”‚               â”‚\n",
    "â”‚  â”‚ - customer.age                      â”‚               â”‚\n",
    "â”‚  â”‚ - customer.lifetime_value           â”‚               â”‚\n",
    "â”‚  â”‚ - product.avg_rating                â”‚               â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚\n",
    "â”‚                â”‚                                        â”‚\n",
    "â”‚                â–¼                                        â”‚\n",
    "â”‚  Dual Serving                                           â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚\n",
    "â”‚  â”‚ Online Serving   â”‚   â”‚ Offline (Batch)  â”‚          â”‚\n",
    "â”‚  â”‚ - Low latency    â”‚   â”‚ - Training data  â”‚          â”‚\n",
    "â”‚  â”‚ - Real-time      â”‚   â”‚ - BigQuery       â”‚          â”‚\n",
    "â”‚  â”‚ - Point lookup   â”‚   â”‚ - Historical     â”‚          â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  Benefits:                                              â”‚\n",
    "â”‚  - Feature reuse across models                         â”‚\n",
    "â”‚  - Consistent online/offline features                  â”‚\n",
    "â”‚  - Point-in-time correctness                           â”‚\n",
    "â”‚  - Feature versioning                                  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Store configuration\n",
    "\n",
    "def create_feature_store():\n",
    "    \"\"\"Configure Vertex AI Feature Store\"\"\"\n",
    "    featurestore_config = {\n",
    "        'name': 'customer_featurestore',\n",
    "        'online_serving_config': {\n",
    "            'fixed_node_count': 1  # Minimum for low traffic\n",
    "        },\n",
    "        'entity_types': [\n",
    "            {\n",
    "                'entity_type_id': 'customer',\n",
    "                'description': 'Customer entity',\n",
    "                'features': [\n",
    "                    {\n",
    "                        'feature_id': 'age',\n",
    "                        'value_type': 'INT64',\n",
    "                        'description': 'Customer age'\n",
    "                    },\n",
    "                    {\n",
    "                        'feature_id': 'lifetime_value',\n",
    "                        'value_type': 'DOUBLE',\n",
    "                        'description': 'Customer lifetime value'\n",
    "                    },\n",
    "                    {\n",
    "                        'feature_id': 'tenure_months',\n",
    "                        'value_type': 'INT64',\n",
    "                        'description': 'Months as customer'\n",
    "                    },\n",
    "                    {\n",
    "                        'feature_id': 'avg_monthly_spend',\n",
    "                        'value_type': 'DOUBLE',\n",
    "                        'description': 'Average monthly spend'\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    return featurestore_config\n",
    "\n",
    "featurestore = create_feature_store()\n",
    "print(\"Feature Store Configuration:\")\n",
    "print(json.dumps(featurestore, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate feature serving\n",
    "\n",
    "class FeatureStore:\n",
    "    \"\"\"Simulates Vertex AI Feature Store\"\"\"\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.features = {}\n",
    "    \n",
    "    def ingest_features(self, entity_type, entity_id, feature_values):\n",
    "        \"\"\"Ingest features for an entity\"\"\"\n",
    "        key = f\"{entity_type}:{entity_id}\"\n",
    "        self.features[key] = {\n",
    "            'entity_type': entity_type,\n",
    "            'entity_id': entity_id,\n",
    "            'features': feature_values,\n",
    "            'timestamp': datetime.now()\n",
    "        }\n",
    "        print(f\"âœ“ Ingested features for {entity_type}:{entity_id}\")\n",
    "    \n",
    "    def read_online(self, entity_type, entity_ids, feature_names):\n",
    "        \"\"\"\n",
    "        Read features for online serving (real-time)\n",
    "        Low latency point lookup\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for entity_id in entity_ids:\n",
    "            key = f\"{entity_type}:{entity_id}\"\n",
    "            if key in self.features:\n",
    "                feature_data = self.features[key]\n",
    "                result = {\n",
    "                    'entity_id': entity_id,\n",
    "                    'features': {\n",
    "                        name: feature_data['features'].get(name)\n",
    "                        for name in feature_names\n",
    "                    }\n",
    "                }\n",
    "                results.append(result)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def batch_read_offline(self, entity_type, feature_names, start_time, end_time):\n",
    "        \"\"\"\n",
    "        Read features for offline training (batch)\n",
    "        Point-in-time correctness\n",
    "        \"\"\"\n",
    "        print(f\"Reading features for training data...\")\n",
    "        print(f\"Entity type: {entity_type}\")\n",
    "        print(f\"Features: {feature_names}\")\n",
    "        print(f\"Time range: {start_time} to {end_time}\")\n",
    "        \n",
    "        # Simulate creating training dataset\n",
    "        num_rows = np.random.randint(1000, 10000)\n",
    "        print(f\"\\nâœ“ Generated {num_rows:,} training examples\")\n",
    "        \n",
    "        return f\"bq://my-project.dataset.training_features_{datetime.now().strftime('%Y%m%d')}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Feature Store\n",
    "fs = FeatureStore('customer_featurestore')\n",
    "\n",
    "# Ingest features\n",
    "print(\"Ingesting features:\\n\")\n",
    "fs.ingest_features('customer', 'C001', {\n",
    "    'age': 45,\n",
    "    'lifetime_value': 5000.0,\n",
    "    'tenure_months': 24,\n",
    "    'avg_monthly_spend': 75.5\n",
    "})\n",
    "\n",
    "fs.ingest_features('customer', 'C002', {\n",
    "    'age': 32,\n",
    "    'lifetime_value': 2500.0,\n",
    "    'tenure_months': 12,\n",
    "    'avg_monthly_spend': 45.0\n",
    "})\n",
    "\n",
    "# Online serving (real-time prediction)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"\\nOnline feature serving (for real-time predictions):\\n\")\n",
    "online_features = fs.read_online(\n",
    "    entity_type='customer',\n",
    "    entity_ids=['C001', 'C002'],\n",
    "    feature_names=['age', 'tenure_months', 'avg_monthly_spend']\n",
    ")\n",
    "\n",
    "for result in online_features:\n",
    "    print(f\"Customer {result['entity_id']}: {result['features']}\")\n",
    "\n",
    "# Offline serving (training data)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"\\nOffline feature serving (for training):\\n\")\n",
    "training_data = fs.batch_read_offline(\n",
    "    entity_type='customer',\n",
    "    feature_names=['age', 'lifetime_value', 'tenure_months', 'avg_monthly_spend'],\n",
    "    start_time=datetime(2024, 1, 1),\n",
    "    end_time=datetime(2024, 12, 31)\n",
    ")\n",
    "print(f\"Training data location: {training_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Pipeline Design\n",
    "\n",
    "Design a complete Vertex AI Pipeline for fraud detection:\n",
    "1. Ingest transaction data from BigQuery\n",
    "2. Read features from Feature Store\n",
    "3. Train model with hyperparameter tuning\n",
    "4. Evaluate on holdout set\n",
    "5. Deploy if precision > 0.90\n",
    "6. Update Feature Store with predictions\n",
    "\n",
    "Define all components and dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "def design_fraud_detection_pipeline():\n",
    "    \"\"\"\n",
    "    Design end-to-end fraud detection pipeline on Vertex AI\n",
    "    \n",
    "    Include:\n",
    "    - Component definitions\n",
    "    - Feature Store integration\n",
    "    - Conditional deployment\n",
    "    - Feedback loop\n",
    "    \n",
    "    Returns:\n",
    "        dict with complete pipeline specification\n",
    "    \"\"\"\n",
    "    # TODO: Design pipeline\n",
    "    pass\n",
    "\n",
    "# Test your pipeline\n",
    "# pipeline = design_fraud_detection_pipeline()\n",
    "# print(json.dumps(pipeline, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: AutoML vs Custom Training\n",
    "\n",
    "Compare AutoML Tables vs Custom Training for a classification task:\n",
    "- Dataset: 100K rows, 20 features\n",
    "- Budget: $200\n",
    "- Timeline: 1 week\n",
    "\n",
    "Analyze trade-offs and recommend an approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "def compare_training_approaches(dataset_size, budget, timeline_days):\n",
    "    \"\"\"\n",
    "    Compare AutoML vs Custom Training\n",
    "    \n",
    "    Consider:\n",
    "    - Cost (AutoML vs compute hours)\n",
    "    - Time to production\n",
    "    - Model performance\n",
    "    - Customization needs\n",
    "    - Maintenance\n",
    "    \n",
    "    Returns:\n",
    "        dict with comparison and recommendation\n",
    "    \"\"\"\n",
    "    # TODO: Analyze trade-offs\n",
    "    pass\n",
    "\n",
    "# Test your analysis\n",
    "# comparison = compare_training_approaches(\n",
    "#     dataset_size=100000,\n",
    "#     budget=200,\n",
    "#     timeline_days=7\n",
    "# )\n",
    "# print(json.dumps(comparison, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Feature Store Strategy\n",
    "\n",
    "Design a Feature Store for an e-commerce recommendation system:\n",
    "- User features (demographics, behavior)\n",
    "- Product features (category, price, ratings)\n",
    "- Interaction features (clicks, purchases)\n",
    "\n",
    "Define entity types, features, and serving strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here\n",
    "def design_recommendation_feature_store():\n",
    "    \"\"\"\n",
    "    Design Feature Store for recommendations\n",
    "    \n",
    "    Include:\n",
    "    - Entity types and features\n",
    "    - Online vs offline serving\n",
    "    - Update frequency\n",
    "    - Point-in-time joins\n",
    "    \n",
    "    Returns:\n",
    "        dict with Feature Store design\n",
    "    \"\"\"\n",
    "    # TODO: Design Feature Store\n",
    "    pass\n",
    "\n",
    "# Test your design\n",
    "# fs_design = design_recommendation_feature_store()\n",
    "# print(json.dumps(fs_design, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. **Vertex AI Workbench**: Unified development environment\n",
    "2. **AutoML**: Automated ML for Tables, Vision, and NLP\n",
    "3. **Custom Training**: Distributed training with full control\n",
    "4. **Vertex AI Pipelines**: Kubeflow-based ML workflows\n",
    "5. **Model Deployment**: Online and batch prediction endpoints\n",
    "6. **Feature Store**: Centralized feature management and serving\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- Vertex AI unifies all ML services in one platform\n",
    "- Use AutoML for quick POCs, custom training for production\n",
    "- Pipelines enable reproducible, automated workflows\n",
    "- Feature Store ensures consistency between training and serving\n",
    "- Always enable auto-scaling and monitoring for endpoints\n",
    "- Start with managed services, customize only when needed\n",
    "\n",
    "### Platform Comparison\n",
    "\n",
    "| Feature | AWS SageMaker | Azure ML | GCP Vertex AI |\n",
    "|---------|--------------|----------|---------------|\n",
    "| Unified Platform | Separate services | Azure ML Studio | **Vertex AI** |\n",
    "| AutoML | Autopilot | AutoML | **Tables, Vision, NLP** |\n",
    "| Feature Store | âœ“ | âœ“ (preview) | **âœ“ (integrated)** |\n",
    "| Pipelines | Step Functions | Pipelines | **Kubeflow** |\n",
    "| BigQuery Integration | - | - | **âœ“ Native** |\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "- Practice building end-to-end ML pipelines\n",
    "- Explore model monitoring and explainability\n",
    "- Experiment with distributed training\n",
    "- Implement MLOps best practices\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [Vertex AI Documentation](https://cloud.google.com/vertex-ai/docs)\n",
    "- [Vertex AI Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines)\n",
    "- [Feature Store Guide](https://cloud.google.com/vertex-ai/docs/featurestore)\n",
    "- [AutoML Tutorial](https://cloud.google.com/vertex-ai/docs/tutorials/tabular-automl)\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You've completed the Cloud ML Platforms module. You now understand how to leverage AWS SageMaker, Azure ML, and GCP Vertex AI for production ML deployments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
