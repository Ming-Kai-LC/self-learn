{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 07: Serverless ML Deployment\n",
    "\n",
    "**Difficulty**: ‚≠ê‚≠ê‚≠ê\n",
    "**Estimated Time**: 90 minutes\n",
    "**Prerequisites**: \n",
    "- [Module 00: Introduction to Cloud ML Services](00_introduction_to_cloud_ml_services.ipynb)\n",
    "- [Module 01: AWS SageMaker Basics](01_aws_sagemaker_basics.ipynb)\n",
    "- Basic understanding of REST APIs\n",
    "- Familiarity with Docker (recommended)\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Understand when serverless is appropriate for ML inference\n",
    "2. Deploy ML models using AWS Lambda, Azure Functions, and Google Cloud Functions\n",
    "3. Optimize cold start times and manage dependencies using Lambda layers\n",
    "4. Integrate API Gateway for production-ready endpoints\n",
    "5. Use container-based serverless for larger ML models\n",
    "6. Compare costs between serverless and dedicated endpoints\n",
    "7. Implement best practices for serverless ML deployment\n",
    "\n",
    "## What is Serverless ML?\n",
    "\n",
    "Serverless computing allows you to run code without managing servers. For ML inference, this means:\n",
    "\n",
    "**Advantages:**\n",
    "- ‚úÖ **Pay-per-use**: Only charged when functions execute\n",
    "- ‚úÖ **Auto-scaling**: Handles 1 or 10,000 requests automatically\n",
    "- ‚úÖ **Zero maintenance**: No server management required\n",
    "- ‚úÖ **Cost-effective**: Ideal for sporadic or low-volume predictions\n",
    "\n",
    "**Limitations:**\n",
    "- ‚ö†Ô∏è **Cold starts**: Initial latency when function hasn't run recently (100ms - 10s)\n",
    "- ‚ö†Ô∏è **Execution limits**: Timeouts (AWS Lambda: 15min max, typically use 30s-3min)\n",
    "- ‚ö†Ô∏è **Memory constraints**: Limited RAM (AWS Lambda: 128MB - 10GB)\n",
    "- ‚ö†Ô∏è **Package size limits**: Deployment package restrictions (50MB zipped, 250MB unzipped for Lambda)\n",
    "\n",
    "**When to Use Serverless for ML:**\n",
    "- Inference frequency: < 1000 requests/day or highly variable traffic\n",
    "- Model size: < 250MB (or use containers for up to 10GB)\n",
    "- Latency tolerance: Can accept 100ms - 1s additional cold start latency\n",
    "- Budget-conscious: Want to minimize costs for low-volume use cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Standard library imports\n",
    "import json\n",
    "import os\n",
    "import base64\n",
    "import time\n",
    "from datetime import datetime\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "\n",
    "# Data science libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "# Cloud SDKs (install only what you need for your chosen platform)\n",
    "# pip install boto3  # For AWS\n",
    "# pip install azure-functions  # For Azure\n",
    "# pip install google-cloud-functions  # For GCP\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "print(f\"Notebook executed on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Training a Lightweight Model for Serverless Deployment\n",
    "\n",
    "For serverless deployment, we want models that are:\n",
    "- **Small**: < 50MB ideally (< 250MB maximum for Lambda)\n",
    "- **Fast**: Inference in < 100ms\n",
    "- **Simple**: Minimal dependencies\n",
    "\n",
    "Let's train a simple Random Forest classifier that meets these criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load sample dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train a lightweight model\n",
    "# Using fewer trees and limited depth for smaller model size\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=10,  # Fewer trees = smaller model\n",
    "    max_depth=5,      # Limited depth = faster inference\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model\n",
    "train_score = model.score(X_train, y_train)\n",
    "test_score = model.score(X_test, y_test)\n",
    "\n",
    "print(f\"Model Training Complete\")\n",
    "print(f\"Train Accuracy: {train_score:.4f}\")\n",
    "print(f\"Test Accuracy: {test_score:.4f}\")\n",
    "\n",
    "# Save model to file\n",
    "model_path = 'iris_model.joblib'\n",
    "joblib.dump(model, model_path)\n",
    "\n",
    "# Check model size (important for serverless)\n",
    "model_size_mb = os.path.getsize(model_path) / (1024 * 1024)\n",
    "print(f\"\\nModel Size: {model_size_mb:.2f} MB\")\n",
    "\n",
    "if model_size_mb < 50:\n",
    "    print(\"‚úÖ Model size is optimal for serverless deployment\")\n",
    "elif model_size_mb < 250:\n",
    "    print(\"‚ö†Ô∏è Model size is acceptable but may need Lambda layers\")\n",
    "else:\n",
    "    print(\"‚ùå Model too large for standard Lambda, consider container-based deployment\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: AWS Lambda for ML Inference\n",
    "\n",
    "AWS Lambda is the most popular serverless platform. Let's create a Lambda function for ML inference.\n",
    "\n",
    "### 2.1: Lambda Function Handler\n",
    "\n",
    "The Lambda handler is the entry point for your function. Here's a production-ready handler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# This code shows what goes inside lambda_function.py\n",
    "# In actual deployment, this would be a separate file\n",
    "\n",
    "lambda_function_code = '''\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Global variable to cache model (persists between warm starts)\n",
    "model = None\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"Load model once and cache for subsequent invocations (warm starts)\"\"\"\n",
    "    global model\n",
    "    if model is None:\n",
    "        logger.info(\"Loading model (cold start)...\")\n",
    "        model = joblib.load(\"/opt/ml/model.joblib\")  # /opt is for Lambda layers\n",
    "        logger.info(\"Model loaded successfully\")\n",
    "    return model\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \"\"\"\n",
    "    AWS Lambda handler for ML inference\n",
    "    \n",
    "    Expected input format:\n",
    "    {\n",
    "        \"features\": [[5.1, 3.5, 1.4, 0.2]]\n",
    "    }\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load model (cached after first call)\n",
    "        clf = load_model()\n",
    "        \n",
    "        # Parse input\n",
    "        if isinstance(event, str):\n",
    "            event = json.loads(event)\n",
    "        \n",
    "        features = event.get('features')\n",
    "        if features is None:\n",
    "            return {\n",
    "                'statusCode': 400,\n",
    "                'body': json.dumps({'error': 'Missing features field'})\n",
    "            }\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        X = np.array(features)\n",
    "        \n",
    "        # Make prediction\n",
    "        predictions = clf.predict(X)\n",
    "        probabilities = clf.predict_proba(X)\n",
    "        \n",
    "        # Format response\n",
    "        response = {\n",
    "            'statusCode': 200,\n",
    "            'headers': {\n",
    "                'Content-Type': 'application/json',\n",
    "                'Access-Control-Allow-Origin': '*'  # For CORS\n",
    "            },\n",
    "            'body': json.dumps({\n",
    "                'predictions': predictions.tolist(),\n",
    "                'probabilities': probabilities.tolist()\n",
    "            })\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"Prediction successful: {predictions}\")\n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during prediction: {str(e)}\")\n",
    "        return {\n",
    "            'statusCode': 500,\n",
    "            'body': json.dumps({'error': str(e)})\n",
    "        }\n",
    "'''\n",
    "\n",
    "# Save to file for reference\n",
    "with open('lambda_function.py', 'w') as f:\n",
    "    f.write(lambda_function_code)\n",
    "\n",
    "print(\"‚úÖ Lambda function code created: lambda_function.py\")\n",
    "print(\"\\nKey features:\")\n",
    "print(\"- Model caching to avoid reloading on warm starts\")\n",
    "print(\"- Proper error handling and logging\")\n",
    "print(\"- CORS headers for web applications\")\n",
    "print(\"- Input validation\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2: Creating a Lambda Deployment Package\n",
    "\n",
    "Lambda requires all code and dependencies in a zip file. Let's create a deployment package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def create_lambda_package(function_file, model_file, output_zip='lambda_deployment.zip'):\n",
    "    \"\"\"\n",
    "    Create a Lambda deployment package with function code and model\n",
    "    \n",
    "    Note: For production, dependencies should be in a Lambda Layer\n",
    "    to keep deployment package small\n",
    "    \"\"\"\n",
    "    with zipfile.ZipFile(output_zip, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        # Add Lambda function\n",
    "        zipf.write(function_file, arcname='lambda_function.py')\n",
    "        print(f\"Added {function_file} to package\")\n",
    "        \n",
    "        # Add model file\n",
    "        zipf.write(model_file, arcname='model.joblib')\n",
    "        print(f\"Added {model_file} to package\")\n",
    "        \n",
    "        # In production, you'd also add:\n",
    "        # - requirements.txt dependencies (in a separate layer)\n",
    "        # - Any helper modules\n",
    "    \n",
    "    package_size = os.path.getsize(output_zip) / (1024 * 1024)\n",
    "    print(f\"\\n‚úÖ Deployment package created: {output_zip}\")\n",
    "    print(f\"Package size: {package_size:.2f} MB\")\n",
    "    \n",
    "    if package_size < 50:\n",
    "        print(\"‚úÖ Package size is optimal for direct upload\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Package is large, consider using S3 for upload\")\n",
    "    \n",
    "    return output_zip\n",
    "\n",
    "# Create deployment package\n",
    "package_path = create_lambda_package('lambda_function.py', 'iris_model.joblib')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3: Lambda Layers for Dependencies\n",
    "\n",
    "Lambda Layers allow you to separate dependencies from your function code. This has several benefits:\n",
    "- Faster deployments (don't re-upload dependencies each time)\n",
    "- Stay under the 50MB direct upload limit\n",
    "- Share dependencies across multiple functions\n",
    "\n",
    "**Creating a Lambda Layer (in production):**\n",
    "\n",
    "```bash\n",
    "# Structure for Python Lambda Layer\n",
    "mkdir -p layer/python/lib/python3.9/site-packages\n",
    "cd layer/python/lib/python3.9/site-packages\n",
    "\n",
    "# Install dependencies\n",
    "pip install numpy scikit-learn joblib -t .\n",
    "\n",
    "# Create layer zip\n",
    "cd ../../../..\n",
    "zip -r sklearn-layer.zip python/\n",
    "\n",
    "# Upload to AWS (using AWS CLI)\n",
    "aws lambda publish-layer-version \\\n",
    "    --layer-name sklearn-numpy \\\n",
    "    --zip-file fileb://sklearn-layer.zip \\\n",
    "    --compatible-runtimes python3.9\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Simulated Lambda Layer structure\n",
    "layer_structure = {\n",
    "    'layer_name': 'sklearn-numpy-layer',\n",
    "    'compatible_runtimes': ['python3.9', 'python3.10', 'python3.11'],\n",
    "    'size_mb': 45.3,\n",
    "    'packages': ['numpy', 'scikit-learn', 'joblib'],\n",
    "    'arn': 'arn:aws:lambda:us-east-1:123456789012:layer:sklearn-numpy-layer:1'\n",
    "}\n",
    "\n",
    "print(\"Lambda Layer Configuration:\")\n",
    "print(json.dumps(layer_structure, indent=2))\n",
    "print(\"\\nüí° Tip: AWS maintains public layers for popular libraries\")\n",
    "print(\"   Check: https://github.com/keithrozario/Klayers for ready-made layers\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4: Simulated Lambda Deployment with boto3\n",
    "\n",
    "Here's how you would deploy to AWS Lambda using boto3 (simulated since we don't have AWS credentials):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Simulated boto3 Lambda deployment\n",
    "# In production, you'd use actual boto3 client\n",
    "\n",
    "class SimulatedLambdaClient:\n",
    "    \"\"\"Simulates AWS Lambda API calls for educational purposes\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.functions = {}\n",
    "    \n",
    "    def create_function(self, FunctionName, Runtime, Role, Handler, Code, \n",
    "                       Timeout=30, MemorySize=512, Environment=None, Layers=None):\n",
    "        \"\"\"Simulate Lambda function creation\"\"\"\n",
    "        function_config = {\n",
    "            'FunctionName': FunctionName,\n",
    "            'FunctionArn': f'arn:aws:lambda:us-east-1:123456789012:function:{FunctionName}',\n",
    "            'Runtime': Runtime,\n",
    "            'Role': Role,\n",
    "            'Handler': Handler,\n",
    "            'CodeSize': len(Code.get('ZipFile', b'')),\n",
    "            'Timeout': Timeout,\n",
    "            'MemorySize': MemorySize,\n",
    "            'LastModified': datetime.now().isoformat(),\n",
    "            'State': 'Active',\n",
    "            'Layers': Layers or []\n",
    "        }\n",
    "        self.functions[FunctionName] = function_config\n",
    "        return function_config\n",
    "    \n",
    "    def invoke(self, FunctionName, Payload):\n",
    "        \"\"\"Simulate Lambda invocation\"\"\"\n",
    "        if FunctionName not in self.functions:\n",
    "            raise Exception(f\"Function {FunctionName} not found\")\n",
    "        \n",
    "        # Simulate cold start on first invocation\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Simulate inference (using our actual model)\n",
    "        event = json.loads(Payload)\n",
    "        features = np.array(event['features'])\n",
    "        predictions = model.predict(features)\n",
    "        probabilities = model.predict_proba(features)\n",
    "        \n",
    "        response_payload = {\n",
    "            'statusCode': 200,\n",
    "            'body': json.dumps({\n",
    "                'predictions': predictions.tolist(),\n",
    "                'probabilities': probabilities.tolist()\n",
    "            })\n",
    "        }\n",
    "        \n",
    "        execution_time = (time.time() - start_time) * 1000  # ms\n",
    "        \n",
    "        return {\n",
    "            'StatusCode': 200,\n",
    "            'ExecutedVersion': '$LATEST',\n",
    "            'Payload': BytesIO(json.dumps(response_payload).encode()),\n",
    "            'ExecutionTime': execution_time\n",
    "        }\n",
    "\n",
    "# Initialize simulated client\n",
    "lambda_client = SimulatedLambdaClient()\n",
    "\n",
    "# Create Lambda function\n",
    "with open(package_path, 'rb') as f:\n",
    "    zip_content = f.read()\n",
    "\n",
    "function_response = lambda_client.create_function(\n",
    "    FunctionName='iris-classifier',\n",
    "    Runtime='python3.9',\n",
    "    Role='arn:aws:iam::123456789012:role/lambda-execution-role',\n",
    "    Handler='lambda_function.lambda_handler',\n",
    "    Code={'ZipFile': zip_content},\n",
    "    Timeout=30,\n",
    "    MemorySize=512,\n",
    "    Layers=[\n",
    "        'arn:aws:lambda:us-east-1:123456789012:layer:sklearn-numpy-layer:1'\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Lambda Function Created (Simulated)\")\n",
    "print(f\"Function Name: {function_response['FunctionName']}\")\n",
    "print(f\"Function ARN: {function_response['FunctionArn']}\")\n",
    "print(f\"Runtime: {function_response['Runtime']}\")\n",
    "print(f\"Memory: {function_response['MemorySize']} MB\")\n",
    "print(f\"Timeout: {function_response['Timeout']} seconds\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5: Testing Lambda Invocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test Lambda invocation\n",
    "test_payload = {\n",
    "    'features': [[5.1, 3.5, 1.4, 0.2]]  # Sample iris flower\n",
    "}\n",
    "\n",
    "response = lambda_client.invoke(\n",
    "    FunctionName='iris-classifier',\n",
    "    Payload=json.dumps(test_payload)\n",
    ")\n",
    "\n",
    "# Parse response\n",
    "result = json.loads(response['Payload'].read())\n",
    "body = json.loads(result['body'])\n",
    "\n",
    "print(\"Lambda Invocation Result:\")\n",
    "print(f\"Status Code: {response['StatusCode']}\")\n",
    "print(f\"Execution Time: {response['ExecutionTime']:.2f} ms\")\n",
    "print(f\"\\nPrediction: {body['predictions']}\")\n",
    "print(f\"Probabilities: {np.array(body['probabilities'])[0]}\")\n",
    "print(f\"\\nPredicted Class: {iris.target_names[body['predictions'][0]]}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: API Gateway Integration\n",
    "\n",
    "API Gateway creates a REST API endpoint for your Lambda function, enabling:\n",
    "- HTTPS endpoints with custom domains\n",
    "- Request/response transformation\n",
    "- Authentication and authorization\n",
    "- Rate limiting and throttling\n",
    "- API keys and usage plans\n",
    "\n",
    "### 3.1: Terraform Configuration for API Gateway + Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Terraform configuration for complete serverless ML API\n",
    "terraform_config = '''\n",
    "# main.tf - Complete serverless ML deployment\n",
    "\n",
    "terraform {\n",
    "  required_providers {\n",
    "    aws = {\n",
    "      source  = \"hashicorp/aws\"\n",
    "      version = \"~> 5.0\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "provider \"aws\" {\n",
    "  region = var.aws_region\n",
    "}\n",
    "\n",
    "# Lambda Function\n",
    "resource \"aws_lambda_function\" \"ml_inference\" {\n",
    "  filename         = \"lambda_deployment.zip\"\n",
    "  function_name    = \"iris-classifier\"\n",
    "  role            = aws_iam_role.lambda_execution.arn\n",
    "  handler         = \"lambda_function.lambda_handler\"\n",
    "  runtime         = \"python3.9\"\n",
    "  timeout         = 30\n",
    "  memory_size     = 512\n",
    "  \n",
    "  # Use Lambda layers for dependencies\n",
    "  layers = [var.sklearn_layer_arn]\n",
    "  \n",
    "  environment {\n",
    "    variables = {\n",
    "      MODEL_PATH = \"/opt/ml/model.joblib\"\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  # Enable function URL (simpler alternative to API Gateway)\n",
    "  # function_url_enabled = true\n",
    "}\n",
    "\n",
    "# IAM Role for Lambda\n",
    "resource \"aws_iam_role\" \"lambda_execution\" {\n",
    "  name = \"iris-classifier-lambda-role\"\n",
    "\n",
    "  assume_role_policy = jsonencode({\n",
    "    Version = \"2012-10-17\"\n",
    "    Statement = [{\n",
    "      Action = \"sts:AssumeRole\"\n",
    "      Effect = \"Allow\"\n",
    "      Principal = {\n",
    "        Service = \"lambda.amazonaws.com\"\n",
    "      }\n",
    "    }]\n",
    "  })\n",
    "}\n",
    "\n",
    "# Attach basic Lambda execution policy\n",
    "resource \"aws_iam_role_policy_attachment\" \"lambda_basic\" {\n",
    "  role       = aws_iam_role.lambda_execution.name\n",
    "  policy_arn = \"arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole\"\n",
    "}\n",
    "\n",
    "# API Gateway REST API\n",
    "resource \"aws_api_gateway_rest_api\" \"ml_api\" {\n",
    "  name        = \"iris-classifier-api\"\n",
    "  description = \"ML Inference API for Iris Classification\"\n",
    "}\n",
    "\n",
    "# API Gateway Resource (/predict)\n",
    "resource \"aws_api_gateway_resource\" \"predict\" {\n",
    "  rest_api_id = aws_api_gateway_rest_api.ml_api.id\n",
    "  parent_id   = aws_api_gateway_rest_api.ml_api.root_resource_id\n",
    "  path_part   = \"predict\"\n",
    "}\n",
    "\n",
    "# POST method\n",
    "resource \"aws_api_gateway_method\" \"predict_post\" {\n",
    "  rest_api_id   = aws_api_gateway_rest_api.ml_api.id\n",
    "  resource_id   = aws_api_gateway_resource.predict.id\n",
    "  http_method   = \"POST\"\n",
    "  authorization = \"NONE\"  # Consider using API keys or Cognito in production\n",
    "}\n",
    "\n",
    "# Lambda integration\n",
    "resource \"aws_api_gateway_integration\" \"lambda\" {\n",
    "  rest_api_id = aws_api_gateway_rest_api.ml_api.id\n",
    "  resource_id = aws_api_gateway_resource.predict.id\n",
    "  http_method = aws_api_gateway_method.predict_post.http_method\n",
    "\n",
    "  integration_http_method = \"POST\"\n",
    "  type                    = \"AWS_PROXY\"  # Lambda proxy integration\n",
    "  uri                     = aws_lambda_function.ml_inference.invoke_arn\n",
    "}\n",
    "\n",
    "# Lambda permission for API Gateway\n",
    "resource \"aws_lambda_permission\" \"api_gateway\" {\n",
    "  statement_id  = \"AllowAPIGatewayInvoke\"\n",
    "  action        = \"lambda:InvokeFunction\"\n",
    "  function_name = aws_lambda_function.ml_inference.function_name\n",
    "  principal     = \"apigateway.amazonaws.com\"\n",
    "  source_arn    = \"${aws_api_gateway_rest_api.ml_api.execution_arn}/*/*\"\n",
    "}\n",
    "\n",
    "# API Gateway Deployment\n",
    "resource \"aws_api_gateway_deployment\" \"prod\" {\n",
    "  depends_on = [\n",
    "    aws_api_gateway_integration.lambda\n",
    "  ]\n",
    "\n",
    "  rest_api_id = aws_api_gateway_rest_api.ml_api.id\n",
    "  stage_name  = \"prod\"\n",
    "}\n",
    "\n",
    "# Outputs\n",
    "output \"api_endpoint\" {\n",
    "  value = \"${aws_api_gateway_deployment.prod.invoke_url}/predict\"\n",
    "  description = \"API Gateway endpoint URL\"\n",
    "}\n",
    "\n",
    "output \"lambda_arn\" {\n",
    "  value = aws_lambda_function.ml_inference.arn\n",
    "}\n",
    "'''\n",
    "\n",
    "# Save Terraform configuration\n",
    "with open('lambda_api_gateway.tf', 'w') as f:\n",
    "    f.write(terraform_config)\n",
    "\n",
    "print(\"‚úÖ Terraform configuration saved: lambda_api_gateway.tf\")\n",
    "print(\"\\nTo deploy:\")\n",
    "print(\"  terraform init\")\n",
    "print(\"  terraform plan\")\n",
    "print(\"  terraform apply\")\n",
    "print(\"\\n‚ö†Ô∏è FREE TIER: AWS offers 1M free Lambda requests/month\")\n",
    "print(\"   API Gateway: 1M free API calls/month (12 months free tier)\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Container-Based Lambda for Larger Models\n",
    "\n",
    "For models > 250MB, use container-based Lambda:\n",
    "- Package size up to 10GB\n",
    "- Full control over runtime environment\n",
    "- Use any base image (must implement Lambda Runtime API)\n",
    "\n",
    "### 4.1: Dockerfile for Lambda Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Dockerfile for containerized Lambda\n",
    "dockerfile_content = '''\n",
    "# Use AWS Lambda Python base image\n",
    "FROM public.ecr.aws/lambda/python:3.9\n",
    "\n",
    "# Install system dependencies if needed\n",
    "RUN yum install -y gcc-c++ && yum clean all\n",
    "\n",
    "# Copy requirements and install Python dependencies\n",
    "COPY requirements.txt ${LAMBDA_TASK_ROOT}/\n",
    "RUN pip install --no-cache-dir -r ${LAMBDA_TASK_ROOT}/requirements.txt\n",
    "\n",
    "# Copy model and function code\n",
    "COPY model.joblib ${LAMBDA_TASK_ROOT}/\n",
    "COPY lambda_function.py ${LAMBDA_TASK_ROOT}/\n",
    "\n",
    "# Set the CMD to your handler\n",
    "CMD [\"lambda_function.lambda_handler\"]\n",
    "'''\n",
    "\n",
    "# Save Dockerfile\n",
    "with open('Dockerfile.lambda', 'w') as f:\n",
    "    f.write(dockerfile_content)\n",
    "\n",
    "print(\"‚úÖ Dockerfile created: Dockerfile.lambda\")\n",
    "print(\"\\nTo build and deploy:\")\n",
    "print(\"  1. Build: docker build -t iris-classifier -f Dockerfile.lambda .\")\n",
    "print(\"  2. Tag: docker tag iris-classifier:latest <account-id>.dkr.ecr.<region>.amazonaws.com/iris-classifier:latest\")\n",
    "print(\"  3. Push to ECR: docker push <account-id>.dkr.ecr.<region>.amazonaws.com/iris-classifier:latest\")\n",
    "print(\"  4. Create Lambda from container image in AWS Console or Terraform\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Azure Functions for ML\n",
    "\n",
    "Azure Functions is Microsoft's serverless platform. Very similar to Lambda but integrates with Azure ecosystem.\n",
    "\n",
    "### 5.1: Azure Function Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Azure Functions code (in __init__.py)\n",
    "azure_function_code = '''\n",
    "import azure.functions as func\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "# Load model once at startup\n",
    "model = joblib.load('model.joblib')\n",
    "\n",
    "def main(req: func.HttpRequest) -> func.HttpResponse:\n",
    "    \"\"\"\n",
    "    Azure Function for ML inference\n",
    "    \"\"\"\n",
    "    logging.info('Python HTTP trigger function processed a request.')\n",
    "    \n",
    "    try:\n",
    "        # Parse request body\n",
    "        req_body = req.get_json()\n",
    "        features = req_body.get('features')\n",
    "        \n",
    "        if features is None:\n",
    "            return func.HttpResponse(\n",
    "                json.dumps({'error': 'Missing features field'}),\n",
    "                status_code=400\n",
    "            )\n",
    "        \n",
    "        # Make prediction\n",
    "        X = np.array(features)\n",
    "        predictions = model.predict(X)\n",
    "        probabilities = model.predict_proba(X)\n",
    "        \n",
    "        # Return response\n",
    "        response = {\n",
    "            'predictions': predictions.tolist(),\n",
    "            'probabilities': probabilities.tolist()\n",
    "        }\n",
    "        \n",
    "        return func.HttpResponse(\n",
    "            json.dumps(response),\n",
    "            mimetype='application/json',\n",
    "            status_code=200\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f'Error: {str(e)}')\n",
    "        return func.HttpResponse(\n",
    "            json.dumps({'error': str(e)}),\n",
    "            status_code=500\n",
    "        )\n",
    "'''\n",
    "\n",
    "# Azure function.json configuration\n",
    "azure_function_config = {\n",
    "    \"scriptFile\": \"__init__.py\",\n",
    "    \"bindings\": [\n",
    "        {\n",
    "            \"authLevel\": \"function\",\n",
    "            \"type\": \"httpTrigger\",\n",
    "            \"direction\": \"in\",\n",
    "            \"name\": \"req\",\n",
    "            \"methods\": [\"post\"]\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"http\",\n",
    "            \"direction\": \"out\",\n",
    "            \"name\": \"$return\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"Azure Functions Configuration:\")\n",
    "print(json.dumps(azure_function_config, indent=2))\n",
    "print(\"\\n‚ö†Ô∏è FREE TIER: Azure offers 1M free executions/month\")\n",
    "print(\"   Plus 400,000 GB-seconds of compute\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Google Cloud Functions\n",
    "\n",
    "Google Cloud Functions (2nd gen) uses Cloud Run underneath and supports up to 16GB memory.\n",
    "\n",
    "### 6.1: Google Cloud Function Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Google Cloud Functions code (main.py)\n",
    "gcp_function_code = '''\n",
    "import functions_framework\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "# Load model at startup\n",
    "model = joblib.load('model.joblib')\n",
    "\n",
    "@functions_framework.http\n",
    "def predict(request):\n",
    "    \"\"\"\n",
    "    HTTP Cloud Function for ML inference\n",
    "    \"\"\"\n",
    "    # Set CORS headers for web applications\n",
    "    if request.method == 'OPTIONS':\n",
    "        headers = {\n",
    "            'Access-Control-Allow-Origin': '*',\n",
    "            'Access-Control-Allow-Methods': 'POST',\n",
    "            'Access-Control-Allow-Headers': 'Content-Type',\n",
    "        }\n",
    "        return ('', 204, headers)\n",
    "    \n",
    "    headers = {'Access-Control-Allow-Origin': '*'}\n",
    "    \n",
    "    try:\n",
    "        request_json = request.get_json(silent=True)\n",
    "        \n",
    "        if request_json is None or 'features' not in request_json:\n",
    "            return (json.dumps({'error': 'Missing features field'}), 400, headers)\n",
    "        \n",
    "        features = request_json['features']\n",
    "        X = np.array(features)\n",
    "        \n",
    "        predictions = model.predict(X)\n",
    "        probabilities = model.predict_proba(X)\n",
    "        \n",
    "        response = {\n",
    "            'predictions': predictions.tolist(),\n",
    "            'probabilities': probabilities.tolist()\n",
    "        }\n",
    "        \n",
    "        return (json.dumps(response), 200, headers)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return (json.dumps({'error': str(e)}), 500, headers)\n",
    "'''\n",
    "\n",
    "print(\"Google Cloud Functions Code:\")\n",
    "print(gcp_function_code[:500] + \"...\")\n",
    "print(\"\\n‚ö†Ô∏è FREE TIER: GCP offers 2M invocations/month\")\n",
    "print(\"   Plus 400,000 GB-seconds, 200,000 GHz-seconds compute\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Cold Start Optimization\n",
    "\n",
    "Cold starts are the biggest challenge with serverless ML. Here are optimization strategies:\n",
    "\n",
    "### 7.1: Measuring Cold Starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def simulate_cold_starts(num_invocations=10, cold_start_probability=0.3):\n",
    "    \"\"\"\n",
    "    Simulate Lambda invocations with cold starts\n",
    "    \n",
    "    Cold starts occur when:\n",
    "    - Function hasn't been invoked recently (5-15 min)\n",
    "    - Scaling up requires new container instances\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i in range(num_invocations):\n",
    "        is_cold_start = np.random.random() < cold_start_probability\n",
    "        \n",
    "        if is_cold_start:\n",
    "            # Cold start: container init + model loading + inference\n",
    "            container_init = np.random.uniform(100, 500)  # ms\n",
    "            model_loading = np.random.uniform(200, 1000)  # ms\n",
    "            inference = np.random.uniform(10, 50)  # ms\n",
    "            total = container_init + model_loading + inference\n",
    "        else:\n",
    "            # Warm start: only inference time\n",
    "            inference = np.random.uniform(10, 50)  # ms\n",
    "            total = inference\n",
    "        \n",
    "        results.append({\n",
    "            'invocation': i + 1,\n",
    "            'cold_start': is_cold_start,\n",
    "            'latency_ms': total\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Simulate invocations\n",
    "latency_data = simulate_cold_starts(num_invocations=100)\n",
    "\n",
    "# Analyze results\n",
    "cold_starts = latency_data[latency_data['cold_start']]\n",
    "warm_starts = latency_data[~latency_data['cold_start']]\n",
    "\n",
    "print(\"Cold Start Analysis:\")\n",
    "print(f\"Total invocations: {len(latency_data)}\")\n",
    "print(f\"Cold starts: {len(cold_starts)} ({len(cold_starts)/len(latency_data)*100:.1f}%)\")\n",
    "print(f\"Warm starts: {len(warm_starts)} ({len(warm_starts)/len(latency_data)*100:.1f}%)\")\n",
    "print(f\"\\nLatency Statistics:\")\n",
    "print(f\"Cold start latency: {cold_starts['latency_ms'].mean():.1f} ms (avg)\")\n",
    "print(f\"Warm start latency: {warm_starts['latency_ms'].mean():.1f} ms (avg)\")\n",
    "print(f\"P95 cold start: {cold_starts['latency_ms'].quantile(0.95):.1f} ms\")\n",
    "print(f\"P95 warm start: {warm_starts['latency_ms'].quantile(0.95):.1f} ms\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Latency over time\n",
    "axes[0].scatter(latency_data[latency_data['cold_start']]['invocation'],\n",
    "                latency_data[latency_data['cold_start']]['latency_ms'],\n",
    "                color='red', label='Cold Start', alpha=0.6, s=50)\n",
    "axes[0].scatter(latency_data[~latency_data['cold_start']]['invocation'],\n",
    "                latency_data[~latency_data['cold_start']]['latency_ms'],\n",
    "                color='green', label='Warm Start', alpha=0.6, s=50)\n",
    "axes[0].set_xlabel('Invocation Number')\n",
    "axes[0].set_ylabel('Latency (ms)')\n",
    "axes[0].set_title('Lambda Invocation Latency')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Latency distribution\n",
    "axes[1].hist(cold_starts['latency_ms'], bins=20, alpha=0.6, color='red', label='Cold Start')\n",
    "axes[1].hist(warm_starts['latency_ms'], bins=20, alpha=0.6, color='green', label='Warm Start')\n",
    "axes[1].set_xlabel('Latency (ms)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Latency Distribution')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2: Cold Start Optimization Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "cold_start_optimizations = [\n",
    "    {\n",
    "        'technique': 'Provisioned Concurrency',\n",
    "        'description': 'Keep N instances warm at all times',\n",
    "        'cost_impact': 'High - pay for idle instances',\n",
    "        'latency_improvement': '95% reduction in cold starts',\n",
    "        'use_case': 'Production APIs with consistent traffic',\n",
    "        'aws_example': '''\n",
    "aws lambda put-provisioned-concurrency-config \\\n",
    "    --function-name iris-classifier \\\n",
    "    --provisioned-concurrent-executions 2\n",
    "'''\n",
    "    },\n",
    "    {\n",
    "        'technique': 'Reduce Package Size',\n",
    "        'description': 'Use Lambda layers, remove unused dependencies',\n",
    "        'cost_impact': 'None',\n",
    "        'latency_improvement': '30-50% faster cold starts',\n",
    "        'use_case': 'All serverless deployments',\n",
    "        'example': '''\n",
    "# Instead of full scikit-learn, use specific modules\n",
    "from sklearn.ensemble import RandomForestClassifier  # Specific import\n",
    "# vs\n",
    "import sklearn  # Imports everything\n",
    "'''\n",
    "    },\n",
    "    {\n",
    "        'technique': 'Lazy Loading',\n",
    "        'description': 'Load model only when needed, cache in global scope',\n",
    "        'cost_impact': 'None',\n",
    "        'latency_improvement': 'Faster cold starts, same warm performance',\n",
    "        'use_case': 'Functions with multiple code paths',\n",
    "        'example': '''\n",
    "model = None  # Global variable\n",
    "\n",
    "def load_model():\n",
    "    global model\n",
    "    if model is None:  # Load only once\n",
    "        model = joblib.load('model.joblib')\n",
    "    return model\n",
    "'''\n",
    "    },\n",
    "    {\n",
    "        'technique': 'Use Smaller Models',\n",
    "        'description': 'Quantize, prune, or use simpler architectures',\n",
    "        'cost_impact': 'None',\n",
    "        'latency_improvement': 'Faster loading and inference',\n",
    "        'use_case': 'When model complexity isn\\'t critical',\n",
    "        'example': '''\n",
    "# Use fewer trees/parameters\n",
    "model = RandomForestClassifier(n_estimators=10, max_depth=5)\n",
    "# vs\n",
    "model = RandomForestClassifier(n_estimators=100, max_depth=None)\n",
    "'''\n",
    "    },\n",
    "    {\n",
    "        'technique': 'Warm-up Pings',\n",
    "        'description': 'Scheduled CloudWatch Events to keep function warm',\n",
    "        'cost_impact': 'Low - minimal invocation costs',\n",
    "        'latency_improvement': 'Reduces cold start frequency',\n",
    "        'use_case': 'Predictable traffic patterns',\n",
    "        'aws_example': '''\n",
    "# CloudWatch Event rule (every 5 minutes)\n",
    "aws events put-rule --schedule-expression \"rate(5 minutes)\" \\\n",
    "    --name WarmupLambda\n",
    "'''\n",
    "    },\n",
    "    {\n",
    "        'technique': 'Increase Memory',\n",
    "        'description': 'More memory = more CPU = faster initialization',\n",
    "        'cost_impact': 'Medium - higher per-invocation cost',\n",
    "        'latency_improvement': '20-40% faster for compute-heavy tasks',\n",
    "        'use_case': 'When initialization is CPU-bound',\n",
    "        'example': 'Set Lambda memory to 1024MB or 2048MB instead of 512MB'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Display as DataFrame\n",
    "optimization_df = pd.DataFrame(cold_start_optimizations)\n",
    "print(\"Cold Start Optimization Strategies:\\n\")\n",
    "for idx, row in optimization_df.iterrows():\n",
    "    print(f\"\\n{idx+1}. {row['technique']}\")\n",
    "    print(f\"   Description: {row['description']}\")\n",
    "    print(f\"   Cost Impact: {row['cost_impact']}\")\n",
    "    print(f\"   Improvement: {row['latency_improvement']}\")\n",
    "    print(f\"   Best For: {row['use_case']}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Cost Comparison - Serverless vs Dedicated Endpoints\n",
    "\n",
    "When should you use serverless vs dedicated endpoints? Let's compare costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def calculate_serverless_cost(requests_per_month, avg_duration_ms=500, memory_mb=512):\n",
    "    \"\"\"\n",
    "    Calculate AWS Lambda costs\n",
    "    \n",
    "    Pricing (as of 2024):\n",
    "    - $0.20 per 1M requests\n",
    "    - $0.0000166667 per GB-second\n",
    "    - Free tier: 1M requests + 400,000 GB-seconds/month\n",
    "    \"\"\"\n",
    "    # Request charges\n",
    "    free_requests = 1_000_000\n",
    "    billable_requests = max(0, requests_per_month - free_requests)\n",
    "    request_cost = (billable_requests / 1_000_000) * 0.20\n",
    "    \n",
    "    # Compute charges\n",
    "    gb_seconds = (memory_mb / 1024) * (avg_duration_ms / 1000) * requests_per_month\n",
    "    free_gb_seconds = 400_000\n",
    "    billable_gb_seconds = max(0, gb_seconds - free_gb_seconds)\n",
    "    compute_cost = billable_gb_seconds * 0.0000166667\n",
    "    \n",
    "    total_cost = request_cost + compute_cost\n",
    "    \n",
    "    return {\n",
    "        'request_cost': request_cost,\n",
    "        'compute_cost': compute_cost,\n",
    "        'total_cost': total_cost,\n",
    "        'free_tier_savings': min(free_requests * 0.20 / 1_000_000 + \n",
    "                                free_gb_seconds * 0.0000166667, total_cost)\n",
    "    }\n",
    "\n",
    "def calculate_endpoint_cost(instance_type='ml.t3.medium', hours_per_month=730):\n",
    "    \"\"\"\n",
    "    Calculate SageMaker endpoint costs\n",
    "    \n",
    "    Pricing examples:\n",
    "    - ml.t3.medium: $0.065/hour\n",
    "    - ml.m5.large: $0.134/hour\n",
    "    - ml.c5.xlarge: $0.238/hour\n",
    "    \"\"\"\n",
    "    pricing = {\n",
    "        'ml.t3.medium': 0.065,\n",
    "        'ml.m5.large': 0.134,\n",
    "        'ml.c5.xlarge': 0.238\n",
    "    }\n",
    "    \n",
    "    hourly_rate = pricing.get(instance_type, 0.065)\n",
    "    total_cost = hourly_rate * hours_per_month\n",
    "    \n",
    "    return total_cost\n",
    "\n",
    "# Compare different traffic levels\n",
    "traffic_scenarios = [\n",
    "    {'name': 'Low Traffic', 'requests_per_month': 10_000},\n",
    "    {'name': 'Medium Traffic', 'requests_per_month': 500_000},\n",
    "    {'name': 'High Traffic', 'requests_per_month': 5_000_000},\n",
    "    {'name': 'Very High Traffic', 'requests_per_month': 50_000_000}\n",
    "]\n",
    "\n",
    "comparison_results = []\n",
    "\n",
    "for scenario in traffic_scenarios:\n",
    "    serverless = calculate_serverless_cost(scenario['requests_per_month'])\n",
    "    endpoint = calculate_endpoint_cost('ml.t3.medium')\n",
    "    \n",
    "    comparison_results.append({\n",
    "        'Scenario': scenario['name'],\n",
    "        'Requests/Month': f\"{scenario['requests_per_month']:,}\",\n",
    "        'Lambda Cost': f\"${serverless['total_cost']:.2f}\",\n",
    "        'SageMaker Endpoint': f\"${endpoint:.2f}\",\n",
    "        'Cheaper Option': 'Lambda' if serverless['total_cost'] < endpoint else 'Endpoint',\n",
    "        'Savings': f\"${abs(serverless['total_cost'] - endpoint):.2f}\"\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_results)\n",
    "print(\"Cost Comparison: Serverless vs Dedicated Endpoint\\n\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"\\nüí° Key Insights:\")\n",
    "print(\"   - Serverless is cheaper for < 1M requests/month\")\n",
    "print(\"   - Dedicated endpoints become cost-effective at high volumes\")\n",
    "print(\"   - Consider latency requirements and traffic predictability\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. **Serverless ML Fundamentals**\n",
    "   - When to use serverless for ML inference\n",
    "   - Advantages and limitations\n",
    "   - Cost-effectiveness for low-volume traffic\n",
    "\n",
    "2. **AWS Lambda Deployment**\n",
    "   - Creating Lambda functions with ML models\n",
    "   - Using Lambda layers for dependencies\n",
    "   - Deployment packages and size limits\n",
    "   - Container-based Lambda for larger models\n",
    "\n",
    "3. **API Gateway Integration**\n",
    "   - Creating production-ready REST APIs\n",
    "   - Terraform infrastructure as code\n",
    "   - Authentication and rate limiting\n",
    "\n",
    "4. **Multi-Cloud Serverless**\n",
    "   - Azure Functions for ML\n",
    "   - Google Cloud Functions\n",
    "   - Platform comparison\n",
    "\n",
    "5. **Cold Start Optimization**\n",
    "   - Measuring and analyzing cold starts\n",
    "   - Provisioned concurrency\n",
    "   - Package size reduction\n",
    "   - Lazy loading and warm-up strategies\n",
    "\n",
    "6. **Cost Analysis**\n",
    "   - Serverless vs dedicated endpoint pricing\n",
    "   - Free tier maximization\n",
    "   - Traffic-based decision making\n",
    "\n",
    "### When to Use Serverless for ML:\n",
    "‚úÖ Low or variable traffic (< 1M requests/month)  \n",
    "‚úÖ Cost optimization is critical  \n",
    "‚úÖ Can tolerate cold start latency  \n",
    "‚úÖ Model size < 10GB  \n",
    "‚úÖ Simple inference (no complex preprocessing)  \n",
    "\n",
    "### When to Use Dedicated Endpoints:\n",
    "‚úÖ High, consistent traffic  \n",
    "‚úÖ Strict latency requirements (< 100ms)  \n",
    "‚úÖ Large models requiring GPU  \n",
    "‚úÖ Complex preprocessing pipelines  \n",
    "‚úÖ Need for auto-scaling with no cold starts  \n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **[Module 08: Cost Optimization Strategies](08_cost_optimization_strategies.ipynb)**: Deep dive into cloud cost management\n",
    "- **[Module 09: Multi-Cloud ML Considerations](09_multi_cloud_ml_considerations.ipynb)**: Cross-platform ML deployment\n",
    "- **Practice**: Deploy your own model to Lambda using free tier\n",
    "- **Explore**: Provisioned concurrency and edge deployment (Lambda@Edge)\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [AWS Lambda Documentation](https://docs.aws.amazon.com/lambda/)\n",
    "- [Serverless Framework](https://www.serverless.com/) - Multi-cloud deployment tool\n",
    "- [AWS Lambda Powertools](https://awslabs.github.io/aws-lambda-powertools-python/) - Best practices utilities\n",
    "- [Azure Functions ML Tutorial](https://learn.microsoft.com/en-us/azure/azure-functions/)\n",
    "- [Google Cloud Functions Python](https://cloud.google.com/functions/docs/create-deploy-http-python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Model Size Optimization ‚≠ê\n",
    "\n",
    "Train two versions of a model:\n",
    "1. Full model with max performance\n",
    "2. Lightweight model optimized for serverless (< 10MB)\n",
    "\n",
    "Compare:\n",
    "- Model sizes\n",
    "- Accuracy differences\n",
    "- Loading times\n",
    "\n",
    "**Hint**: Use fewer estimators, lower max_depth, or try a simpler algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Your code here\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Lambda Handler with Validation ‚≠ê‚≠ê\n",
    "\n",
    "Enhance the Lambda handler to include:\n",
    "1. Input validation (check feature count, data types)\n",
    "2. Response time logging\n",
    "3. Error handling for malformed requests\n",
    "4. Return confidence scores only if above threshold\n",
    "\n",
    "Test with various inputs including edge cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Your code here\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Cost Analysis for Your Use Case ‚≠ê‚≠ê\n",
    "\n",
    "Create a cost calculator that:\n",
    "1. Takes your expected monthly requests as input\n",
    "2. Calculates costs for Lambda, Azure Functions, and Google Cloud Functions\n",
    "3. Calculates equivalent SageMaker/Azure ML/Vertex AI endpoint costs\n",
    "4. Recommends the most cost-effective option\n",
    "5. Shows cost at different traffic levels (plot)\n",
    "\n",
    "Consider:\n",
    "- Free tier benefits\n",
    "- Traffic variability\n",
    "- Geographic region pricing differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Your code here\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Cold Start Mitigation Strategy ‚≠ê‚≠ê‚≠ê\n",
    "\n",
    "Design and implement a complete cold start mitigation strategy:\n",
    "\n",
    "1. **Measure baseline**: Simulate cold vs warm starts\n",
    "2. **Apply optimizations**:\n",
    "   - Reduce package size\n",
    "   - Implement lazy loading\n",
    "   - Use global variable caching\n",
    "3. **Implement warm-up logic**: Scheduled pings to keep function warm\n",
    "4. **Compare before/after**: Plot latency distributions\n",
    "5. **Calculate ROI**: Cost of optimizations vs latency improvement\n",
    "\n",
    "**Bonus**: Implement provisioned concurrency logic and calculate when it's worth the cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Your code here\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Multi-Cloud Deployment Comparison ‚≠ê‚≠ê‚≠ê\n",
    "\n",
    "Create a comprehensive comparison of deploying the same model to:\n",
    "- AWS Lambda\n",
    "- Azure Functions\n",
    "- Google Cloud Functions\n",
    "\n",
    "Compare:\n",
    "1. Deployment complexity (steps required)\n",
    "2. Package size limits\n",
    "3. Memory and timeout limits\n",
    "4. Cold start characteristics\n",
    "5. Pricing at different traffic levels\n",
    "6. Integration with other services\n",
    "7. Free tier benefits\n",
    "\n",
    "Present findings in a decision matrix to help choose the best platform for different scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Your code here\n"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
