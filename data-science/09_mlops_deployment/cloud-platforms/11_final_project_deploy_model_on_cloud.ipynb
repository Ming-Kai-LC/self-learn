{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 11: Final Project - Deploy ML Model to Cloud\n",
    "\n",
    "**Difficulty**: ‚≠ê‚≠ê‚≠ê Advanced\n",
    "\n",
    "**Estimated Time**: 120-180 minutes\n",
    "\n",
    "**Prerequisites**: \n",
    "- All previous modules (00-10)\n",
    "- Cloud account (AWS, Azure, or GCP)\n",
    "- Understanding of ML deployment concepts\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By completing this capstone project, you will:\n",
    "1. Build an end-to-end ML pipeline from data to deployment\n",
    "2. Deploy a production-ready model to your chosen cloud platform\n",
    "3. Implement monitoring, logging, and alerting\n",
    "4. Apply cost optimization techniques\n",
    "5. Create API endpoints for model serving\n",
    "6. Document your deployment for production use\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "### The Challenge\n",
    "\n",
    "You are a Machine Learning Engineer tasked with deploying a customer churn prediction model for a telecommunications company. The model must:\n",
    "\n",
    "**Business Requirements**:\n",
    "- Predict customer churn with >85% accuracy\n",
    "- Provide predictions within 100ms\n",
    "- Handle 1000+ daily predictions\n",
    "- Cost <$100/month for infrastructure\n",
    "- 99.9% uptime SLA\n",
    "\n",
    "**Technical Requirements**:\n",
    "- RESTful API for predictions\n",
    "- Real-time monitoring dashboard\n",
    "- Automated alerting for errors\n",
    "- A/B testing capability\n",
    "- Comprehensive documentation\n",
    "\n",
    "### Deployment Architecture\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                   Cloud ML Deployment Pipeline                  ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                                 ‚îÇ\n",
    "‚îÇ  Data                Training            Deployment             ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îÇ\n",
    "‚îÇ  ‚îÇ  S3 /  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Train  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Endpoint ‚îÇ           ‚îÇ\n",
    "‚îÇ  ‚îÇ Blob / ‚îÇ         ‚îÇ  Job   ‚îÇ         ‚îÇ  (API)   ‚îÇ           ‚îÇ\n",
    "‚îÇ  ‚îÇ  GCS   ‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ                    ‚îÇ                 ‚îÇ\n",
    "‚îÇ                          ‚ñº                    ‚ñº                 ‚îÇ\n",
    "‚îÇ                   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ\n",
    "‚îÇ                   ‚îÇ   Model    ‚îÇ      ‚îÇ  Monitoring  ‚îÇ        ‚îÇ\n",
    "‚îÇ                   ‚îÇ  Registry  ‚îÇ      ‚îÇ  Dashboard   ‚îÇ        ‚îÇ\n",
    "‚îÇ                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ\n",
    "‚îÇ                                              ‚îÇ                  ‚îÇ\n",
    "‚îÇ                                              ‚ñº                  ‚îÇ\n",
    "‚îÇ                                       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ\n",
    "‚îÇ                                       ‚îÇ    Alerts    ‚îÇ         ‚îÇ\n",
    "‚îÇ                                       ‚îÇ  (Email/SMS) ‚îÇ         ‚îÇ\n",
    "‚îÇ                                       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Data Preparation\n",
    "\n",
    "### 1.1 Choose Your Cloud Platform\n",
    "\n",
    "This project can be completed on AWS, Azure, or GCP. Choose based on:\n",
    "- Available free credits\n",
    "- Target job market\n",
    "- Existing familiarity\n",
    "\n",
    "**Platform-Specific Guides**:\n",
    "- AWS: Use Modules 01-02\n",
    "- Azure: Use Modules 03-04  \n",
    "- GCP: Use Modules 05-06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Setup and imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    ")\n",
    "import joblib\n",
    "import json\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure visualization\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")\n",
    "print(f\"Project initialized at: {datetime.now()}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Configuration - Update with your cloud platform choice\n",
    "\n",
    "PROJECT_CONFIG = {\n",
    "    'platform': 'AWS',  # Options: 'AWS', 'Azure', 'GCP'\n",
    "    'project_name': 'customer-churn-predictor',\n",
    "    'model_version': 'v1.0',\n",
    "    'deployment_environment': 'production',  # 'development' or 'production'\n",
    "    \n",
    "    # Performance targets\n",
    "    'target_accuracy': 0.85,\n",
    "    'max_latency_ms': 100,\n",
    "    'daily_predictions': 1000,\n",
    "    'monthly_budget': 100,  # USD\n",
    "    \n",
    "    # Paths\n",
    "    'data_dir': 'data',\n",
    "    'model_dir': 'models',\n",
    "    'logs_dir': 'logs'\n",
    "}\n",
    "\n",
    "# Create necessary directories\n",
    "for dir_path in [PROJECT_CONFIG['data_dir'], PROJECT_CONFIG['model_dir'], PROJECT_CONFIG['logs_dir']]:\n",
    "    Path(dir_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Project Configuration:\")\n",
    "print(json.dumps(PROJECT_CONFIG, indent=2))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Load and Explore Data\n",
    "\n",
    "We'll create a synthetic telecom customer dataset for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate synthetic customer churn dataset\n",
    "\n",
    "def generate_telecom_data(n_samples=10000):\n",
    "    \"\"\"\n",
    "    Generate synthetic telecom customer data for churn prediction.\n",
    "    \n",
    "    Features:\n",
    "    - Customer demographics\n",
    "    - Account information\n",
    "    - Usage patterns\n",
    "    - Service details\n",
    "    \"\"\"\n",
    "    np.random.seed(RANDOM_STATE)\n",
    "    \n",
    "    # Demographics\n",
    "    tenure_months = np.random.randint(1, 72, n_samples)\n",
    "    age = np.random.randint(18, 80, n_samples)\n",
    "    is_senior = (age >= 65).astype(int)\n",
    "    \n",
    "    # Account info\n",
    "    monthly_charges = np.random.uniform(20, 120, n_samples)\n",
    "    total_charges = monthly_charges * tenure_months + np.random.normal(0, 50, n_samples)\n",
    "    total_charges = np.maximum(total_charges, 0)  # No negative charges\n",
    "    \n",
    "    # Contract type: 0=Month-to-month, 1=One year, 2=Two year\n",
    "    contract = np.random.choice([0, 1, 2], n_samples, p=[0.55, 0.25, 0.20])\n",
    "    \n",
    "    # Services\n",
    "    internet_service = np.random.choice([0, 1, 2], n_samples, p=[0.2, 0.4, 0.4])  # 0=No, 1=DSL, 2=Fiber\n",
    "    phone_service = np.random.choice([0, 1], n_samples, p=[0.1, 0.9])\n",
    "    multiple_lines = np.random.choice([0, 1], n_samples, p=[0.5, 0.5])\n",
    "    online_security = np.random.choice([0, 1], n_samples, p=[0.5, 0.5])\n",
    "    tech_support = np.random.choice([0, 1], n_samples, p=[0.5, 0.5])\n",
    "    \n",
    "    # Payment\n",
    "    paperless_billing = np.random.choice([0, 1], n_samples, p=[0.4, 0.6])\n",
    "    payment_method = np.random.choice([0, 1, 2, 3], n_samples)  # Electronic check, Mailed check, Bank transfer, Credit card\n",
    "    \n",
    "    # Calculate churn probability based on features\n",
    "    # Higher churn for: short tenure, month-to-month, high charges, no services\n",
    "    churn_prob = (\n",
    "        0.1  # Base rate\n",
    "        + 0.3 * (tenure_months < 12)\n",
    "        + 0.2 * (contract == 0)\n",
    "        + 0.15 * (monthly_charges > 80)\n",
    "        + 0.1 * (internet_service == 2)  # Fiber optic issues\n",
    "        - 0.15 * (tech_support == 1)\n",
    "        - 0.1 * (online_security == 1)\n",
    "    )\n",
    "    churn_prob = np.clip(churn_prob, 0, 1)\n",
    "    churn = (np.random.random(n_samples) < churn_prob).astype(int)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    data = pd.DataFrame({\n",
    "        'customer_id': [f'CUST{str(i).zfill(6)}' for i in range(n_samples)],\n",
    "        'tenure_months': tenure_months,\n",
    "        'age': age,\n",
    "        'is_senior': is_senior,\n",
    "        'monthly_charges': monthly_charges.round(2),\n",
    "        'total_charges': total_charges.round(2),\n",
    "        'contract_type': contract,\n",
    "        'internet_service': internet_service,\n",
    "        'phone_service': phone_service,\n",
    "        'multiple_lines': multiple_lines,\n",
    "        'online_security': online_security,\n",
    "        'tech_support': tech_support,\n",
    "        'paperless_billing': paperless_billing,\n",
    "        'payment_method': payment_method,\n",
    "        'churn': churn\n",
    "    })\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Generate dataset\n",
    "df = generate_telecom_data(10000)\n",
    "\n",
    "# Save to disk\n",
    "data_file = Path(PROJECT_CONFIG['data_dir']) / 'telecom_churn.csv'\n",
    "df.to_csv(data_file, index=False)\n",
    "\n",
    "print(f\"Dataset generated: {len(df)} customers\")\n",
    "print(f\"Saved to: {data_file}\")\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"\\nChurn rate: {df['churn'].mean()*100:.2f}%\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "# Basic statistics\n",
    "print(\"Dataset Summary:\")\n",
    "print(\"=\"*60)\n",
    "print(df.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"\\nMissing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Feature distributions\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "fig.suptitle('Feature Distributions', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Tenure\n",
    "axes[0, 0].hist([df[df['churn']==0]['tenure_months'], \n",
    "                 df[df['churn']==1]['tenure_months']], \n",
    "                label=['No Churn', 'Churn'], bins=20, alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Tenure (months)')\n",
    "axes[0, 0].set_ylabel('Count')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].set_title('Tenure Distribution')\n",
    "\n",
    "# Monthly Charges\n",
    "axes[0, 1].hist([df[df['churn']==0]['monthly_charges'], \n",
    "                 df[df['churn']==1]['monthly_charges']], \n",
    "                label=['No Churn', 'Churn'], bins=20, alpha=0.7)\n",
    "axes[0, 1].set_xlabel('Monthly Charges ($)')\n",
    "axes[0, 1].set_ylabel('Count')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].set_title('Monthly Charges Distribution')\n",
    "\n",
    "# Contract Type\n",
    "contract_churn = df.groupby('contract_type')['churn'].mean()\n",
    "axes[0, 2].bar(contract_churn.index, contract_churn.values, alpha=0.7)\n",
    "axes[0, 2].set_xlabel('Contract Type')\n",
    "axes[0, 2].set_ylabel('Churn Rate')\n",
    "axes[0, 2].set_title('Churn Rate by Contract Type')\n",
    "axes[0, 2].set_xticks([0, 1, 2])\n",
    "axes[0, 2].set_xticklabels(['Month-to-month', 'One year', 'Two year'])\n",
    "\n",
    "# Internet Service\n",
    "internet_churn = df.groupby('internet_service')['churn'].mean()\n",
    "axes[1, 0].bar(internet_churn.index, internet_churn.values, alpha=0.7)\n",
    "axes[1, 0].set_xlabel('Internet Service')\n",
    "axes[1, 0].set_ylabel('Churn Rate')\n",
    "axes[1, 0].set_title('Churn Rate by Internet Service')\n",
    "axes[1, 0].set_xticks([0, 1, 2])\n",
    "axes[1, 0].set_xticklabels(['No', 'DSL', 'Fiber'])\n",
    "\n",
    "# Tech Support\n",
    "tech_churn = df.groupby('tech_support')['churn'].mean()\n",
    "axes[1, 1].bar(tech_churn.index, tech_churn.values, alpha=0.7)\n",
    "axes[1, 1].set_xlabel('Tech Support')\n",
    "axes[1, 1].set_ylabel('Churn Rate')\n",
    "axes[1, 1].set_title('Churn Rate by Tech Support')\n",
    "axes[1, 1].set_xticks([0, 1])\n",
    "axes[1, 1].set_xticklabels(['No', 'Yes'])\n",
    "\n",
    "# Overall churn rate\n",
    "churn_counts = df['churn'].value_counts()\n",
    "axes[1, 2].pie(churn_counts, labels=['No Churn', 'Churn'], autopct='%1.1f%%', startangle=90)\n",
    "axes[1, 2].set_title('Overall Churn Rate')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ EDA complete\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Model Development\n",
    "\n",
    "### 2.1 Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Prepare features and target\n",
    "\n",
    "# Drop customer_id (not a feature)\n",
    "feature_columns = [col for col in df.columns if col not in ['customer_id', 'churn']]\n",
    "X = df[feature_columns]\n",
    "y = df['churn']\n",
    "\n",
    "print(f\"Features: {len(feature_columns)}\")\n",
    "print(f\"Feature names: {feature_columns}\")\n",
    "print(f\"\\nTarget variable (churn): {y.value_counts()}\")\n",
    "print(f\"Class balance: {y.value_counts(normalize=True)}\")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set: {len(X_train)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")\n",
    "\n",
    "# Feature scaling (important for some models)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\n‚úÖ Data preprocessing complete\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Model Training and Selection\n",
    "\n",
    "We'll train multiple models and select the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Train multiple models\n",
    "\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=RANDOM_STATE, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=RANDOM_STATE)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(\"Training models...\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Train\n",
    "    if name == 'Logistic Regression':\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Evaluate\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'auc': auc,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall:    {recall:.4f}\")\n",
    "    print(f\"  F1 Score:  {f1:.4f}\")\n",
    "    print(f\"  AUC:       {auc:.4f}\")\n",
    "    \n",
    "    # Check if meets target\n",
    "    if accuracy >= PROJECT_CONFIG['target_accuracy']:\n",
    "        print(f\"  ‚úÖ Meets accuracy target ({PROJECT_CONFIG['target_accuracy']})\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå Below accuracy target ({PROJECT_CONFIG['target_accuracy']})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compare models visually\n",
    "\n",
    "# Metrics comparison\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'Accuracy': [r['accuracy'] for r in results.values()],\n",
    "    'Precision': [r['precision'] for r in results.values()],\n",
    "    'Recall': [r['recall'] for r in results.values()],\n",
    "    'F1 Score': [r['f1'] for r in results.values()],\n",
    "    'AUC': [r['auc'] for r in results.values()]\n",
    "})\n",
    "\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Metrics comparison\n",
    "metrics_df.set_index('Model')[['Accuracy', 'Precision', 'Recall', 'F1 Score']].plot(\n",
    "    kind='bar', ax=axes[0], alpha=0.8\n",
    ")\n",
    "axes[0].set_title('Model Performance Comparison', fontweight='bold')\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].set_ylim(0, 1)\n",
    "axes[0].legend(loc='lower right')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "axes[0].axhline(y=PROJECT_CONFIG['target_accuracy'], color='red', \n",
    "                linestyle='--', label=f\"Target ({PROJECT_CONFIG['target_accuracy']})\")\n",
    "\n",
    "# ROC curves\n",
    "for name, result in results.items():\n",
    "    fpr, tpr, _ = roc_curve(y_test, result['y_pred_proba'])\n",
    "    axes[1].plot(fpr, tpr, label=f\"{name} (AUC={result['auc']:.3f})\")\n",
    "\n",
    "axes[1].plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "axes[1].set_xlabel('False Positive Rate')\n",
    "axes[1].set_ylabel('True Positive Rate')\n",
    "axes[1].set_title('ROC Curves', fontweight='bold')\n",
    "axes[1].legend(loc='lower right')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Select best model based on F1 score (balance of precision and recall)\n",
    "best_model_name = max(results.items(), key=lambda x: x[1]['f1'])[0]\n",
    "best_model = results[best_model_name]['model']\n",
    "\n",
    "print(f\"\\nüèÜ Best model selected: {best_model_name}\")\n",
    "print(f\"   F1 Score: {results[best_model_name]['f1']:.4f}\")\n",
    "print(f\"   Accuracy: {results[best_model_name]['accuracy']:.4f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Detailed evaluation of best model\n",
    "\n",
    "best_result = results[best_model_name]\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, best_result['y_pred'])\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Confusion matrix heatmap\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
    "axes[0].set_title(f'Confusion Matrix - {best_model_name}', fontweight='bold')\n",
    "axes[0].set_ylabel('Actual')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_xticklabels(['No Churn', 'Churn'])\n",
    "axes[0].set_yticklabels(['No Churn', 'Churn'])\n",
    "\n",
    "# Feature importance (if available)\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_columns,\n",
    "        'Importance': best_model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    axes[1].barh(importance_df['Feature'], importance_df['Importance'])\n",
    "    axes[1].set_xlabel('Importance')\n",
    "    axes[1].set_title('Feature Importance', fontweight='bold')\n",
    "    axes[1].invert_yaxis()\n",
    "else:\n",
    "    axes[1].text(0.5, 0.5, 'Feature importance\\nnot available\\nfor this model',\n",
    "                ha='center', va='center', fontsize=12)\n",
    "    axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(f\"\\nClassification Report - {best_model_name}:\")\n",
    "print(classification_report(y_test, best_result['y_pred'], \n",
    "                           target_names=['No Churn', 'Churn']))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Save Model for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save model and preprocessing artifacts\n",
    "\n",
    "model_dir = Path(PROJECT_CONFIG['model_dir'])\n",
    "\n",
    "# Save model\n",
    "model_file = model_dir / f\"churn_model_{PROJECT_CONFIG['model_version']}.joblib\"\n",
    "joblib.dump(best_model, model_file)\n",
    "print(f\"‚úÖ Model saved: {model_file}\")\n",
    "\n",
    "# Save scaler\n",
    "scaler_file = model_dir / f\"scaler_{PROJECT_CONFIG['model_version']}.joblib\"\n",
    "joblib.dump(scaler, scaler_file)\n",
    "print(f\"‚úÖ Scaler saved: {scaler_file}\")\n",
    "\n",
    "# Save feature names\n",
    "feature_file = model_dir / f\"features_{PROJECT_CONFIG['model_version']}.json\"\n",
    "with open(feature_file, 'w') as f:\n",
    "    json.dump({\n",
    "        'features': feature_columns,\n",
    "        'model_type': best_model_name,\n",
    "        'version': PROJECT_CONFIG['model_version']\n",
    "    }, f, indent=2)\n",
    "print(f\"‚úÖ Features saved: {feature_file}\")\n",
    "\n",
    "# Save model metadata\n",
    "metadata = {\n",
    "    'model_name': best_model_name,\n",
    "    'model_version': PROJECT_CONFIG['model_version'],\n",
    "    'training_date': datetime.now().isoformat(),\n",
    "    'training_samples': len(X_train),\n",
    "    'test_samples': len(X_test),\n",
    "    'features': feature_columns,\n",
    "    'metrics': {\n",
    "        'accuracy': float(best_result['accuracy']),\n",
    "        'precision': float(best_result['precision']),\n",
    "        'recall': float(best_result['recall']),\n",
    "        'f1_score': float(best_result['f1']),\n",
    "        'auc': float(best_result['auc'])\n",
    "    },\n",
    "    'target_accuracy': PROJECT_CONFIG['target_accuracy'],\n",
    "    'meets_target': bool(best_result['accuracy'] >= PROJECT_CONFIG['target_accuracy'])\n",
    "}\n",
    "\n",
    "metadata_file = model_dir / f\"model_metadata_{PROJECT_CONFIG['model_version']}.json\"\n",
    "with open(metadata_file, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(f\"‚úÖ Metadata saved: {metadata_file}\")\n",
    "\n",
    "print(f\"\\nüì¶ Model artifacts ready for deployment\")\n",
    "print(f\"   Total files: {len(list(model_dir.glob('*')))}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Create Prediction Interface\n",
    "\n",
    "### 3.1 Build Inference Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create prediction pipeline\n",
    "\n",
    "class ChurnPredictor:\n",
    "    \"\"\"\n",
    "    Production-ready churn prediction pipeline.\n",
    "    \n",
    "    Handles:\n",
    "    - Input validation\n",
    "    - Preprocessing\n",
    "    - Prediction\n",
    "    - Error handling\n",
    "    - Logging\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_dir):\n",
    "        \"\"\"Load model artifacts\"\"\"\n",
    "        self.model_dir = Path(model_dir)\n",
    "        \n",
    "        # Load metadata\n",
    "        metadata_file = list(self.model_dir.glob('model_metadata_*.json'))[0]\n",
    "        with open(metadata_file) as f:\n",
    "            self.metadata = json.load(f)\n",
    "        \n",
    "        # Load model\n",
    "        model_file = list(self.model_dir.glob('churn_model_*.joblib'))[0]\n",
    "        self.model = joblib.load(model_file)\n",
    "        \n",
    "        # Load scaler (if needed)\n",
    "        scaler_files = list(self.model_dir.glob('scaler_*.joblib'))\n",
    "        self.scaler = joblib.load(scaler_files[0]) if scaler_files else None\n",
    "        \n",
    "        # Feature names\n",
    "        self.feature_names = self.metadata['features']\n",
    "        \n",
    "        print(f\"‚úÖ ChurnPredictor initialized\")\n",
    "        print(f\"   Model: {self.metadata['model_name']}\")\n",
    "        print(f\"   Version: {self.metadata['model_version']}\")\n",
    "        print(f\"   Accuracy: {self.metadata['metrics']['accuracy']:.4f}\")\n",
    "    \n",
    "    def validate_input(self, data):\n",
    "        \"\"\"\n",
    "        Validate input data.\n",
    "        \n",
    "        Args:\n",
    "            data: dict or pd.DataFrame\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: Validated data\n",
    "        \"\"\"\n",
    "        if isinstance(data, dict):\n",
    "            data = pd.DataFrame([data])\n",
    "        \n",
    "        # Check required features\n",
    "        missing_features = set(self.feature_names) - set(data.columns)\n",
    "        if missing_features:\n",
    "            raise ValueError(f\"Missing features: {missing_features}\")\n",
    "        \n",
    "        # Select only required features in correct order\n",
    "        data = data[self.feature_names]\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def predict(self, data):\n",
    "        \"\"\"\n",
    "        Make churn prediction.\n",
    "        \n",
    "        Args:\n",
    "            data: Customer data (dict or DataFrame)\n",
    "        \n",
    "        Returns:\n",
    "            dict: Prediction results\n",
    "        \"\"\"\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Validate input\n",
    "            df = self.validate_input(data)\n",
    "            \n",
    "            # Apply scaling if needed\n",
    "            if self.scaler and self.metadata['model_name'] == 'Logistic Regression':\n",
    "                X = self.scaler.transform(df)\n",
    "            else:\n",
    "                X = df.values\n",
    "            \n",
    "            # Predict\n",
    "            prediction = int(self.model.predict(X)[0])\n",
    "            probability = float(self.model.predict_proba(X)[0, 1])\n",
    "            \n",
    "            # Calculate latency\n",
    "            latency_ms = (time.time() - start_time) * 1000\n",
    "            \n",
    "            return {\n",
    "                'prediction': prediction,\n",
    "                'churn_probability': probability,\n",
    "                'latency_ms': latency_ms,\n",
    "                'model_version': self.metadata['model_version'],\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'error': str(e),\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "\n",
    "# Initialize predictor\n",
    "predictor = ChurnPredictor(PROJECT_CONFIG['model_dir'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test prediction pipeline\n",
    "\n",
    "# Sample customer (high churn risk)\n",
    "high_risk_customer = {\n",
    "    'tenure_months': 3,\n",
    "    'age': 25,\n",
    "    'is_senior': 0,\n",
    "    'monthly_charges': 95.50,\n",
    "    'total_charges': 285.00,\n",
    "    'contract_type': 0,  # Month-to-month\n",
    "    'internet_service': 2,  # Fiber\n",
    "    'phone_service': 1,\n",
    "    'multiple_lines': 0,\n",
    "    'online_security': 0,\n",
    "    'tech_support': 0,\n",
    "    'paperless_billing': 1,\n",
    "    'payment_method': 0\n",
    "}\n",
    "\n",
    "# Sample customer (low churn risk)\n",
    "low_risk_customer = {\n",
    "    'tenure_months': 48,\n",
    "    'age': 55,\n",
    "    'is_senior': 0,\n",
    "    'monthly_charges': 45.00,\n",
    "    'total_charges': 2160.00,\n",
    "    'contract_type': 2,  # Two year\n",
    "    'internet_service': 1,  # DSL\n",
    "    'phone_service': 1,\n",
    "    'multiple_lines': 1,\n",
    "    'online_security': 1,\n",
    "    'tech_support': 1,\n",
    "    'paperless_billing': 0,\n",
    "    'payment_method': 3\n",
    "}\n",
    "\n",
    "print(\"Testing prediction pipeline...\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test high risk customer\n",
    "result1 = predictor.predict(high_risk_customer)\n",
    "print(\"\\nHigh Risk Customer:\")\n",
    "print(json.dumps(result1, indent=2))\n",
    "print(f\"Risk Level: {'HIGH' if result1['churn_probability'] > 0.5 else 'LOW'}\")\n",
    "\n",
    "# Test low risk customer\n",
    "result2 = predictor.predict(low_risk_customer)\n",
    "print(\"\\nLow Risk Customer:\")\n",
    "print(json.dumps(result2, indent=2))\n",
    "print(f\"Risk Level: {'HIGH' if result2['churn_probability'] > 0.5 else 'LOW'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Check latency requirement\n",
    "avg_latency = (result1['latency_ms'] + result2['latency_ms']) / 2\n",
    "print(f\"\\nAverage latency: {avg_latency:.2f}ms\")\n",
    "\n",
    "if avg_latency < PROJECT_CONFIG['max_latency_ms']:\n",
    "    print(f\"‚úÖ Meets latency requirement (<{PROJECT_CONFIG['max_latency_ms']}ms)\")\n",
    "else:\n",
    "    print(f\"‚ùå Exceeds latency requirement (<{PROJECT_CONFIG['max_latency_ms']}ms)\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Deployment Guide\n",
    "\n",
    "### 4.1 Platform-Specific Deployment\n",
    "\n",
    "Choose your deployment platform and follow the corresponding guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option A: AWS SageMaker Deployment\n",
    "\n",
    "**Steps**:\n",
    "\n",
    "1. **Package Model for SageMaker**:\n",
    "```bash\n",
    "# Create model.tar.gz\n",
    "tar -czf model.tar.gz -C models .\n",
    "```\n",
    "\n",
    "2. **Upload to S3**:\n",
    "```python\n",
    "import boto3\n",
    "s3 = boto3.client('s3')\n",
    "bucket = 'your-sagemaker-bucket'\n",
    "s3.upload_file('model.tar.gz', bucket, 'models/churn/model.tar.gz')\n",
    "```\n",
    "\n",
    "3. **Create Inference Script** (`inference.py`):\n",
    "```python\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"Load model\"\"\"\n",
    "    model = joblib.load(f\"{model_dir}/churn_model_v1.0.joblib\")\n",
    "    return model\n",
    "\n",
    "def input_fn(request_body, content_type):\n",
    "    \"\"\"Parse input\"\"\"\n",
    "    if content_type == 'application/json':\n",
    "        return json.loads(request_body)\n",
    "    raise ValueError(f\"Unsupported content type: {content_type}\")\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    \"\"\"Make prediction\"\"\"\n",
    "    # Your prediction logic here\n",
    "    return model.predict([input_data])\n",
    "```\n",
    "\n",
    "4. **Deploy Endpoint**:\n",
    "```python\n",
    "from sagemaker.sklearn import SKLearnModel\n",
    "\n",
    "model = SKLearnModel(\n",
    "    model_data='s3://your-bucket/models/churn/model.tar.gz',\n",
    "    role='your-sagemaker-role',\n",
    "    entry_point='inference.py',\n",
    "    framework_version='1.0-1'\n",
    ")\n",
    "\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.t2.medium'\n",
    ")\n",
    "```\n",
    "\n",
    "5. **Test Endpoint**:\n",
    "```python\n",
    "result = predictor.predict(test_data)\n",
    "print(result)\n",
    "```\n",
    "\n",
    "**Cost Estimate**: ~$47/month for ml.t2.medium endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option B: Azure ML Deployment\n",
    "\n",
    "**Steps**:\n",
    "\n",
    "1. **Register Model**:\n",
    "```python\n",
    "from azureml.core import Workspace, Model\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "model = Model.register(\n",
    "    workspace=ws,\n",
    "    model_path='models/churn_model_v1.0.joblib',\n",
    "    model_name='churn-predictor'\n",
    ")\n",
    "```\n",
    "\n",
    "2. **Create Scoring Script** (`score.py`):\n",
    "```python\n",
    "import json\n",
    "import joblib\n",
    "from azureml.core.model import Model\n",
    "\n",
    "def init():\n",
    "    global model\n",
    "    model_path = Model.get_model_path('churn-predictor')\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "def run(data):\n",
    "    data = json.loads(data)\n",
    "    result = model.predict([data])\n",
    "    return json.dumps({\"prediction\": int(result[0])})\n",
    "```\n",
    "\n",
    "3. **Deploy to Azure**:\n",
    "```python\n",
    "from azureml.core.webservice import AciWebservice, Webservice\n",
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "inference_config = InferenceConfig(\n",
    "    entry_script='score.py',\n",
    "    environment=env\n",
    ")\n",
    "\n",
    "aci_config = AciWebservice.deploy_configuration(\n",
    "    cpu_cores=1,\n",
    "    memory_gb=1\n",
    ")\n",
    "\n",
    "service = Model.deploy(\n",
    "    workspace=ws,\n",
    "    name='churn-service',\n",
    "    models=[model],\n",
    "    inference_config=inference_config,\n",
    "    deployment_config=aci_config\n",
    ")\n",
    "```\n",
    "\n",
    "**Cost Estimate**: ~$30-40/month for basic ACI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option C: GCP Vertex AI Deployment\n",
    "\n",
    "**Steps**:\n",
    "\n",
    "1. **Upload Model to GCS**:\n",
    "```bash\n",
    "gsutil cp models/* gs://your-bucket/churn-model/\n",
    "```\n",
    "\n",
    "2. **Create Custom Predictor**:\n",
    "```python\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project='your-project', location='us-central1')\n",
    "\n",
    "model = aiplatform.Model.upload(\n",
    "    display_name='churn-predictor',\n",
    "    artifact_uri='gs://your-bucket/churn-model/',\n",
    "    serving_container_image_uri='us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest'\n",
    ")\n",
    "```\n",
    "\n",
    "3. **Deploy Endpoint**:\n",
    "```python\n",
    "endpoint = model.deploy(\n",
    "    deployed_model_display_name='churn-v1',\n",
    "    machine_type='n1-standard-2',\n",
    "    min_replica_count=1,\n",
    "    max_replica_count=3\n",
    ")\n",
    "```\n",
    "\n",
    "**Cost Estimate**: ~$50/month for n1-standard-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Monitoring and Maintenance\n",
    "\n",
    "### 5.1 Performance Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create monitoring dashboard simulation\n",
    "\n",
    "class ModelMonitor:\n",
    "    \"\"\"\n",
    "    Monitor model performance in production.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.predictions = []\n",
    "        self.latencies = []\n",
    "        self.errors = []\n",
    "    \n",
    "    def log_prediction(self, result):\n",
    "        \"\"\"Log prediction metrics\"\"\"\n",
    "        if 'error' in result:\n",
    "            self.errors.append(result)\n",
    "        else:\n",
    "            self.predictions.append(result)\n",
    "            self.latencies.append(result['latency_ms'])\n",
    "    \n",
    "    def get_metrics(self):\n",
    "        \"\"\"Calculate monitoring metrics\"\"\"\n",
    "        if not self.predictions:\n",
    "            return {'error': 'No predictions logged'}\n",
    "        \n",
    "        return {\n",
    "            'total_predictions': len(self.predictions),\n",
    "            'total_errors': len(self.errors),\n",
    "            'error_rate': len(self.errors) / (len(self.predictions) + len(self.errors)),\n",
    "            'avg_latency_ms': np.mean(self.latencies),\n",
    "            'p50_latency_ms': np.percentile(self.latencies, 50),\n",
    "            'p95_latency_ms': np.percentile(self.latencies, 95),\n",
    "            'p99_latency_ms': np.percentile(self.latencies, 99),\n",
    "            'max_latency_ms': np.max(self.latencies),\n",
    "            'churn_rate': np.mean([p['prediction'] for p in self.predictions])\n",
    "        }\n",
    "    \n",
    "    def check_alerts(self, thresholds):\n",
    "        \"\"\"Check if metrics exceed thresholds\"\"\"\n",
    "        metrics = self.get_metrics()\n",
    "        alerts = []\n",
    "        \n",
    "        if metrics['error_rate'] > thresholds.get('max_error_rate', 0.01):\n",
    "            alerts.append(f\"‚ö†Ô∏è High error rate: {metrics['error_rate']*100:.2f}%\")\n",
    "        \n",
    "        if metrics['avg_latency_ms'] > thresholds.get('max_latency_ms', 100):\n",
    "            alerts.append(f\"‚ö†Ô∏è High latency: {metrics['avg_latency_ms']:.2f}ms\")\n",
    "        \n",
    "        return alerts\n",
    "\n",
    "# Simulate production traffic\n",
    "monitor = ModelMonitor()\n",
    "\n",
    "print(\"Simulating production traffic...\\n\")\n",
    "\n",
    "# Simulate 100 predictions\n",
    "for i in range(100):\n",
    "    # Random customer\n",
    "    customer = {\n",
    "        'tenure_months': np.random.randint(1, 72),\n",
    "        'age': np.random.randint(18, 80),\n",
    "        'is_senior': np.random.choice([0, 1]),\n",
    "        'monthly_charges': np.random.uniform(20, 120),\n",
    "        'total_charges': np.random.uniform(100, 5000),\n",
    "        'contract_type': np.random.choice([0, 1, 2]),\n",
    "        'internet_service': np.random.choice([0, 1, 2]),\n",
    "        'phone_service': np.random.choice([0, 1]),\n",
    "        'multiple_lines': np.random.choice([0, 1]),\n",
    "        'online_security': np.random.choice([0, 1]),\n",
    "        'tech_support': np.random.choice([0, 1]),\n",
    "        'paperless_billing': np.random.choice([0, 1]),\n",
    "        'payment_method': np.random.choice([0, 1, 2, 3])\n",
    "    }\n",
    "    \n",
    "    result = predictor.predict(customer)\n",
    "    monitor.log_prediction(result)\n",
    "\n",
    "# Display metrics\n",
    "metrics = monitor.get_metrics()\n",
    "print(\"Production Metrics:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total Predictions: {metrics['total_predictions']}\")\n",
    "print(f\"Total Errors: {metrics['total_errors']}\")\n",
    "print(f\"Error Rate: {metrics['error_rate']*100:.2f}%\")\n",
    "print(f\"\\nLatency Statistics:\")\n",
    "print(f\"  Average: {metrics['avg_latency_ms']:.2f}ms\")\n",
    "print(f\"  P50: {metrics['p50_latency_ms']:.2f}ms\")\n",
    "print(f\"  P95: {metrics['p95_latency_ms']:.2f}ms\")\n",
    "print(f\"  P99: {metrics['p99_latency_ms']:.2f}ms\")\n",
    "print(f\"  Max: {metrics['max_latency_ms']:.2f}ms\")\n",
    "print(f\"\\nPredicted Churn Rate: {metrics['churn_rate']*100:.2f}%\")\n",
    "\n",
    "# Check alerts\n",
    "alerts = monitor.check_alerts({\n",
    "    'max_error_rate': 0.01,\n",
    "    'max_latency_ms': PROJECT_CONFIG['max_latency_ms']\n",
    "})\n",
    "\n",
    "print(\"\\nAlerts:\")\n",
    "if alerts:\n",
    "    for alert in alerts:\n",
    "        print(f\"  {alert}\")\n",
    "else:\n",
    "    print(\"  ‚úÖ All metrics within acceptable ranges\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Documentation and Handoff\n",
    "\n",
    "### 6.1 Create Deployment Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate deployment documentation\n",
    "\n",
    "documentation = f\"\"\"\n",
    "# Customer Churn Prediction Model - Deployment Documentation\n",
    "\n",
    "## Model Overview\n",
    "\n",
    "**Model Name**: {metadata['model_name']}\n",
    "**Version**: {metadata['model_version']}\n",
    "**Training Date**: {metadata['training_date']}\n",
    "**Purpose**: Predict customer churn for proactive retention\n",
    "\n",
    "## Performance Metrics\n",
    "\n",
    "- **Accuracy**: {metadata['metrics']['accuracy']:.4f}\n",
    "- **Precision**: {metadata['metrics']['precision']:.4f}\n",
    "- **Recall**: {metadata['metrics']['recall']:.4f}\n",
    "- **F1 Score**: {metadata['metrics']['f1_score']:.4f}\n",
    "- **AUC**: {metadata['metrics']['auc']:.4f}\n",
    "\n",
    "**Status**: {'‚úÖ Meets' if metadata['meets_target'] else '‚ùå Below'} target accuracy ({PROJECT_CONFIG['target_accuracy']})\n",
    "\n",
    "## Model Input\n",
    "\n",
    "The model requires the following features:\n",
    "\n",
    "```json\n",
    "{{\n",
    "  \"tenure_months\": \"int (1-72)\",\n",
    "  \"age\": \"int (18-80)\",\n",
    "  \"is_senior\": \"int (0 or 1)\",\n",
    "  \"monthly_charges\": \"float (20-120)\",\n",
    "  \"total_charges\": \"float\",\n",
    "  \"contract_type\": \"int (0=month-to-month, 1=one year, 2=two year)\",\n",
    "  \"internet_service\": \"int (0=no, 1=DSL, 2=fiber)\",\n",
    "  \"phone_service\": \"int (0 or 1)\",\n",
    "  \"multiple_lines\": \"int (0 or 1)\",\n",
    "  \"online_security\": \"int (0 or 1)\",\n",
    "  \"tech_support\": \"int (0 or 1)\",\n",
    "  \"paperless_billing\": \"int (0 or 1)\",\n",
    "  \"payment_method\": \"int (0-3)\"\n",
    "}}\n",
    "```\n",
    "\n",
    "## Model Output\n",
    "\n",
    "```json\n",
    "{{\n",
    "  \"prediction\": \"int (0=no churn, 1=churn)\",\n",
    "  \"churn_probability\": \"float (0.0-1.0)\",\n",
    "  \"latency_ms\": \"float\",\n",
    "  \"model_version\": \"string\",\n",
    "  \"timestamp\": \"ISO 8601 datetime\"\n",
    "}}\n",
    "```\n",
    "\n",
    "## Deployment Configuration\n",
    "\n",
    "**Platform**: {PROJECT_CONFIG['platform']}\n",
    "**Environment**: {PROJECT_CONFIG['deployment_environment']}\n",
    "**Monthly Budget**: ${PROJECT_CONFIG['monthly_budget']}\n",
    "**SLA**: 99.9% uptime\n",
    "**Max Latency**: {PROJECT_CONFIG['max_latency_ms']}ms\n",
    "\n",
    "## Monitoring\n",
    "\n",
    "Key metrics to monitor:\n",
    "1. **Error Rate**: Should be <1%\n",
    "2. **Latency**: P95 should be <{PROJECT_CONFIG['max_latency_ms']}ms\n",
    "3. **Prediction Volume**: Expected {PROJECT_CONFIG['daily_predictions']}/day\n",
    "4. **Predicted Churn Rate**: Monitor for drift\n",
    "\n",
    "## Alerting\n",
    "\n",
    "Set up alerts for:\n",
    "- Error rate >1%\n",
    "- P95 latency >{PROJECT_CONFIG['max_latency_ms']}ms\n",
    "- Prediction volume drop >50%\n",
    "- Churn rate drift >10% from baseline\n",
    "\n",
    "## Maintenance\n",
    "\n",
    "- **Retraining**: Monthly or when performance degrades\n",
    "- **Data Updates**: Weekly data refresh recommended\n",
    "- **Model Validation**: Compare predictions vs. actual outcomes\n",
    "\n",
    "## Cost Optimization\n",
    "\n",
    "Estimated monthly costs:\n",
    "- Compute: $40-50 (auto-scaling)\n",
    "- Storage: $5-10\n",
    "- Data transfer: $5\n",
    "- **Total**: ~${PROJECT_CONFIG['monthly_budget']}/month\n",
    "\n",
    "## Support\n",
    "\n",
    "For issues or questions:\n",
    "- Technical: ml-team@company.com\n",
    "- Business: retention-team@company.com\n",
    "\n",
    "## Change Log\n",
    "\n",
    "- **v1.0** ({metadata['training_date'][:10]}): Initial deployment\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Save documentation\n",
    "doc_file = Path(PROJECT_CONFIG['model_dir']) / 'DEPLOYMENT_GUIDE.md'\n",
    "with open(doc_file, 'w') as f:\n",
    "    f.write(documentation)\n",
    "\n",
    "print(\"‚úÖ Deployment documentation created\")\n",
    "print(f\"   File: {doc_file}\")\n",
    "print(\"\\nDocumentation Preview:\")\n",
    "print(\"=\"*60)\n",
    "print(documentation[:1000] + \"\\n...\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Summary and Next Steps\n",
    "\n",
    "### What You've Accomplished\n",
    "\n",
    "‚úÖ **Built an end-to-end ML pipeline**:\n",
    "- Generated and analyzed telecom customer data\n",
    "- Trained and compared multiple models\n",
    "- Selected best model based on business metrics\n",
    "\n",
    "‚úÖ **Created production-ready inference pipeline**:\n",
    "- Input validation\n",
    "- Error handling\n",
    "- Performance monitoring\n",
    "- Latency tracking\n",
    "\n",
    "‚úÖ **Prepared for cloud deployment**:\n",
    "- Platform-specific deployment guides (AWS/Azure/GCP)\n",
    "- Cost optimization strategies\n",
    "- Monitoring and alerting setup\n",
    "\n",
    "‚úÖ **Documented everything**:\n",
    "- Model metadata and artifacts\n",
    "- Deployment guide\n",
    "- API specifications\n",
    "- Maintenance procedures\n",
    "\n",
    "### Production Deployment Checklist\n",
    "\n",
    "Before going live, ensure:\n",
    "\n",
    "- [ ] Model meets accuracy target (>85%)\n",
    "- [ ] Latency is acceptable (<100ms)\n",
    "- [ ] Error handling is robust\n",
    "- [ ] Monitoring dashboards are configured\n",
    "- [ ] Alerts are set up\n",
    "- [ ] Cost tracking is enabled\n",
    "- [ ] Documentation is complete\n",
    "- [ ] Team is trained on maintenance procedures\n",
    "- [ ] Rollback plan is documented\n",
    "- [ ] Security review completed\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Deploy to Staging**:\n",
    "   - Use smaller instance type\n",
    "   - Test with synthetic traffic\n",
    "   - Validate monitoring\n",
    "\n",
    "2. **Production Deployment**:\n",
    "   - Follow platform-specific guide\n",
    "   - Start with canary deployment (10% traffic)\n",
    "   - Monitor closely for 24-48 hours\n",
    "   - Gradually increase to 100%\n",
    "\n",
    "3. **Continuous Improvement**:\n",
    "   - Collect feedback labels\n",
    "   - Retrain monthly\n",
    "   - A/B test new models\n",
    "   - Optimize features\n",
    "\n",
    "### Congratulations!\n",
    "\n",
    "You've completed the Cloud ML Deployment learning path. You now have:\n",
    "- Understanding of cloud ML platforms (AWS, Azure, GCP)\n",
    "- Hands-on experience with deployment patterns\n",
    "- Production-ready ML deployment skills\n",
    "- Cost optimization knowledge\n",
    "- Monitoring and maintenance expertise\n",
    "\n",
    "**You're ready to deploy ML models to production!** üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
