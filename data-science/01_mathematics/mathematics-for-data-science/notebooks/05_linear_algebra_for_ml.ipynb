{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Module 05: Linear Algebra for Machine Learning\n\n**Difficulty**: ⭐⭐⭐ Advanced\n\n**Estimated Time**: 90 minutes\n\n**Prerequisites**: \n- Module 04: Linear Algebra Foundations\n- Understanding of matrices, vectors, and matrix operations\n\n## Learning Objectives\n\nBy the end of this notebook, you will be able to:\n1. Understand eigenvalues and eigenvectors and their significance\n2. Perform matrix decomposition using SVD\n3. Apply Principal Component Analysis (PCA) for dimensionality reduction\n4. Understand the mathematical foundation of PCA\n5. Use PCA on real datasets\n6. Interpret PCA results for data science applications"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import load_iris, load_wine\n\n# Configure visualization\n%matplotlib inline\nplt.style.use('seaborn-v0_8-whitegrid')\nsns.set_palette(\"husl\")\n\n# Set random seed\nnp.random.seed(42)\n\n# Display options\nnp.set_printoptions(precision=4, suppress=True)\npd.set_option('display.precision', 4)\n\nprint(\"Setup complete!\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Eigenvalues and Eigenvectors\n\n**Definition**: For a square matrix $, if there exists a non-zero vector $\u000bec{v}$ and scalar $\\lambda$ such that:\n\n4390A\u000bec{v} = \\lambda\u000bec{v}4390\n\nthen:\n- $\u000bec{v}$ is an **eigenvector** of $\n- $\\lambda$ is the corresponding **eigenvalue**\n\n**Intuition**: Eigenvectors are special directions that don't change direction when the matrix transformation is applied - they only get scaled by $\\lambda$.\n\n**Why Important in ML:**\n- PCA: Find principal components (eigenvectors of covariance matrix)\n- PageRank: Dominant eigenvector of web link matrix\n- Spectral clustering: Eigenvectors reveal data structure\n- Neural networks: Eigenvalues affect training stability"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Example: Find eigenvalues and eigenvectors\n\nA = np.array([[4, 2],\n              [1, 3]])\n\nprint(\"Matrix A:\")\nprint(A)\n\n# Calculate eigenvalues and eigenvectors\neigenvalues, eigenvectors = np.linalg.eig(A)\n\nprint(\"\\nEigenvalues:\")\nprint(eigenvalues)\n\nprint(\"\\nEigenvectors (columns):\")\nprint(eigenvectors)\n\n# Verify: A * v = λ * v for first eigenvector\nv1 = eigenvectors[:, 0]\nlambda1 = eigenvalues[0]\n\nAv1 = A @ v1\nlambda_v1 = lambda1 * v1\n\nprint(f\"\\nVerification for first eigenvector:\")\nprint(f\"A * v1 = {Av1}\")\nprint(f\"λ1 * v1 = {lambda_v1}\")\nprint(f\"Equal? {np.allclose(Av1, lambda_v1)}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Singular Value Decomposition (SVD)\n\n**SVD** decomposes any matrix {m \times n}$ into three matrices:\n\n4390A = U \\Sigma V^T4390\n\nwhere:\n- {m \times m}$: Left singular vectors (orthogonal)\n- $\\Sigma_{m \times n}$: Diagonal matrix of singular values\n- ^T_{n \times n}$: Right singular vectors (orthogonal)\n\n**Properties:**\n- Works for any matrix (not just square!)\n- Singular values in $\\Sigma$ are non-negative\n- Reveals the rank of the matrix\n- Used in: PCA, image compression, recommender systems"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# SVD example\n\nA = np.array([[1, 2, 3],\n              [4, 5, 6],\n              [7, 8, 9],\n              [10, 11, 12]])\n\nprint(\"Matrix A (4×3):\")\nprint(A)\n\n# Perform SVD\nU, S, VT = np.linalg.svd(A, full_matrices=False)\n\nprint(f\"\\nU shape: {U.shape}\")\nprint(\"U (left singular vectors):\")\nprint(U)\n\nprint(f\"\\nS shape: {S.shape}\")\nprint(\"Singular values:\")\nprint(S)\n\nprint(f\"\\nV^T shape: {VT.shape}\")\nprint(\"V^T (right singular vectors):\")\nprint(VT)\n\n# Reconstruct A\nSigma = np.diag(S)\nA_reconstructed = U @ Sigma @ VT\n\nprint(\"\\nReconstructed A (should match original):\")\nprint(A_reconstructed)\nprint(f\"Reconstruction error: {np.linalg.norm(A - A_reconstructed):.10f}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Principal Component Analysis (PCA)\n\n**PCA** is a technique for dimensionality reduction that:\n1. Finds directions of maximum variance in data\n2. Projects data onto these directions (principal components)\n3. Reduces dimensions while preserving most information\n\n**Mathematical Steps:**\n1. Center the data: {centered} = X - \bar{X}$\n2. Compute covariance matrix:  = \frac{1}{n-1}X_{centered}^T X_{centered}$\n3. Find eigenvectors of $ (these are the principal components)\n4. Project data: {pca} = X_{centered} \\cdot \text{eigenvectors}$\n\n**Applications:**\n- Data visualization (reduce to 2D/3D)\n- Feature extraction\n- Noise reduction\n- Data compression"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# PCA from scratch\n\n# Generate sample data\nnp.random.seed(42)\nmean = [0, 0]\ncov = [[3, 1.5], [1.5, 1]]\nX = np.random.multivariate_normal(mean, cov, 200)\n\nprint(\"Original data shape:\", X.shape)\n\n# Step 1: Center the data\nX_mean = np.mean(X, axis=0)\nX_centered = X - X_mean\n\nprint(f\"\\nData mean before centering: {X_mean}\")\nprint(f\"Data mean after centering: {np.mean(X_centered, axis=0)}\")\n\n# Step 2: Compute covariance matrix\ncov_matrix = np.cov(X_centered.T)\nprint(\"\\nCovariance matrix:\")\nprint(cov_matrix)\n\n# Step 3: Compute eigenvectors and eigenvalues\neigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n\n# Sort by eigenvalues (descending)\nidx = eigenvalues.argsort()[::-1]\neigenvalues = eigenvalues[idx]\neigenvectors = eigenvectors[:, idx]\n\nprint(\"\\nEigenvalues (variance explained):\")\nprint(eigenvalues)\nprint(\"\\nEigenvectors (principal components):\")\nprint(eigenvectors)\n\n# Variance explained ratio\nvar_explained = eigenvalues / np.sum(eigenvalues)\nprint(\"\\nVariance explained ratio:\")\nfor i, var in enumerate(var_explained):\n    print(f\"PC{i+1}: {var:.4f} ({var*100:.2f}%)\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Visualize PCA\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# Original data\naxes[0].scatter(X[:, 0], X[:, 1], alpha=0.6)\naxes[0].scatter(X_mean[0], X_mean[1], c='red', s=200, marker='x', linewidths=3, label='Mean')\n\n# Plot principal components\nscale = 3\nfor i in range(2):\n    axes[0].arrow(X_mean[0], X_mean[1], \n                 eigenvectors[0, i] * scale * np.sqrt(eigenvalues[i]),\n                 eigenvectors[1, i] * scale * np.sqrt(eigenvalues[i]),\n                 head_width=0.3, head_length=0.3, fc=f'C{i+2}', ec=f'C{i+2}', linewidth=2.5,\n                 label=f'PC{i+1}')\n\naxes[0].set_xlabel('Feature 1', fontsize=12)\naxes[0].set_ylabel('Feature 2', fontsize=12)\naxes[0].set_title('Original Data with Principal Components', fontsize=13, fontweight='bold')\naxes[0].legend(fontsize=10)\naxes[0].grid(True, alpha=0.3)\naxes[0].set_aspect('equal')\n\n# Project onto principal components\nX_pca = X_centered @ eigenvectors\n\naxes[1].scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.6)\naxes[1].axhline(0, color='k', linewidth=0.5)\naxes[1].axvline(0, color='k', linewidth=0.5)\naxes[1].set_xlabel('PC1', fontsize=12)\naxes[1].set_ylabel('PC2', fontsize=12)\naxes[1].set_title('Data in PCA Space', fontsize=13, fontweight='bold')\naxes[1].grid(True, alpha=0.3)\naxes[1].set_aspect('equal')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"PC1 captures the direction of maximum variance.\")\nprint(\"PC2 is orthogonal to PC1 and captures the next most variance.\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. PCA with scikit-learn\n\nWhile understanding the math is important, we typically use scikit-learn's PCA implementation in practice."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# PCA using scikit-learn\n\nfrom sklearn.decomposition import PCA\n\n# Load Iris dataset\niris = load_iris()\nX_iris = iris.data\ny_iris = iris.target\n\nprint(\"Iris dataset:\")\nprint(f\"Shape: {X_iris.shape} (150 samples, 4 features)\")\nprint(f\"Features: {iris.feature_names}\")\n\n# Standardize the data\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_iris)\n\n# Apply PCA\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n\nprint(f\"\\nPCA reduced shape: {X_pca.shape} (150 samples, 2 components)\")\nprint(\"\\nVariance explained by each component:\")\nfor i, var in enumerate(pca.explained_variance_ratio_):\n    print(f\"PC{i+1}: {var:.4f} ({var*100:.2f}%)\")\nprint(f\"\\nTotal variance explained: {np.sum(pca.explained_variance_ratio_)*100:.2f}%\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Visualize Iris dataset in PCA space\n\nplt.figure(figsize=(10, 7))\n\n# Plot each class\ncolors = ['red', 'green', 'blue']\ntargets = iris.target_names\n\nfor target, color in zip([0, 1, 2], colors):\n    indices = y_iris == target\n    plt.scatter(X_pca[indices, 0], X_pca[indices, 1], \n               c=color, label=targets[target], s=50, alpha=0.7, edgecolors='black')\n\nplt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% variance)', fontsize=12)\nplt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% variance)', fontsize=12)\nplt.title('Iris Dataset - PCA Projection', fontsize=14, fontweight='bold')\nplt.legend(fontsize=11)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\"PCA successfully reduced 4D data to 2D while preserving class separability!\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Choosing Number of Components\n\nHow many principal components should we keep?\n\n**Methods:**\n1. **Variance threshold**: Keep components that explain X% of variance (e.g., 95%)\n2. **Scree plot**: Look for elbow in variance explained\n3. **Cross-validation**: Choose number that gives best model performance"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Scree plot - visualizing variance explained\n\n# Fit PCA with all components\npca_full = PCA()\npca_full.fit(X_scaled)\n\n# Calculate cumulative variance explained\ncumsum_var = np.cumsum(pca_full.explained_variance_ratio_)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Individual variance explained\naxes[0].bar(range(1, len(pca_full.explained_variance_ratio_) + 1),\n           pca_full.explained_variance_ratio_, alpha=0.7, edgecolor='black')\naxes[0].set_xlabel('Principal Component', fontsize=12)\naxes[0].set_ylabel('Variance Explained Ratio', fontsize=12)\naxes[0].set_title('Scree Plot', fontsize=13, fontweight='bold')\naxes[0].set_xticks(range(1, len(pca_full.explained_variance_ratio_) + 1))\naxes[0].grid(True, alpha=0.3, axis='y')\n\n# Cumulative variance explained\naxes[1].plot(range(1, len(cumsum_var) + 1), cumsum_var, \n            marker='o', linewidth=2.5, markersize=8)\naxes[1].axhline(0.95, color='red', linestyle='--', linewidth=2, label='95% threshold')\naxes[1].set_xlabel('Number of Components', fontsize=12)\naxes[1].set_ylabel('Cumulative Variance Explained', fontsize=12)\naxes[1].set_title('Cumulative Variance Explained', fontsize=13, fontweight='bold')\naxes[1].set_xticks(range(1, len(cumsum_var) + 1))\naxes[1].legend(fontsize=11)\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Find number of components for 95% variance\nn_components_95 = np.argmax(cumsum_var >= 0.95) + 1\nprint(f\"\\nNumber of components needed for 95% variance: {n_components_95}\")\nprint(f\"With {n_components_95} components: {cumsum_var[n_components_95-1]*100:.2f}% variance explained\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Practice Exercises"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercise 1: Eigenvalues and Eigenvectors\n\nGiven matrix:\n4390A = \begin{bmatrix} 3 & 1 \\ 1 & 3 \\end{bmatrix}4390\n\nTasks:\n1. Calculate eigenvalues and eigenvectors\n2. Verify that \u000bec{v} = \\lambda\u000bec{v}$ for each eigenpair\n3. Visualize the eigenvectors"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Exercise 1 Solution\nA = np.array([[3, 1],\n              [1, 3]])\n\nprint(\"=== Exercise 1 Solution ===\")\nprint(\"Matrix A:\")\nprint(A)\n\n# 1. Calculate eigenvalues and eigenvectors\neigenvalues, eigenvectors = np.linalg.eig(A)\n\nprint(\"\\n1. Eigenvalues:\", eigenvalues)\nprint(\"   Eigenvectors:\")\nprint(eigenvectors)\n\n# 2. Verify\nprint(\"\\n2. Verification:\")\nfor i in range(len(eigenvalues)):\n    v = eigenvectors[:, i]\n    lam = eigenvalues[i]\n    Av = A @ v\n    lam_v = lam * v\n    print(f\"\\n   Eigenpair {i+1}:\")\n    print(f\"   A*v = {Av}\")\n    print(f\"   λ*v = {lam_v}\")\n    print(f\"   Equal? {np.allclose(Av, lam_v)}\")\n\n# 3. Visualize\nfig, ax = plt.subplots(figsize=(8, 8))\n\n# Plot eigenvectors\norigin = np.array([[0, 0], [0, 0]])\nfor i in range(2):\n    v = eigenvectors[:, i]\n    ax.quiver(*origin, v[0], v[1], angles='xy', scale_units='xy', scale=0.3,\n             color=f'C{i}', width=0.012, label=f'v{i+1} (λ={eigenvalues[i]:.2f})', linewidth=2)\n\nax.set_xlim(-2, 2)\nax.set_ylim(-2, 2)\nax.set_xlabel('x', fontsize=12)\nax.set_ylabel('y', fontsize=12)\nax.set_title('Eigenvectors of A', fontsize=14, fontweight='bold')\nax.legend(fontsize=11)\nax.grid(True, alpha=0.3)\nax.axhline(0, color='k', linewidth=0.5)\nax.axvline(0, color='k', linewidth=0.5)\nax.set_aspect('equal')\nplt.tight_layout()\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercise 2: PCA on Wine Dataset\n\nUse PCA on the wine dataset:\n1. Load and standardize the data\n2. Apply PCA to reduce to 2 components\n3. Visualize the data in PCA space\n4. Calculate how much variance is explained"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Exercise 2 Solution\nfrom sklearn.datasets import load_wine\n\nprint(\"=== Exercise 2 Solution ===\")\n\n# 1. Load and standardize\nwine = load_wine()\nX_wine = wine.data\ny_wine = wine.target\n\nprint(f\"Wine dataset shape: {X_wine.shape}\")\nprint(f\"Number of classes: {len(wine.target_names)}\")\nprint(f\"Classes: {wine.target_names}\")\n\nscaler = StandardScaler()\nX_wine_scaled = scaler.fit_transform(X_wine)\n\n# 2. Apply PCA\npca_wine = PCA(n_components=2)\nX_wine_pca = pca_wine.fit_transform(X_wine_scaled)\n\nprint(f\"\\nPCA shape: {X_wine_pca.shape}\")\n\n# 3. Visualize\nplt.figure(figsize=(10, 7))\n\ncolors = ['red', 'green', 'blue']\nfor target, color in zip([0, 1, 2], colors):\n    indices = y_wine == target\n    plt.scatter(X_wine_pca[indices, 0], X_wine_pca[indices, 1],\n               c=color, label=wine.target_names[target], s=50, alpha=0.7, edgecolors='black')\n\nplt.xlabel(f'PC1 ({pca_wine.explained_variance_ratio_[0]*100:.1f}% variance)', fontsize=12)\nplt.ylabel(f'PC2 ({pca_wine.explained_variance_ratio_[1]*100:.1f}% variance)', fontsize=12)\nplt.title('Wine Dataset - PCA Projection', fontsize=14, fontweight='bold')\nplt.legend(fontsize=11)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# 4. Variance explained\nprint(f\"\\n4. Variance explained:\")\nprint(f\"   PC1: {pca_wine.explained_variance_ratio_[0]*100:.2f}%\")\nprint(f\"   PC2: {pca_wine.explained_variance_ratio_[1]*100:.2f}%\")\nprint(f\"   Total: {np.sum(pca_wine.explained_variance_ratio_)*100:.2f}%\")\nprint(f\"\\n   Reduced from {X_wine.shape[1]} to 2 dimensions while retaining\")\nprint(f\"   {np.sum(pca_wine.explained_variance_ratio_)*100:.2f}% of the variance!\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Summary and Key Takeaways\n\nIn this module, you learned:\n\n✅ **Eigenvalues and Eigenvectors**\n- Special vectors that don't change direction under transformation\n- \u000bec{v} = \\lambda\u000bec{v}$\n- Foundation for PCA and many ML algorithms\n\n✅ **Singular Value Decomposition (SVD)**\n- Decomposes any matrix:  = U\\Sigma V^T$\n- Generalizes eigendecomposition\n- Applications: dimensionality reduction, compression\n\n✅ **Principal Component Analysis (PCA)**\n- Finds directions of maximum variance\n- Reduces dimensionality while preserving information\n- Steps: center → covariance → eigenvectors → project\n\n✅ **PCA Applications**\n- Data visualization (high-D → 2D/3D)\n- Feature extraction and selection\n- Noise reduction\n- Data compression\n\n✅ **Choosing Components**\n- Variance explained threshold (e.g., 95%)\n- Scree plot for visual inspection\n- Cross-validation for optimal performance\n\n### What's Next?\n\nIn **Module 06: Calculus Basics**, you'll learn:\n- Limits and derivatives\n- Chain rule and partial derivatives\n- Gradient descent optimization\n- Applications to neural networks\n\n### Additional Resources\n\n- [3Blue1Brown - Eigenvectors and Eigenvalues](https://www.youtube.com/watch?v=PFDu9oVAE-g)\n- [StatQuest - PCA](https://www.youtube.com/watch?v=FgakZw6K1QQ)\n- [Scikit-learn PCA Documentation](https://scikit-learn.org/stable/modules/decomposition.html#pca)\n- [Stanford CS229 - PCA](http://cs229.stanford.edu/notes/cs229-notes10.pdf)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n**Fantastic work!** You now understand the linear algebra behind dimensionality reduction and PCA - crucial for modern machine learning.\n\n**Next**: Proceed to "
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}