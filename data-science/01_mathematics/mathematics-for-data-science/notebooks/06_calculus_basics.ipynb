{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Module 06: Calculus Basics\n\n**Difficulty**: ⭐⭐⭐ Advanced\n\n**Estimated Time**: 90 minutes\n\n**Prerequisites**: \n- Module 00: Setup and Introduction\n- Basic understanding of functions and algebra\n\n## Learning Objectives\n\nBy the end of this notebook, you will be able to:\n1. Understand limits and continuity concepts\n2. Calculate derivatives using various rules\n3. Apply the chain rule and partial derivatives\n4. Understand gradient descent optimization\n5. Visualize optimization problems\n6. Apply calculus concepts to neural network training"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import optimize\nfrom matplotlib import animation\nfrom IPython.display import HTML\n\n# Configure visualization\n%matplotlib inline\nplt.style.use('seaborn-v0_8-whitegrid')\nsns.set_palette(\"husl\")\n\n# Set random seed\nnp.random.seed(42)\n\n# Display options\nnp.set_printoptions(precision=4, suppress=True)\npd.set_option('display.precision', 4)\n\nprint(\"Setup complete!\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Limits and Continuity\n\n**Limit**: The value a function approaches as the input approaches some value.\n\n$$\\lim_{x \\to a} f(x) = L$$\n\n**Continuity**: A function is continuous at $x=a$ if:\n1. $f(a)$ exists\n2. $\\lim_{x \\to a} f(x)$ exists\n3. $\\lim_{x \\to a} f(x) = f(a)$\n\n**Why important in ML:**\n- Ensures smooth optimization landscapes\n- Guarantees convergence of gradient descent\n- Essential for theoretical guarantees"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Visualizing limits\n\ndef f(x):\n    \"\"\"Example function with a discontinuity\"\"\"\n    return np.where(x < 0, x**2, 2*x + 1)\n\nx = np.linspace(-3, 3, 1000)\ny = f(x)\n\nplt.figure(figsize=(12, 6))\nplt.plot(x[x < 0], y[x < 0], 'b-', linewidth=2.5, label='f(x) = x² for x < 0')\nplt.plot(x[x >= 0], y[x >= 0], 'r-', linewidth=2.5, label='f(x) = 2x + 1 for x ≥ 0')\n\n# Mark the point at x=0\nplt.scatter([0], [0], s=100, c='blue', zorder=5, marker='o')\nplt.scatter([0], [1], s=100, c='red', zorder=5, marker='o')\n\nplt.axhline(y=0, color='k', linewidth=0.5)\nplt.axvline(x=0, color='k', linewidth=0.5)\nplt.xlabel('x', fontsize=12)\nplt.ylabel('f(x)', fontsize=12)\nplt.title('Piecewise Function with Discontinuity at x=0', fontsize=14, fontweight='bold')\nplt.legend(fontsize=11)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\"The function jumps from 0 to 1 at x=0 (discontinuous).\")\nprint(\"Left limit: lim(x→0⁻) f(x) = 0\")\nprint(\"Right limit: lim(x→0⁺) f(x) = 1\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Derivatives\n\n**Derivative**: Rate of change of a function. The slope of the tangent line.\n\n$$f'(x) = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}$$\n\n**Notation**: $f'(x)$, $\\frac{df}{dx}$, $\\frac{d}{dx}f(x)$\n\n**Basic Rules:**\n- Constant: $(c)' = 0$\n- Power rule: $(x^n)' = nx^{n-1}$\n- Sum: $(f+g)' = f' + g'$\n- Product: $(fg)' = f'g + fg'$\n- Quotient: $(\\frac{f}{g})' = \\frac{f'g - fg'}{g^2}$\n\n**Why important:**\n- Find minima/maxima of loss functions\n- Gradient descent optimization\n- Backpropagation in neural networks"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Numerical derivative approximation\n\ndef f(x):\n    return x**2\n\ndef numerical_derivative(f, x, h=1e-5):\n    \"\"\"Approximate derivative using finite differences\"\"\"\n    return (f(x + h) - f(x)) / h\n\n# Test at x = 3\nx_val = 3\nanalytical_deriv = 2 * x_val  # f'(x) = 2x for f(x) = x²\nnumerical_deriv = numerical_derivative(f, x_val)\n\nprint(f\"Function: f(x) = x²\")\nprint(f\"Point: x = {x_val}\")\nprint(f\"\\nAnalytical derivative f'({x_val}) = {analytical_deriv}\")\nprint(f\"Numerical derivative f'({x_val}) ≈ {numerical_deriv:.6f}\")\nprint(f\"Difference: {abs(analytical_deriv - numerical_deriv):.10f}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Visualize derivative as slope\n\nx = np.linspace(-2, 4, 100)\ny = x**2\n\n# Point of interest\nx0 = 1.5\ny0 = x0**2\nslope = 2 * x0  # Derivative at x0\n\n# Tangent line: y - y0 = slope * (x - x0)\ntangent_line = slope * (x - x0) + y0\n\nplt.figure(figsize=(10, 7))\nplt.plot(x, y, 'b-', linewidth=2.5, label='f(x) = x²')\nplt.plot(x, tangent_line, 'r--', linewidth=2, label=f'Tangent line (slope = {slope:.2f})')\nplt.scatter([x0], [y0], s=150, c='red', zorder=5, edgecolors='black', linewidths=2)\nplt.text(x0 + 0.2, y0 + 0.3, f'({x0}, {y0:.2f})', fontsize=11, fontweight='bold')\n\nplt.axhline(y=0, color='k', linewidth=0.5)\nplt.axvline(x=0, color='k', linewidth=0.5)\nplt.xlabel('x', fontsize=12)\nplt.ylabel('f(x)', fontsize=12)\nplt.title('Derivative as Slope of Tangent Line', fontsize=14, fontweight='bold')\nplt.legend(fontsize=11)\nplt.grid(True, alpha=0.3)\nplt.xlim(-2, 4)\nplt.ylim(-1, 10)\nplt.tight_layout()\nplt.show()\n\nprint(f\"At x = {x0}, the derivative (slope) is {slope:.2f}\")\nprint(f\"This means the function is increasing at a rate of {slope:.2f} units per unit x.\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Chain Rule and Partial Derivatives\n\n**Chain Rule**: For composite functions $f(g(x))$:\n\n$$\\frac{d}{dx}f(g(x)) = f'(g(x)) \\cdot g'(x)$$\n\n**Example**: $h(x) = (2x + 1)^3$\n- Let $u = 2x + 1$, so $h = u^3$\n- $\\frac{dh}{dx} = \\frac{dh}{du} \\cdot \\frac{du}{dx} = 3u^2 \\cdot 2 = 6(2x+1)^2$\n\n**Partial Derivatives**: For functions of multiple variables $f(x, y)$:\n\n$$\\frac{\\partial f}{\\partial x} = \\text{derivative with respect to } x \\text{ (treat } y \\text{ as constant)}$$\n\n**Applications:**\n- Backpropagation (chain rule through network layers)\n- Gradient computation for optimization"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Chain rule example\n\n# Function: h(x) = (2x + 1)³\ndef h(x):\n    return (2*x + 1)**3\n\n# Analytical derivative: h'(x) = 6(2x + 1)²\ndef h_prime(x):\n    return 6 * (2*x + 1)**2\n\n# Test\nx_test = 2\nanalytical = h_prime(x_test)\nnumerical = numerical_derivative(h, x_test)\n\nprint(\"Function: h(x) = (2x + 1)³\")\nprint(f\"\\nUsing chain rule:\")\nprint(f\"h'(x) = 3(2x + 1)² × 2 = 6(2x + 1)²\")\nprint(f\"\\nAt x = {x_test}:\")\nprint(f\"Analytical: h'({x_test}) = {analytical}\")\nprint(f\"Numerical: h'({x_test}) ≈ {numerical:.6f}\")\nprint(f\"Match: {np.isclose(analytical, numerical)}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Partial derivatives\n\ndef f(x, y):\n    \"\"\"Example: f(x, y) = x² + 2xy + y²\"\"\"\n    return x**2 + 2*x*y + y**2\n\n# Partial derivatives\n# ∂f/∂x = 2x + 2y\n# ∂f/∂y = 2x + 2y\n\nx0, y0 = 1, 2\n\n# Numerical partial derivatives\nh = 1e-5\ndf_dx = (f(x0 + h, y0) - f(x0, y0)) / h\ndf_dy = (f(x0, y0 + h) - f(x0, y0)) / h\n\n# Analytical\ndf_dx_analytical = 2*x0 + 2*y0\ndf_dy_analytical = 2*x0 + 2*y0\n\nprint(f\"Function: f(x, y) = x² + 2xy + y²\")\nprint(f\"\\nAt point ({x0}, {y0}):\")\nprint(f\"\\n∂f/∂x = 2x + 2y\")\nprint(f\"Analytical: {df_dx_analytical}\")\nprint(f\"Numerical: {df_dx:.6f}\")\nprint(f\"\\n∂f/∂y = 2x + 2y\")\nprint(f\"Analytical: {df_dy_analytical}\")\nprint(f\"Numerical: {df_dy:.6f}\")\nprint(f\"\\nGradient vector: ∇f = [{df_dx_analytical}, {df_dy_analytical}]\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Gradient Descent Optimization\n\n**Gradient**: Vector of partial derivatives\n\n$$\\nabla f = \\left[\\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\ldots, \\frac{\\partial f}{\\partial x_n}\\right]$$\n\n**Gradient Descent Algorithm**:\n1. Start with initial guess $x_0$\n2. Compute gradient $\\nabla f(x_t)$\n3. Update: $x_{t+1} = x_t - \\alpha \\nabla f(x_t)$\n4. Repeat until convergence\n\nwhere $\\alpha$ is the learning rate.\n\n**Key insight**: Gradient points in direction of steepest increase, so we go opposite direction to minimize.\n\n**Applications:**\n- Training neural networks\n- Optimizing loss functions\n- Finding minimum of any differentiable function"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Gradient descent on simple function\n\ndef f(x):\n    \"\"\"Objective function: f(x) = (x - 3)² + 5\"\"\"\n    return (x - 3)**2 + 5\n\ndef df_dx(x):\n    \"\"\"Gradient: f'(x) = 2(x - 3)\"\"\"\n    return 2 * (x - 3)\n\n# Gradient descent\nx = 0.0  # Starting point\nlearning_rate = 0.1\nnum_iterations = 20\n\nhistory = [x]\n\nprint(\"=== Gradient Descent ===\")\nprint(f\"Function: f(x) = (x - 3)² + 5\")\nprint(f\"Starting point: x = {x}\")\nprint(f\"Learning rate: α = {learning_rate}\")\nprint(f\"\\nIterations:\")\n\nfor i in range(num_iterations):\n    grad = df_dx(x)\n    x = x - learning_rate * grad\n    history.append(x)\n    if i < 5 or i == num_iterations - 1:\n        print(f\"  {i+1}: x = {x:.6f}, f(x) = {f(x):.6f}, gradient = {grad:.6f}\")\n\nprint(f\"\\nConverged to: x = {x:.6f}\")\nprint(f\"Minimum value: f({x:.6f}) = {f(x):.6f}\")\nprint(f\"True minimum: x = 3.0, f(3) = 5.0\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Visualize gradient descent\n\nx_range = np.linspace(-1, 7, 200)\ny_range = f(x_range)\n\nplt.figure(figsize=(12, 7))\nplt.plot(x_range, y_range, 'b-', linewidth=2.5, label='f(x) = (x-3)² + 5')\n\n# Plot gradient descent path\nhistory_array = np.array(history)\nplt.plot(history_array, f(history_array), 'ro-', markersize=6, linewidth=1.5, \n        label='Gradient descent path', alpha=0.7)\n\n# Mark start and end\nplt.scatter([history[0]], [f(history[0])], s=200, c='green', marker='*', \n           zorder=5, edgecolors='black', linewidths=2, label='Start')\nplt.scatter([history[-1]], [f(history[-1])], s=200, c='red', marker='*', \n           zorder=5, edgecolors='black', linewidths=2, label='End')\n\nplt.xlabel('x', fontsize=12)\nplt.ylabel('f(x)', fontsize=12)\nplt.title('Gradient Descent Optimization', fontsize=14, fontweight='bold')\nplt.legend(fontsize=11)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(f\"Converged in {len(history)-1} iterations!\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Application to Neural Networks\n\nIn neural networks, calculus is used for:\n\n**Forward pass**: Compute predictions\n$$y = f(Wx + b)$$\n\n**Backward pass (Backpropagation)**: Compute gradients using chain rule\n$$\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial W}$$\n\nwhere:\n- $L$ is the loss function\n- $W$ are the weights\n- Chain rule connects layers\n\n**Optimization**: Update weights using gradient descent\n$$W_{new} = W_{old} - \\alpha \\frac{\\partial L}{\\partial W}$$"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Simple neural network gradient example\n\n# Simple model: y = sigmoid(wx + b)\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n\ndef sigmoid_derivative(z):\n    s = sigmoid(z)\n    return s * (1 - s)\n\n# Data\nx = np.array([0.5, 1.0, 1.5])\ny_true = np.array([0, 1, 1])\n\n# Initial parameters\nw = 0.5\nb = 0.0\nlearning_rate = 0.5\n\nprint(\"=== Simple Neural Network Training ===\")\nprint(f\"Data: x = {x}, y_true = {y_true}\")\nprint(f\"\\nInitial: w = {w}, b = {b}\")\nprint(f\"Learning rate: {learning_rate}\\n\")\n\n# Training\nfor epoch in range(10):\n    # Forward pass\n    z = w * x + b\n    y_pred = sigmoid(z)\n    \n    # Loss (MSE)\n    loss = np.mean((y_pred - y_true)**2)\n    \n    # Backward pass (gradients)\n    dL_dy = 2 * (y_pred - y_true) / len(x)\n    dy_dz = sigmoid_derivative(z)\n    dL_dz = dL_dy * dy_dz\n    \n    dL_dw = np.sum(dL_dz * x)\n    dL_db = np.sum(dL_dz)\n    \n    # Update weights\n    w = w - learning_rate * dL_dw\n    b = b - learning_rate * dL_db\n    \n    if epoch < 3 or epoch == 9:\n        print(f\"Epoch {epoch+1}: Loss = {loss:.6f}, w = {w:.4f}, b = {b:.4f}\")\n\nprint(f\"\\nFinal predictions: {sigmoid(w * x + b)}\")\nprint(f\"True values: {y_true}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Practice Exercises"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercise 1: Compute Derivatives\n\nCompute the derivatives of the following functions:\n\n1. $f(x) = 3x^4 - 2x^2 + 5$\n2. $g(x) = \\sin(2x)$ (use chain rule)\n3. $h(x, y) = x^2y + xy^2$ (partial derivatives)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Exercise 1 Solution\nprint(\"=== Exercise 1 Solution ===\\n\")\n\n# 1. f(x) = 3x⁴ - 2x² + 5\nprint(\"1. f(x) = 3x⁴ - 2x² + 5\")\nprint(\"   Using power rule:\")\nprint(\"   f'(x) = 12x³ - 4x\")\nprint(\"   At x = 2:\")\nx = 2\nf_prime = 12 * x**3 - 4 * x\nprint(f\"   f'(2) = {f_prime}\")\n\n# 2. g(x) = sin(2x)\nprint(\"\\n2. g(x) = sin(2x)\")\nprint(\"   Using chain rule:\")\nprint(\"   g'(x) = cos(2x) × 2 = 2cos(2x)\")\nprint(\"   At x = π/4:\")\nx = np.pi / 4\ng_prime = 2 * np.cos(2 * x)\nprint(f\"   g'(π/4) = {g_prime:.6f}\")\n\n# 3. h(x, y) = x²y + xy²\nprint(\"\\n3. h(x, y) = x²y + xy²\")\nprint(\"   Partial derivatives:\")\nprint(\"   ∂h/∂x = 2xy + y²\")\nprint(\"   ∂h/∂y = x² + 2xy\")\nprint(\"   At (x, y) = (1, 2):\")\nx, y = 1, 2\ndh_dx = 2*x*y + y**2\ndh_dy = x**2 + 2*x*y\nprint(f\"   ∂h/∂x = {dh_dx}\")\nprint(f\"   ∂h/∂y = {dh_dy}\")\nprint(f\"   Gradient: ∇h = [{dh_dx}, {dh_dy}]\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Exercise 2: Gradient Descent\n\nUse gradient descent to minimize: $f(x) = x^4 - 4x^2 + 5$\n\nTasks:\n1. Compute the derivative\n2. Implement gradient descent\n3. Find local minima starting from different points\n4. Visualize the optimization paths"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Exercise 2 Solution\ndef f_ex2(x):\n    return x**4 - 4*x**2 + 5\n\ndef df_ex2(x):\n    return 4*x**3 - 8*x\n\ndef gradient_descent_ex2(start_x, lr=0.1, iterations=50):\n    x = start_x\n    path = [x]\n    for _ in range(iterations):\n        grad = df_ex2(x)\n        x = x - lr * grad\n        path.append(x)\n    return np.array(path)\n\nprint(\"=== Exercise 2 Solution ===\")\nprint(\"Function: f(x) = x⁴ - 4x² + 5\")\nprint(\"Derivative: f'(x) = 4x³ - 8x\\n\")\n\n# Try different starting points\nstarting_points = [-2.5, 0.5, 2.5]\npaths = []\n\nfor start in starting_points:\n    path = gradient_descent_ex2(start, lr=0.1, iterations=50)\n    paths.append(path)\n    print(f\"Starting from x = {start}:\")\n    print(f\"  Converged to: x = {path[-1]:.6f}\")\n    print(f\"  Minimum value: f(x) = {f_ex2(path[-1]):.6f}\\n\")\n\n# Visualize\nx_range = np.linspace(-3, 3, 300)\ny_range = f_ex2(x_range)\n\nplt.figure(figsize=(12, 7))\nplt.plot(x_range, y_range, 'b-', linewidth=2.5, label='f(x) = x⁴ - 4x² + 5')\n\ncolors = ['red', 'green', 'orange']\nfor i, (start, path) in enumerate(zip(starting_points, paths)):\n    plt.plot(path, f_ex2(path), 'o-', color=colors[i], markersize=4, linewidth=1.5,\n            alpha=0.7, label=f'Path from x={start}')\n    plt.scatter([path[-1]], [f_ex2(path[-1])], s=150, c=colors[i], \n               marker='*', zorder=5, edgecolors='black', linewidths=2)\n\nplt.xlabel('x', fontsize=12)\nplt.ylabel('f(x)', fontsize=12)\nplt.title('Gradient Descent from Different Starting Points', fontsize=14, fontweight='bold')\nplt.legend(fontsize=10)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\"Notice: The function has two local minima at x ≈ ±√2\")\nprint(\"Gradient descent finds different minima depending on starting point!\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Summary and Key Takeaways\n\nIn this module, you learned:\n\n✅ **Limits and Continuity**\n- Foundation of calculus\n- Ensures smooth optimization\n- Critical for convergence guarantees\n\n✅ **Derivatives**\n- Rate of change of functions\n- Power rule, chain rule, product rule\n- Geometric interpretation as slope\n\n✅ **Partial Derivatives**\n- Derivatives for multivariable functions\n- Gradient vector points uphill\n- Essential for multivariate optimization\n\n✅ **Gradient Descent**\n- Iterative optimization algorithm\n- Update: $x_{t+1} = x_t - \\alpha \\nabla f(x_t)$\n- Foundation of neural network training\n\n✅ **Applications to ML**\n- Backpropagation uses chain rule\n- Weight updates use gradient descent\n- All modern deep learning relies on these concepts\n\n### What's Next?\n\nIn **Module 07: Final Project**, you'll:\n- Combine all mathematical concepts learned\n- Analyze a real dataset\n- Implement ML algorithms from scratch\n- Apply statistics, linear algebra, and calculus together\n\n### Additional Resources\n\n- [3Blue1Brown - Essence of Calculus](https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr)\n- [Khan Academy - Calculus](https://www.khanacademy.org/math/calculus-1)\n- [MIT OpenCourseWare - Single Variable Calculus](https://ocw.mit.edu/courses/18-01-single-variable-calculus-fall-2006/)\n- [Gradient Descent Visualization](https://www.benfrederickson.com/numerical-optimization/)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n**Phenomenal work!** You now understand the calculus behind machine learning optimization and neural network training.\n\n**Next**: Proceed to `07_final_project.ipynb`"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}