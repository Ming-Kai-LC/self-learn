{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 10: Text Classification with Transformers\n",
    "\n",
    "**Difficulty**: ⭐⭐ Intermediate  \n",
    "**Estimated Time**: 100 minutes  \n",
    "**Prerequisites**: [Module 09: Fine-Tuning Transformers](09_fine_tuning_transformers.ipynb)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Build production-ready text classifiers using transformer models\n",
    "2. Implement sentiment analysis systems with BERT\n",
    "3. Handle multi-class and multi-label classification problems\n",
    "4. Evaluate classification models with appropriate metrics\n",
    "5. Fine-tune pre-trained models on custom classification tasks\n",
    "\n",
    "## Why Text Classification?\n",
    "\n",
    "Text classification is one of the most common NLP tasks with wide-ranging applications:\n",
    "\n",
    "- **Sentiment Analysis**: Product reviews, social media monitoring\n",
    "- **Topic Classification**: News categorization, document routing\n",
    "- **Intent Detection**: Chatbots, customer service automation\n",
    "- **Spam Detection**: Email filtering, content moderation\n",
    "- **Content Moderation**: Identifying toxic or inappropriate content\n",
    "\n",
    "With transformers, we can achieve state-of-the-art accuracy with minimal training data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Hugging Face\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    pipeline\n",
    ")\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "# Sklearn for metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "\n",
    "# Visualization\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Random seed\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"✓ All libraries imported successfully!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Quick Start: Sentiment Analysis with Pipelines\n",
    "\n",
    "The easiest way to get started is using Hugging Face pipelines - they handle tokenization, inference, and post-processing automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load pre-trained sentiment analysis pipeline\n",
    "# This uses distilbert-base-uncased-finetuned-sst-2-english by default\n",
    "sentiment_classifier = pipeline(\n",
    "    'sentiment-analysis',\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "print(\"✓ Sentiment classifier loaded!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test on various examples\n",
    "test_reviews = [\n",
    "    \"I absolutely love this product! Best purchase ever!\",\n",
    "    \"This is terrible. Waste of money.\",\n",
    "    \"It's okay, nothing special. Gets the job done.\",\n",
    "    \"Exceeded my expectations! Highly recommend.\",\n",
    "    \"Disappointed. Poor quality and overpriced.\"\n",
    "]\n",
    "\n",
    "print(\"Sentiment Analysis Results:\\n\")\n",
    "print(f\"{'Review':<50} {'Sentiment':<12} {'Confidence':<10}\")\n",
    "print(\"-\" * 72)\n",
    "\n",
    "for review in test_reviews:\n",
    "    result = sentiment_classifier(review)[0]\n",
    "    label = result['label']\n",
    "    score = result['score']\n",
    "    print(f\"{review:<50} {label:<12} {score:.2%}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1**: Test edge cases\n",
    "\n",
    "Test the sentiment classifier on these challenging cases:\n",
    "1. Sarcasm: \"Oh great, another software update that breaks everything!\"\n",
    "2. Negation: \"This is not bad at all\"\n",
    "3. Mixed sentiment: \"The food was great but the service was terrible\"\n",
    "4. Neutral: \"The package arrived on Tuesday\"\n",
    "\n",
    "Discuss where the model succeeds and where it struggles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# YOUR CODE HERE\n",
    "edge_cases = [\n",
    "    \"Oh great, another software update that breaks everything!\",\n",
    "    \"This is not bad at all\",\n",
    "    \"The food was great but the service was terrible\",\n",
    "    \"The package arrived on Tuesday\"\n",
    "]\n",
    "\n",
    "# Test and analyze results"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fine-Tuning BERT for Multi-Class Classification\n",
    "\n",
    "Now let's fine-tune BERT on a multi-class classification task: categorizing news articles into topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load and Explore the AG News Dataset\n",
    "\n",
    "AG News is a collection of news articles classified into 4 categories: World, Sports, Business, Sci/Tech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load AG News dataset\n",
    "# We'll use a subset for faster training\n",
    "dataset = load_dataset('ag_news')\n",
    "\n",
    "# Explore the data\n",
    "print(\"Dataset structure:\")\n",
    "print(dataset)\n",
    "print(f\"\\nClasses: {dataset['train'].features['label'].names}\")\n",
    "print(f\"\\nTraining samples: {len(dataset['train'])}\")\n",
    "print(f\"Test samples: {len(dataset['test'])}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Look at some examples\n",
    "print(\"Sample news articles:\\n\")\n",
    "class_names = dataset['train'].features['label'].names\n",
    "\n",
    "for i in range(5):\n",
    "    example = dataset['train'][i]\n",
    "    label = class_names[example['label']]\n",
    "    text = example['text'][:150] + \"...\"  # Truncate for display\n",
    "    print(f\"Category: {label}\")\n",
    "    print(f\"Text: {text}\\n\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Class distribution\n",
    "train_labels = [example['label'] for example in dataset['train']]\n",
    "label_counts = pd.Series(train_labels).value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(range(len(class_names)), label_counts.values, color='skyblue')\n",
    "plt.xticks(range(len(class_names)), class_names)\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Number of Articles')\n",
    "plt.title('AG News Dataset - Class Distribution')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Class distribution:\")\n",
    "for i, name in enumerate(class_names):\n",
    "    print(f\"{name}: {label_counts[i]:,} articles\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Prepare Data for Training\n",
    "\n",
    "We need to tokenize the text and prepare it in the format BERT expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load BERT tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(f\"✓ Loaded tokenizer: {model_name}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def tokenize_function(examples):\n",
    "    \"\"\"\n",
    "    Tokenize text for BERT.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    examples : dict\n",
    "        Batch of examples with 'text' field\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Tokenized inputs (input_ids, attention_mask)\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=128  # Shorter for faster training\n",
    "    )\n",
    "\n",
    "# For faster training, use a subset\n",
    "# Remove this for production use\n",
    "small_train = dataset['train'].shuffle(seed=42).select(range(5000))\n",
    "small_test = dataset['test'].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "# Tokenize datasets\n",
    "print(\"Tokenizing datasets...\")\n",
    "tokenized_train = small_train.map(tokenize_function, batched=True)\n",
    "tokenized_test = small_test.map(tokenize_function, batched=True)\n",
    "\n",
    "print(\"✓ Tokenization complete!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Load Pre-trained BERT for Classification\n",
    "\n",
    "We'll load BERT and add a classification head on top (this is done automatically)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load BERT for sequence classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=4  # 4 classes in AG News\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "print(f\"✓ Model loaded with {model.num_parameters():,} parameters\")\n",
    "print(f\"✓ Model moved to {device}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Define Training Configuration\n",
    "\n",
    "Hugging Face Trainer makes training very simple - we just need to specify hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define metrics for evaluation\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute accuracy and F1 score for evaluation.\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average='weighted')\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "print(\"✓ Metrics function defined\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"✓ Trainer initialized!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Train the Model\n",
    "\n",
    "Now we can start training! This will take a few minutes on CPU, much faster on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Train the model\n",
    "print(\"Starting training...\\n\")\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n✓ Training complete!\")\n",
    "print(f\"Training time: {train_result.metrics['train_runtime']:.2f} seconds\")\n",
    "print(f\"Final loss: {train_result.metrics['train_loss']:.4f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Evaluate on test set\n",
    "eval_result = trainer.evaluate()\n",
    "\n",
    "print(\"Test Set Performance:\")\n",
    "print(f\"  Accuracy: {eval_result['eval_accuracy']:.2%}\")\n",
    "print(f\"  F1 Score: {eval_result['eval_f1']:.4f}\")\n",
    "print(f\"  Loss: {eval_result['eval_loss']:.4f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Get predictions for detailed analysis\n",
    "predictions = trainer.predict(tokenized_test)\n",
    "y_pred = np.argmax(predictions.predictions, axis=1)\n",
    "y_true = predictions.label_ids\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nDetailed Classification Report:\\n\")\n",
    "print(classification_report(y_true, y_pred, target_names=class_names))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=class_names,\n",
    "    yticklabels=class_names\n",
    ")\n",
    "plt.title('Confusion Matrix - AG News Classification')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze confusion\n",
    "print(\"\\nMost confused pairs:\")\n",
    "for i in range(len(class_names)):\n",
    "    for j in range(len(class_names)):\n",
    "        if i != j and cm[i,j] > 10:\n",
    "            print(f\"  {class_names[i]} → {class_names[j]}: {cm[i,j]} errors\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2**: Error analysis\n",
    "\n",
    "Find and analyze misclassified examples:\n",
    "1. Extract 5 examples where the model was most confident but wrong\n",
    "2. Identify patterns in the errors\n",
    "3. Suggest improvements to reduce these errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# YOUR CODE HERE\n",
    "# Hint: Use predictions.predictions to get confidence scores\n",
    "# Find examples with high confidence but incorrect predictions"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Use the Trained Model for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create pipeline from our fine-tuned model\n",
    "classifier = pipeline(\n",
    "    'text-classification',\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# Test on new articles\n",
    "new_articles = [\n",
    "    \"Apple announces new iPhone with revolutionary features\",\n",
    "    \"Stock market hits record high amid economic recovery\",\n",
    "    \"Scientists discover new exoplanet in habitable zone\",\n",
    "    \"Manchester United wins Champions League final\"\n",
    "]\n",
    "\n",
    "print(\"Predictions on new articles:\\n\")\n",
    "for article in new_articles:\n",
    "    result = classifier(article)[0]\n",
    "    label_id = int(result['label'].split('_')[1])  # Extract label ID\n",
    "    predicted_class = class_names[label_id]\n",
    "    confidence = result['score']\n",
    "    print(f\"Article: {article}\")\n",
    "    print(f\"Category: {predicted_class} (confidence: {confidence:.2%})\\n\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multi-Label Classification\n",
    "\n",
    "Sometimes a text can belong to multiple categories. For example, a movie review might be both \"Action\" and \"Sci-Fi\".\n",
    "\n",
    "**Key Difference**: \n",
    "- **Multi-class**: One label per example (softmax)\n",
    "- **Multi-label**: Multiple labels per example (sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Example: Movie genre classification\n",
    "# Each movie can have multiple genres\n",
    "sample_data = {\n",
    "    'text': [\n",
    "        \"A thrilling space adventure with amazing visual effects\",\n",
    "        \"Romantic comedy set in New York City\",\n",
    "        \"Dark psychological thriller with unexpected twists\",\n",
    "        \"Action-packed superhero origin story\"\n",
    "    ],\n",
    "    'labels': [\n",
    "        [1, 0, 1, 0],  # Action, Sci-Fi\n",
    "        [0, 1, 0, 1],  # Romance, Comedy\n",
    "        [0, 0, 1, 0],  # Thriller\n",
    "        [1, 0, 0, 0],  # Action\n",
    "    ]\n",
    "}\n",
    "\n",
    "genre_names = ['Action', 'Romance', 'Thriller', 'Comedy']\n",
    "\n",
    "# For multi-label, we use BCEWithLogitsLoss instead of CrossEntropyLoss\n",
    "print(\"Multi-label classification requires:\")\n",
    "print(\"1. Sigmoid activation (not softmax)\")\n",
    "print(\"2. BCEWithLogitsLoss\")\n",
    "print(\"3. Independent probability for each label\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3**: Implement multi-label classification\n",
    "\n",
    "Create a multi-label text classifier:\n",
    "1. Modify the model to use sigmoid instead of softmax\n",
    "2. Train on a multi-label dataset\n",
    "3. Evaluate using multi-label metrics (Hamming loss, Jaccard score)\n",
    "4. Predict multiple labels for new examples\n",
    "\n",
    "Hint: Use `problem_type=\"multi_label_classification\"` in the model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# YOUR CODE HERE"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tips for Production Text Classification\n",
    "\n",
    "### 4.1 Handling Imbalanced Data\n",
    "\n",
    "Real-world datasets often have imbalanced classes. Solutions:\n",
    "- **Weighted loss**: Give more weight to minority classes\n",
    "- **Oversampling**: Duplicate minority class examples\n",
    "- **Undersampling**: Reduce majority class examples\n",
    "- **Data augmentation**: Create synthetic examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Example: Weighted loss for imbalanced data\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# Calculate class weights (inverse frequency)\n",
    "class_counts = np.bincount(train_labels)\n",
    "class_weights = 1.0 / class_counts\n",
    "class_weights = class_weights / class_weights.sum()  # Normalize\n",
    "\n",
    "print(\"Class weights:\")\n",
    "for i, name in enumerate(class_names):\n",
    "    print(f\"{name}: {class_weights[i]:.4f}\")\n",
    "\n",
    "# Use weighted loss in training\n",
    "# loss_fn = CrossEntropyLoss(weight=torch.tensor(class_weights).to(device))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Model Selection\n",
    "\n",
    "Different models have different trade-offs:\n",
    "\n",
    "| Model | Size | Speed | Accuracy | Best For |\n",
    "|-------|------|-------|----------|----------|\n",
    "| DistilBERT | 66M | Fast | Good | Production, limited resources |\n",
    "| BERT-base | 110M | Medium | Very Good | Balanced accuracy/speed |\n",
    "| RoBERTa | 125M | Medium | Excellent | Maximum accuracy |\n",
    "| ALBERT | 12M | Fast | Good | Mobile/edge deployment |\n",
    "| DeBERTa | 140M | Slow | Excellent | Research, competitions |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4**: Model comparison\n",
    "\n",
    "Compare 3 different models on the same task:\n",
    "1. Fine-tune DistilBERT, BERT, and RoBERTa on AG News\n",
    "2. Measure accuracy, inference time, and model size\n",
    "3. Create a comparison table\n",
    "4. Recommend which to use for different scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# YOUR CODE HERE"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts Covered:\n",
    "\n",
    "1. **Quick Classification with Pipelines**:\n",
    "   - Zero-code inference with pre-trained models\n",
    "   - Sentiment analysis out of the box\n",
    "   - Limitations of pre-trained models\n",
    "\n",
    "2. **Fine-Tuning for Custom Tasks**:\n",
    "   - AG News topic classification\n",
    "   - Hugging Face Trainer API\n",
    "   - Evaluation metrics (accuracy, F1, confusion matrix)\n",
    "   - Error analysis and interpretation\n",
    "\n",
    "3. **Advanced Techniques**:\n",
    "   - Multi-label classification\n",
    "   - Handling imbalanced data\n",
    "   - Model selection criteria\n",
    "   - Production considerations\n",
    "\n",
    "### Important Takeaways:\n",
    "\n",
    "- **Transformers achieve 90%+ accuracy** with minimal training\n",
    "- **Fine-tuning is fast**: Even 1-2 epochs can work well\n",
    "- **Use pipelines for quick experiments**, Trainer for production\n",
    "- **Monitor both accuracy and F1** for imbalanced data\n",
    "- **Error analysis is crucial** for improving models\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In **Module 11: Named Entity Recognition**, we'll learn:\n",
    "- Token-level classification (vs sequence-level)\n",
    "- BIO tagging scheme\n",
    "- Extracting entities from text (people, organizations, locations)\n",
    "- Building custom NER systems\n",
    "- Combining NER with text classification\n",
    "\n",
    "### Additional Resources:\n",
    "\n",
    "- **Hugging Face Tasks**: [huggingface.co/tasks/text-classification](https://huggingface.co/tasks/text-classification)\n",
    "- **GLUE Benchmark**: [gluebenchmark.com](https://gluebenchmark.com/)\n",
    "- **Best Practices**: [hf.co/docs/transformers/training](https://huggingface.co/docs/transformers/training)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
