{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 00: Introduction to NLP and Text Processing\n",
    "\n",
    "**Difficulty**: ⭐⭐ Intermediate  \n",
    "**Estimated Time**: 90 minutes  \n",
    "**Prerequisites**: Deep Learning Fundamentals, Python proficiency\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Understand the fundamental challenges in Natural Language Processing\n",
    "2. Implement basic text processing pipelines\n",
    "3. Apply traditional text representation methods (Bag of Words, TF-IDF)\n",
    "4. Build a simple text classification model using traditional methods\n",
    "5. Understand the evolution from traditional NLP to modern transformers\n",
    "\n",
    "## What is Natural Language Processing?\n",
    "\n",
    "Natural Language Processing (NLP) is a field at the intersection of computer science, artificial intelligence, and linguistics. The goal is to enable computers to understand, interpret, and generate human language in a valuable way.\n",
    "\n",
    "### Why is NLP Challenging?\n",
    "\n",
    "Unlike structured data, human language is:\n",
    "- **Ambiguous**: \"I saw her duck\" - Did I see her bend down or her pet bird?\n",
    "- **Context-dependent**: \"That's sick!\" can be positive or negative\n",
    "- **Evolving**: New words and meanings emerge constantly\n",
    "- **High-dimensional**: Vocabulary sizes can be 50,000+ words\n",
    "- **Sequential**: Word order matters - \"dog bites man\" ≠ \"man bites dog\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# NLP libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Sklearn for traditional ML\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Visualization settings\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"✓ All libraries imported successfully!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Download required NLTK data\n",
    "# These are standard datasets needed for tokenization and processing\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "\n",
    "print(\"✓ NLTK data downloaded successfully!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Text Processing\n",
    "\n",
    "Before we can apply any machine learning algorithm to text, we need to process it. Let's explore the fundamental steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Tokenization\n",
    "\n",
    "**Tokenization** is the process of breaking text into individual units (tokens), typically words or sentences.\n",
    "\n",
    "**Why it's important**: Machine learning models work with numerical data, so we need to break text into manageable pieces first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Sample text for demonstration\n",
    "sample_text = \"\"\"\n",
    "Natural Language Processing is fascinating! It enables computers to understand human language.\n",
    "This technology powers chatbots, translation systems, and sentiment analysis tools.\n",
    "Modern NLP has evolved dramatically with the advent of transformers.\n",
    "\"\"\"\n",
    "\n",
    "# Sentence tokenization - split into sentences\n",
    "sentences = sent_tokenize(sample_text)\n",
    "print(\"Number of sentences:\", len(sentences))\n",
    "print(\"\\nSentences:\")\n",
    "for i, sent in enumerate(sentences, 1):\n",
    "    print(f\"{i}. {sent.strip()}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Word tokenization - split into words\n",
    "words = word_tokenize(sample_text)\n",
    "print(\"Number of tokens:\", len(words))\n",
    "print(\"\\nFirst 20 tokens:\")\n",
    "print(words[:20])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: Notice that punctuation marks are treated as separate tokens. This is intentional because punctuation can carry meaning (\"I love this!\" vs \"I love this.\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Lowercasing and Cleaning\n",
    "\n",
    "We typically convert all text to lowercase to avoid treating \"The\" and \"the\" as different words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Convert to lowercase\n",
    "words_lower = [word.lower() for word in words]\n",
    "\n",
    "# Remove punctuation and keep only alphabetic tokens\n",
    "words_alpha = [word for word in words_lower if word.isalpha()]\n",
    "\n",
    "print(\"Original token count:\", len(words))\n",
    "print(\"After removing punctuation:\", len(words_alpha))\n",
    "print(\"\\nCleaned tokens:\")\n",
    "print(words_alpha[:20])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Stop Words Removal\n",
    "\n",
    "**Stop words** are common words (like \"the\", \"is\", \"and\") that appear frequently but carry little meaningful information for many NLP tasks.\n",
    "\n",
    "**When to remove them**: \n",
    "- Text classification: Usually beneficial\n",
    "- Sentiment analysis: Sometimes harmful (\"not good\" becomes \"good\")\n",
    "- Language modeling: Keep them (they're part of natural language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Get English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "print(\"Number of stop words in NLTK:\", len(stop_words))\n",
    "print(\"\\nExample stop words:\")\n",
    "print(list(stop_words)[:20])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Remove stop words\n",
    "words_no_stop = [word for word in words_alpha if word not in stop_words]\n",
    "\n",
    "print(\"Tokens before stop word removal:\", len(words_alpha))\n",
    "print(\"Tokens after stop word removal:\", len(words_no_stop))\n",
    "print(\"\\nRemaining meaningful words:\")\n",
    "print(words_no_stop)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Stemming and Lemmatization\n",
    "\n",
    "Both reduce words to their base form, but differently:\n",
    "\n",
    "**Stemming**: Crude chopping of word endings  \n",
    "- \"running\" → \"run\"\n",
    "- \"studies\" → \"studi\" (not a real word!)\n",
    "- Fast but less accurate\n",
    "\n",
    "**Lemmatization**: Linguistic analysis to find the root  \n",
    "- \"running\" → \"run\"\n",
    "- \"studies\" → \"study\"\n",
    "- Slower but more accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize stemmer and lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Test words\n",
    "test_words = ['running', 'runs', 'ran', 'studies', 'studying', 'better', 'worse']\n",
    "\n",
    "print(\"Word\\t\\tStemmed\\t\\tLemmatized\")\n",
    "print(\"-\" * 50)\n",
    "for word in test_words:\n",
    "    stemmed = stemmer.stem(word)\n",
    "    lemmatized = lemmatizer.lemmatize(word, pos='v')  # pos='v' for verb\n",
    "    print(f\"{word:12}\\t{stemmed:12}\\t{lemmatized}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1**: Create a text processing pipeline\n",
    "\n",
    "Write a function that takes raw text and applies all preprocessing steps:\n",
    "1. Tokenization\n",
    "2. Lowercasing\n",
    "3. Remove punctuation\n",
    "4. Remove stop words\n",
    "5. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Apply standard NLP preprocessing pipeline to text.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        Raw input text\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    list : Cleaned and processed tokens\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "# Test your function\n",
    "test_text = \"The scientists are studying the effects of climate change on polar bears!\"\n",
    "processed = preprocess_text(test_text)\n",
    "print(\"Processed tokens:\", processed)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Representation: From Words to Numbers\n",
    "\n",
    "Machine learning models require numerical input. How do we convert text to numbers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Bag of Words (BoW)\n",
    "\n",
    "**Bag of Words** represents text as a vector of word counts, ignoring grammar and word order.\n",
    "\n",
    "**Advantages**: Simple, easy to understand  \n",
    "**Disadvantages**: Loses word order, high dimensionality, no semantic meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Sample documents\n",
    "documents = [\n",
    "    \"I love machine learning\",\n",
    "    \"I love natural language processing\",\n",
    "    \"Machine learning is part of AI\",\n",
    "    \"Deep learning is a subset of machine learning\"\n",
    "]\n",
    "\n",
    "# Create Bag of Words representation\n",
    "vectorizer = CountVectorizer()\n",
    "bow_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get feature names (vocabulary)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"Vocabulary size:\", len(feature_names))\n",
    "print(\"\\nVocabulary:\")\n",
    "print(feature_names)\n",
    "print(\"\\nBag of Words matrix shape:\", bow_matrix.shape)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize BoW as DataFrame\n",
    "bow_df = pd.DataFrame(\n",
    "    bow_matrix.toarray(),\n",
    "    columns=feature_names,\n",
    "    index=[f\"Doc {i+1}\" for i in range(len(documents))]\n",
    ")\n",
    "\n",
    "print(\"Bag of Words representation:\")\n",
    "print(bow_df)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: Each column represents a word in our vocabulary, and each row represents a document. The values are word counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "\n",
    "**TF-IDF** weights words by how important they are to a document relative to the entire corpus.\n",
    "\n",
    "**Formula**: TF-IDF(word, document) = TF(word, document) × IDF(word, corpus)\n",
    "\n",
    "- **TF (Term Frequency)**: How often does the word appear in this document?\n",
    "- **IDF (Inverse Document Frequency)**: How rare is the word across all documents?\n",
    "\n",
    "**Why it's better**: Common words get lower weights, rare (potentially more informative) words get higher weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create TF-IDF representation\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Visualize TF-IDF\n",
    "tfidf_df = pd.DataFrame(\n",
    "    tfidf_matrix.toarray(),\n",
    "    columns=tfidf_vectorizer.get_feature_names_out(),\n",
    "    index=[f\"Doc {i+1}\" for i in range(len(documents))]\n",
    ")\n",
    "\n",
    "print(\"TF-IDF representation:\")\n",
    "print(tfidf_df.round(3))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize TF-IDF scores for first document\n",
    "plt.figure(figsize=(12, 5))\n",
    "doc1_scores = tfidf_df.iloc[0].sort_values(ascending=False)\n",
    "doc1_scores[doc1_scores > 0].plot(kind='bar')\n",
    "plt.title('TF-IDF Scores for Document 1: \"I love machine learning\"')\n",
    "plt.xlabel('Word')\n",
    "plt.ylabel('TF-IDF Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2**: Compare BoW and TF-IDF\n",
    "\n",
    "Given the following documents, create both BoW and TF-IDF representations and explain the differences:\n",
    "\n",
    "```python\n",
    "docs = [\n",
    "    \"The cat sat on the mat\",\n",
    "    \"The dog sat on the log\",\n",
    "    \"Cats and dogs are enemies\"\n",
    "]\n",
    "```\n",
    "\n",
    "Which words get higher TF-IDF scores and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# YOUR CODE HERE\n",
    "docs = [\n",
    "    \"The cat sat on the mat\",\n",
    "    \"The dog sat on the log\",\n",
    "    \"Cats and dogs are enemies\"\n",
    "]\n",
    "\n",
    "# Create BoW\n",
    "\n",
    "# Create TF-IDF\n",
    "\n",
    "# Compare and explain"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building a Simple Text Classifier\n",
    "\n",
    "Let's put everything together and build a sentiment classifier using traditional methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a small sentiment dataset\n",
    "# In practice, you'd use datasets like IMDB or Twitter sentiment\n",
    "texts = [\n",
    "    \"I love this product, it's amazing!\",\n",
    "    \"Terrible experience, would not recommend\",\n",
    "    \"Best purchase ever, highly satisfied\",\n",
    "    \"Waste of money, very disappointing\",\n",
    "    \"Excellent quality and fast shipping\",\n",
    "    \"Poor customer service and defective item\",\n",
    "    \"Absolutely fantastic, exceeded expectations\",\n",
    "    \"Horrible quality, broke after one use\",\n",
    "    \"Great value for money, very pleased\",\n",
    "    \"Do not buy, complete garbage\",\n",
    "    \"Outstanding product, worth every penny\",\n",
    "    \"Regret buying this, total disappointment\",\n",
    "    \"Superb quality, would buy again\",\n",
    "    \"Awful product, returned immediately\",\n",
    "    \"Impressive features, works perfectly\",\n",
    "    \"Broke within days, very upset\",\n",
    "    \"Highly recommend, exceptional quality\",\n",
    "    \"Useless product, wasted my time\",\n",
    "    \"Brilliant purchase, very happy\",\n",
    "    \"Terrible quality, avoid at all costs\"\n",
    "]\n",
    "\n",
    "# Labels: 1 for positive, 0 for negative\n",
    "labels = [1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
    "\n",
    "print(f\"Dataset size: {len(texts)} reviews\")\n",
    "print(f\"Positive reviews: {sum(labels)}\")\n",
    "print(f\"Negative reviews: {len(labels) - sum(labels)}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    texts, labels, test_size=0.3, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Testing samples: {len(X_test)}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create TF-IDF features\n",
    "vectorizer = TfidfVectorizer(max_features=100, ngram_range=(1, 2))\n",
    "\n",
    "# Fit on training data and transform both train and test\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"Feature matrix shape: {X_train_tfidf.shape}\")\n",
    "print(f\"Vocabulary size: {len(vectorizer.get_feature_names_out())}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Train Naive Bayes classifier\n",
    "# Naive Bayes works well for text classification\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Negative', 'Positive']))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Negative', 'Positive'],\n",
    "            yticklabels=['Negative', 'Positive'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test on new examples\n",
    "new_reviews = [\n",
    "    \"This is absolutely wonderful, best thing ever!\",\n",
    "    \"Complete waste of money, very disappointed\",\n",
    "    \"Not bad, but could be better\"\n",
    "]\n",
    "\n",
    "# Transform and predict\n",
    "new_reviews_tfidf = vectorizer.transform(new_reviews)\n",
    "predictions = classifier.predict(new_reviews_tfidf)\n",
    "probabilities = classifier.predict_proba(new_reviews_tfidf)\n",
    "\n",
    "print(\"Predictions on new reviews:\\n\")\n",
    "for review, pred, prob in zip(new_reviews, predictions, probabilities):\n",
    "    sentiment = \"Positive\" if pred == 1 else \"Negative\"\n",
    "    confidence = max(prob) * 100\n",
    "    print(f\"Review: {review}\")\n",
    "    print(f\"Sentiment: {sentiment} (Confidence: {confidence:.1f}%)\\n\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3**: Feature importance analysis\n",
    "\n",
    "Extract the top 10 features (words/n-grams) that are most indicative of positive and negative sentiments. Hint: Use the classifier's `feature_log_prob_` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# YOUR CODE HERE\n",
    "# Get feature names\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Extract top features for each class\n",
    "# Hint: classifier.feature_log_prob_[0] = negative class\n",
    "#       classifier.feature_log_prob_[1] = positive class"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Limitations of Traditional NLP\n",
    "\n",
    "While traditional methods work reasonably well, they have significant limitations:\n",
    "\n",
    "### 4.1 Loss of Context and Word Order\n",
    "\n",
    "Bag of Words treats \"not good\" and \"good\" almost the same way!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Demonstrate order independence\n",
    "sentences = [\n",
    "    \"The movie was not good at all\",\n",
    "    \"The movie was good not at all\",  # Ungrammatical but same BoW\n",
    "    \"All good was the not movie at\"   # Nonsense but same BoW\n",
    "]\n",
    "\n",
    "vec = CountVectorizer()\n",
    "bow = vec.fit_transform(sentences)\n",
    "\n",
    "print(\"All three sentences have identical BoW representations!\")\n",
    "print(pd.DataFrame(\n",
    "    bow.toarray(),\n",
    "    columns=vec.get_feature_names_out(),\n",
    "    index=sentences\n",
    "))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 No Semantic Understanding\n",
    "\n",
    "Traditional methods treat \"king\" and \"queen\" as completely unrelated words, even though they're semantically similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Similar meaning but different representations\n",
    "similar_sentences = [\n",
    "    \"The cat is sleeping\",\n",
    "    \"The feline is resting\"\n",
    "]\n",
    "\n",
    "vec = CountVectorizer()\n",
    "bow = vec.fit_transform(similar_sentences)\n",
    "\n",
    "print(\"Despite similar meanings, no word overlap:\")\n",
    "print(pd.DataFrame(\n",
    "    bow.toarray(),\n",
    "    columns=vec.get_feature_names_out(),\n",
    "    index=similar_sentences\n",
    "))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 High Dimensionality and Sparsity\n",
    "\n",
    "With vocabularies of 50,000+ words, most features are zero (sparse), making models inefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Demonstrate sparsity\n",
    "large_corpus = [\n",
    "    \"This is a simple example with many different words to demonstrate sparsity\",\n",
    "    \"Another sentence with completely different vocabulary and terms\",\n",
    "    \"Yet more text using unique words not seen before\"\n",
    "]\n",
    "\n",
    "vec = CountVectorizer()\n",
    "bow = vec.fit_transform(large_corpus)\n",
    "\n",
    "# Calculate sparsity\n",
    "total_elements = bow.shape[0] * bow.shape[1]\n",
    "non_zero_elements = bow.nnz\n",
    "sparsity = (1 - non_zero_elements / total_elements) * 100\n",
    "\n",
    "print(f\"Matrix shape: {bow.shape}\")\n",
    "print(f\"Total elements: {total_elements}\")\n",
    "print(f\"Non-zero elements: {non_zero_elements}\")\n",
    "print(f\"Sparsity: {sparsity:.1f}%\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. The Evolution to Modern NLP\n",
    "\n",
    "To address these limitations, NLP has evolved through several paradigms:\n",
    "\n",
    "### Historical Timeline:\n",
    "\n",
    "1. **1990s-2000s**: Rule-based systems and statistical methods\n",
    "   - Hand-crafted rules\n",
    "   - Bag of Words, TF-IDF\n",
    "   - N-grams and language models\n",
    "\n",
    "2. **2013-2017**: Word Embeddings\n",
    "   - Word2Vec (2013): Dense vector representations\n",
    "   - GloVe (2014): Global vectors for word representation\n",
    "   - FastText (2016): Subword embeddings\n",
    "   - **Breakthrough**: Words with similar meanings have similar vectors!\n",
    "\n",
    "3. **2014-2017**: Recurrent Neural Networks\n",
    "   - LSTM (1997, popular 2014+): Handle sequences better\n",
    "   - GRU (2014): Simpler alternative to LSTM\n",
    "   - Seq2Seq (2014): Machine translation breakthrough\n",
    "   - **Achievement**: Can model word order and context\n",
    "\n",
    "4. **2017-2018**: Attention Mechanism\n",
    "   - Attention (2015): Focus on relevant parts of input\n",
    "   - Transformer (2017): \"Attention is All You Need\"\n",
    "   - **Revolution**: Parallel processing + long-range dependencies\n",
    "\n",
    "5. **2018-Present**: Transfer Learning Era\n",
    "   - BERT (2018): Bidirectional transformers\n",
    "   - GPT (2018, 2019, 2020): Autoregressive generation\n",
    "   - T5, RoBERTa, ALBERT, etc.\n",
    "   - **Current**: LLMs (GPT-4, Claude, LLaMA) with billions of parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Makes Transformers Special?\n",
    "\n",
    "1. **Self-Attention**: Understand relationships between all words in a sentence\n",
    "2. **Parallel Processing**: Process entire sequences at once (vs sequential in RNNs)\n",
    "3. **Transfer Learning**: Pre-train on massive data, fine-tune on specific tasks\n",
    "4. **Contextual Embeddings**: Same word has different representations based on context\n",
    "\n",
    "**Example**: In \"The bank is closed\" vs \"The river bank is muddy\", transformers understand \"bank\" differently!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4**: Research and compare\n",
    "\n",
    "Research and write a brief comparison (in the cell below) between:\n",
    "1. Traditional BoW/TF-IDF approaches\n",
    "2. Word embeddings (Word2Vec, GloVe)\n",
    "3. Transformer-based models (BERT, GPT)\n",
    "\n",
    "Consider: representation, context handling, training requirements, performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your comparison here:**\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts Covered:\n",
    "\n",
    "1. **Text Processing Pipeline**:\n",
    "   - Tokenization (words and sentences)\n",
    "   - Lowercasing and cleaning\n",
    "   - Stop words removal\n",
    "   - Stemming and lemmatization\n",
    "\n",
    "2. **Text Representation**:\n",
    "   - Bag of Words: Simple word counts\n",
    "   - TF-IDF: Weighted by importance\n",
    "   - Both lose word order and context\n",
    "\n",
    "3. **Traditional Classification**:\n",
    "   - TF-IDF + Naive Bayes\n",
    "   - Works for simple tasks\n",
    "   - Struggles with nuance and context\n",
    "\n",
    "4. **Limitations and Evolution**:\n",
    "   - No semantic understanding\n",
    "   - Loss of word order\n",
    "   - High dimensionality and sparsity\n",
    "   - Evolution → Word Embeddings → RNNs → Transformers\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In **Module 01: Text Preprocessing**, we'll dive deeper into:\n",
    "- Advanced tokenization strategies (BPE, WordPiece)\n",
    "- Handling special cases (URLs, mentions, hashtags)\n",
    "- Regular expressions for text cleaning\n",
    "- Building production-ready preprocessing pipelines\n",
    "\n",
    "### Additional Resources:\n",
    "\n",
    "- **NLTK Book**: [nltk.org/book](https://www.nltk.org/book/)\n",
    "- **Speech and Language Processing** by Jurafsky & Martin (free online)\n",
    "- **Scikit-learn Text Processing**: [sklearn documentation](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
