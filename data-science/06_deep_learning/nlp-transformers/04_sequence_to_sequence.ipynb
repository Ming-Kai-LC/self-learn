{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Module 04: Sequence-to-Sequence Models\n\n**Difficulty**: \u2b50\u2b50\u2b50 Advanced  \n**Estimated Time**: 120 minutes  \n**Prerequisites**: [Module 03: Recurrent Neural Networks](03_recurrent_neural_networks.ipynb)\n\n## Learning Objectives\n\nBy the end of this notebook, you will be able to:\n\n1. Understand the encoder-decoder architecture for sequence-to-sequence tasks\n2. Implement seq2seq models from scratch in PyTorch\n3. Apply teacher forcing for stable training\n4. Implement greedy and beam search decoding\n5. Build a translation system using seq2seq\n6. Understand limitations that led to attention mechanisms\n\n## What are Sequence-to-Sequence Models?\n\n**Seq2Seq** models map input sequences to output sequences of potentially different lengths.\n\n### Examples:\n- Machine translation: \"Hello\" \u2192 \"Bonjour\"\n- Summarization: [Long article] \u2192 [Short summary]\n- Question answering: \"What is NLP?\" \u2192 \"Natural Language Processing...\"\n- Dialogue: \"How are you?\" \u2192 \"I'm doing well, thanks!\"\n\n### The Challenge:\n\nTraditional RNNs have fixed output size. How do we handle:\n- Variable-length inputs AND outputs?\n- Different input/output lengths?\n- One-to-many, many-to-one, many-to-many mappings?\n\n**Solution**: Encoder-Decoder Architecture!"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Setup and Imports"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Core libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nfrom collections import Counter\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# PyTorch\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\n# Visualization\n%matplotlib inline\nplt.style.use('seaborn-v0_8-darkgrid')\n\n# Random seeds\nnp.random.seed(42)\nrandom.seed(42)\ntorch.manual_seed(42)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\nprint(\"\u2713 All libraries imported successfully!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Encoder-Decoder Architecture\n\nThe seq2seq model consists of two RNNs:\n\n1. **Encoder**: Reads input sequence \u2192 produces context vector\n2. **Decoder**: Takes context vector \u2192 generates output sequence\n\n**Key insight**: Context vector = fixed-size representation of entire input!\n\n### Mathematics:\n\n**Encoder**:\n- For each input word $x_t$: $h_t^{enc} = \\text{LSTM}(x_t, h_{t-1}^{enc})$\n- Final hidden state = context: $c = h_T^{enc}$\n\n**Decoder**:\n- Initialize with context: $h_0^{dec} = c$\n- Generate each output: $y_t, h_t^{dec} = \\text{LSTM}(y_{t-1}, h_{t-1}^{dec})$"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Teacher Forcing\n\n**Problem**: During training, decoder errors compound.\n\n**Teacher Forcing**: Feed ground truth (not predictions) as decoder input!\n\n```python\n# Without teacher forcing (exposure bias)\nfor t in range(len(target)):\n    output = decoder(previous_output)  # Uses its own prediction\n    previous_output = output\n\n# With teacher forcing (stable training)\nfor t in range(len(target)):\n    output = decoder(target[t-1])  # Uses ground truth\n```\n\n**Trade-off**: Faster convergence but train/test mismatch."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Implementing Seq2Seq\n\nLet's build a complete seq2seq model for machine translation."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Decoding Strategies\n\n**How to generate output sequences?**\n\n### 1. Greedy Decoding\n- At each step, pick most probable word\n- Fast but suboptimal\n\n### 2. Beam Search\n- Keep top-k hypotheses at each step\n- Better quality, slower\n- Beam size = trade-off between quality and speed"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Application: Machine Translation\n\nBuild a simple English-to-French translator."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Summary\n\n### Key Concepts:\n\n1. **Encoder-Decoder**: Two RNNs for sequence transformation\n2. **Context Vector**: Fixed-size bottleneck (limitation!)\n3. **Teacher Forcing**: Stable training technique\n4. **Beam Search**: Better decoding than greedy\n5. **Limitations**: Context bottleneck, long sequences\n\n### What's Next?\n\nIn **Module 05: Attention Mechanism**, we solve the context bottleneck!"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}