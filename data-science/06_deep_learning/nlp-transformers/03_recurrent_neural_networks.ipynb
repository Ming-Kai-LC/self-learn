{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 03: Recurrent Neural Networks\n",
    "\n",
    "**Difficulty**: ⭐⭐⭐ Advanced  \n",
    "**Estimated Time**: 120 minutes  \n",
    "**Prerequisites**: [Module 02: Word Embeddings](02_word_embeddings.ipynb), Deep Learning Fundamentals\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Understand the architecture and mathematics of vanilla RNNs\n",
    "2. Implement RNN, LSTM, and GRU from scratch in PyTorch\n",
    "3. Understand and solve the vanishing gradient problem\n",
    "4. Build bidirectional RNNs for better context understanding\n",
    "5. Apply RNNs to sequence classification (sentiment analysis)\n",
    "6. Compare different RNN architectures and their trade-offs\n",
    "\n",
    "## Why Recurrent Neural Networks?\n",
    "\n",
    "Traditional feedforward neural networks have limitations:\n",
    "- **Fixed input size**: Can't handle variable-length sequences\n",
    "- **No memory**: Each input processed independently\n",
    "- **No temporal dynamics**: Can't model sequential patterns\n",
    "\n",
    "**RNNs solve this** by:\n",
    "- Maintaining hidden state (memory)\n",
    "- Processing sequences one step at a time\n",
    "- Sharing parameters across time steps\n",
    "\n",
    "### Applications:\n",
    "- Language modeling (predict next word)\n",
    "- Sentiment analysis (classify text)\n",
    "- Machine translation\n",
    "- Speech recognition\n",
    "- Time series forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "# NLP\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Visualization\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"✓ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Vanilla RNN: Architecture and Mathematics\n",
    "\n",
    "### 1.1 The RNN Cell\n",
    "\n",
    "At each time step $t$, an RNN:\n",
    "1. Takes input $x_t$ (current word)\n",
    "2. Takes previous hidden state $h_{t-1}$ (memory)\n",
    "3. Produces new hidden state $h_t$\n",
    "4. Optionally produces output $y_t$\n",
    "\n",
    "**Mathematics**:\n",
    "\n",
    "$$h_t = \\tanh(W_{xh} x_t + W_{hh} h_{t-1} + b_h)$$\n",
    "\n",
    "$$y_t = W_{hy} h_t + b_y$$\n",
    "\n",
    "Where:\n",
    "- $W_{xh}$: Input-to-hidden weights\n",
    "- $W_{hh}$: Hidden-to-hidden weights (memory)\n",
    "- $W_{hy}$: Hidden-to-output weights\n",
    "- $\\tanh$: Activation function\n",
    "\n",
    "### Key Insight: Weight Sharing\n",
    "\n",
    "The same weights ($W_{xh}$, $W_{hh}$, $W_{hy}$) are used at every time step!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Implementing Vanilla RNN from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Vanilla RNN implementation from scratch.\n",
    "    \n",
    "    This educational implementation shows the inner workings of an RNN.\n",
    "    In practice, use nn.RNN for efficiency.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_size : int\n",
    "            Dimension of input vectors (e.g., embedding size)\n",
    "        hidden_size : int\n",
    "            Dimension of hidden state\n",
    "        output_size : int\n",
    "            Dimension of output (e.g., number of classes)\n",
    "        \"\"\"\n",
    "        super(VanillaRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Input to hidden\n",
    "        self.W_xh = nn.Linear(input_size, hidden_size)\n",
    "        # Hidden to hidden (recurrent connection)\n",
    "        self.W_hh = nn.Linear(hidden_size, hidden_size)\n",
    "        # Hidden to output\n",
    "        self.W_hy = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x, h_prev=None):\n",
    "        \"\"\"\n",
    "        Forward pass through RNN.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : torch.Tensor\n",
    "            Input sequence of shape (batch_size, seq_len, input_size)\n",
    "        h_prev : torch.Tensor or None\n",
    "            Previous hidden state (batch_size, hidden_size)\n",
    "            If None, initialize to zeros\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        output : torch.Tensor\n",
    "            Output at final time step (batch_size, output_size)\n",
    "        hidden_states : list\n",
    "            Hidden states at all time steps\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        # Initialize hidden state if not provided\n",
    "        if h_prev is None:\n",
    "            h = torch.zeros(batch_size, self.hidden_size).to(x.device)\n",
    "        else:\n",
    "            h = h_prev\n",
    "        \n",
    "        hidden_states = []\n",
    "        \n",
    "        # Process sequence one time step at a time\n",
    "        for t in range(seq_len):\n",
    "            x_t = x[:, t, :]  # Current input (batch_size, input_size)\n",
    "            \n",
    "            # RNN equation: h_t = tanh(W_xh * x_t + W_hh * h_{t-1})\n",
    "            h = torch.tanh(self.W_xh(x_t) + self.W_hh(h))\n",
    "            \n",
    "            hidden_states.append(h)\n",
    "        \n",
    "        # Output from final hidden state\n",
    "        output = self.W_hy(h)\n",
    "        \n",
    "        return output, hidden_states\n",
    "\n",
    "print(\"✓ VanillaRNN class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the vanilla RNN\n",
    "input_size = 10   # e.g., word embedding dimension\n",
    "hidden_size = 20  # hidden state dimension\n",
    "output_size = 3   # e.g., 3 classes for classification\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "\n",
    "# Create random input\n",
    "x = torch.randn(batch_size, seq_len, input_size)\n",
    "\n",
    "# Initialize RNN\n",
    "rnn = VanillaRNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Forward pass\n",
    "output, hidden_states = rnn(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Number of hidden states: {len(hidden_states)}\")\n",
    "print(f\"Each hidden state shape: {hidden_states[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1**: Analyze RNN computation\n",
    "\n",
    "1. Count the number of parameters in the vanilla RNN\n",
    "2. Trace the computation graph for a sequence of length 3\n",
    "3. Explain why the same weights are applied at each time step\n",
    "4. What happens if you pass in sequences of different lengths?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in rnn.parameters())\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "\n",
    "# Explain weight sharing\n",
    "# YOUR EXPLANATION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Vanishing Gradient Problem\n",
    "\n",
    "### Why Vanilla RNNs Struggle with Long Sequences\n",
    "\n",
    "During backpropagation through time (BPTT), gradients are computed as:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial h_t} = \\frac{\\partial L}{\\partial h_{t+1}} \\cdot \\frac{\\partial h_{t+1}}{\\partial h_t}$$\n",
    "\n",
    "For long sequences, this becomes:\n",
    "\n",
    "$$\\frac{\\partial h_T}{\\partial h_0} = \\prod_{t=1}^{T} \\frac{\\partial h_t}{\\partial h_{t-1}}$$\n",
    "\n",
    "**Problem**: If $\\frac{\\partial h_t}{\\partial h_{t-1}} < 1$, the gradient vanishes (→ 0)\n",
    "\n",
    "**Result**: RNN can't learn long-range dependencies!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate vanishing gradient\n",
    "def demonstrate_vanishing_gradient(seq_length=50):\n",
    "    \"\"\"\n",
    "    Show how gradients vanish in vanilla RNN.\n",
    "    \"\"\"\n",
    "    rnn = VanillaRNN(input_size=10, hidden_size=20, output_size=1)\n",
    "    \n",
    "    # Create dummy input and target\n",
    "    x = torch.randn(1, seq_length, 10)\n",
    "    target = torch.randn(1, 1)\n",
    "    \n",
    "    # Forward pass\n",
    "    output, hidden_states = rnn(x)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = F.mse_loss(output, target)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Analyze gradient magnitudes\n",
    "    grad_magnitudes = []\n",
    "    for name, param in rnn.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            grad_mag = param.grad.norm().item()\n",
    "            grad_magnitudes.append((name, grad_mag))\n",
    "    \n",
    "    return grad_magnitudes\n",
    "\n",
    "# Test with different sequence lengths\n",
    "for seq_len in [10, 50, 100]:\n",
    "    grads = demonstrate_vanishing_gradient(seq_len)\n",
    "    print(f\"\\nSequence length: {seq_len}\")\n",
    "    for name, mag in grads:\n",
    "        print(f\"  {name:20} gradient magnitude: {mag:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LSTM: Long Short-Term Memory\n",
    "\n",
    "**LSTM** (Hochreiter & Schmidhuber, 1997) solves the vanishing gradient problem with gating mechanisms.\n",
    "\n",
    "### LSTM Architecture\n",
    "\n",
    "LSTM has:\n",
    "1. **Cell state** ($c_t$): Long-term memory highway\n",
    "2. **Hidden state** ($h_t$): Short-term memory\n",
    "3. **Three gates**:\n",
    "   - **Forget gate** ($f_t$): What to forget from cell state\n",
    "   - **Input gate** ($i_t$): What new information to add\n",
    "   - **Output gate** ($o_t$): What to output\n",
    "\n",
    "### LSTM Equations:\n",
    "\n",
    "$$f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$$ (Forget gate)\n",
    "\n",
    "$$i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$$ (Input gate)\n",
    "\n",
    "$$\\tilde{c}_t = \\tanh(W_c \\cdot [h_{t-1}, x_t] + b_c)$$ (Candidate values)\n",
    "\n",
    "$$c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t$$ (Update cell state)\n",
    "\n",
    "$$o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)$$ (Output gate)\n",
    "\n",
    "$$h_t = o_t \\odot \\tanh(c_t)$$ (New hidden state)\n",
    "\n",
    "Where $\\sigma$ is sigmoid, $\\odot$ is element-wise multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "    \"\"\"\n",
    "    Single LSTM cell implementation.\n",
    "    \n",
    "    Educational implementation to understand LSTM internals.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(LSTMCell, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Combined linear layer for all gates (more efficient)\n",
    "        # Computes forget, input, output gates, and candidate values\n",
    "        self.linear = nn.Linear(input_size + hidden_size, 4 * hidden_size)\n",
    "        \n",
    "    def forward(self, x, states=None):\n",
    "        \"\"\"\n",
    "        Forward pass through LSTM cell.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : torch.Tensor\n",
    "            Input (batch_size, input_size)\n",
    "        states : tuple or None\n",
    "            Previous (hidden, cell) states\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        h_new : torch.Tensor\n",
    "            New hidden state\n",
    "        c_new : torch.Tensor\n",
    "            New cell state\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Initialize states if not provided\n",
    "        if states is None:\n",
    "            h = torch.zeros(batch_size, self.hidden_size).to(x.device)\n",
    "            c = torch.zeros(batch_size, self.hidden_size).to(x.device)\n",
    "        else:\n",
    "            h, c = states\n",
    "        \n",
    "        # Concatenate input and hidden state\n",
    "        combined = torch.cat([x, h], dim=1)\n",
    "        \n",
    "        # Compute all gates and candidate values\n",
    "        gates = self.linear(combined)\n",
    "        \n",
    "        # Split into individual gates\n",
    "        forget_gate, input_gate, output_gate, candidate = gates.chunk(4, dim=1)\n",
    "        \n",
    "        # Apply activations\n",
    "        f_t = torch.sigmoid(forget_gate)\n",
    "        i_t = torch.sigmoid(input_gate)\n",
    "        o_t = torch.sigmoid(output_gate)\n",
    "        c_tilde = torch.tanh(candidate)\n",
    "        \n",
    "        # Update cell state (key innovation of LSTM!)\n",
    "        c_new = f_t * c + i_t * c_tilde\n",
    "        \n",
    "        # Compute new hidden state\n",
    "        h_new = o_t * torch.tanh(c_new)\n",
    "        \n",
    "        return h_new, c_new\n",
    "\n",
    "print(\"✓ LSTMCell class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build full LSTM using our cell\n",
    "class SimpleLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete LSTM that processes sequences.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm_cell = LSTMCell(input_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x, states=None):\n",
    "        \"\"\"\n",
    "        Process sequence through LSTM.\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        h, c = states if states else (None, None)\n",
    "        \n",
    "        # Process sequence\n",
    "        for t in range(seq_len):\n",
    "            h, c = self.lstm_cell(x[:, t, :], (h, c) if h is not None else None)\n",
    "        \n",
    "        # Output from final hidden state\n",
    "        output = self.fc(h)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Test LSTM\n",
    "lstm = SimpleLSTM(input_size=10, hidden_size=20, output_size=3)\n",
    "x = torch.randn(2, 5, 10)\n",
    "output = lstm(x)\n",
    "print(f\"LSTM output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. GRU: Gated Recurrent Unit\n",
    "\n",
    "**GRU** (Cho et al., 2014) is a simpler alternative to LSTM.\n",
    "\n",
    "### Differences from LSTM:\n",
    "- Only 2 gates (vs 3 in LSTM): Reset and Update\n",
    "- No separate cell state (combines $h$ and $c$)\n",
    "- Fewer parameters → faster training\n",
    "\n",
    "### GRU Equations:\n",
    "\n",
    "$$r_t = \\sigma(W_r \\cdot [h_{t-1}, x_t])$$ (Reset gate)\n",
    "\n",
    "$$z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t])$$ (Update gate)\n",
    "\n",
    "$$\\tilde{h}_t = \\tanh(W \\cdot [r_t \\odot h_{t-1}, x_t])$$ (Candidate)\n",
    "\n",
    "$$h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t$$ (New hidden state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUCell(nn.Module):\n",
    "    \"\"\"\n",
    "    GRU cell implementation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(GRUCell, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Gates and candidate\n",
    "        self.linear_gates = nn.Linear(input_size + hidden_size, 2 * hidden_size)\n",
    "        self.linear_candidate = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        \n",
    "    def forward(self, x, h=None):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        if h is None:\n",
    "            h = torch.zeros(batch_size, self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Compute reset and update gates\n",
    "        combined = torch.cat([x, h], dim=1)\n",
    "        gates = self.linear_gates(combined)\n",
    "        reset_gate, update_gate = gates.chunk(2, dim=1)\n",
    "        \n",
    "        r_t = torch.sigmoid(reset_gate)\n",
    "        z_t = torch.sigmoid(update_gate)\n",
    "        \n",
    "        # Compute candidate hidden state\n",
    "        combined_reset = torch.cat([x, r_t * h], dim=1)\n",
    "        h_tilde = torch.tanh(self.linear_candidate(combined_reset))\n",
    "        \n",
    "        # Update hidden state (interpolation between old and new)\n",
    "        h_new = (1 - z_t) * h + z_t * h_tilde\n",
    "        \n",
    "        return h_new\n",
    "\n",
    "print(\"✓ GRUCell class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2**: Compare RNN architectures\n",
    "\n",
    "1. Count parameters in vanilla RNN, LSTM, and GRU with same hidden size\n",
    "2. Create a table comparing:\n",
    "   - Number of parameters\n",
    "   - Number of gates\n",
    "   - Computational complexity\n",
    "3. Discuss: When would you use each architecture?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Compare parameter counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Bidirectional RNNs\n",
    "\n",
    "**Problem**: Standard RNNs only use past context.\n",
    "\n",
    "**Solution**: Process sequence in both directions!\n",
    "\n",
    "### Bidirectional RNN:\n",
    "- **Forward RNN**: Processes left-to-right → $\\overrightarrow{h}_t$\n",
    "- **Backward RNN**: Processes right-to-left ← $\\overleftarrow{h}_t$\n",
    "- **Combine**: $h_t = [\\overrightarrow{h}_t; \\overleftarrow{h}_t]$\n",
    "\n",
    "**Use case**: Sentence classification, NER (where future context helps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    Bidirectional LSTM for sequence classification.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        \n",
    "        # Use PyTorch's efficient LSTM implementation\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True  # Key: bidirectional!\n",
    "        )\n",
    "        \n",
    "        # Output layer (hidden_size * 2 because bidirectional)\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # LSTM output: (batch_size, seq_len, hidden_size * 2)\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "        \n",
    "        # Use final time step from both directions\n",
    "        # h_n shape: (num_layers * 2, batch_size, hidden_size)\n",
    "        forward_hidden = h_n[-2, :, :]\n",
    "        backward_hidden = h_n[-1, :, :]\n",
    "        \n",
    "        # Concatenate forward and backward\n",
    "        hidden = torch.cat([forward_hidden, backward_hidden], dim=1)\n",
    "        \n",
    "        # Output\n",
    "        output = self.fc(hidden)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Test BiLSTM\n",
    "bilstm = BiLSTM(input_size=10, hidden_size=20, output_size=3)\n",
    "x = torch.randn(2, 5, 10)\n",
    "output = bilstm(x)\n",
    "print(f\"BiLSTM output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Application: Sentiment Analysis\n",
    "\n",
    "Let's apply RNNs to a real task: classifying movie reviews as positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic sentiment dataset\n",
    "# In practice, use IMDB or SST dataset\n",
    "positive_reviews = [\n",
    "    \"This movie is amazing and wonderful\",\n",
    "    \"I absolutely loved this film\",\n",
    "    \"Best movie I have seen in years\",\n",
    "    \"Incredible performance by all actors\",\n",
    "    \"Highly recommended, fantastic story\",\n",
    "] * 20\n",
    "\n",
    "negative_reviews = [\n",
    "    \"This movie is terrible and boring\",\n",
    "    \"I hated every minute of it\",\n",
    "    \"Worst film I have ever watched\",\n",
    "    \"Poor acting and weak plot\",\n",
    "    \"Do not waste your time on this\",\n",
    "] * 20\n",
    "\n",
    "# Combine and create labels\n",
    "texts = positive_reviews + negative_reviews\n",
    "labels = [1] * len(positive_reviews) + [0] * len(negative_reviews)\n",
    "\n",
    "print(f\"Dataset size: {len(texts)} reviews\")\n",
    "print(f\"Positive: {sum(labels)}, Negative: {len(labels) - sum(labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary\n",
    "def build_vocab(texts, min_freq=1):\n",
    "    \"\"\"\n",
    "    Build vocabulary from texts.\n",
    "    \"\"\"\n",
    "    word_counts = Counter()\n",
    "    for text in texts:\n",
    "        words = text.lower().split()\n",
    "        word_counts.update(words)\n",
    "    \n",
    "    # Filter by frequency\n",
    "    vocab = {word: idx + 2 for idx, (word, count) in enumerate(word_counts.items())\n",
    "             if count >= min_freq}\n",
    "    \n",
    "    # Add special tokens\n",
    "    vocab['<PAD>'] = 0\n",
    "    vocab['<UNK>'] = 1\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "vocab = build_vocab(texts)\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(f\"Sample words: {list(vocab.keys())[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text to indices\n",
    "def text_to_indices(text, vocab, max_len=20):\n",
    "    \"\"\"\n",
    "    Convert text to indices with padding.\n",
    "    \"\"\"\n",
    "    words = text.lower().split()\n",
    "    indices = [vocab.get(word, vocab['<UNK>']) for word in words]\n",
    "    \n",
    "    # Pad or truncate\n",
    "    if len(indices) < max_len:\n",
    "        indices += [vocab['<PAD>']] * (max_len - len(indices))\n",
    "    else:\n",
    "        indices = indices[:max_len]\n",
    "    \n",
    "    return indices\n",
    "\n",
    "# Convert all texts\n",
    "X = np.array([text_to_indices(text, vocab) for text in texts])\n",
    "y = np.array(labels)\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"\\nSample encoded text: {X[0]}\")\n",
    "print(f\"Original text: {texts[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Create PyTorch dataset\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = torch.LongTensor(texts)\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.labels[idx]\n",
    "\n",
    "train_dataset = SentimentDataset(X_train, y_train)\n",
    "test_dataset = SentimentDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build sentiment classifier\n",
    "class SentimentLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM-based sentiment classifier.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(SentimentLSTM, self).__init__()\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # Bidirectional LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=2,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=0.3\n",
    "        )\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Embed: (batch, seq_len) -> (batch, seq_len, embedding_dim)\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        # LSTM: (batch, seq_len, embedding_dim) -> (batch, seq_len, hidden_dim * 2)\n",
    "        lstm_out, (h_n, c_n) = self.lstm(embedded)\n",
    "        \n",
    "        # Use final hidden states from both directions\n",
    "        forward = h_n[-2, :, :]\n",
    "        backward = h_n[-1, :, :]\n",
    "        hidden = torch.cat([forward, backward], dim=1)\n",
    "        \n",
    "        # Dropout and output\n",
    "        hidden = self.dropout(hidden)\n",
    "        output = self.fc(hidden)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Initialize model\n",
    "model = SentimentLSTM(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=50,\n",
    "    hidden_dim=64,\n",
    "    output_dim=2  # Binary classification\n",
    ").to(device)\n",
    "\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_epoch(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for texts, labels in dataloader:\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(texts)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    \n",
    "    return total_loss / len(dataloader), correct / total\n",
    "\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for texts, labels in dataloader:\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            outputs = model(texts)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    return total_loss / len(dataloader), correct / total\n",
    "\n",
    "print(\"✓ Training functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, train_accs = [], []\n",
    "test_losses, test_accs = [], []\n",
    "\n",
    "print(\"Training sentiment classifier...\\n\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion)\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accs.append(test_acc)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"  Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "print(\"\\n✓ Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "ax1.plot(train_losses, label='Train Loss')\n",
    "ax1.plot(test_losses, label='Test Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Test Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "ax2.plot(train_accs, label='Train Accuracy')\n",
    "ax2.plot(test_accs, label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Training and Test Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on new examples\n",
    "def predict_sentiment(text, model, vocab):\n",
    "    model.eval()\n",
    "    indices = text_to_indices(text, vocab)\n",
    "    tensor = torch.LongTensor([indices]).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(tensor)\n",
    "        probs = F.softmax(output, dim=1)\n",
    "        pred = output.argmax(1).item()\n",
    "    \n",
    "    sentiment = \"Positive\" if pred == 1 else \"Negative\"\n",
    "    confidence = probs[0][pred].item()\n",
    "    \n",
    "    return sentiment, confidence\n",
    "\n",
    "# Test examples\n",
    "test_reviews = [\n",
    "    \"This film is absolutely wonderful and amazing\",\n",
    "    \"Terrible movie, complete waste of time\",\n",
    "    \"Not great but not terrible either\",\n",
    "]\n",
    "\n",
    "print(\"Sentiment predictions:\\n\")\n",
    "for review in test_reviews:\n",
    "    sentiment, conf = predict_sentiment(review, model, vocab)\n",
    "    print(f\"Review: {review}\")\n",
    "    print(f\"Sentiment: {sentiment} (confidence: {conf:.2%})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3**: Improve the sentiment classifier\n",
    "\n",
    "Try these improvements:\n",
    "1. Add more layers or increase hidden size\n",
    "2. Try GRU instead of LSTM\n",
    "3. Use pre-trained word embeddings (GloVe)\n",
    "4. Implement attention mechanism (preview of next module!)\n",
    "5. Compare performance of different architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Experiment with different architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4**: Visualize attention\n",
    "\n",
    "Although we haven't covered attention yet:\n",
    "1. Extract LSTM hidden states at each time step\n",
    "2. Visualize which words contribute most to the final prediction\n",
    "3. Use gradient-based attribution or simple averaging\n",
    "4. Discuss: Which words are most important for sentiment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Visualize word importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts Covered:\n",
    "\n",
    "1. **Vanilla RNN**:\n",
    "   - Recurrent connections for sequence processing\n",
    "   - Hidden state as memory\n",
    "   - Vanishing gradient problem\n",
    "\n",
    "2. **LSTM**:\n",
    "   - Gating mechanisms (forget, input, output)\n",
    "   - Cell state as long-term memory\n",
    "   - Solves vanishing gradients\n",
    "\n",
    "3. **GRU**:\n",
    "   - Simplified LSTM with 2 gates\n",
    "   - Fewer parameters, faster training\n",
    "   - Often comparable performance to LSTM\n",
    "\n",
    "4. **Bidirectional RNNs**:\n",
    "   - Process sequences in both directions\n",
    "   - Better context understanding\n",
    "   - Essential for many NLP tasks\n",
    "\n",
    "5. **Practical Application**:\n",
    "   - Sentiment analysis pipeline\n",
    "   - Data preprocessing and batching\n",
    "   - Training and evaluation\n",
    "\n",
    "### Architecture Comparison:\n",
    "\n",
    "| Architecture | Parameters | Speed | Long-term Memory | Use Case |\n",
    "|--------------|-----------|-------|------------------|----------|\n",
    "| Vanilla RNN | Low | Fast | Poor | Simple tasks |\n",
    "| LSTM | High | Slower | Excellent | Complex sequences |\n",
    "| GRU | Medium | Medium | Very good | Good default choice |\n",
    "| BiLSTM | High | Slowest | Excellent | Classification, NER |\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In **Module 04: Sequence-to-Sequence Models**, we'll learn:\n",
    "- Encoder-decoder architecture\n",
    "- Machine translation with RNNs\n",
    "- Teacher forcing and beam search\n",
    "- Setting the stage for attention mechanisms\n",
    "\n",
    "### Additional Resources:\n",
    "\n",
    "- **LSTM Paper**: [Long Short-Term Memory](http://www.bioinf.jku.at/publications/older/2604.pdf)\n",
    "- **GRU Paper**: [Learning Phrase Representations using RNN](https://arxiv.org/abs/1406.1078)\n",
    "- **Understanding LSTMs**: [Chris Olah's Blog](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "- **PyTorch RNN Tutorial**: [pytorch.org/tutorials](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
