{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 02: Word Embeddings\n",
    "\n",
    "**Difficulty**: ⭐⭐ Intermediate  \n",
    "**Estimated Time**: 120 minutes  \n",
    "**Prerequisites**: [Module 01: Text Preprocessing](01_text_preprocessing.ipynb)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Understand the concept and mathematics of word embeddings\n",
    "2. Implement and train Word2Vec (Skip-gram and CBOW) models\n",
    "3. Use pre-trained GloVe and FastText embeddings\n",
    "4. Discover semantic relationships (analogies like king-man+woman=queen)\n",
    "5. Visualize embeddings using t-SNE and PCA\n",
    "6. Compare different embedding methods and their trade-offs\n",
    "\n",
    "## What are Word Embeddings?\n",
    "\n",
    "**Word embeddings** are dense vector representations of words in a continuous vector space, where semantically similar words are mapped to nearby points.\n",
    "\n",
    "### From Sparse to Dense Representations\n",
    "\n",
    "**Traditional (One-hot encoding)**:\n",
    "- \"cat\" = [0, 0, 1, 0, 0, ..., 0] (vocabulary size = 50,000)\n",
    "- **Problem**: No notion of similarity, very sparse\n",
    "\n",
    "**Modern (Word embeddings)**:\n",
    "- \"cat\" = [0.2, -0.4, 0.7, ..., 0.1] (typically 100-300 dimensions)\n",
    "- **Benefit**: Similar words have similar vectors\n",
    "\n",
    "### The Distributional Hypothesis\n",
    "\n",
    "> \"You shall know a word by the company it keeps\" - J.R. Firth (1957)\n",
    "\n",
    "Words that appear in similar contexts have similar meanings:\n",
    "- \"The cat sits on the mat\" ≈ \"The dog sits on the mat\"\n",
    "- Therefore: cat ≈ dog (in vector space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NLP and embeddings\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# PyTorch for custom implementation\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Dimensionality reduction\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Visualization\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"✓ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('brown', quiet=True)\n",
    "\n",
    "print(\"✓ NLTK data downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Word Embeddings: A Visual Introduction\n",
    "\n",
    "Let's start by creating a simple example to understand how embeddings capture meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple corpus for demonstration\n",
    "corpus = [\n",
    "    \"The cat sits on the mat\",\n",
    "    \"The dog sits on the log\",\n",
    "    \"Cats and dogs are animals\",\n",
    "    \"The cat and dog play together\",\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"A cat and a fox are different animals\",\n",
    "]\n",
    "\n",
    "# Tokenize\n",
    "tokenized_corpus = [word_tokenize(sentence.lower()) for sentence in corpus]\n",
    "\n",
    "print(\"Sample corpus:\")\n",
    "for i, tokens in enumerate(tokenized_corpus, 1):\n",
    "    print(f\"{i}. {tokens}\")\n",
    "\n",
    "# Build vocabulary\n",
    "vocab = set()\n",
    "for tokens in tokenized_corpus:\n",
    "    vocab.update(tokens)\n",
    "\n",
    "print(f\"\\nVocabulary size: {len(vocab)}\")\n",
    "print(f\"Vocabulary: {sorted(vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Co-occurrence Matrix\n",
    "\n",
    "Before learning about Word2Vec, let's understand co-occurrence: how often words appear together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cooccurrence_matrix(tokenized_corpus, window_size=2):\n",
    "    \"\"\"\n",
    "    Build word co-occurrence matrix.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    tokenized_corpus : list of list of str\n",
    "        Tokenized sentences\n",
    "    window_size : int\n",
    "        Context window size (words before/after)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame : Co-occurrence matrix\n",
    "    \"\"\"\n",
    "    vocab = sorted(set([word for tokens in tokenized_corpus for word in tokens]))\n",
    "    word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
    "    \n",
    "    # Initialize matrix\n",
    "    cooc_matrix = np.zeros((len(vocab), len(vocab)))\n",
    "    \n",
    "    # Count co-occurrences\n",
    "    for tokens in tokenized_corpus:\n",
    "        for i, word in enumerate(tokens):\n",
    "            # Get context words within window\n",
    "            start = max(0, i - window_size)\n",
    "            end = min(len(tokens), i + window_size + 1)\n",
    "            \n",
    "            for j in range(start, end):\n",
    "                if i != j:\n",
    "                    word_idx = word_to_idx[word]\n",
    "                    context_idx = word_to_idx[tokens[j]]\n",
    "                    cooc_matrix[word_idx, context_idx] += 1\n",
    "    \n",
    "    return pd.DataFrame(cooc_matrix, index=vocab, columns=vocab)\n",
    "\n",
    "# Build co-occurrence matrix\n",
    "cooc_df = build_cooccurrence_matrix(tokenized_corpus, window_size=2)\n",
    "\n",
    "# Show subset for key words\n",
    "key_words = ['cat', 'dog', 'fox', 'animal', 'sits']\n",
    "subset = cooc_df.loc[key_words, key_words]\n",
    "\n",
    "print(\"Co-occurrence matrix (subset):\")\n",
    "print(subset.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize co-occurrence matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(subset, annot=True, fmt='g', cmap='YlOrRd', cbar_kws={'label': 'Co-occurrence count'})\n",
    "plt.title('Word Co-occurrence Matrix (Window=2)')\n",
    "plt.xlabel('Context Words')\n",
    "plt.ylabel('Target Words')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservation: 'cat' and 'dog' appear in similar contexts (high co-occurrence with 'the', 'and')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Word2Vec: Skip-gram and CBOW\n",
    "\n",
    "**Word2Vec** (Mikolov et al., 2013) learns word embeddings by predicting context from words or vice versa.\n",
    "\n",
    "### Two Architectures:\n",
    "\n",
    "**1. Skip-gram**: Predict context words from target word\n",
    "- Input: \"cat\"\n",
    "- Output: [\"the\", \"sits\", \"on\", \"the\"]\n",
    "- Better for rare words, larger datasets\n",
    "\n",
    "**2. CBOW (Continuous Bag of Words)**: Predict target word from context\n",
    "- Input: [\"the\", \"sits\", \"on\", \"the\"]\n",
    "- Output: \"cat\"\n",
    "- Faster, better for frequent words\n",
    "\n",
    "### Training Objective:\n",
    "\n",
    "Maximize the probability of observing actual context words given the target word (Skip-gram):\n",
    "\n",
    "$$\\text{maximize} \\sum_{t=1}^{T} \\sum_{-c \\leq j \\leq c, j \\neq 0} \\log p(w_{t+j} | w_t)$$\n",
    "\n",
    "Where:\n",
    "- $w_t$ = target word at position t\n",
    "- $w_{t+j}$ = context word\n",
    "- $c$ = context window size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Training Word2Vec with Gensim\n",
    "\n",
    "Let's train Word2Vec on a larger corpus using Gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Brown corpus for training\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# Get sentences from Brown corpus (more data for better embeddings)\n",
    "brown_sentences = brown.sents()[:10000]  # Use first 10,000 sentences\n",
    "\n",
    "# Lowercase all words\n",
    "brown_sentences = [[word.lower() for word in sent] for sent in brown_sentences]\n",
    "\n",
    "print(f\"Training corpus: {len(brown_sentences)} sentences\")\n",
    "print(f\"Sample sentence: {brown_sentences[0][:15]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Skip-gram model\n",
    "skipgram_model = Word2Vec(\n",
    "    sentences=brown_sentences,\n",
    "    vector_size=100,      # Embedding dimension\n",
    "    window=5,             # Context window size\n",
    "    min_count=5,          # Ignore words with frequency < 5\n",
    "    sg=1,                 # 1 = Skip-gram, 0 = CBOW\n",
    "    workers=4,            # Number of threads\n",
    "    epochs=10,            # Training epochs\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"✓ Skip-gram model trained!\")\n",
    "print(f\"Vocabulary size: {len(skipgram_model.wv)}\")\n",
    "print(f\"Embedding dimension: {skipgram_model.wv.vector_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CBOW model for comparison\n",
    "cbow_model = Word2Vec(\n",
    "    sentences=brown_sentences,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=5,\n",
    "    sg=0,  # CBOW\n",
    "    workers=4,\n",
    "    epochs=10,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"✓ CBOW model trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Exploring Word Similarities\n",
    "\n",
    "The magic of word embeddings: finding similar words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find similar words\n",
    "test_words = ['king', 'computer', 'happy', 'run']\n",
    "\n",
    "for word in test_words:\n",
    "    if word in skipgram_model.wv:\n",
    "        similar = skipgram_model.wv.most_similar(word, topn=5)\n",
    "        print(f\"\\nWords similar to '{word}':\")\n",
    "        for sim_word, score in similar:\n",
    "            print(f\"  {sim_word:15} (similarity: {score:.3f})\")\n",
    "    else:\n",
    "        print(f\"\\n'{word}' not in vocabulary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Skip-gram vs CBOW\n",
    "test_word = 'good'\n",
    "\n",
    "if test_word in skipgram_model.wv and test_word in cbow_model.wv:\n",
    "    sg_similar = skipgram_model.wv.most_similar(test_word, topn=5)\n",
    "    cbow_similar = cbow_model.wv.most_similar(test_word, topn=5)\n",
    "    \n",
    "    print(f\"Similar words to '{test_word}':\\n\")\n",
    "    print(\"Skip-gram:\".ljust(30) + \"CBOW:\")\n",
    "    print(\"-\" * 60)\n",
    "    for (sg_word, sg_score), (cbow_word, cbow_score) in zip(sg_similar, cbow_similar):\n",
    "        print(f\"{sg_word:15} ({sg_score:.3f})    {cbow_word:15} ({cbow_score:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Word Analogies: The Famous King - Man + Woman = Queen\n",
    "\n",
    "Word embeddings capture semantic relationships through vector arithmetic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_analogy(model, word_a, word_b, word_c, topn=5):\n",
    "    \"\"\"\n",
    "    Solve word analogy: word_a is to word_b as word_c is to ?\n",
    "    \n",
    "    Example: king is to man as queen is to woman\n",
    "    Formula: king - man + woman ≈ queen\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = model.wv.most_similar(\n",
    "            positive=[word_a, word_c],  # king + woman\n",
    "            negative=[word_b],           # - man\n",
    "            topn=topn\n",
    "        )\n",
    "        return result\n",
    "    except KeyError as e:\n",
    "        return f\"Word not in vocabulary: {e}\"\n",
    "\n",
    "# Test analogies\n",
    "analogies = [\n",
    "    ('king', 'man', 'woman'),     # king - man + woman = ?\n",
    "    ('good', 'better', 'bad'),    # good - better + bad = ?\n",
    "    ('france', 'paris', 'italy'), # france - paris + italy = ?\n",
    "]\n",
    "\n",
    "for word_a, word_b, word_c in analogies:\n",
    "    print(f\"\\n{word_a} - {word_b} + {word_c} = ?\")\n",
    "    result = solve_analogy(skipgram_model, word_a, word_b, word_c, topn=3)\n",
    "    if isinstance(result, list):\n",
    "        for word, score in result:\n",
    "            print(f\"  {word:15} (score: {score:.3f})\")\n",
    "    else:\n",
    "        print(f\"  {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1**: Create and test your own analogies\n",
    "\n",
    "Come up with 3 word analogies and test them using the Skip-gram model. Try different types:\n",
    "1. Gender relationships (king/queen, man/woman)\n",
    "2. Comparative forms (good/better, bad/worse)\n",
    "3. Geography (country/capital)\n",
    "4. Tense (walk/walked, run/ran)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Test your own analogies\n",
    "\n",
    "my_analogies = [\n",
    "    # Add your analogies as (word_a, word_b, word_c) tuples\n",
    "]\n",
    "\n",
    "# Test them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. GloVe: Global Vectors for Word Representation\n",
    "\n",
    "**GloVe** (Pennington et al., 2014) combines:\n",
    "- Global matrix factorization (like LSA)\n",
    "- Local context window methods (like Word2Vec)\n",
    "\n",
    "**Key insight**: Ratios of co-occurrence probabilities encode meaning.\n",
    "\n",
    "### Using Pre-trained GloVe Embeddings\n",
    "\n",
    "Pre-trained embeddings are trained on massive corpora (billions of words) and work better than training from scratch on small datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download GloVe embeddings (this uses a small version)\n",
    "# In practice, download from: https://nlp.stanford.edu/projects/glove/\n",
    "# For this demo, we'll use gensim's downloader\n",
    "\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Load pre-trained GloVe (this may take a few minutes first time)\n",
    "print(\"Loading GloVe embeddings... (this may take a minute)\")\n",
    "glove_model = api.load('glove-wiki-gigaword-100')  # 100-dim GloVe trained on Wikipedia\n",
    "\n",
    "print(f\"✓ GloVe loaded!\")\n",
    "print(f\"Vocabulary size: {len(glove_model)}\")\n",
    "print(f\"Embedding dimension: {glove_model.vector_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test GloVe on analogies\n",
    "print(\"Testing GloVe on word analogies:\\n\")\n",
    "\n",
    "# King - Man + Woman = ?\n",
    "result = glove_model.most_similar(\n",
    "    positive=['king', 'woman'],\n",
    "    negative=['man'],\n",
    "    topn=5\n",
    ")\n",
    "\n",
    "print(\"king - man + woman = ?\")\n",
    "for word, score in result:\n",
    "    print(f\"  {word:15} (score: {score:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More GloVe examples\n",
    "test_words = ['python', 'neural', 'learning', 'beautiful']\n",
    "\n",
    "for word in test_words:\n",
    "    similar = glove_model.most_similar(word, topn=5)\n",
    "    print(f\"\\nWords similar to '{word}':\")\n",
    "    for sim_word, score in similar:\n",
    "        print(f\"  {sim_word:15} (similarity: {score:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2**: Compare Word2Vec and GloVe\n",
    "\n",
    "For the same set of words, compare the similar words found by:\n",
    "1. Your trained Skip-gram model\n",
    "2. Pre-trained GloVe\n",
    "\n",
    "Discuss:\n",
    "- Which gives more meaningful similarities?\n",
    "- Why might GloVe perform better?\n",
    "- When would you use each?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Compare Skip-gram and GloVe on the same words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. FastText: Subword Embeddings\n",
    "\n",
    "**FastText** (Bojanowski et al., 2016) extends Word2Vec by representing words as bags of character n-grams.\n",
    "\n",
    "**Example**: \"running\" = [\"run\", \"runn\", \"unni\", \"nnin\", \"ning\", \"running\"]\n",
    "\n",
    "**Advantages**:\n",
    "- Handles out-of-vocabulary (OOV) words\n",
    "- Better for morphologically rich languages\n",
    "- Can generate embeddings for misspelled words\n",
    "\n",
    "**Example**:\n",
    "- Word2Vec: \"running\" seen, \"runned\" (wrong) → OOV error\n",
    "- FastText: \"runned\" → composed from \"run\", \"runn\", \"nned\" → valid embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train FastText model\n",
    "fasttext_model = FastText(\n",
    "    sentences=brown_sentences,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=5,\n",
    "    workers=4,\n",
    "    sg=1,  # Skip-gram\n",
    "    epochs=10,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"✓ FastText model trained!\")\n",
    "print(f\"Vocabulary size: {len(fasttext_model.wv)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test FastText on OOV words\n",
    "# Create misspelled/new words\n",
    "oov_words = ['runned', 'computering', 'happyness']  # Not in original vocab\n",
    "\n",
    "print(\"Testing out-of-vocabulary words:\\n\")\n",
    "\n",
    "for word in oov_words:\n",
    "    # Word2Vec would fail on OOV\n",
    "    in_w2v = word in skipgram_model.wv\n",
    "    \n",
    "    # FastText can handle OOV via subwords\n",
    "    if not in_w2v:\n",
    "        try:\n",
    "            # FastText can generate embedding even for OOV\n",
    "            embedding = fasttext_model.wv[word]\n",
    "            similar = fasttext_model.wv.most_similar(word, topn=3)\n",
    "            \n",
    "            print(f\"'{word}' (OOV):\")\n",
    "            print(f\"  Embedding shape: {embedding.shape}\")\n",
    "            print(f\"  Similar words:\")\n",
    "            for sim_word, score in similar:\n",
    "                print(f\"    {sim_word:15} ({score:.3f})\")\n",
    "            print()\n",
    "        except:\n",
    "            print(f\"'{word}': Could not generate embedding\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3**: Subword analysis\n",
    "\n",
    "1. Create a function to extract character n-grams from a word (like FastText does)\n",
    "2. Compare embeddings for morphologically related words:\n",
    "   - \"happy\", \"happier\", \"happiest\", \"happiness\", \"unhappy\"\n",
    "3. Calculate cosine similarities between them\n",
    "4. Discuss: Do morphologically related words have similar embeddings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "def extract_ngrams(word, n=3):\n",
    "    \"\"\"\n",
    "    Extract character n-grams from a word.\n",
    "    \n",
    "    Example: extract_ngrams('cat', n=3) → ['<ca', 'cat', 'at>']\n",
    "    \"\"\"\n",
    "    # Add special boundary markers\n",
    "    word = f'<{word}>'\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "# Test on morphological variants\n",
    "variants = ['happy', 'happier', 'happiest', 'happiness', 'unhappy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualizing Embeddings\n",
    "\n",
    "Embeddings live in high-dimensional space (typically 100-300 dimensions). We can visualize them in 2D using dimensionality reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 t-SNE Visualization\n",
    "\n",
    "**t-SNE** (t-Distributed Stochastic Neighbor Embedding) preserves local structure, making it great for visualizing clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select words to visualize\n",
    "words_to_plot = [\n",
    "    # Animals\n",
    "    'dog', 'cat', 'horse', 'lion', 'tiger',\n",
    "    # Countries\n",
    "    'france', 'germany', 'italy', 'spain',\n",
    "    # Numbers\n",
    "    'one', 'two', 'three', 'four', 'five',\n",
    "    # Colors\n",
    "    'red', 'blue', 'green', 'yellow',\n",
    "    # Emotions\n",
    "    'happy', 'sad', 'angry', 'excited'\n",
    "]\n",
    "\n",
    "# Filter to words in vocabulary\n",
    "words_in_vocab = [w for w in words_to_plot if w in glove_model]\n",
    "\n",
    "# Get embeddings\n",
    "embeddings = np.array([glove_model[word] for word in words_in_vocab])\n",
    "\n",
    "print(f\"Visualizing {len(words_in_vocab)} words\")\n",
    "print(f\"Embedding matrix shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=5)\n",
    "embeddings_2d = tsne.fit_transform(embeddings)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(14, 10))\n",
    "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], alpha=0.7, s=100)\n",
    "\n",
    "# Annotate points\n",
    "for i, word in enumerate(words_in_vocab):\n",
    "    plt.annotate(word, (embeddings_2d[i, 0], embeddings_2d[i, 1]),\n",
    "                fontsize=12, alpha=0.8)\n",
    "\n",
    "plt.title('t-SNE Visualization of Word Embeddings (GloVe)', fontsize=16)\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observation: Semantically similar words cluster together!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 PCA Visualization\n",
    "\n",
    "**PCA** (Principal Component Analysis) preserves global structure and is faster than t-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "embeddings_pca = pca.fit_transform(embeddings)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(14, 10))\n",
    "plt.scatter(embeddings_pca[:, 0], embeddings_pca[:, 1], alpha=0.7, s=100)\n",
    "\n",
    "for i, word in enumerate(words_in_vocab):\n",
    "    plt.annotate(word, (embeddings_pca[i, 0], embeddings_pca[i, 1]),\n",
    "                fontsize=12, alpha=0.8)\n",
    "\n",
    "plt.title('PCA Visualization of Word Embeddings (GloVe)', fontsize=16)\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4**: Create a semantic visualization\n",
    "\n",
    "1. Choose a semantic category (e.g., programming languages, foods, sports)\n",
    "2. Select 20-30 related words\n",
    "3. Visualize using both t-SNE and PCA\n",
    "4. Use different colors for subcategories\n",
    "5. Discuss: Which visualization method works better for your data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Create your own semantic visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Embedding Quality and Evaluation\n",
    "\n",
    "How do we measure if embeddings are good?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Intrinsic Evaluation: Word Similarity\n",
    "\n",
    "Compare embedding similarities with human judgments using benchmark datasets (e.g., WordSim-353)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on word similarity task\n",
    "# SimLex-999 dataset is built into Gensim\n",
    "correlation = glove_model.evaluate_word_pairs(\n",
    "    'wordsim353.tsv',\n",
    "    dummy4unknown=True\n",
    ")[0][0]  # Pearson correlation\n",
    "\n",
    "print(f\"Word similarity correlation: {correlation:.3f}\")\n",
    "print(\"(Higher is better; > 0.6 is good)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Analogy Accuracy\n",
    "\n",
    "Test on standard analogy datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simple analogy test\n",
    "test_analogies = [\n",
    "    # Format: (word_a, word_b, word_c, expected_word_d)\n",
    "    ('man', 'woman', 'king', 'queen'),\n",
    "    ('man', 'woman', 'boy', 'girl'),\n",
    "    ('good', 'better', 'bad', 'worse'),\n",
    "    ('big', 'bigger', 'small', 'smaller'),\n",
    "]\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for a, b, c, expected in test_analogies:\n",
    "    if all(w in glove_model for w in [a, b, c, expected]):\n",
    "        result = glove_model.most_similar(positive=[c, b], negative=[a], topn=3)\n",
    "        predicted = result[0][0]\n",
    "        \n",
    "        total += 1\n",
    "        if predicted == expected:\n",
    "            correct += 1\n",
    "            status = \"✓\"\n",
    "        else:\n",
    "            status = \"✗\"\n",
    "        \n",
    "        print(f\"{status} {a}:{b} :: {c}:{expected} → predicted: {predicted}\")\n",
    "\n",
    "accuracy = correct / total if total > 0 else 0\n",
    "print(f\"\\nAnalogy accuracy: {accuracy:.1%} ({correct}/{total})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Limitations of Static Word Embeddings\n",
    "\n",
    "Despite their power, Word2Vec/GloVe/FastText have critical limitations:\n",
    "\n",
    "### 7.1 No Context Awareness\n",
    "\n",
    "The word \"bank\" has the same embedding whether it means:\n",
    "- Financial institution: \"I went to the **bank** to deposit money\"\n",
    "- River edge: \"We sat on the river **bank**\"\n",
    "\n",
    "**Problem**: One vector per word, regardless of context!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the problem\n",
    "sentences = [\n",
    "    \"The bank approved my loan application\",\n",
    "    \"We walked along the river bank at sunset\"\n",
    "]\n",
    "\n",
    "# Get embedding for 'bank' (same for both contexts!)\n",
    "if 'bank' in glove_model:\n",
    "    bank_embedding = glove_model['bank']\n",
    "    similar_words = glove_model.most_similar('bank', topn=5)\n",
    "    \n",
    "    print(\"The word 'bank' has only ONE embedding, regardless of context:\")\n",
    "    print(f\"\\nEmbedding shape: {bank_embedding.shape}\")\n",
    "    print(f\"\\nMost similar words to 'bank':\")\n",
    "    for word, score in similar_words:\n",
    "        print(f\"  {word:15} ({score:.3f})\")\n",
    "    \n",
    "    print(\"\\n⚠ This single embedding must represent BOTH financial and geographical meanings!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Other Limitations\n",
    "\n",
    "1. **Fixed vocabulary**: Can't easily add new words after training\n",
    "2. **Training data bias**: Embeddings inherit biases from training data\n",
    "3. **No sentence/document representation**: Must aggregate word vectors somehow\n",
    "4. **Polysemy**: Multiple meanings per word not distinguished\n",
    "\n",
    "### The Solution: Contextual Embeddings\n",
    "\n",
    "Modern models (BERT, GPT, etc.) generate **different** embeddings for the same word in different contexts!\n",
    "\n",
    "We'll learn about these in **Module 07: BERT and Masked Language Modeling**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5**: Bias exploration\n",
    "\n",
    "Word embeddings can contain societal biases from training data. Investigate:\n",
    "\n",
    "1. Test analogies like: \"man is to doctor as woman is to ?\"\n",
    "2. Compare similar words for gendered terms: \"man\", \"woman\", \"he\", \"she\"\n",
    "3. Test occupation analogies and observe any gender biases\n",
    "4. Discuss: What are the implications of these biases in real applications?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Explore potential biases in embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts Covered:\n",
    "\n",
    "1. **Word Embeddings Fundamentals**:\n",
    "   - Dense vector representations of words\n",
    "   - Distributional hypothesis: similar contexts → similar meanings\n",
    "   - Reduces dimensionality while capturing semantics\n",
    "\n",
    "2. **Word2Vec**:\n",
    "   - Skip-gram: Predict context from word\n",
    "   - CBOW: Predict word from context\n",
    "   - Training objective: Maximize context prediction probability\n",
    "\n",
    "3. **Other Methods**:\n",
    "   - GloVe: Global co-occurrence statistics\n",
    "   - FastText: Subword embeddings for OOV handling\n",
    "\n",
    "4. **Applications**:\n",
    "   - Semantic similarity\n",
    "   - Word analogies (king - man + woman = queen)\n",
    "   - Clustering and visualization\n",
    "\n",
    "5. **Limitations**:\n",
    "   - No context awareness (same word = same embedding)\n",
    "   - Polysemy problem\n",
    "   - Potential biases\n",
    "\n",
    "### Comparison Table:\n",
    "\n",
    "| Method | Pros | Cons | Best For |\n",
    "|--------|------|------|----------|\n",
    "| Word2Vec | Fast, efficient, good analogies | No context, OOV issues | Large datasets |\n",
    "| GloVe | Captures global statistics | Slower training | Pre-trained use |\n",
    "| FastText | Handles OOV, morphology | Larger model size | Morphologically rich languages |\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In **Module 03: Recurrent Neural Networks**, we'll learn:\n",
    "- How to process sequential data (sentences)\n",
    "- LSTM and GRU architectures\n",
    "- Sequence classification and generation\n",
    "- Moving beyond static embeddings\n",
    "\n",
    "### Additional Resources:\n",
    "\n",
    "- **Word2Vec Paper**: [Efficient Estimation of Word Representations](https://arxiv.org/abs/1301.3781)\n",
    "- **GloVe Paper**: [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf)\n",
    "- **FastText Paper**: [Enriching Word Vectors with Subword Information](https://arxiv.org/abs/1607.04606)\n",
    "- **Interactive Demo**: [Embedding Projector](https://projector.tensorflow.org/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
