{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Module 07: BERT and Masked Language Modeling\n\n**Difficulty**: \u2b50\u2b50\u2b50 Advanced  \n**Estimated Time**: 120 minutes  \n**Prerequisites**: [Module 06: Transformer Architecture](06_transformer_architecture.ipynb)\n\n## Learning Objectives\n\n1. Understand BERT's bidirectional pre-training approach\n2. Implement masked language modeling (MLM)\n3. Understand next sentence prediction (NSP)\n4. Use pre-trained BERT from Hugging Face\n5. Fine-tune BERT for downstream tasks\n6. Compare BERT with other pre-training approaches\n\n## BERT: Bidirectional Encoder Representations from Transformers\n\n**Key Innovation**: Pre-train bidirectional representations by masking random tokens.\n\n### Why BERT Matters:\n\n- **Before BERT**: Models were either left-to-right (GPT) or shallow bidirectional\n- **BERT**: Deep bidirectional understanding\n- **Result**: State-of-the-art on 11 NLP tasks\n\n### Architecture:\n\n- **Encoder-only** Transformer (no decoder)\n- **BERT-Base**: 12 layers, 768 hidden, 12 heads = 110M parameters\n- **BERT-Large**: 24 layers, 1024 hidden, 16 heads = 340M parameters"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Setup and Imports"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import BertTokenizer, BertModel, BertForMaskedLM, BertConfig\nfrom transformers import AutoTokenizer, AutoModel\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Using device: {device}')\nprint('\u2713 Libraries imported!')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Masked Language Modeling (MLM)\n\n**Training objective**: Predict randomly masked tokens.\n\n**Example**:\n- Input: \"The [MASK] sat on the [MASK]\"\n- Target: \"cat\", \"mat\"\n\n**Masking strategy** (15% of tokens):\n- 80%: Replace with [MASK]\n- 10%: Replace with random word\n- 10%: Keep unchanged\n\n**Why?** Forces bidirectional understanding!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load BERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# Example sentence\ntext = \"The quick brown fox jumps over the lazy dog\"\n\n# Tokenize\ntokens = tokenizer.tokenize(text)\nprint(f'Tokens: {tokens}')\n\n# Convert to IDs\ninput_ids = tokenizer.encode(text, add_special_tokens=True)\nprint(f'\\nInput IDs: {input_ids}')\n\n# Decode back\ndecoded = tokenizer.decode(input_ids)\nprint(f'Decoded: {decoded}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Demonstrate masking\nfrom transformers import pipeline\n\n# Load masked LM pipeline\nmlm = pipeline('fill-mask', model='bert-base-uncased')\n\n# Test MLM\ntest_sentences = [\n    \"The cat [MASK] on the mat.\",\n    \"Paris is the [MASK] of France.\",\n    \"I love [MASK] learning.\"\n]\n\nfor sent in test_sentences:\n    results = mlm(sent)\n    print(f\"\\nSentence: {sent}\")\n    print(\"Top predictions:\")\n    for i, result in enumerate(results[:3], 1):\n        print(f\"  {i}. {result['token_str']:15} (score: {result['score']:.3f})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Using Pre-trained BERT\n\n**Hugging Face** provides easy access to pre-trained models."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load pre-trained BERT\nmodel = BertModel.from_pretrained('bert-base-uncased')\nmodel.to(device)\nmodel.eval()\n\nprint(f'\u2713 BERT loaded!')\nprint(f'Parameters: {sum(p.numel() for p in model.parameters()):,}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Extract embeddings\ntext = \"BERT provides contextualized word embeddings\"\n\n# Tokenize\ninputs = tokenizer(text, return_tensors='pt', padding=True).to(device)\n\n# Get BERT outputs\nwith torch.no_grad():\n    outputs = model(**inputs)\n\n# Extract embeddings\nlast_hidden_state = outputs.last_hidden_state  # (batch, seq_len, hidden_dim)\ncls_embedding = last_hidden_state[:, 0, :]  # [CLS] token embedding\n\nprint(f'Last hidden state shape: {last_hidden_state.shape}')\nprint(f'[CLS] embedding shape: {cls_embedding.shape}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Fine-Tuning BERT\n\n**Transfer learning workflow**:\n1. Load pre-trained BERT\n2. Add task-specific head\n3. Fine-tune on target task\n4. Achieve SOTA with less data!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from transformers import BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n\n# Load BERT for classification\nmodel = BertForSequenceClassification.from_pretrained(\n    'bert-base-uncased',\n    num_labels=2,  # Binary classification\n    output_attentions=False,\n    output_hidden_states=False\n)\n\nmodel.to(device)\nprint('\u2713 BERT classifier loaded!')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Exercise 1**: Fine-tune BERT for sentiment analysis\n\n1. Load IMDB or SST dataset\n2. Tokenize with BERT tokenizer\n3. Fine-tune BERT classifier\n4. Evaluate performance\n5. Compare with RNN baseline"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# YOUR CODE HERE\n# Fine-tune BERT for sentiment analysis"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. BERT Variants\n\n### Popular BERT derivatives:\n\n- **RoBERTa**: Optimized training (no NSP, larger batches)\n- **ALBERT**: Parameter sharing for efficiency\n- **DistilBERT**: Smaller, faster (66% size, 95% performance)\n- **ELECTRA**: Discriminative pre-training\n- **DeBERTa**: Disentangled attention"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compare different BERT models\nmodels_to_compare = [\n    'bert-base-uncased',\n    'distilbert-base-uncased',\n    'roberta-base'\n]\n\nfor model_name in models_to_compare:\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModel.from_pretrained(model_name)\n    \n    params = sum(p.numel() for p in model.parameters())\n    print(f'{model_name:30} Parameters: {params:,}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\n### Key Concepts:\n\n1. **Masked Language Modeling**: Pre-training via masking\n2. **Bidirectional Context**: Deep understanding\n3. **Transfer Learning**: Pre-train then fine-tune\n4. **Contextualized Embeddings**: Same word, different vectors\n\n### BERT Impact:\n\n\u2705 State-of-the-art on many tasks  \n\u2705 Efficient transfer learning  \n\u2705 Spawned many variants  \n\u2705 Foundation for modern NLP  \n\n### What's Next?\n\nIn **Module 08: GPT**, we'll learn about autoregressive (decoder-only) models.\n\n### Resources:\n\n- **BERT Paper**: [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805)\n- **Illustrated BERT**: [Jay Alammar's Blog](http://jalammar.github.io/illustrated-bert/)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}