{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Module 05: Attention Mechanism\n\n**Difficulty**: \u2b50\u2b50\u2b50 Advanced  \n**Estimated Time**: 130 minutes  \n**Prerequisites**: [Module 04: Sequence-to-Sequence Models](04_sequence_to_sequence.ipynb)\n\n## Learning Objectives\n\nBy the end of this notebook, you will be able to:\n\n1. Understand the motivation and intuition behind attention mechanisms\n2. Implement Bahdanau (additive) attention from scratch\n3. Implement Luong (multiplicative) attention\n4. Visualize attention weights to interpret model decisions\n5. Understand self-attention as precursor to transformers\n6. Compare different attention mechanisms and their trade-offs\n\n## The Attention Revolution\n\n### Problem with Standard Seq2Seq:\n\nIn vanilla seq2seq, the **entire input** is compressed into a **single fixed-size context vector**.\n\n**Issues**:\n- Information bottleneck for long sequences\n- Encoder must remember everything\n- Performance degrades with sequence length\n\n### Solution: Attention Mechanism\n\n**Key insight**: Let the decoder **attend to different parts** of the input at each decoding step!\n\n**Analogy**: When translating \"The cat sat on the mat\" to French:\n- When generating \"chat\" (cat), focus on \"cat\"\n- When generating \"le\" (the), focus on \"the\"\n- Dynamic focus on relevant input parts!"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Setup and Imports"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Core libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# PyTorch\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\n# Visualization\n%matplotlib inline\nplt.style.use('seaborn-v0_8-darkgrid')\n\n# Random seeds\nnp.random.seed(42)\nrandom.seed(42)\ntorch.manual_seed(42)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\nprint(\"\u2713 All libraries imported successfully!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Attention Intuition\n\n### How Attention Works:\n\n1. **Query (Q)**: Current decoder state (what we're looking for)\n2. **Keys (K)**: Encoder hidden states (what we can attend to)\n3. **Values (V)**: Also encoder hidden states (what we retrieve)\n\n**Steps**:\n1. Compute alignment scores: $e_{ij} = \\text{score}(h_i^{dec}, h_j^{enc})$\n2. Normalize with softmax: $\\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_k \\exp(e_{ik})}$ (attention weights)\n3. Weighted sum: $c_i = \\sum_j \\alpha_{ij} h_j^{enc}$ (context vector)\n\n**Result**: Different context vector for each decoder step!"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Bahdanau Attention (Additive)\n\n**Bahdanau et al., 2015**: First attention mechanism for NMT.\n\n**Score function**:\n$$\\text{score}(h_i^{dec}, h_j^{enc}) = v^T \\tanh(W_1 h_i^{dec} + W_2 h_j^{enc})$$\n\nWhere $v$, $W_1$, $W_2$ are learned parameters."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class BahdanauAttention(nn.Module):\n    \"\"\"\n    Bahdanau (additive) attention mechanism.\n    \n    Reference: \"Neural Machine Translation by Jointly Learning to Align and Translate\"\n    Bahdanau et al., ICLR 2015\n    \"\"\"\n    \n    def __init__(self, hidden_dim):\n        \"\"\"\n        Parameters:\n        -----------\n        hidden_dim : int\n            Dimension of hidden states\n        \"\"\"\n        super(BahdanauAttention, self).__init__()\n        \n        # Learned parameters\n        self.W_query = nn.Linear(hidden_dim, hidden_dim, bias=False)\n        self.W_key = nn.Linear(hidden_dim, hidden_dim, bias=False)\n        self.v = nn.Linear(hidden_dim, 1, bias=False)\n        \n    def forward(self, query, keys, values, mask=None):\n        \"\"\"\n        Compute attention.\n        \n        Parameters:\n        -----------\n        query : torch.Tensor\n            Decoder hidden state (batch_size, hidden_dim)\n        keys : torch.Tensor\n            Encoder hidden states (batch_size, seq_len, hidden_dim)\n        values : torch.Tensor\n            Encoder hidden states (batch_size, seq_len, hidden_dim)\n        mask : torch.Tensor or None\n            Padding mask (batch_size, seq_len)\n            \n        Returns:\n        --------\n        context : torch.Tensor\n            Attention-weighted context (batch_size, hidden_dim)\n        attention_weights : torch.Tensor\n            Attention distribution (batch_size, seq_len)\n        \"\"\"\n        # Expand query to match keys shape\n        # query: (batch, hidden) -> (batch, 1, hidden) -> (batch, seq_len, hidden)\n        query_expanded = query.unsqueeze(1)\n        \n        # Compute alignment scores\n        # score = v^T * tanh(W_query * query + W_key * keys)\n        scores = self.v(torch.tanh(\n            self.W_query(query_expanded) + self.W_key(keys)\n        )).squeeze(-1)  # (batch, seq_len)\n        \n        # Apply mask if provided (set padded positions to -inf)\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, float('-inf'))\n        \n        # Softmax to get attention weights\n        attention_weights = F.softmax(scores, dim=1)  # (batch, seq_len)\n        \n        # Weighted sum of values\n        context = torch.bmm(\n            attention_weights.unsqueeze(1),  # (batch, 1, seq_len)\n            values  # (batch, seq_len, hidden)\n        ).squeeze(1)  # (batch, hidden)\n        \n        return context, attention_weights\n\nprint(\"\u2713 BahdanauAttention class defined!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test Bahdanau attention\nbatch_size = 2\nseq_len = 5\nhidden_dim = 8\n\n# Create dummy data\nquery = torch.randn(batch_size, hidden_dim)  # Current decoder state\nkeys = torch.randn(batch_size, seq_len, hidden_dim)  # Encoder states\nvalues = keys  # Usually same as keys\n\n# Initialize attention\nattention = BahdanauAttention(hidden_dim)\n\n# Forward pass\ncontext, weights = attention(query, keys, values)\n\nprint(f\"Query shape: {query.shape}\")\nprint(f\"Keys shape: {keys.shape}\")\nprint(f\"Context shape: {context.shape}\")\nprint(f\"Attention weights shape: {weights.shape}\")\nprint(f\"\\nAttention weights (should sum to 1):\")\nprint(weights)\nprint(f\"Sum: {weights.sum(dim=1)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Luong Attention (Multiplicative)\n\n**Luong et al., 2015**: Simpler, more efficient attention.\n\n**Three variants**:\n\n1. **Dot**: $\\text{score}(h_i^{dec}, h_j^{enc}) = h_i^{dec} \\cdot h_j^{enc}$\n2. **General**: $\\text{score}(h_i^{dec}, h_j^{enc}) = h_i^{dec} W h_j^{enc}$\n3. **Concat**: Similar to Bahdanau\n\nMost common: **General (multiplicative)**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class LuongAttention(nn.Module):\n    \"\"\"\n    Luong (multiplicative) attention mechanism.\n    \n    Reference: \"Effective Approaches to Attention-based Neural Machine Translation\"\n    Luong et al., EMNLP 2015\n    \"\"\"\n    \n    def __init__(self, hidden_dim, method='general'):\n        \"\"\"\n        Parameters:\n        -----------\n        hidden_dim : int\n            Dimension of hidden states\n        method : str\n            Attention method: 'dot', 'general', or 'concat'\n        \"\"\"\n        super(LuongAttention, self).__init__()\n        \n        self.method = method\n        self.hidden_dim = hidden_dim\n        \n        if method == 'general':\n            self.W = nn.Linear(hidden_dim, hidden_dim, bias=False)\n        elif method == 'concat':\n            self.W = nn.Linear(hidden_dim * 2, hidden_dim)\n            self.v = nn.Linear(hidden_dim, 1, bias=False)\n        \n    def forward(self, query, keys, values, mask=None):\n        \"\"\"\n        Compute Luong attention.\n        \"\"\"\n        # Compute scores based on method\n        if self.method == 'dot':\n            # Simple dot product\n            scores = torch.bmm(\n                query.unsqueeze(1),  # (batch, 1, hidden)\n                keys.transpose(1, 2)  # (batch, hidden, seq_len)\n            ).squeeze(1)  # (batch, seq_len)\n            \n        elif self.method == 'general':\n            # Learned transformation then dot product\n            scores = torch.bmm(\n                self.W(query).unsqueeze(1),\n                keys.transpose(1, 2)\n            ).squeeze(1)\n            \n        elif self.method == 'concat':\n            # Concatenate and feed through network\n            query_expanded = query.unsqueeze(1).expand(-1, keys.size(1), -1)\n            combined = torch.cat([query_expanded, keys], dim=2)\n            scores = self.v(torch.tanh(self.W(combined))).squeeze(-1)\n        \n        # Apply mask\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, float('-inf'))\n        \n        # Softmax\n        attention_weights = F.softmax(scores, dim=1)\n        \n        # Weighted sum\n        context = torch.bmm(\n            attention_weights.unsqueeze(1),\n            values\n        ).squeeze(1)\n        \n        return context, attention_weights\n\nprint(\"\u2713 LuongAttention class defined!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Exercise 1**: Compare attention mechanisms\n\n1. Implement all three Luong attention variants\n2. Compare their computational complexity\n3. Test on same input and compare results\n4. Which is fastest? Which uses most parameters?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# YOUR CODE HERE\n# Compare different attention methods"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Visualizing Attention Weights\n\nAttention weights show **which input words** the model focuses on when generating each output word.\n\n**Interpretability**: We can visualize and understand model decisions!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def visualize_attention(input_words, output_words, attention_weights):\n    \"\"\"\n    Visualize attention weights as heatmap.\n    \n    Parameters:\n    -----------\n    input_words : list of str\n        Source sentence words\n    output_words : list of str\n        Target sentence words\n    attention_weights : np.ndarray\n        Attention matrix (output_len, input_len)\n    \"\"\"\n    plt.figure(figsize=(10, 8))\n    \n    sns.heatmap(\n        attention_weights,\n        xticklabels=input_words,\n        yticklabels=output_words,\n        cmap='YlOrRd',\n        cbar_kws={'label': 'Attention Weight'},\n        annot=True,\n        fmt='.2f'\n    )\n    \n    plt.xlabel('Input Sequence')\n    plt.ylabel('Output Sequence')\n    plt.title('Attention Weights Visualization')\n    plt.tight_layout()\n    plt.show()\n\n# Example: English to French translation\ninput_sent = ['the', 'cat', 'sat', 'on', 'the', 'mat']\noutput_sent = ['le', 'chat', 'assis', 'sur', 'le', 'tapis']\n\n# Simulate attention (in practice, this comes from trained model)\nattention_matrix = np.array([\n    [0.8, 0.1, 0.0, 0.0, 0.1, 0.0],  # 'le' attends to 'the'\n    [0.1, 0.8, 0.1, 0.0, 0.0, 0.0],  # 'chat' attends to 'cat'\n    [0.0, 0.2, 0.7, 0.1, 0.0, 0.0],  # 'assis' attends to 'sat'\n    [0.0, 0.0, 0.2, 0.7, 0.1, 0.0],  # 'sur' attends to 'on'\n    [0.1, 0.0, 0.0, 0.1, 0.8, 0.0],  # 'le' attends to 'the'\n    [0.0, 0.0, 0.0, 0.1, 0.1, 0.8],  # 'tapis' attends to 'mat'\n])\n\nvisualize_attention(input_sent, output_sent, attention_matrix)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Observation**: The attention learns **alignments** between source and target words!\n\nFor translation, we see diagonal pattern (monotonic alignment)."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Self-Attention: Introduction\n\n**Self-attention**: Attention where query, keys, and values all come from the **same sequence**!\n\n**Purpose**: Model relationships between words in same sentence.\n\n**Example**: \"The animal didn't cross the street because **it** was too tired\"\n- What does \"it\" refer to?\n- Self-attention: \"it\" attends strongly to \"animal\"\n\n**This is the core of Transformers!** (Module 06)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class SelfAttention(nn.Module):\n    \"\"\"\n    Self-attention mechanism (simplified scaled dot-product).\n    \n    This is the building block of Transformers!\n    \"\"\"\n    \n    def __init__(self, hidden_dim):\n        super(SelfAttention, self).__init__()\n        \n        self.hidden_dim = hidden_dim\n        \n        # Linear projections for Q, K, V\n        self.query_proj = nn.Linear(hidden_dim, hidden_dim)\n        self.key_proj = nn.Linear(hidden_dim, hidden_dim)\n        self.value_proj = nn.Linear(hidden_dim, hidden_dim)\n        \n    def forward(self, x, mask=None):\n        \"\"\"\n        Compute self-attention.\n        \n        Parameters:\n        -----------\n        x : torch.Tensor\n            Input sequence (batch, seq_len, hidden_dim)\n        mask : torch.Tensor or None\n            Attention mask\n            \n        Returns:\n        --------\n        output : torch.Tensor\n            Attention output (batch, seq_len, hidden_dim)\n        attention_weights : torch.Tensor\n            Attention weights (batch, seq_len, seq_len)\n        \"\"\"\n        batch_size, seq_len, _ = x.size()\n        \n        # Project to Q, K, V\n        Q = self.query_proj(x)  # (batch, seq_len, hidden)\n        K = self.key_proj(x)    # (batch, seq_len, hidden)\n        V = self.value_proj(x)  # (batch, seq_len, hidden)\n        \n        # Scaled dot-product attention\n        # scores = Q * K^T / sqrt(d_k)\n        scores = torch.bmm(Q, K.transpose(1, 2)) / np.sqrt(self.hidden_dim)\n        # (batch, seq_len, seq_len)\n        \n        # Apply mask if provided\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, float('-inf'))\n        \n        # Softmax over keys dimension\n        attention_weights = F.softmax(scores, dim=-1)\n        # (batch, seq_len, seq_len)\n        \n        # Weighted sum of values\n        output = torch.bmm(attention_weights, V)\n        # (batch, seq_len, hidden)\n        \n        return output, attention_weights\n\nprint(\"\u2713 SelfAttention class defined!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test self-attention\nseq_len = 6\nhidden_dim = 8\nbatch_size = 2\n\n# Input sequence\nx = torch.randn(batch_size, seq_len, hidden_dim)\n\n# Self-attention\nself_attn = SelfAttention(hidden_dim)\noutput, attn_weights = self_attn(x)\n\nprint(f\"Input shape: {x.shape}\")\nprint(f\"Output shape: {output.shape}\")\nprint(f\"Attention weights shape: {attn_weights.shape}\")\nprint(f\"\\nAttention weights for first sample:\")\nprint(attn_weights[0].detach().numpy().round(2))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Exercise 2**: Analyze self-attention\n\n1. Visualize self-attention weights for a sentence\n2. Identify which words attend to which other words\n3. Compare with encoder-decoder attention\n4. Explain: Why is self-attention O(n\u00b2) in sequence length?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# YOUR CODE HERE\n# Visualize and analyze self-attention patterns"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\n### Key Concepts Covered:\n\n1. **Attention Motivation**:\n   - Solves context bottleneck in seq2seq\n   - Dynamic focus on relevant input parts\n   - Different context for each decoder step\n\n2. **Bahdanau Attention**:\n   - Additive attention mechanism\n   - First successful attention for NMT\n   - Uses tanh and learned parameters\n\n3. **Luong Attention**:\n   - Multiplicative attention\n   - Simpler and more efficient\n   - Three variants: dot, general, concat\n\n4. **Attention Visualization**:\n   - Interpretable alignments\n   - Shows what model focuses on\n   - Useful for debugging and analysis\n\n5. **Self-Attention**:\n   - Query, key, value from same sequence\n   - Models intra-sentence dependencies\n   - Core building block of Transformers\n\n### Attention Benefits:\n\n\u2705 Better long-range dependencies  \n\u2705 Interpretability through visualization  \n\u2705 Parallelizable (self-attention)  \n\u2705 State-of-the-art performance  \n\n### What's Next?\n\nIn **Module 06: Transformer Architecture**, we'll learn:\n- Full transformer architecture using self-attention\n- Multi-head attention\n- Positional encoding\n- The \"Attention is All You Need\" revolution\n\n### Additional Resources:\n\n- **Bahdanau Paper**: [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)\n- **Luong Paper**: [Effective Approaches to Attention-based NMT](https://arxiv.org/abs/1508.04025)\n- **Blog**: [Jay Alammar's Visualizing Attention](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}