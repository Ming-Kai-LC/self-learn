{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 01: Text Preprocessing and Tokenization\n",
    "\n",
    "**Difficulty**: ‚≠ê‚≠ê Intermediate  \n",
    "**Estimated Time**: 100 minutes  \n",
    "**Prerequisites**: [Module 00: Introduction to NLP](00_introduction_to_nlp.ipynb)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Apply advanced text cleaning techniques using regular expressions\n",
    "2. Understand and implement modern tokenization strategies (BPE, WordPiece)\n",
    "3. Handle special text elements (URLs, mentions, emojis, hashtags)\n",
    "4. Build production-ready text preprocessing pipelines\n",
    "5. Compare different tokenization methods and their use cases\n",
    "\n",
    "## Why Preprocessing Matters\n",
    "\n",
    "Text preprocessing is the **foundation** of any NLP pipeline. Poor preprocessing can:\n",
    "- Introduce noise and reduce model accuracy\n",
    "- Create inconsistent representations\n",
    "- Waste computational resources\n",
    "- Cause failures in production\n",
    "\n",
    "**Good preprocessing**:\n",
    "- Standardizes inputs\n",
    "- Reduces vocabulary size\n",
    "- Improves model generalization\n",
    "- Handles edge cases gracefully"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# NLP libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer\n",
    "import spacy\n",
    "\n",
    "# Hugging Face tokenizers\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE, WordPiece\n",
    "from tokenizers.trainers import BpeTrainer, WordPieceTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Visualization\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "# Random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úì Libraries imported successfully!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Download NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "# Load spaCy model (install with: python -m spacy download en_core_web_sm)\n",
    "try:\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    print(\"‚úì spaCy model loaded successfully!\")\n",
    "except OSError:\n",
    "    print(\"‚ö† spaCy model not found. Install with: python -m spacy download en_core_web_sm\")\n",
    "    nlp = None"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Advanced Text Cleaning with Regular Expressions\n",
    "\n",
    "Regular expressions (regex) are powerful tools for pattern matching and text manipulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Cleaning Social Media Text\n",
    "\n",
    "Social media text contains many special elements that need careful handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Sample social media text\n",
    "social_media_text = \"\"\"\n",
    "@john_doe Check out this amazing article! https://example.com/article123 \n",
    "#NLP #MachineLearning #AI üöÄüî•\n",
    "Email me at contact@example.com for more info!!!\n",
    "Price: $99.99 (50% OFF) - Limited time only!!!\n",
    "RT @jane_smith: This is sooo cool üòçüòçüòç\n",
    "\"\"\"\n",
    "\n",
    "print(\"Original text:\")\n",
    "print(social_media_text)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def clean_social_media_text(text, remove_urls=True, remove_mentions=True, \n",
    "                           remove_hashtags=False, remove_emojis=True,\n",
    "                           remove_emails=True, normalize_whitespace=True):\n",
    "    \"\"\"\n",
    "    Clean social media text with configurable options.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        Input text to clean\n",
    "    remove_urls : bool\n",
    "        Whether to remove URLs\n",
    "    remove_mentions : bool\n",
    "        Whether to remove @mentions\n",
    "    remove_hashtags : bool\n",
    "        Whether to remove #hashtags (keep False to preserve topics)\n",
    "    remove_emojis : bool\n",
    "        Whether to remove emoji characters\n",
    "    remove_emails : bool\n",
    "        Whether to remove email addresses\n",
    "    normalize_whitespace : bool\n",
    "        Whether to normalize multiple spaces to single space\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str : Cleaned text\n",
    "    \"\"\"\n",
    "    \n",
    "    # Remove URLs\n",
    "    if remove_urls:\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove email addresses\n",
    "    if remove_emails:\n",
    "        text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # Remove @mentions\n",
    "    if remove_mentions:\n",
    "        text = re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    # Remove or clean hashtags\n",
    "    if remove_hashtags:\n",
    "        text = re.sub(r'#\\w+', '', text)\n",
    "    else:\n",
    "        # Keep hashtag content but remove the # symbol\n",
    "        text = re.sub(r'#(\\w+)', r'\\1', text)\n",
    "    \n",
    "    # Remove emojis\n",
    "    if remove_emojis:\n",
    "        # Emoji pattern covering most common emojis\n",
    "        emoji_pattern = re.compile(\n",
    "            \"[\"\n",
    "            \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "            \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "            \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "            \"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
    "            \"\\U00002702-\\U000027B0\"\n",
    "            \"\\U000024C2-\\U0001F251\"\n",
    "            \"]+\", flags=re.UNICODE\n",
    "        )\n",
    "        text = emoji_pattern.sub(r'', text)\n",
    "    \n",
    "    # Remove RT (retweet indicator)\n",
    "    text = re.sub(r'\\bRT\\b', '', text)\n",
    "    \n",
    "    # Normalize whitespace\n",
    "    if normalize_whitespace:\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# Test the cleaning function\n",
    "cleaned_text = clean_social_media_text(social_media_text)\n",
    "print(\"Cleaned text:\")\n",
    "print(cleaned_text)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1**: Custom text cleaner\n",
    "\n",
    "Modify the cleaning function to:\n",
    "1. Replace repeated punctuation (\"!!!\", \"???\") with single instances\n",
    "2. Expand contractions (\"don't\" ‚Üí \"do not\", \"won't\" ‚Üí \"will not\")\n",
    "3. Remove or replace price mentions (\"$99.99\") with a token like \"[PRICE]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# YOUR CODE HERE\n",
    "def advanced_clean_text(text):\n",
    "    \"\"\"\n",
    "    Apply advanced cleaning including:\n",
    "    - Repeated punctuation normalization\n",
    "    - Contraction expansion\n",
    "    - Price tokenization\n",
    "    \"\"\"\n",
    "    # Hint: Use re.sub() with appropriate patterns\n",
    "    pass\n",
    "\n",
    "# Test your function\n",
    "test_text = \"This is sooo cool!!! It won't cost $99.99 anymore!\"\n",
    "# Expected output: \"This is so cool! It will not cost [PRICE] anymore!\""
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Unicode Normalization\n",
    "\n",
    "Text from different sources may use different Unicode representations. Normalization ensures consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import unicodedata\n",
    "\n",
    "# Examples of Unicode variations\n",
    "text1 = \"caf√©\"  # √© as single character (U+00E9)\n",
    "text2 = \"caf√©\"  # √© as e + combining accent (U+0065 + U+0301)\n",
    "\n",
    "print(f\"Text 1: {text1} (length: {len(text1)})\")\n",
    "print(f\"Text 2: {text2} (length: {len(text2)})\")\n",
    "print(f\"Are they equal? {text1 == text2}\")\n",
    "\n",
    "# Normalize both to NFC (Canonical Decomposition followed by Canonical Composition)\n",
    "normalized1 = unicodedata.normalize('NFC', text1)\n",
    "normalized2 = unicodedata.normalize('NFC', text2)\n",
    "\n",
    "print(f\"\\nAfter normalization: {normalized1 == normalized2}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def normalize_unicode(text, form='NFC'):\n",
    "    \"\"\"\n",
    "    Normalize Unicode text.\n",
    "    \n",
    "    Forms:\n",
    "    - NFC: Canonical Decomposition + Composition (recommended)\n",
    "    - NFD: Canonical Decomposition\n",
    "    - NFKC: Compatibility Decomposition + Composition\n",
    "    - NFKD: Compatibility Decomposition\n",
    "    \"\"\"\n",
    "    return unicodedata.normalize(form, text)\n",
    "\n",
    "# Test with accented characters\n",
    "test_texts = [\"na√Øve caf√©\", \"Z√ºrich\", \"se√±or\"]\n",
    "for text in test_texts:\n",
    "    normalized = normalize_unicode(text)\n",
    "    print(f\"{text:15} ‚Üí {normalized:15} (length: {len(text)} ‚Üí {len(normalized)})\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modern Tokenization Strategies\n",
    "\n",
    "While simple word tokenization works for basic cases, modern NLP uses more sophisticated methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Comparison of Tokenization Methods\n",
    "\n",
    "Let's compare different tokenization approaches on the same text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "sample_text = \"The quick-brown fox jumps over the lazy dog. It's running at 25mph!\"\n",
    "\n",
    "# Method 1: Simple split on whitespace\n",
    "simple_tokens = sample_text.split()\n",
    "\n",
    "# Method 2: NLTK word tokenizer\n",
    "nltk_tokens = word_tokenize(sample_text)\n",
    "\n",
    "# Method 3: Tweet tokenizer (preserves hashtags, mentions)\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "tweet_tokens = tweet_tokenizer.tokenize(sample_text)\n",
    "\n",
    "# Method 4: spaCy tokenizer\n",
    "if nlp:\n",
    "    doc = nlp(sample_text)\n",
    "    spacy_tokens = [token.text for token in doc]\n",
    "else:\n",
    "    spacy_tokens = [\"spaCy not available\"]\n",
    "\n",
    "# Compare results\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Method': ['Simple Split', 'NLTK', 'TweetTokenizer', 'spaCy'],\n",
    "    'Token Count': [len(simple_tokens), len(nltk_tokens), \n",
    "                   len(tweet_tokens), len(spacy_tokens)],\n",
    "    'Sample Tokens': [\n",
    "        str(simple_tokens[:5]),\n",
    "        str(nltk_tokens[:5]),\n",
    "        str(tweet_tokens[:5]),\n",
    "        str(spacy_tokens[:5])\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(comparison_df.to_string(index=False))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**:\n",
    "- Simple split fails on punctuation\n",
    "- NLTK and spaCy handle contractions better\n",
    "- Different tokenizers make different decisions about hyphenated words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Subword Tokenization: BPE (Byte Pair Encoding)\n",
    "\n",
    "**Why subword tokenization?**\n",
    "\n",
    "Word-level tokenization has problems:\n",
    "- **Large vocabulary**: English has 170,000+ words\n",
    "- **Out-of-vocabulary (OOV)**: Can't handle new or misspelled words\n",
    "- **Morphology**: \"run\", \"running\", \"runner\" treated as completely different\n",
    "\n",
    "**BPE Solution**: Break words into subword units\n",
    "- \"running\" ‚Üí [\"run\", \"##ning\"]\n",
    "- \"unhappiness\" ‚Üí [\"un\", \"##happi\", \"##ness\"]\n",
    "- \"coronavirus\" (new word) ‚Üí [\"coron\", \"##avirus\"] (can be understood from parts)\n",
    "\n",
    "**Used by**: GPT, GPT-2, RoBERTa, BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Train a simple BPE tokenizer\n",
    "# First, create training data\n",
    "training_corpus = [\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"Natural language processing is amazing\",\n",
    "    \"Machine learning transforms how we process text\",\n",
    "    \"Deep learning models require lots of data\",\n",
    "    \"Transformers revolutionized natural language understanding\",\n",
    "    \"BERT and GPT are popular transformer models\",\n",
    "    \"Fine-tuning pre-trained models saves time and resources\",\n",
    "    \"Tokenization is a crucial preprocessing step\",\n",
    "] * 10  # Repeat for better training\n",
    "\n",
    "# Save to file (BPE trainer needs file input)\n",
    "with open('/tmp/training_data.txt', 'w') as f:\n",
    "    for text in training_corpus:\n",
    "        f.write(text + '\\n')\n",
    "\n",
    "print(f\"Training corpus: {len(training_corpus)} sentences\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize BPE tokenizer\n",
    "bpe_tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "bpe_tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# Train BPE with small vocabulary\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=100,  # Small vocab for demonstration\n",
    "    special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
    ")\n",
    "\n",
    "bpe_tokenizer.train(['/tmp/training_data.txt'], trainer)\n",
    "print(\"‚úì BPE tokenizer trained!\")\n",
    "\n",
    "# Get vocabulary\n",
    "vocab = bpe_tokenizer.get_vocab()\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(f\"\\nSample vocabulary (first 20 tokens):\")\n",
    "print(list(vocab.keys())[:20])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test BPE tokenization\n",
    "test_sentences = [\n",
    "    \"Natural language processing\",\n",
    "    \"Transformers are revolutionary\",  # 'revolutionary' might be split\n",
    "    \"Preprocessing text data\",\n",
    "]\n",
    "\n",
    "print(\"BPE Tokenization Results:\\n\")\n",
    "for sentence in test_sentences:\n",
    "    encoding = bpe_tokenizer.encode(sentence)\n",
    "    print(f\"Input: {sentence}\")\n",
    "    print(f\"Tokens: {encoding.tokens}\")\n",
    "    print(f\"IDs: {encoding.ids}\")\n",
    "    print()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2**: Analyze BPE behavior\n",
    "\n",
    "Test the BPE tokenizer on words that weren't in the training data:\n",
    "1. \"coronavirus\" (new word)\n",
    "2. \"antidisestablishmentarianism\" (very long word)\n",
    "3. \"happily\" vs \"unhappily\" (morphological variants)\n",
    "\n",
    "Observe how BPE breaks them into subwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# YOUR CODE HERE\n",
    "oov_words = [\"coronavirus\", \"antidisestablishmentarianism\", \"happily\", \"unhappily\"]\n",
    "\n",
    "# Tokenize each and observe the subword breakdown"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 WordPiece Tokenization\n",
    "\n",
    "**WordPiece** is similar to BPE but uses a different merging criterion (likelihood-based).\n",
    "\n",
    "**Used by**: BERT, DistilBERT, Electra\n",
    "\n",
    "**Key difference**: Instead of frequency-based merging, WordPiece maximizes the likelihood of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize WordPiece tokenizer\n",
    "wordpiece_tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "wordpiece_tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# Train WordPiece\n",
    "wp_trainer = WordPieceTrainer(\n",
    "    vocab_size=100,\n",
    "    special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
    ")\n",
    "\n",
    "wordpiece_tokenizer.train(['/tmp/training_data.txt'], wp_trainer)\n",
    "print(\"‚úì WordPiece tokenizer trained!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compare BPE vs WordPiece\n",
    "test_text = \"Preprocessing transformers for natural language understanding\"\n",
    "\n",
    "bpe_encoding = bpe_tokenizer.encode(test_text)\n",
    "wp_encoding = wordpiece_tokenizer.encode(test_text)\n",
    "\n",
    "print(\"Input text:\", test_text)\n",
    "print(\"\\nBPE tokens:\", bpe_encoding.tokens)\n",
    "print(\"WordPiece tokens:\", wp_encoding.tokens)\n",
    "print(f\"\\nToken count - BPE: {len(bpe_encoding.tokens)}, WordPiece: {len(wp_encoding.tokens)}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Using Pre-trained Tokenizers\n",
    "\n",
    "In practice, we use tokenizers from pre-trained models like BERT, GPT-2, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load BERT tokenizer (WordPiece)\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load GPT-2 tokenizer (BPE)\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "\n",
    "print(\"‚úì Pre-trained tokenizers loaded!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compare BERT vs GPT-2 tokenization\n",
    "test_text = \"The unhappiest preprocessing experience!\"\n",
    "\n",
    "bert_tokens = bert_tokenizer.tokenize(test_text)\n",
    "gpt2_tokens = gpt2_tokenizer.tokenize(test_text)\n",
    "\n",
    "print(f\"Input: {test_text}\\n\")\n",
    "print(f\"BERT (WordPiece): {bert_tokens}\")\n",
    "print(f\"GPT-2 (BPE): {gpt2_tokens}\")\n",
    "\n",
    "# Get IDs (what the model actually sees)\n",
    "bert_ids = bert_tokenizer.encode(test_text)\n",
    "gpt2_ids = gpt2_tokenizer.encode(test_text)\n",
    "\n",
    "print(f\"\\nBERT IDs: {bert_ids}\")\n",
    "print(f\"GPT-2 IDs: {gpt2_ids}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Observations**:\n",
    "- BERT uses `##` prefix for subword continuations\n",
    "- GPT-2 uses `ƒ†` prefix for spaces (byte-level BPE)\n",
    "- Both can handle OOV words by breaking them into subwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3**: Tokenization comparison\n",
    "\n",
    "Compare how BERT and GPT-2 tokenize these challenging cases:\n",
    "1. \"COVID-19\"\n",
    "2. \"don't\", \"won't\", \"I'm\"\n",
    "3. \"antidisestablishmentarianism\"\n",
    "4. \"üöÄ rocket emoji\"\n",
    "\n",
    "Explain the differences you observe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# YOUR CODE HERE\n",
    "challenging_texts = [\n",
    "    \"COVID-19\",\n",
    "    \"don't won't I'm\",\n",
    "    \"antidisestablishmentarianism\",\n",
    "    \"üöÄ rocket emoji\"\n",
    "]\n",
    "\n",
    "# Compare BERT and GPT-2 tokenization for each"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building a Production-Ready Preprocessing Pipeline\n",
    "\n",
    "Let's combine everything into a robust, reusable pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class TextPreprocessor:\n",
    "    \"\"\"\n",
    "    Production-ready text preprocessing pipeline.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 lowercase=True,\n",
    "                 remove_urls=True,\n",
    "                 remove_mentions=True,\n",
    "                 remove_hashtags=False,\n",
    "                 remove_emojis=False,\n",
    "                 normalize_unicode=True,\n",
    "                 tokenizer_name='bert-base-uncased'):\n",
    "        \"\"\"\n",
    "        Initialize preprocessor with configuration.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        lowercase : bool\n",
    "            Convert text to lowercase\n",
    "        remove_urls : bool\n",
    "            Remove URLs from text\n",
    "        remove_mentions : bool\n",
    "            Remove @mentions\n",
    "        remove_hashtags : bool\n",
    "            Remove #hashtags\n",
    "        remove_emojis : bool\n",
    "            Remove emoji characters\n",
    "        normalize_unicode : bool\n",
    "            Apply Unicode normalization\n",
    "        tokenizer_name : str\n",
    "            Name of Hugging Face tokenizer to use\n",
    "        \"\"\"\n",
    "        self.lowercase = lowercase\n",
    "        self.remove_urls = remove_urls\n",
    "        self.remove_mentions = remove_mentions\n",
    "        self.remove_hashtags = remove_hashtags\n",
    "        self.remove_emojis = remove_emojis\n",
    "        self.normalize_unicode = normalize_unicode\n",
    "        \n",
    "        # Load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "        \n",
    "    def clean(self, text):\n",
    "        \"\"\"\n",
    "        Apply cleaning rules to text.\n",
    "        \"\"\"\n",
    "        if self.normalize_unicode:\n",
    "            text = unicodedata.normalize('NFC', text)\n",
    "        \n",
    "        if self.lowercase:\n",
    "            text = text.lower()\n",
    "        \n",
    "        if self.remove_urls:\n",
    "            text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "        \n",
    "        if self.remove_mentions:\n",
    "            text = re.sub(r'@\\w+', '', text)\n",
    "        \n",
    "        if self.remove_hashtags:\n",
    "            text = re.sub(r'#\\w+', '', text)\n",
    "        \n",
    "        if self.remove_emojis:\n",
    "            emoji_pattern = re.compile(\n",
    "                \"[\"\n",
    "                \"\\U0001F600-\\U0001F64F\"\n",
    "                \"\\U0001F300-\\U0001F5FF\"\n",
    "                \"\\U0001F680-\\U0001F6FF\"\n",
    "                \"\\U0001F1E0-\\U0001F1FF\"\n",
    "                \"]+\", flags=re.UNICODE\n",
    "            )\n",
    "            text = emoji_pattern.sub(r'', text)\n",
    "        \n",
    "        # Normalize whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    def tokenize(self, text, return_tensors=None):\n",
    "        \"\"\"\n",
    "        Tokenize text using configured tokenizer.\n",
    "        \"\"\"\n",
    "        return self.tokenizer(\n",
    "            text,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=return_tensors\n",
    "        )\n",
    "    \n",
    "    def preprocess(self, text, return_tokens=False):\n",
    "        \"\"\"\n",
    "        Complete preprocessing pipeline: clean + tokenize.\n",
    "        \"\"\"\n",
    "        cleaned_text = self.clean(text)\n",
    "        \n",
    "        if return_tokens:\n",
    "            return cleaned_text, self.tokenizer.tokenize(cleaned_text)\n",
    "        else:\n",
    "            return cleaned_text\n",
    "    \n",
    "    def batch_preprocess(self, texts, return_tensors='pt'):\n",
    "        \"\"\"\n",
    "        Preprocess a batch of texts.\n",
    "        \"\"\"\n",
    "        cleaned_texts = [self.clean(text) for text in texts]\n",
    "        return self.tokenizer(\n",
    "            cleaned_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=return_tensors\n",
    "        )\n",
    "\n",
    "print(\"‚úì TextPreprocessor class defined!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test the preprocessor\n",
    "preprocessor = TextPreprocessor(\n",
    "    lowercase=True,\n",
    "    remove_urls=True,\n",
    "    remove_mentions=True,\n",
    "    remove_emojis=True,\n",
    "    tokenizer_name='bert-base-uncased'\n",
    ")\n",
    "\n",
    "# Test on social media text\n",
    "test_text = \"\"\"\n",
    "@john Check out this NLP tutorial! https://example.com #NLP #AI üöÄ\n",
    "It's really amazing and helpful!!!\n",
    "\"\"\"\n",
    "\n",
    "cleaned, tokens = preprocessor.preprocess(test_text, return_tokens=True)\n",
    "\n",
    "print(\"Original text:\")\n",
    "print(test_text)\n",
    "print(\"\\nCleaned text:\")\n",
    "print(cleaned)\n",
    "print(\"\\nTokens:\")\n",
    "print(tokens)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Batch processing example\n",
    "batch_texts = [\n",
    "    \"I love natural language processing!\",\n",
    "    \"@user This is an amazing tutorial https://example.com\",\n",
    "    \"#NLP #MachineLearning #DeepLearning üî•\",\n",
    "]\n",
    "\n",
    "# Process batch\n",
    "batch_output = preprocessor.batch_preprocess(batch_texts)\n",
    "\n",
    "print(\"Batch processing results:\")\n",
    "print(f\"Input IDs shape: {batch_output['input_ids'].shape}\")\n",
    "print(f\"Attention mask shape: {batch_output['attention_mask'].shape}\")\n",
    "print(\"\\nFirst text tokens:\")\n",
    "print(preprocessor.tokenizer.convert_ids_to_tokens(batch_output['input_ids'][0]))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4**: Custom preprocessing pipeline\n",
    "\n",
    "Extend the `TextPreprocessor` class to:\n",
    "1. Add a method to handle contractions expansion\n",
    "2. Add statistics tracking (number of URLs removed, mentions removed, etc.)\n",
    "3. Add a method to save/load configuration from JSON\n",
    "\n",
    "Test your extended preprocessor on a sample dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# YOUR CODE HERE\n",
    "class ExtendedTextPreprocessor(TextPreprocessor):\n",
    "    \"\"\"\n",
    "    Extended preprocessor with additional features.\n",
    "    \"\"\"\n",
    "    pass"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts Covered:\n",
    "\n",
    "1. **Advanced Text Cleaning**:\n",
    "   - Regular expressions for pattern matching\n",
    "   - Handling social media elements (URLs, mentions, hashtags, emojis)\n",
    "   - Unicode normalization for consistency\n",
    "\n",
    "2. **Tokenization Methods**:\n",
    "   - Word-level: Simple but limited\n",
    "   - Subword tokenization: BPE and WordPiece\n",
    "   - Pre-trained tokenizers from BERT, GPT-2\n",
    "   - Trade-offs: vocabulary size vs OOV handling\n",
    "\n",
    "3. **Production Pipeline**:\n",
    "   - Configurable preprocessing\n",
    "   - Batch processing support\n",
    "   - Integration with Hugging Face tokenizers\n",
    "   - Reusable and maintainable design\n",
    "\n",
    "### Important Takeaways:\n",
    "\n",
    "- **Always clean before tokenizing**: Garbage in, garbage out\n",
    "- **Use subword tokenization**: Better OOV handling, smaller vocabulary\n",
    "- **Match tokenizer to model**: BERT uses WordPiece, GPT uses BPE\n",
    "- **Batch processing**: Much faster than processing one at a time\n",
    "- **Make it configurable**: Different tasks need different preprocessing\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In **Module 02: Word Embeddings**, we'll learn:\n",
    "- How to convert tokens into dense vector representations\n",
    "- Word2Vec, GloVe, and FastText algorithms\n",
    "- Semantic relationships in vector space\n",
    "- Visualizing embeddings with t-SNE\n",
    "- Limitations that led to contextual embeddings\n",
    "\n",
    "### Additional Resources:\n",
    "\n",
    "- **Hugging Face Tokenizers**: [huggingface.co/docs/tokenizers](https://huggingface.co/docs/tokenizers)\n",
    "- **BPE Paper**: [Neural Machine Translation of Rare Words](https://arxiv.org/abs/1508.07909)\n",
    "- **Regular Expressions**: [regex101.com](https://regex101.com/) (interactive tester)\n",
    "- **Unicode Normalization**: [unicode.org/reports/tr15](https://www.unicode.org/reports/tr15/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
