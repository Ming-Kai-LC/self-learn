{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Module 08: GPT and Autoregressive Models\n\n**Difficulty**: \u2b50\u2b50\u2b50 Advanced  \n**Estimated Time**: 120 minutes  \n**Prerequisites**: [Module 06-07: Transformers and BERT](07_bert_masked_lm.ipynb)\n\n## Learning Objectives\n\n1. Understand autoregressive language modeling\n2. Learn GPT architecture (decoder-only transformer)\n3. Implement causal (masked) attention\n4. Generate text with different sampling strategies\n5. Compare GPT vs BERT approaches\n6. Understand scaling laws and GPT evolution\n\n## GPT: Generative Pre-trained Transformer\n\n**Key difference from BERT**:\n- **BERT**: Bidirectional, encoder-only, masked LM\n- **GPT**: Unidirectional, decoder-only, causal LM\n\n### Autoregressive Modeling:\n\nPredict next token given previous tokens:\n\n$$P(x_1, x_2, ..., x_n) = \\prod_{i=1}^{n} P(x_i | x_1, ..., x_{i-1})$$\n\n**Training**: Maximize likelihood of next token\n\n**Generation**: Sample tokens one by one"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Setup"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config\nfrom transformers import pipeline\nimport matplotlib.pyplot as plt\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Using device: {device}')\nprint('\u2713 Libraries imported!')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Causal (Masked) Attention\n\n**Causal masking**: Token can only attend to previous tokens (not future).\n\n**Prevents cheating**: Model can't see the answer during training!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def create_causal_mask(seq_len):\n    \"\"\"\n    Create causal attention mask.\n    \n    Returns lower triangular matrix:\n    [[1, 0, 0],\n     [1, 1, 0],\n     [1, 1, 1]]\n    \"\"\"\n    mask = torch.tril(torch.ones(seq_len, seq_len))\n    return mask\n\n# Visualize causal mask\nmask = create_causal_mask(10)\nplt.figure(figsize=(8, 8))\nplt.imshow(mask, cmap='Blues')\nplt.title('Causal Attention Mask')\nplt.xlabel('Key Position')\nplt.ylabel('Query Position')\nplt.colorbar(label='Can Attend')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Using Pre-trained GPT-2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load GPT-2\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\nmodel = GPT2LMHeadModel.from_pretrained('gpt2')\nmodel.to(device)\nmodel.eval()\n\nprint(f'\u2713 GPT-2 loaded!')\nprint(f'Parameters: {sum(p.numel() for p in model.parameters()):,}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Text Generation Strategies\n\n### 1. Greedy Decoding\n- Always pick most probable token\n- Deterministic but repetitive\n\n### 2. Temperature Sampling\n- $P(x_i) = \\frac{\\exp(\\text{logit}_i / T)}{\\sum_j \\exp(\\text{logit}_j / T)}$\n- Lower T = more conservative\n- Higher T = more creative\n\n### 3. Top-k Sampling\n- Sample from top k most probable tokens\n\n### 4. Top-p (Nucleus) Sampling\n- Sample from smallest set with cumulative probability > p"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Text generation with different strategies\nfrom transformers import set_seed\nset_seed(42)\n\nprompt = \"Artificial intelligence will\"\n\nprint(\"=\" * 60)\nprint(f\"Prompt: {prompt}\\n\")\n\n# Greedy\nprint(\"1. GREEDY DECODING:\")\ngenerated = pipeline('text-generation', model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\nresult = generated(prompt, max_length=50, num_return_sequences=1, do_sample=False)\nprint(result[0]['generated_text'])\n\n# Temperature sampling\nprint(\"\\n2. TEMPERATURE SAMPLING (T=0.7):\")\nresult = generated(prompt, max_length=50, temperature=0.7, do_sample=True)\nprint(result[0]['generated_text'])\n\n# Top-k\nprint(\"\\n3. TOP-K SAMPLING (k=50):\")\nresult = generated(prompt, max_length=50, top_k=50, do_sample=True)\nprint(result[0]['generated_text'])\n\n# Top-p\nprint(\"\\n4. TOP-P SAMPLING (p=0.9):\")\nresult = generated(prompt, max_length=50, top_p=0.9, do_sample=True)\nprint(result[0]['generated_text'])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Exercise 1**: Compare generation strategies\n\n1. Try different temperatures (0.1, 0.5, 1.0, 2.0)\n2. Compare top-k with different k values\n3. Analyze diversity vs quality trade-off\n4. Find optimal parameters for your use case"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# YOUR CODE HERE"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. GPT Evolution\n\n### Timeline:\n\n- **GPT-1** (2018): 117M params, proof of concept\n- **GPT-2** (2019): 1.5B params, \"too dangerous to release\"\n- **GPT-3** (2020): 175B params, few-shot learning\n- **GPT-4** (2023): Multimodal, RLHF\n\n### Scaling Laws:\n\n**Bigger is better** (up to a point):\n- More parameters \u2192 better performance\n- More data \u2192 better performance\n- More compute \u2192 better performance"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\n### Key Concepts:\n\n1. **Autoregressive Modeling**: Predict next token\n2. **Causal Attention**: Can't see future tokens\n3. **Generation Strategies**: Greedy, sampling, top-k, top-p\n4. **Scaling**: Larger models perform better\n\n### GPT vs BERT:\n\n| Aspect | GPT | BERT |\n|--------|-----|------|\n| Architecture | Decoder | Encoder |\n| Attention | Causal | Bidirectional |\n| Training | Next token | Masked LM |\n| Best for | Generation | Understanding |\n\n### What's Next?\n\nIn **Module 09: Fine-Tuning**, we'll learn to adapt pre-trained models to specific tasks.\n\n### Resources:\n\n- **GPT-3 Paper**: [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)\n- **GPT-2 Blog**: [Better Language Models](https://openai.com/blog/better-language-models/)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}