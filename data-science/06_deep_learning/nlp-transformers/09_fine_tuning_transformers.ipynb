{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Module 09: Fine-Tuning Transformers\n\n**Difficulty**: \u2b50\u2b50\u2b50 Advanced  \n**Estimated Time**: 140 minutes  \n**Prerequisites**: Modules 07-08\n\n## Learning Objectives\n\n1. Understand fine-tuning vs pre-training\n2. Use Hugging Face Trainer API\n3. Implement parameter-efficient fine-tuning (LoRA, adapters)\n4. Optimize hyperparameters for fine-tuning\n5. Handle common fine-tuning challenges\n6. Evaluate fine-tuned models\n\n## Fine-Tuning: Transfer Learning for NLP\n\n**Idea**: Leverage pre-trained knowledge for specific tasks.\n\n### Why Fine-Tune?\n\n\u2705 Better performance with less data  \n\u2705 Faster training (vs from scratch)  \n\u2705 State-of-the-art results  \n\u2705 Accessible with limited compute  \n\n### Fine-Tuning Workflow:\n\n1. Choose pre-trained model\n2. Prepare task-specific dataset\n3. Add task head (if needed)\n4. Fine-tune with smaller learning rate\n5. Evaluate and iterate"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Setup"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import Trainer, TrainingArguments\nfrom transformers import DataCollatorWithPadding\nfrom datasets import load_dataset\nimport evaluate\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('\u2713 Ready!')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Using Hugging Face Trainer\n\n**Trainer API**: Simplifies training loop."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load dataset\ndataset = load_dataset('imdb', split='train[:1000]')\n\n# Load tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\nmodel = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n\n# Tokenize\ndef tokenize_function(examples):\n    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=512)\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\n\nprint('\u2713 Data prepared!')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    evaluation_strategy='epoch',\n    learning_rate=2e-5,\n    per_device_train_batch_size=8,\n    num_train_epochs=3,\n    weight_decay=0.01,\n)\n\n# Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets,\n    tokenizer=tokenizer,\n)\n\nprint('\u2713 Trainer ready! (Run trainer.train() to start)')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Parameter-Efficient Fine-Tuning (PEFT)\n\n**Problem**: Fine-tuning all parameters is expensive.\n\n**Solution**: Only update small number of parameters!\n\n### LoRA (Low-Rank Adaptation)\n\nInstead of updating $W$, learn $\\Delta W = AB$ where $A, B$ are small matrices.\n\n**Benefits**:\n- 10,000x fewer parameters\n- Same performance\n- Multiple task adapters"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from peft import LoraConfig, get_peft_model\n\n# Configure LoRA\nlora_config = LoraConfig(\n    r=8,  # Rank\n    lora_alpha=32,\n    target_modules=['q_lin', 'v_lin'],\n    lora_dropout=0.1,\n    bias='none',\n)\n\n# Apply LoRA\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()\nprint('\u2713 LoRA applied!')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Exercise**: Fine-tune with LoRA\n\n1. Fine-tune BERT with LoRA on sentiment analysis\n2. Compare with full fine-tuning\n3. Measure: performance, training time, memory\n4. Try different LoRA ranks"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# YOUR CODE HERE"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\n### Key Concepts:\n\n1. **Transfer Learning**: Pre-train \u2192 Fine-tune\n2. **Trainer API**: Simplified training\n3. **LoRA**: Parameter-efficient fine-tuning\n4. **Hyperparameter Tuning**: Learning rate, batch size, epochs\n\n### Best Practices:\n\n- Use smaller LR than pre-training\n- Warm-up for stability\n- Monitor for overfitting\n- Save checkpoints\n\n### What's Next?\n\nModules 10-14: Applications (classification, NER, QA, generation, project)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}