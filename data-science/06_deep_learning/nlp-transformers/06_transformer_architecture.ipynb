{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Module 06: Transformer Architecture\n\n**Difficulty**: \u2b50\u2b50\u2b50 Advanced  \n**Estimated Time**: 150 minutes  \n**Prerequisites**: [Module 05: Attention Mechanism](05_attention_mechanism.ipynb)\n\n## Learning Objectives\n\nBy the end of this notebook, you will be able to:\n\n1. Understand the complete Transformer architecture from \"Attention is All You Need\"\n2. Implement multi-head attention from scratch in PyTorch\n3. Understand and implement positional encoding\n4. Build encoder and decoder layers with layer normalization\n5. Implement a complete Transformer for sequence-to-sequence tasks\n6. Understand why Transformers revolutionized NLP\n\n## The Transformer Revolution\n\n**\"Attention is All You Need\"** (Vaswani et al., 2017) changed everything.\n\n### Why Transformers?\n\n**Problems with RNNs**:\n- Sequential processing (can't parallelize)\n- Vanishing gradients for long sequences\n- Limited context window\n\n**Transformer advantages**:\n- \u2705 Fully parallelizable (process all positions simultaneously)\n- \u2705 Direct connections between any two positions\n- \u2705 Better at capturing long-range dependencies\n- \u2705 Faster training on modern hardware (GPUs/TPUs)\n\n### Core Innovation:\n\n**Replace recurrence with self-attention!**\n\n- No more sequential processing\n- Attention connects all positions directly\n- Positional encoding adds sequence information"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Setup and Imports"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\n%matplotlib inline\nplt.style.use('seaborn-v0_8-darkgrid')\n\nnp.random.seed(42)\ntorch.manual_seed(42)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Using device: {device}')\nprint('\u2713 Libraries imported!')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Multi-Head Attention\n\n**Idea**: Instead of single attention, use multiple attention \"heads\" in parallel!\n\n**Benefits**:\n- Different heads can attend to different aspects\n- Head 1: Syntactic relationships\n- Head 2: Semantic relationships\n- Head 3: Long-range dependencies\n\n**Mathematics**:\n\n$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h) W^O$$\n\nWhere each head:\n$$\\text{head}_i = \\text{Attention}(Q W_i^Q, K W_i^K, V W_i^V)$$"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        assert d_model % num_heads == 0\n        \n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.d_k = d_model // num_heads\n        \n        # Q, K, V projections for all heads\n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        \n        # Output projection\n        self.W_o = nn.Linear(d_model, d_model)\n        \n    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n        # Q, K, V: (batch, num_heads, seq_len, d_k)\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n        \n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, float('-inf'))\n        \n        attn_weights = F.softmax(scores, dim=-1)\n        output = torch.matmul(attn_weights, V)\n        \n        return output, attn_weights\n    \n    def forward(self, Q, K, V, mask=None):\n        batch_size = Q.size(0)\n        \n        # Linear projections and split into heads\n        Q = self.W_q(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        K = self.W_k(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        V = self.W_v(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        \n        # Apply attention\n        attn_output, attn_weights = self.scaled_dot_product_attention(Q, K, V, mask)\n        \n        # Concatenate heads\n        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n        \n        # Final linear layer\n        output = self.W_o(attn_output)\n        \n        return output, attn_weights\n\nprint('\u2713 MultiHeadAttention defined!')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Positional Encoding\n\n**Problem**: Attention has no notion of position/order!\n\n**Solution**: Add positional information to embeddings.\n\n**Sinusoidal encoding** (Vaswani et al.):\n\n$$PE_{(pos, 2i)} = \\sin(pos / 10000^{2i/d_{model}})$$\n$$PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i/d_{model}})$$\n\n**Why sine/cosine?**\n- Allows model to learn relative positions\n- Works for sequences longer than training"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super().__init__()\n        \n        # Create positional encoding matrix\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        \n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        \n        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n        self.register_buffer('pe', pe)\n        \n    def forward(self, x):\n        # x: (batch, seq_len, d_model)\n        return x + self.pe[:, :x.size(1), :]\n\nprint('\u2713 PositionalEncoding defined!')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Feed-Forward Networks\n\n**Position-wise FFN**: Applied to each position independently.\n\n$$\\text{FFN}(x) = \\text{ReLU}(x W_1 + b_1) W_2 + b_2$$\n\nTypically: $d_{ff} = 4 \\times d_{model}$"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class PositionwiseFeedForward(nn.Module):\n    def __init__(self, d_model, d_ff, dropout=0.1):\n        super().__init__()\n        self.linear1 = nn.Linear(d_model, d_ff)\n        self.linear2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x):\n        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n\nprint('\u2713 PositionwiseFeedForward defined!')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Encoder Layer\n\n**Each encoder layer has**:\n1. Multi-head self-attention\n2. Add & Norm (residual + layer norm)\n3. Feed-forward network\n4. Add & Norm"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class EncoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n        super().__init__()\n        \n        self.self_attn = MultiHeadAttention(d_model, num_heads)\n        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n        \n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        \n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n        \n    def forward(self, x, mask=None):\n        # Self-attention\n        attn_output, _ = self.self_attn(x, x, x, mask)\n        x = self.norm1(x + self.dropout1(attn_output))\n        \n        # Feed-forward\n        ff_output = self.feed_forward(x)\n        x = self.norm2(x + self.dropout2(ff_output))\n        \n        return x\n\nprint('\u2713 EncoderLayer defined!')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Complete Transformer Encoder"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class TransformerEncoder(nn.Module):\n    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n        super().__init__()\n        \n        self.embedding = nn.Embedding(vocab_size, d_model)\n        self.pos_encoding = PositionalEncoding(d_model)\n        \n        self.layers = nn.ModuleList([\n            EncoderLayer(d_model, num_heads, d_ff, dropout)\n            for _ in range(num_layers)\n        ])\n        \n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x, mask=None):\n        # Embed and add positional encoding\n        x = self.embedding(x) * math.sqrt(self.embedding.embedding_dim)\n        x = self.pos_encoding(x)\n        x = self.dropout(x)\n        \n        # Pass through encoder layers\n        for layer in self.layers:\n            x = layer(x, mask)\n        \n        return x\n\nprint('\u2713 TransformerEncoder defined!')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Decoder Layer\n\n**Decoder adds**:\n- Masked self-attention (can't see future)\n- Cross-attention (attend to encoder output)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class DecoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n        super().__init__()\n        \n        self.self_attn = MultiHeadAttention(d_model, num_heads)\n        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n        \n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        \n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n        self.dropout3 = nn.Dropout(dropout)\n        \n    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n        # Masked self-attention\n        attn_output, _ = self.self_attn(x, x, x, tgt_mask)\n        x = self.norm1(x + self.dropout1(attn_output))\n        \n        # Cross-attention\n        attn_output, _ = self.cross_attn(x, enc_output, enc_output, src_mask)\n        x = self.norm2(x + self.dropout2(attn_output))\n        \n        # Feed-forward\n        ff_output = self.feed_forward(x)\n        x = self.norm3(x + self.dropout3(ff_output))\n        \n        return x\n\nprint('\u2713 DecoderLayer defined!')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Complete Transformer"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class Transformer(nn.Module):\n    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, num_heads=8,\n                 d_ff=2048, num_encoder_layers=6, num_decoder_layers=6, dropout=0.1):\n        super().__init__()\n        \n        # Encoder\n        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n        self.encoder_pos = PositionalEncoding(d_model)\n        self.encoder_layers = nn.ModuleList([\n            EncoderLayer(d_model, num_heads, d_ff, dropout)\n            for _ in range(num_encoder_layers)\n        ])\n        \n        # Decoder\n        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n        self.decoder_pos = PositionalEncoding(d_model)\n        self.decoder_layers = nn.ModuleList([\n            DecoderLayer(d_model, num_heads, d_ff, dropout)\n            for _ in range(num_decoder_layers)\n        ])\n        \n        # Output projection\n        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n        self.dropout = nn.Dropout(dropout)\n        \n    def generate_mask(self, src, tgt):\n        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n        \n        seq_length = tgt.size(1)\n        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n        tgt_mask = tgt_mask & nopeak_mask.to(tgt.device)\n        \n        return src_mask, tgt_mask\n    \n    def forward(self, src, tgt):\n        src_mask, tgt_mask = self.generate_mask(src, tgt)\n        \n        # Encode\n        enc_output = self.encoder_embedding(src) * math.sqrt(self.encoder_embedding.embedding_dim)\n        enc_output = self.encoder_pos(enc_output)\n        enc_output = self.dropout(enc_output)\n        \n        for layer in self.encoder_layers:\n            enc_output = layer(enc_output, src_mask)\n        \n        # Decode\n        dec_output = self.decoder_embedding(tgt) * math.sqrt(self.decoder_embedding.embedding_dim)\n        dec_output = self.decoder_pos(dec_output)\n        dec_output = self.dropout(dec_output)\n        \n        for layer in self.decoder_layers:\n            dec_output = layer(dec_output, enc_output, src_mask, tgt_mask)\n        \n        # Project to vocabulary\n        output = self.fc_out(dec_output)\n        \n        return output\n\nprint('\u2713 Complete Transformer defined!')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Exercise 1**: Test the Transformer\n\n1. Initialize a small Transformer\n2. Pass dummy input through it\n3. Count total parameters\n4. Compare with RNN/LSTM of similar capacity"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# YOUR CODE HERE"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\n### Key Concepts:\n\n1. **Multi-Head Attention**: Multiple attention mechanisms in parallel\n2. **Positional Encoding**: Inject sequence order information\n3. **Layer Normalization**: Stabilize training\n4. **Residual Connections**: Enable deep networks\n5. **Encoder-Decoder**: Process and generate sequences\n\n### Transformer Advantages:\n\n\u2705 Parallelizable (fast training)  \n\u2705 Long-range dependencies  \n\u2705 State-of-the-art performance  \n\u2705 Transfer learning (BERT, GPT)  \n\n### What's Next?\n\nIn **Module 07: BERT**, we'll learn about encoder-only transformers for understanding tasks.\n\n### Resources:\n\n- **Original Paper**: [Attention is All You Need](https://arxiv.org/abs/1706.03762)\n- **Illustrated Transformer**: [Jay Alammar's Blog](http://jalammar.github.io/illustrated-transformer/)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}