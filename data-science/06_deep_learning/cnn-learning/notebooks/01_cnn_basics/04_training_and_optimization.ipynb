{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 04: Training & Optimization Techniques\n",
    "\n",
    "Learn advanced techniques to train better, faster CNNs.\n",
    "\n",
    "## Topics Covered\n",
    "- Optimizers (SGD, Adam, RMSprop)\n",
    "- Learning rate scheduling\n",
    "- Batch normalization\n",
    "- Dropout for regularization\n",
    "- Data augmentation\n",
    "- Detecting and fixing overfitting\n",
    "\n",
    "## Time: 45 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Optimizers\n",
    "\n",
    "### Common Optimizers\n",
    "\n",
    "**SGD (Stochastic Gradient Descent)**\n",
    "- Classic, simple, reliable\n",
    "- Requires careful learning rate tuning\n",
    "- With momentum: smoother updates\n",
    "\n",
    "**Adam (Adaptive Moment Estimation)**\n",
    "- Most popular!\n",
    "- Adapts learning rate per parameter\n",
    "- Works well out-of-the-box\n",
    "\n",
    "**RMSprop**\n",
    "- Good for RNNs\n",
    "- Adaptive learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare optimizers\n",
    "model = nn.Linear(10, 1)\n",
    "\n",
    "# SGD\n",
    "optimizer_sgd = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Adam\n",
    "optimizer_adam = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# RMSprop\n",
    "optimizer_rmsprop = optim.RMSprop(model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Optimizers created!\")\n",
    "print(\"\\nWhen to use:\")\n",
    "print(\"- Adam: Default choice, works well for most tasks\")\n",
    "print(\"- SGD with momentum: When you need better generalization\")\n",
    "print(\"- RMSprop: Recurrent networks, online learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Learning Rate Scheduling\n",
    "\n",
    "Start with higher learning rate, gradually decrease.\n",
    "\n",
    "**Benefits:**\n",
    "- Fast initial learning\n",
    "- Fine-tuning at the end\n",
    "- Better final accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Step LR: Reduce by factor every N epochs\n",
    "scheduler_step = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# Exponential: Smooth decay\n",
    "scheduler_exp = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "\n",
    "# Reduce on plateau: Reduce when loss stops improving\n",
    "scheduler_plateau = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", patience=5)\n",
    "\n",
    "print(\"Learning rate schedulers created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Batch Normalization\n",
    "\n",
    "Normalizes layer inputs for stable, faster training.\n",
    "\n",
    "**Benefits:**\n",
    "- Faster convergence\n",
    "- Higher learning rates possible\n",
    "- Acts as regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNWithBatchNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)  # Batch norm after conv\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model_bn = CNNWithBatchNorm()\n",
    "print(model_bn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Dropout for Regularization\n",
    "\n",
    "Randomly \"drops\" neurons during training to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNWithDropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout1 = nn.Dropout(0.25)  # Drop 25% of neurons\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.dropout2 = nn.Dropout(0.5)  # Drop 50% of neurons\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "print(\"Dropout helps prevent overfitting!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Data Augmentation\n",
    "\n",
    "Artificially increase training data by applying transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation for training\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomRotation(10),  # Rotate Â±10 degrees\n",
    "        transforms.RandomAffine(0, translate=(0.1, 0.1)),  # Shift\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# No augmentation for testing\n",
    "test_transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    ")\n",
    "\n",
    "print(\"Data augmentation increases effective dataset size!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Techniques:\n",
    "1. **Adam optimizer** - Great default choice\n",
    "2. **Learning rate scheduling** - Improve final accuracy\n",
    "3. **Batch normalization** - Faster, more stable training\n",
    "4. **Dropout** - Prevent overfitting\n",
    "5. **Data augmentation** - More training data\n",
    "\n",
    "### Next: Module 05 - CNN Architectures\n",
    "Learn famous CNN architectures used in production!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
