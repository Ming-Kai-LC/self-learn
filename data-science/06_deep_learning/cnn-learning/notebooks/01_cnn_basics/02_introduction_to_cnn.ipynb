{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 02: Introduction to Convolutional Neural Networks\n",
    "\n",
    "**Understanding the Magic of Convolution**\n",
    "\n",
    "Welcome to the heart of computer vision! In this module, you'll learn why CNNs revolutionized image processing and how they work.\n",
    "\n",
    "## What You'll Learn\n",
    "- Why regular neural networks struggle with images\n",
    "- What is a convolution operation?\n",
    "- Filters and feature maps explained\n",
    "- Pooling layers and their purpose\n",
    "- Complete CNN architecture overview\n",
    "- Real-world applications\n",
    "\n",
    "## Prerequisites\n",
    "- Completed Modules 00-01\n",
    "- Understanding of basic neural networks\n",
    "\n",
    "## Time Required\n",
    "45 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from scipy.signal import convolve2d\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Why Regular Neural Networks Struggle with Images\n",
    "\n",
    "### The Problem\n",
    "\n",
    "In Module 01, we built a simple neural network that flattened images into vectors. This works, but has major issues:\n",
    "\n",
    "#### 1. **Loses Spatial Information**\n",
    "- Flattening destroys the 2D structure of images\n",
    "- Nearby pixels are no longer \"neighbors\"\n",
    "- The network can't understand spatial relationships\n",
    "\n",
    "#### 2. **Too Many Parameters**\n",
    "- A small 28×28 image = 784 pixels\n",
    "- With 128 hidden neurons: 100,480 parameters just for first layer!\n",
    "- A 224×224 RGB image would need: 224×224×3 = 150,528 inputs\n",
    "- With 1000 hidden neurons: 150 million parameters for first layer alone!\n",
    "\n",
    "#### 3. **Not Translation Invariant**\n",
    "- If a cat appears in the top-left vs bottom-right, it's treated as completely different\n",
    "- Network must learn to recognize cats in every possible position\n",
    "- Extremely inefficient!\n",
    "\n",
    "### The Solution: Convolutional Neural Networks\n",
    "\n",
    "CNNs solve all these problems by:\n",
    "1. **Preserving spatial structure** - Keep the 2D arrangement\n",
    "2. **Local connectivity** - Each neuron only looks at a small region\n",
    "3. **Parameter sharing** - Use the same filter across the entire image\n",
    "4. **Translation invariance** - Detect features regardless of position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Understanding Convolution\n",
    "\n",
    "### What is Convolution?\n",
    "\n",
    "**Convolution** is a mathematical operation that:\n",
    "- Takes an image (input)\n",
    "- Applies a **filter** (also called **kernel**)\n",
    "- Produces a **feature map** (output)\n",
    "\n",
    "### Visual Analogy\n",
    "\n",
    "Think of it like:\n",
    "- **Image**: A large piece of paper with a picture\n",
    "- **Filter**: A small magnifying glass\n",
    "- **Process**: Slide the magnifying glass across the image, looking for specific patterns\n",
    "\n",
    "### How It Works (Step by Step)\n",
    "\n",
    "1. **Place** the filter (e.g., 3×3) on the top-left corner of the image\n",
    "2. **Multiply** each filter value with the corresponding image pixel\n",
    "3. **Sum** all these products to get a single number\n",
    "4. **Slide** the filter one position to the right\n",
    "5. **Repeat** until you've covered the entire image\n",
    "\n",
    "The result is a **feature map** showing where the pattern was detected!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see convolution in action with a simple example\n",
    "\n",
    "# Create a simple 5×5 image\n",
    "image = np.array(\n",
    "    [[0, 0, 0, 0, 0], [0, 1, 1, 1, 0], [0, 1, 1, 1, 0], [0, 1, 1, 1, 0], [0, 0, 0, 0, 0]],\n",
    "    dtype=np.float32,\n",
    ")\n",
    "\n",
    "# Create a simple 3×3 filter (detects vertical edges)\n",
    "filter_vertical = np.array([[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]], dtype=np.float32)\n",
    "\n",
    "# Apply convolution\n",
    "feature_map_vertical = convolve2d(image, filter_vertical, mode=\"valid\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Original image\n",
    "axes[0].imshow(image, cmap=\"gray\", vmin=0, vmax=1)\n",
    "axes[0].set_title(\"Original Image\", fontsize=14, fontweight=\"bold\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "# Filter\n",
    "axes[1].imshow(filter_vertical, cmap=\"RdBu\", vmin=-1, vmax=1)\n",
    "axes[1].set_title(\"Vertical Edge Filter\\n(3×3)\", fontsize=14, fontweight=\"bold\")\n",
    "axes[1].axis(\"off\")\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        axes[1].text(\n",
    "            j,\n",
    "            i,\n",
    "            f\"{filter_vertical[i,j]:.0f}\",\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            fontsize=12,\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "\n",
    "# Feature map\n",
    "axes[2].imshow(feature_map_vertical, cmap=\"RdBu\")\n",
    "axes[2].set_title(\"Feature Map\\n(Vertical Edges Detected)\", fontsize=14, fontweight=\"bold\")\n",
    "axes[2].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice how the filter detected vertical edges!\")\n",
    "print(f\"Original image size: {image.shape}\")\n",
    "print(f\"Filter size: {filter_vertical.shape}\")\n",
    "print(f\"Feature map size: {feature_map_vertical.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Filters and What They Detect\n",
    "\n",
    "Different filters detect different features:\n",
    "\n",
    "#### 1. **Edge Detection Filters**\n",
    "- **Vertical edges**: Detect left-to-right changes\n",
    "- **Horizontal edges**: Detect top-to-bottom changes\n",
    "- **Diagonal edges**: Detect diagonal patterns\n",
    "\n",
    "#### 2. **Blur Filters**\n",
    "- Average nearby pixels\n",
    "- Smoothen the image\n",
    "\n",
    "#### 3. **Sharpen Filters**\n",
    "- Enhance edges and details\n",
    "- Make images crisper\n",
    "\n",
    "**In CNNs, the network LEARNS what filters to use!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try different filters on a real image\n",
    "\n",
    "# Load an MNIST digit\n",
    "mnist = datasets.MNIST(\"../data/datasets\", train=True, download=True)\n",
    "sample_image = mnist[0][0]  # First digit\n",
    "img_array = np.array(sample_image, dtype=np.float32) / 255.0\n",
    "\n",
    "# Define various filters\n",
    "filters = {\n",
    "    \"Vertical Edge\": np.array([[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]]),\n",
    "    \"Horizontal Edge\": np.array([[-1, -1, -1], [0, 0, 0], [1, 1, 1]]),\n",
    "    \"Sobel X\": np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]),\n",
    "    \"Blur\": np.ones((3, 3)) / 9,\n",
    "    \"Sharpen\": np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]]),\n",
    "}\n",
    "\n",
    "# Apply filters\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Original image\n",
    "axes[0].imshow(img_array, cmap=\"gray\")\n",
    "axes[0].set_title(\"Original MNIST Digit\", fontsize=12, fontweight=\"bold\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "# Apply each filter\n",
    "for idx, (name, filt) in enumerate(filters.items(), start=1):\n",
    "    feature_map = convolve2d(img_array, filt, mode=\"same\")\n",
    "    axes[idx].imshow(feature_map, cmap=\"gray\")\n",
    "    axes[idx].set_title(f\"{name} Filter\", fontsize=12, fontweight=\"bold\")\n",
    "    axes[idx].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Each filter highlights different features!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Convolution in PyTorch\n",
    "\n",
    "### Conv2d Layer\n",
    "\n",
    "PyTorch provides `nn.Conv2d` for 2D convolution:\n",
    "\n",
    "```python\n",
    "nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "```\n",
    "\n",
    "**Parameters:**\n",
    "- `in_channels`: Number of input channels (1 for grayscale, 3 for RGB)\n",
    "- `out_channels`: Number of filters (feature maps to create)\n",
    "- `kernel_size`: Size of the filter (e.g., 3 means 3×3)\n",
    "- `stride`: Step size when sliding the filter (default=1)\n",
    "- `padding`: Add zeros around the image (to preserve size)\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "#### **Stride**\n",
    "- How many pixels to move the filter each step\n",
    "- Stride=1: Move one pixel at a time (detailed)\n",
    "- Stride=2: Skip pixels (faster, smaller output)\n",
    "\n",
    "#### **Padding**\n",
    "- Add zeros around the image border\n",
    "- Prevents shrinking of feature maps\n",
    "- Allows filters to process edge pixels properly\n",
    "\n",
    "#### **Output Size Calculation**\n",
    "```\n",
    "Output_size = (Input_size - Kernel_size + 2*Padding) / Stride + 1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple convolutional layer\n",
    "\n",
    "# Define a Conv2d layer\n",
    "# Input: 1 channel (grayscale), Output: 6 filters, Kernel: 3×3\n",
    "conv_layer = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "print(\"Convolutional Layer:\")\n",
    "print(\"=\" * 50)\n",
    "print(conv_layer)\n",
    "print(f\"\\nNumber of parameters: {sum(p.numel() for p in conv_layer.parameters())}\")\n",
    "print(f\"\\nFilter weights shape: {conv_layer.weight.shape}\")\n",
    "print(f\"Interpretation: (out_channels=6, in_channels=1, height=3, width=3)\")\n",
    "\n",
    "# Test with a sample image\n",
    "sample_input = torch.randn(1, 1, 28, 28)  # (batch, channels, height, width)\n",
    "output = conv_layer(sample_input)\n",
    "\n",
    "print(f\"\\nInput shape: {sample_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nOutput interpretation: (batch=1, channels=6, height=28, width=28)\")\n",
    "print(\"\\nWe went from 1 channel to 6 channels (6 different feature maps)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Pooling Layers\n",
    "\n",
    "### What is Pooling?\n",
    "\n",
    "**Pooling** reduces the spatial dimensions of feature maps while keeping important information.\n",
    "\n",
    "### Why Pooling?\n",
    "\n",
    "1. **Reduce computational cost** - Fewer pixels to process\n",
    "2. **Reduce overfitting** - Less parameters\n",
    "3. **Increase receptive field** - Each neuron \"sees\" more of the image\n",
    "4. **Add translation invariance** - Small shifts don't matter\n",
    "\n",
    "### Types of Pooling\n",
    "\n",
    "#### **Max Pooling**\n",
    "- Takes the maximum value in each region\n",
    "- Most common type\n",
    "- Keeps the strongest activation (most important feature)\n",
    "\n",
    "#### **Average Pooling**\n",
    "- Takes the average value in each region\n",
    "- Smoother, but less commonly used\n",
    "\n",
    "### Example: 2×2 Max Pooling\n",
    "\n",
    "```\n",
    "Input (4×4):          Output (2×2):\n",
    "[1, 3, 2, 4]          [3, 4]\n",
    "[2, 1, 5, 3]    -->   [6, 8]\n",
    "[4, 6, 1, 2]\n",
    "[3, 2, 7, 8]\n",
    "```\n",
    "\n",
    "Divide into 2×2 regions, take max from each!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize max pooling\n",
    "\n",
    "# Create a sample feature map\n",
    "feature_map = np.array(\n",
    "    [\n",
    "        [1, 3, 2, 4, 1, 2],\n",
    "        [2, 1, 5, 3, 4, 1],\n",
    "        [4, 6, 1, 2, 3, 5],\n",
    "        [3, 2, 7, 8, 2, 1],\n",
    "        [5, 1, 3, 2, 6, 4],\n",
    "        [2, 4, 1, 5, 3, 7],\n",
    "    ],\n",
    "    dtype=np.float32,\n",
    ")\n",
    "\n",
    "# Apply max pooling using PyTorch\n",
    "feature_tensor = torch.from_numpy(feature_map).unsqueeze(0).unsqueeze(0)\n",
    "pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "pooled = pool(feature_tensor)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Original\n",
    "im1 = axes[0].imshow(feature_map, cmap=\"YlOrRd\", vmin=0, vmax=8)\n",
    "axes[0].set_title(\"Original Feature Map (6×6)\", fontsize=14, fontweight=\"bold\")\n",
    "for i in range(6):\n",
    "    for j in range(6):\n",
    "        axes[0].text(\n",
    "            j,\n",
    "            i,\n",
    "            f\"{feature_map[i,j]:.0f}\",\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            fontsize=11,\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "# Add grid to show pooling regions\n",
    "for i in range(0, 7, 2):\n",
    "    axes[0].axhline(i - 0.5, color=\"blue\", linewidth=2)\n",
    "    axes[0].axvline(i - 0.5, color=\"blue\", linewidth=2)\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "# Pooled\n",
    "pooled_array = pooled.squeeze().numpy()\n",
    "im2 = axes[1].imshow(pooled_array, cmap=\"YlOrRd\", vmin=0, vmax=8)\n",
    "axes[1].set_title(\"After Max Pooling (3×3)\", fontsize=14, fontweight=\"bold\")\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        axes[1].text(\n",
    "            j,\n",
    "            i,\n",
    "            f\"{pooled_array[i,j]:.0f}\",\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            fontsize=14,\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Original size: {feature_map.shape}\")\n",
    "print(f\"After pooling: {pooled_array.shape}\")\n",
    "print(f\"Size reduction: {(1 - pooled_array.size / feature_map.size) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Complete CNN Architecture\n",
    "\n",
    "### Standard CNN Structure\n",
    "\n",
    "A typical CNN consists of:\n",
    "\n",
    "```\n",
    "Input Image\n",
    "    ↓\n",
    "[Conv → ReLU → Pool] ← Repeat multiple times\n",
    "    ↓\n",
    "[Conv → ReLU → Pool]\n",
    "    ↓\n",
    "[Conv → ReLU → Pool]\n",
    "    ↓\n",
    "Flatten\n",
    "    ↓\n",
    "[Fully Connected → ReLU] ← Regular neural network layers\n",
    "    ↓\n",
    "[Fully Connected → Softmax]\n",
    "    ↓\n",
    "Output (Class Probabilities)\n",
    "```\n",
    "\n",
    "### What Each Part Does\n",
    "\n",
    "#### **Early Layers** (Close to input)\n",
    "- Detect simple features: edges, corners, colors\n",
    "- Small receptive field\n",
    "- Many feature maps (many different simple features)\n",
    "\n",
    "#### **Middle Layers**\n",
    "- Combine simple features into patterns\n",
    "- Detect textures, parts of objects\n",
    "- Medium receptive field\n",
    "\n",
    "#### **Deep Layers** (Close to output)\n",
    "- Detect complex patterns and whole objects\n",
    "- Large receptive field\n",
    "- \"See\" the entire image context\n",
    "\n",
    "#### **Fully Connected Layers**\n",
    "- Combine all features to make final decision\n",
    "- Act like a regular neural network classifier\n",
    "\n",
    "### Hierarchical Feature Learning\n",
    "\n",
    "**This is the magic of CNNs!**\n",
    "\n",
    "- **Layer 1**: Learns edges and simple patterns\n",
    "- **Layer 2**: Combines edges into curves and corners\n",
    "- **Layer 3**: Combines curves into parts (ears, eyes, wheels)\n",
    "- **Layer 4**: Combines parts into objects (cats, cars)\n",
    "- **Output**: Classifies the object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You now understand how CNNs work!\n",
    "\n",
    "### What You Learned:\n",
    "\n",
    "1. **Why CNNs?**\n",
    "   - Regular NNs lose spatial information\n",
    "   - Too many parameters for images\n",
    "   - Not translation invariant\n",
    "\n",
    "2. **Convolution Operation**\n",
    "   - Slide a filter across an image\n",
    "   - Detect specific patterns\n",
    "   - Create feature maps\n",
    "\n",
    "3. **Filters**\n",
    "   - Small matrices (e.g., 3×3)\n",
    "   - Learn what patterns to detect\n",
    "   - Different filters detect different features\n",
    "\n",
    "4. **Pooling**\n",
    "   - Reduces spatial dimensions\n",
    "   - Keeps important information\n",
    "   - Adds translation invariance\n",
    "\n",
    "5. **CNN Architecture**\n",
    "   - Stack Conv-ReLU-Pool blocks\n",
    "   - Hierarchical feature learning\n",
    "   - Flatten and classify with FC layers\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "- CNNs preserve spatial structure of images\n",
    "- Convolution + pooling reduces parameters dramatically\n",
    "- Early layers detect simple features, deep layers detect complex patterns\n",
    "- The same filter is used across the entire image (parameter sharing)\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "Now you understand CNN theory! In Module 03, you'll:\n",
    "- Build your first CNN from scratch\n",
    "- Train it on MNIST digits\n",
    "- Visualize what the network learns\n",
    "- Achieve state-of-the-art performance!\n",
    "\n",
    "**Ready to build your own CNN? Proceed to Module 03!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
