{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 09: Introduction to Image Segmentation\n",
    "\n",
    "**Pixel-Level Understanding**\n",
    "\n",
    "Classify every pixel in an image!\n",
    "\n",
    "## What You'll Learn\n",
    "- What is image segmentation?\n",
    "- Semantic vs instance segmentation\n",
    "- U-Net architecture\n",
    "- Using pre-trained segmentation models\n",
    "- Real-world applications\n",
    "\n",
    "## Time: 45 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.segmentation import deeplabv3_resnet50\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: What is Image Segmentation?\n",
    "\n",
    "### Progression of Vision Tasks\n",
    "\n",
    "**1. Classification:**\n",
    "- \"What is in the image?\"\n",
    "- Output: One label\n",
    "\n",
    "**2. Object Detection:**\n",
    "- \"What and where are objects?\"\n",
    "- Output: Boxes + labels\n",
    "\n",
    "**3. Segmentation:**\n",
    "- \"Which pixels belong to which object?\"\n",
    "- Output: Labeled mask for every pixel\n",
    "\n",
    "### Types of Segmentation\n",
    "\n",
    "#### **Semantic Segmentation**\n",
    "- Classify each pixel into a class\n",
    "- Don't distinguish between instances\n",
    "- Example: All people labeled as \"person\", all cars as \"car\"\n",
    "\n",
    "#### **Instance Segmentation**\n",
    "- Classify AND separate individual instances\n",
    "- Example: Person1, Person2, Car1, Car2\n",
    "- More complex but more informative\n",
    "\n",
    "#### **Panoptic Segmentation**\n",
    "- Combines semantic and instance\n",
    "- Labels \"stuff\" (road, sky) semantically\n",
    "- Labels \"things\" (people, cars) by instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: U-Net Architecture\n",
    "\n",
    "**Most popular architecture for segmentation!**\n",
    "\n",
    "### Structure\n",
    "\n",
    "```\n",
    "Input Image\n",
    "    ↓\n",
    "[Encoder Path] → Downsample, extract features\n",
    "    ↓\n",
    "[Bottleneck] → Deepest features\n",
    "    ↓\n",
    "[Decoder Path] → Upsample, restore resolution\n",
    "    ↑ ↖ (skip connections from encoder)\n",
    "    ↓\n",
    "Output Segmentation Map\n",
    "```\n",
    "\n",
    "### Key Features\n",
    "\n",
    "1. **Encoder (Contracting Path)**\n",
    "   - Similar to normal CNN\n",
    "   - Downsample with pooling\n",
    "   - Increase channels\n",
    "   - Extract high-level features\n",
    "\n",
    "2. **Decoder (Expanding Path)**\n",
    "   - Upsample with transposed convolutions\n",
    "   - Decrease channels\n",
    "   - Restore spatial resolution\n",
    "\n",
    "3. **Skip Connections**\n",
    "   - Connect encoder to decoder\n",
    "   - Preserve fine details\n",
    "   - Help recover spatial information\n",
    "\n",
    "### Why U-Net?\n",
    "\n",
    "- **Works with small datasets** (originally for medical imaging)\n",
    "- **Precise localization** thanks to skip connections\n",
    "- **Fast** and efficient\n",
    "- **Easy to train**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Using Pre-Trained Segmentation Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained DeepLabV3\n",
    "model = deeplabv3_resnet50(pretrained=True)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"DeepLabV3 loaded!\")\n",
    "print(\"Trained on COCO dataset\")\n",
    "print(\"Can segment 21 classes including:\")\n",
    "print(\"- person, car, bicycle, dog, cat\")\n",
    "print(\"- chair, table, potted plant\")\n",
    "print(\"- background, and more...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Segmentation Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_image(image_path, model):\n",
    "    \"\"\"\n",
    "    Perform semantic segmentation on an image\n",
    "\n",
    "    Args:\n",
    "        image_path: Path to image\n",
    "        model: Segmentation model\n",
    "\n",
    "    Returns:\n",
    "        Original image and segmentation mask\n",
    "    \"\"\"\n",
    "    # Load and preprocess image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    transform = torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    # Segment\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)[\"out\"][0]\n",
    "\n",
    "    # Get class predictions\n",
    "    output_predictions = output.argmax(0).cpu().numpy()\n",
    "\n",
    "    return image, output_predictions\n",
    "\n",
    "\n",
    "def visualize_segmentation(image, mask, num_classes=21):\n",
    "    \"\"\"\n",
    "    Visualize segmentation results\n",
    "    \"\"\"\n",
    "    # Create colormap\n",
    "    colors = plt.cm.tab20(np.linspace(0, 1, num_classes))\n",
    "\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "    # Original image\n",
    "    ax1.imshow(image)\n",
    "    ax1.set_title(\"Original Image\", fontsize=14, fontweight=\"bold\")\n",
    "    ax1.axis(\"off\")\n",
    "\n",
    "    # Segmentation mask\n",
    "    ax2.imshow(mask, cmap=\"tab20\")\n",
    "    ax2.set_title(\"Segmentation Mask\", fontsize=14, fontweight=\"bold\")\n",
    "    ax2.axis(\"off\")\n",
    "\n",
    "    # Overlay\n",
    "    ax3.imshow(image)\n",
    "    ax3.imshow(mask, alpha=0.5, cmap=\"tab20\")\n",
    "    ax3.set_title(\"Overlay\", fontsize=14, fontweight=\"bold\")\n",
    "    ax3.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(\"Segmentation functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Real-World Applications\n",
    "\n",
    "### Medical Imaging\n",
    "- **Tumor detection**: Segment cancerous regions\n",
    "- **Organ segmentation**: Identify organs in CT/MRI scans\n",
    "- **Cell counting**: Count and segment individual cells\n",
    "\n",
    "### Autonomous Vehicles\n",
    "- **Road scene understanding**: Segment road, lanes, vehicles, pedestrians\n",
    "- **Drivable area detection**: Where can the car go?\n",
    "- **Obstacle detection**: Identify and segment obstacles\n",
    "\n",
    "### Satellite Imagery\n",
    "- **Land use classification**: Forest, urban, agriculture\n",
    "- **Building detection**: Segment buildings from aerial images\n",
    "- **Crop monitoring**: Identify crop types and health\n",
    "\n",
    "### Photo/Video Editing\n",
    "- **Background removal**: Segment person from background\n",
    "- **Object selection**: Select objects for editing\n",
    "- **Depth estimation**: Estimate depth from segmentation\n",
    "\n",
    "### Manufacturing\n",
    "- **Defect detection**: Segment defective areas\n",
    "- **Quality control**: Identify and segment anomalies\n",
    "- **Part inspection**: Segment and measure components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What You Learned:\n",
    "\n",
    "1. **Segmentation Types**\n",
    "   - Semantic: Classify pixels by class\n",
    "   - Instance: Separate individual objects\n",
    "   - Panoptic: Combine both\n",
    "\n",
    "2. **U-Net Architecture**\n",
    "   - Encoder-decoder structure\n",
    "   - Skip connections for detail\n",
    "   - Gold standard for segmentation\n",
    "\n",
    "3. **Practical Implementation**\n",
    "   - Used pre-trained DeepLabV3\n",
    "   - Segmented images pixel-by-pixel\n",
    "   - Visualized results\n",
    "\n",
    "4. **Applications**\n",
    "   - Medical imaging\n",
    "   - Autonomous vehicles\n",
    "   - Satellite imagery\n",
    "   - Many more!\n",
    "\n",
    "### Key Insight:\n",
    "Segmentation provides the most detailed understanding of images - every pixel is classified!\n",
    "\n",
    "### Next: Module 10 - Final Projects & Next Steps\n",
    "Wrap up and plan your deep learning journey!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
