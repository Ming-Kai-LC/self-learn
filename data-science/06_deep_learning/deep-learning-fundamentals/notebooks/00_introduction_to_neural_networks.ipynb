{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 00: Introduction to Neural Networks and Deep Learning\n",
    "\n",
    "**Difficulty**: ⭐⭐ (Intermediate)\n",
    "**Estimated Time**: 30-40 minutes\n",
    "**Prerequisites**: \n",
    "- Basic Python programming\n",
    "- Understanding of linear algebra (vectors, matrices)\n",
    "- Basic calculus (derivatives)\n",
    "- Familiarity with NumPy and Matplotlib\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. **Explain** the biological inspiration behind artificial neural networks\n",
    "2. **Describe** the historical evolution of deep learning from perceptrons to modern architectures\n",
    "3. **Identify** key applications of deep learning across different domains\n",
    "4. **Understand** the mathematical notation and basic concepts used in neural networks\n",
    "5. **Visualize** simple neural network structures and their components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "Let's start by importing the necessary libraries for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard scientific computing libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib for better-looking plots\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Display settings\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"Setup complete! NumPy version:\", np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What Are Neural Networks?\n",
    "\n",
    "### 2.1 Biological Inspiration\n",
    "\n",
    "Neural networks are computing systems inspired by the biological neural networks in animal brains. Let's understand the key components:\n",
    "\n",
    "**Biological Neuron Components:**\n",
    "- **Dendrites**: Receive signals from other neurons\n",
    "- **Cell Body (Soma)**: Processes incoming signals\n",
    "- **Axon**: Transmits output signals to other neurons\n",
    "- **Synapses**: Connections between neurons that can strengthen or weaken\n",
    "\n",
    "**Artificial Neuron (Perceptron) Components:**\n",
    "- **Inputs**: Similar to dendrites, receive data ($x_1, x_2, ..., x_n$)\n",
    "- **Weights**: Similar to synaptic strengths ($w_1, w_2, ..., w_n$)\n",
    "- **Summation**: Weighted sum of inputs ($\\sum w_i x_i$)\n",
    "- **Activation Function**: Determines if neuron \"fires\" (produces output)\n",
    "- **Output**: Signal sent to next layer ($y$)\n",
    "\n",
    "The mathematical model of a single neuron:\n",
    "\n",
    "$$y = f\\left(\\sum_{i=1}^{n} w_i x_i + b\\right)$$\n",
    "\n",
    "Where:\n",
    "- $x_i$ are input features\n",
    "- $w_i$ are weights (learnable parameters)\n",
    "- $b$ is bias (learnable parameter)\n",
    "- $f$ is the activation function\n",
    "- $y$ is the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_neuron():\n",
    "    \"\"\"\n",
    "    Visualize a simple artificial neuron structure.\n",
    "    This shows the flow from inputs through weights to output.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "    \n",
    "    # Define positions for components\n",
    "    input_x = 0.1\n",
    "    neuron_x = 0.5\n",
    "    output_x = 0.9\n",
    "    \n",
    "    # Draw inputs\n",
    "    num_inputs = 3\n",
    "    input_y = np.linspace(0.2, 0.8, num_inputs)\n",
    "    \n",
    "    for i, y in enumerate(input_y):\n",
    "        # Input nodes\n",
    "        circle = plt.Circle((input_x, y), 0.04, color='lightblue', ec='black', zorder=3)\n",
    "        ax.add_patch(circle)\n",
    "        ax.text(input_x - 0.08, y, f'$x_{i+1}$', fontsize=14, ha='center', va='center')\n",
    "        \n",
    "        # Connections to neuron\n",
    "        ax.plot([input_x + 0.04, neuron_x - 0.08], [y, 0.5], 'k-', alpha=0.5, linewidth=2)\n",
    "        ax.text((input_x + neuron_x) / 2, (y + 0.5) / 2 + 0.05, f'$w_{i+1}$', \n",
    "                fontsize=11, ha='center', style='italic')\n",
    "    \n",
    "    # Draw neuron (cell body)\n",
    "    neuron = plt.Circle((neuron_x, 0.5), 0.08, color='coral', ec='black', linewidth=2, zorder=3)\n",
    "    ax.add_patch(neuron)\n",
    "    ax.text(neuron_x, 0.5, '$\\\\Sigma$', fontsize=20, ha='center', va='center', weight='bold')\n",
    "    ax.text(neuron_x, 0.35, '$f(\\\\cdot)$', fontsize=12, ha='center', va='center')\n",
    "    \n",
    "    # Draw bias\n",
    "    bias_circle = plt.Circle((neuron_x, 0.15), 0.03, color='lightyellow', ec='black', zorder=3)\n",
    "    ax.add_patch(bias_circle)\n",
    "    ax.text(neuron_x, 0.05, '$b$', fontsize=12, ha='center', va='center')\n",
    "    ax.plot([neuron_x, neuron_x], [0.18, 0.42], 'k-', alpha=0.5, linewidth=2)\n",
    "    \n",
    "    # Draw output\n",
    "    ax.plot([neuron_x + 0.08, output_x - 0.04], [0.5, 0.5], 'k-', linewidth=3)\n",
    "    output_circle = plt.Circle((output_x, 0.5), 0.04, color='lightgreen', ec='black', zorder=3)\n",
    "    ax.add_patch(output_circle)\n",
    "    ax.text(output_x + 0.08, 0.5, '$y$', fontsize=14, ha='center', va='center')\n",
    "    \n",
    "    # Labels\n",
    "    ax.text(input_x, 0.95, 'Inputs', fontsize=13, ha='center', weight='bold')\n",
    "    ax.text(neuron_x, 0.95, 'Neuron', fontsize=13, ha='center', weight='bold')\n",
    "    ax.text(output_x, 0.95, 'Output', fontsize=13, ha='center', weight='bold')\n",
    "    \n",
    "    ax.set_xlim(-0.1, 1.1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.axis('off')\n",
    "    ax.set_title('Structure of an Artificial Neuron', fontsize=15, weight='bold', pad=20)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the neuron structure\n",
    "visualize_neuron()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. History of Deep Learning\n",
    "\n",
    "Deep learning has evolved significantly over the past 70+ years. Here's a timeline of major milestones:\n",
    "\n",
    "### Historical Timeline\n",
    "\n",
    "**1943 - McCulloch-Pitts Neuron**\n",
    "- First mathematical model of a biological neuron\n",
    "- Binary threshold unit (outputs 0 or 1)\n",
    "\n",
    "**1958 - The Perceptron (Frank Rosenblatt)**\n",
    "- First learning algorithm for neural networks\n",
    "- Could learn simple linear decision boundaries\n",
    "- Limitation: Cannot solve XOR problem (not linearly separable)\n",
    "\n",
    "**1969 - AI Winter Begins**\n",
    "- Minsky & Papert showed limitations of single-layer perceptrons\n",
    "- Research funding decreased dramatically\n",
    "\n",
    "**1986 - Backpropagation Algorithm**\n",
    "- Rumelhart, Hinton, and Williams popularized backpropagation\n",
    "- Enabled training of multi-layer networks\n",
    "- Solved the XOR problem and more complex patterns\n",
    "\n",
    "**1998 - LeNet (Convolutional Neural Networks)**\n",
    "- Yann LeCun developed LeNet-5 for handwritten digit recognition\n",
    "- Used by banks to read checks\n",
    "- Foundation for modern CNNs\n",
    "\n",
    "**2006 - Deep Learning Renaissance**\n",
    "- Geoffrey Hinton introduced \"Deep Belief Networks\"\n",
    "- Showed deep networks could be trained effectively\n",
    "- Term \"Deep Learning\" gained popularity\n",
    "\n",
    "**2012 - AlexNet (ImageNet Revolution)**\n",
    "- Alex Krizhevsky's CNN won ImageNet competition by large margin\n",
    "- Used GPUs for training (breakthrough in computational efficiency)\n",
    "- Proved deep learning's superiority in computer vision\n",
    "\n",
    "**2014-2017 - Transformers and Attention**\n",
    "- Attention mechanisms revolutionize NLP\n",
    "- Transformer architecture (\"Attention is All You Need\")\n",
    "- Foundation for BERT, GPT, and modern language models\n",
    "\n",
    "**2020s - Foundation Models Era**\n",
    "- GPT-3, GPT-4: Large language models with emergent abilities\n",
    "- DALL-E, Stable Diffusion: Text-to-image generation\n",
    "- AlphaFold: Protein structure prediction\n",
    "- Multimodal models combining vision, language, and more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dl_history_timeline():\n",
    "    \"\"\"\n",
    "    Visualize the timeline of deep learning milestones.\n",
    "    Shows the progression from early neural networks to modern architectures.\n",
    "    \"\"\"\n",
    "    # Define milestones\n",
    "    milestones = [\n",
    "        (1958, \"Perceptron\", 1),\n",
    "        (1986, \"Backpropagation\", 2),\n",
    "        (1998, \"LeNet (CNN)\", 3),\n",
    "        (2006, \"Deep Learning\\nRevival\", 4),\n",
    "        (2012, \"AlexNet\", 5),\n",
    "        (2017, \"Transformers\", 6),\n",
    "        (2020, \"GPT-3\", 7),\n",
    "        (2023, \"GPT-4\\nMultimodal\", 8)\n",
    "    ]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    # Draw timeline\n",
    "    years = [m[0] for m in milestones]\n",
    "    ax.plot(years, [0] * len(years), 'k-', linewidth=2, zorder=1)\n",
    "    \n",
    "    # Add milestones\n",
    "    colors = plt.cm.viridis(np.linspace(0.2, 0.9, len(milestones)))\n",
    "    \n",
    "    for i, (year, label, importance) in enumerate(milestones):\n",
    "        # Alternate positions for readability\n",
    "        y_pos = 0.3 if i % 2 == 0 else -0.3\n",
    "        \n",
    "        # Draw milestone point\n",
    "        ax.scatter(year, 0, s=200 + importance * 50, c=[colors[i]], \n",
    "                  edgecolors='black', linewidth=2, zorder=3)\n",
    "        \n",
    "        # Draw connecting line and label\n",
    "        ax.plot([year, year], [0, y_pos], 'k--', alpha=0.3, linewidth=1)\n",
    "        ax.text(year, y_pos + (0.1 if y_pos > 0 else -0.1), label, \n",
    "               ha='center', va='bottom' if y_pos > 0 else 'top',\n",
    "               fontsize=10, weight='bold')\n",
    "        \n",
    "        # Add year labels\n",
    "        ax.text(year, -0.05, str(year), ha='center', va='top', fontsize=9)\n",
    "    \n",
    "    ax.set_ylim(-0.6, 0.6)\n",
    "    ax.set_xlim(1950, 2030)\n",
    "    ax.axis('off')\n",
    "    ax.set_title('Deep Learning Historical Milestones', fontsize=16, weight='bold', pad=20)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot the timeline\n",
    "plot_dl_history_timeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Applications of Deep Learning\n",
    "\n",
    "Deep learning has revolutionized numerous fields. Here are the major application domains:\n",
    "\n",
    "### 4.1 Computer Vision\n",
    "- **Image Classification**: Identifying objects in images (cats vs dogs, disease detection)\n",
    "- **Object Detection**: Locating and classifying multiple objects (self-driving cars, surveillance)\n",
    "- **Semantic Segmentation**: Pixel-level classification (medical imaging, satellite imagery)\n",
    "- **Face Recognition**: Identity verification, photo organization\n",
    "- **Image Generation**: Creating realistic images (DALL-E, Stable Diffusion, Midjourney)\n",
    "\n",
    "### 4.2 Natural Language Processing (NLP)\n",
    "- **Machine Translation**: Google Translate, DeepL\n",
    "- **Text Generation**: GPT models, chatbots, content creation\n",
    "- **Sentiment Analysis**: Understanding opinions in reviews, social media\n",
    "- **Question Answering**: Search engines, virtual assistants\n",
    "- **Text Summarization**: Automatic document summarization\n",
    "\n",
    "### 4.3 Speech and Audio\n",
    "- **Speech Recognition**: Siri, Alexa, Google Assistant\n",
    "- **Text-to-Speech**: Natural-sounding voice synthesis\n",
    "- **Music Generation**: AI composers (Amper, AIVA)\n",
    "- **Audio Enhancement**: Noise reduction, voice separation\n",
    "\n",
    "### 4.4 Healthcare and Medicine\n",
    "- **Medical Image Analysis**: Tumor detection, X-ray interpretation\n",
    "- **Drug Discovery**: Predicting molecular properties\n",
    "- **Protein Folding**: AlphaFold solving 50-year-old problem\n",
    "- **Disease Prediction**: Early detection from symptoms and biomarkers\n",
    "\n",
    "### 4.5 Autonomous Systems\n",
    "- **Self-Driving Cars**: Tesla, Waymo perception systems\n",
    "- **Robotics**: Robot manipulation and navigation\n",
    "- **Drones**: Autonomous flight and obstacle avoidance\n",
    "\n",
    "### 4.6 Gaming and Entertainment\n",
    "- **Game AI**: AlphaGo, AlphaStar defeating world champions\n",
    "- **Recommendation Systems**: Netflix, YouTube, Spotify\n",
    "- **Content Moderation**: Detecting inappropriate content\n",
    "\n",
    "### 4.7 Finance and Business\n",
    "- **Fraud Detection**: Identifying suspicious transactions\n",
    "- **Algorithmic Trading**: Predicting market movements\n",
    "- **Credit Scoring**: Assessing loan risk\n",
    "- **Customer Service**: Chatbots and virtual assistants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_applications():\n",
    "    \"\"\"\n",
    "    Create a bar chart showing the impact/adoption of deep learning across domains.\n",
    "    Note: These are illustrative values representing relative maturity and adoption.\n",
    "    \"\"\"\n",
    "    domains = [\n",
    "        'Computer\\nVision',\n",
    "        'Natural\\nLanguage',\n",
    "        'Speech\\n& Audio',\n",
    "        'Healthcare',\n",
    "        'Autonomous\\nSystems',\n",
    "        'Gaming',\n",
    "        'Finance'\n",
    "    ]\n",
    "    \n",
    "    # Adoption/maturity scores (0-100 scale)\n",
    "    adoption = [95, 90, 85, 70, 65, 88, 75]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    colors = plt.cm.viridis(np.linspace(0.2, 0.9, len(domains)))\n",
    "    bars = ax.barh(domains, adoption, color=colors, edgecolor='black', linewidth=1.5)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (bar, value) in enumerate(zip(bars, adoption)):\n",
    "        ax.text(value - 5, i, f'{value}%', va='center', ha='right', \n",
    "               fontsize=11, weight='bold', color='white')\n",
    "    \n",
    "    ax.set_xlabel('Maturity & Adoption Level (%)', fontsize=12, weight='bold')\n",
    "    ax.set_title('Deep Learning Impact Across Different Domains', \n",
    "                fontsize=14, weight='bold', pad=20)\n",
    "    ax.set_xlim(0, 100)\n",
    "    ax.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize applications\n",
    "visualize_applications()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Mathematical Notation and Prerequisites\n",
    "\n",
    "Before diving deeper, let's review the mathematical notation commonly used in deep learning.\n",
    "\n",
    "### 5.1 Vectors and Matrices\n",
    "\n",
    "**Scalar** (single number):\n",
    "$$x \\in \\mathbb{R}$$\n",
    "\n",
    "**Vector** (1D array):\n",
    "$$\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} \\in \\mathbb{R}^n$$\n",
    "\n",
    "**Matrix** (2D array):\n",
    "$$\\mathbf{W} = \\begin{bmatrix} w_{11} & w_{12} & \\cdots & w_{1n} \\\\ w_{21} & w_{22} & \\cdots & w_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ w_{m1} & w_{m2} & \\cdots & w_{mn} \\end{bmatrix} \\in \\mathbb{R}^{m \\times n}$$\n",
    "\n",
    "**Tensor** (multi-dimensional array):\n",
    "$$\\mathcal{T} \\in \\mathbb{R}^{d_1 \\times d_2 \\times \\cdots \\times d_n}$$\n",
    "\n",
    "### 5.2 Common Operations\n",
    "\n",
    "**Dot Product** (inner product of vectors):\n",
    "$$\\mathbf{x} \\cdot \\mathbf{w} = \\sum_{i=1}^{n} x_i w_i$$\n",
    "\n",
    "**Matrix Multiplication**:\n",
    "$$\\mathbf{Y} = \\mathbf{X} \\mathbf{W}$$\n",
    "where if $\\mathbf{X} \\in \\mathbb{R}^{m \\times n}$ and $\\mathbf{W} \\in \\mathbb{R}^{n \\times p}$, then $\\mathbf{Y} \\in \\mathbb{R}^{m \\times p}$\n",
    "\n",
    "**Element-wise Operations** (Hadamard product):\n",
    "$$\\mathbf{C} = \\mathbf{A} \\odot \\mathbf{B}$$\n",
    "where $c_{ij} = a_{ij} \\times b_{ij}$\n",
    "\n",
    "### 5.3 Derivatives and Gradients\n",
    "\n",
    "**Derivative** (rate of change):\n",
    "$$\\frac{df}{dx} = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}$$\n",
    "\n",
    "**Partial Derivative** (derivative with respect to one variable):\n",
    "$$\\frac{\\partial f}{\\partial x_i}$$\n",
    "\n",
    "**Gradient** (vector of all partial derivatives):\n",
    "$$\\nabla f(\\mathbf{x}) = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_n} \\end{bmatrix}$$\n",
    "\n",
    "**Chain Rule** (fundamental for backpropagation):\n",
    "$$\\frac{dz}{dx} = \\frac{dz}{dy} \\cdot \\frac{dy}{dx}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate basic mathematical operations used in neural networks\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MATHEMATICAL OPERATIONS IN NEURAL NETWORKS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Vector operations\n",
    "print(\"\\n1. VECTOR OPERATIONS\")\n",
    "print(\"-\" * 60)\n",
    "x = np.array([1.0, 2.0, 3.0])  # Input vector\n",
    "w = np.array([0.5, -1.0, 0.8])  # Weight vector\n",
    "b = 0.3  # Bias\n",
    "\n",
    "print(f\"Input vector x: {x}\")\n",
    "print(f\"Weight vector w: {w}\")\n",
    "print(f\"Bias b: {b}\")\n",
    "\n",
    "# Dot product (weighted sum)\n",
    "dot_product = np.dot(x, w)\n",
    "print(f\"\\nDot product (x · w): {dot_product:.4f}\")\n",
    "print(f\"With bias (x · w + b): {dot_product + b:.4f}\")\n",
    "\n",
    "# 2. Matrix multiplication (multiple neurons)\n",
    "print(\"\\n2. MATRIX MULTIPLICATION (Layer with 3 inputs, 2 neurons)\")\n",
    "print(\"-\" * 60)\n",
    "X = np.array([[1.0, 2.0, 3.0],      # Sample 1\n",
    "              [4.0, 5.0, 6.0]])     # Sample 2\n",
    "W = np.array([[0.5, -1.0],          # Weights for neuron 1 and 2\n",
    "              [-0.2, 0.8],          # from input 1\n",
    "              [0.3, -0.5]])         # from input 2, 3\n",
    "B = np.array([0.1, -0.2])           # Biases for 2 neurons\n",
    "\n",
    "print(f\"Input matrix X (2 samples, 3 features):\\n{X}\")\n",
    "print(f\"\\nWeight matrix W (3 inputs, 2 neurons):\\n{W}\")\n",
    "print(f\"\\nBias vector B: {B}\")\n",
    "\n",
    "# Compute layer output\n",
    "output = np.dot(X, W) + B\n",
    "print(f\"\\nLayer output (X @ W + B):\\n{output}\")\n",
    "print(f\"Shape: {output.shape} (2 samples, 2 neurons)\")\n",
    "\n",
    "# 3. Element-wise operations\n",
    "print(\"\\n3. ELEMENT-WISE OPERATIONS\")\n",
    "print(\"-\" * 60)\n",
    "a = np.array([1, 2, 3, 4])\n",
    "b = np.array([2, 2, 2, 2])\n",
    "\n",
    "print(f\"Array a: {a}\")\n",
    "print(f\"Array b: {b}\")\n",
    "print(f\"Element-wise multiplication (a * b): {a * b}\")\n",
    "print(f\"Element-wise power (a ** 2): {a ** 2}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Simple Neural Network Architecture\n",
    "\n",
    "A typical neural network consists of multiple layers:\n",
    "\n",
    "1. **Input Layer**: Receives the raw data (features)\n",
    "2. **Hidden Layers**: Process the information (can have multiple layers)\n",
    "3. **Output Layer**: Produces the final prediction\n",
    "\n",
    "### Network Terminology\n",
    "\n",
    "- **Depth**: Number of layers (deep = many layers)\n",
    "- **Width**: Number of neurons per layer\n",
    "- **Parameters**: Weights and biases (learnable)\n",
    "- **Hyperparameters**: Learning rate, number of layers, neurons per layer (set by user)\n",
    "- **Forward Propagation**: Data flows from input to output\n",
    "- **Backward Propagation**: Gradients flow from output to input (for learning)\n",
    "\n",
    "### Example Architecture\n",
    "\n",
    "A simple 3-layer network:\n",
    "- Input: 4 features\n",
    "- Hidden: 8 neurons\n",
    "- Output: 3 classes\n",
    "\n",
    "Total parameters:\n",
    "- Layer 1: $(4 \\times 8) + 8 = 40$ (weights + biases)\n",
    "- Layer 2: $(8 \\times 3) + 3 = 27$ (weights + biases)\n",
    "- **Total: 67 parameters to learn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_network_architecture(layer_sizes=[4, 8, 6, 3]):\n",
    "    \"\"\"\n",
    "    Visualize a multi-layer neural network architecture.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    layer_sizes : list\n",
    "        Number of neurons in each layer (input, hidden1, hidden2, ..., output)\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    \n",
    "    num_layers = len(layer_sizes)\n",
    "    layer_spacing = 0.8 / (num_layers - 1) if num_layers > 1 else 0.5\n",
    "    \n",
    "    # Calculate total parameters\n",
    "    total_params = 0\n",
    "    for i in range(len(layer_sizes) - 1):\n",
    "        total_params += (layer_sizes[i] * layer_sizes[i+1]) + layer_sizes[i+1]\n",
    "    \n",
    "    # Draw layers\n",
    "    for layer_idx, size in enumerate(layer_sizes):\n",
    "        x = 0.1 + layer_idx * layer_spacing\n",
    "        neuron_spacing = 0.8 / (size + 1)\n",
    "        \n",
    "        # Determine layer color\n",
    "        if layer_idx == 0:\n",
    "            color = 'lightblue'\n",
    "            label = 'Input'\n",
    "        elif layer_idx == num_layers - 1:\n",
    "            color = 'lightgreen'\n",
    "            label = 'Output'\n",
    "        else:\n",
    "            color = 'coral'\n",
    "            label = f'Hidden {layer_idx}'\n",
    "        \n",
    "        # Draw neurons in this layer\n",
    "        for neuron_idx in range(size):\n",
    "            y = 0.1 + (neuron_idx + 1) * neuron_spacing\n",
    "            \n",
    "            # Draw neuron\n",
    "            circle = plt.Circle((x, y), 0.025, color=color, ec='black', linewidth=1.5, zorder=3)\n",
    "            ax.add_patch(circle)\n",
    "            \n",
    "            # Draw connections to next layer\n",
    "            if layer_idx < num_layers - 1:\n",
    "                next_x = x + layer_spacing\n",
    "                next_size = layer_sizes[layer_idx + 1]\n",
    "                next_neuron_spacing = 0.8 / (next_size + 1)\n",
    "                \n",
    "                for next_neuron_idx in range(next_size):\n",
    "                    next_y = 0.1 + (next_neuron_idx + 1) * next_neuron_spacing\n",
    "                    # Only draw a subset of connections for clarity\n",
    "                    if neuron_idx == 0 or neuron_idx == size - 1 or next_neuron_idx % 2 == 0:\n",
    "                        ax.plot([x + 0.025, next_x - 0.025], [y, next_y], \n",
    "                               'gray', alpha=0.2, linewidth=0.5, zorder=1)\n",
    "        \n",
    "        # Add layer label\n",
    "        ax.text(x, 0.05, f\"{label}\\n{size} neurons\", ha='center', va='top', \n",
    "               fontsize=10, weight='bold')\n",
    "    \n",
    "    # Add parameter count\n",
    "    param_text = f\"Total Trainable Parameters: {total_params:,}\"\n",
    "    ax.text(0.5, 0.95, param_text, ha='center', va='bottom', \n",
    "           fontsize=12, weight='bold', \n",
    "           bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f'Neural Network Architecture: {layer_sizes}', \n",
    "                fontsize=14, weight='bold', pad=20)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print parameter breakdown\n",
    "    print(\"\\nParameter Breakdown:\")\n",
    "    print(\"=\" * 50)\n",
    "    total = 0\n",
    "    for i in range(len(layer_sizes) - 1):\n",
    "        weights = layer_sizes[i] * layer_sizes[i+1]\n",
    "        biases = layer_sizes[i+1]\n",
    "        layer_total = weights + biases\n",
    "        total += layer_total\n",
    "        print(f\"Layer {i+1} ({layer_sizes[i]} → {layer_sizes[i+1]}):\")\n",
    "        print(f\"  Weights: {weights:,} | Biases: {biases} | Total: {layer_total:,}\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Total Parameters: {total:,}\")\n",
    "\n",
    "# Visualize a sample architecture\n",
    "visualize_network_architecture([4, 8, 6, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Exercises\n",
    "\n",
    "Now it's your turn to practice! Complete the following exercises to reinforce your understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Calculate Neuron Output\n",
    "\n",
    "Given a neuron with:\n",
    "- Inputs: $x = [2.0, 3.0, -1.0]$\n",
    "- Weights: $w = [0.5, -0.3, 0.8]$\n",
    "- Bias: $b = 0.2$\n",
    "- Activation function: $f(z) = \\max(0, z)$ (ReLU)\n",
    "\n",
    "Calculate the output of this neuron manually, then verify with NumPy.\n",
    "\n",
    "**Steps:**\n",
    "1. Calculate weighted sum: $z = \\sum w_i x_i + b$\n",
    "2. Apply activation: $y = f(z)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your solution here\n",
    "\n",
    "# Given values\n",
    "x = np.array([2.0, 3.0, -1.0])\n",
    "w = np.array([0.5, -0.3, 0.8])\n",
    "b = 0.2\n",
    "\n",
    "# TODO: Calculate weighted sum (z)\n",
    "z = None  # Replace with your calculation\n",
    "\n",
    "# TODO: Apply ReLU activation\n",
    "# ReLU(z) = max(0, z)\n",
    "y = None  # Replace with your calculation\n",
    "\n",
    "# Print results\n",
    "print(f\"Weighted sum (z): {z}\")\n",
    "print(f\"Output after ReLU (y): {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to Exercise 1\n",
    "\n",
    "x = np.array([2.0, 3.0, -1.0])\n",
    "w = np.array([0.5, -0.3, 0.8])\n",
    "b = 0.2\n",
    "\n",
    "# Calculate weighted sum\n",
    "z = np.dot(x, w) + b\n",
    "print(f\"Step 1 - Weighted sum calculation:\")\n",
    "print(f\"  z = (2.0 × 0.5) + (3.0 × -0.3) + (-1.0 × 0.8) + 0.2\")\n",
    "print(f\"  z = 1.0 + (-0.9) + (-0.8) + 0.2\")\n",
    "print(f\"  z = {z:.4f}\")\n",
    "\n",
    "# Apply ReLU activation\n",
    "y = np.maximum(0, z)\n",
    "print(f\"\\nStep 2 - Apply ReLU activation:\")\n",
    "print(f\"  y = max(0, {z:.4f})\")\n",
    "print(f\"  y = {y:.4f}\")\n",
    "\n",
    "if z < 0:\n",
    "    print(f\"\\nSince z is negative, ReLU outputs 0 (neuron is not activated)\")\n",
    "else:\n",
    "    print(f\"\\nSince z is positive, ReLU outputs z (neuron is activated)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Count Network Parameters\n",
    "\n",
    "For a neural network with the following architecture:\n",
    "- Input layer: 10 features\n",
    "- Hidden layer 1: 20 neurons\n",
    "- Hidden layer 2: 15 neurons\n",
    "- Output layer: 5 classes\n",
    "\n",
    "Calculate:\n",
    "1. Total number of weights\n",
    "2. Total number of biases\n",
    "3. Total number of trainable parameters\n",
    "\n",
    "**Hint**: For each layer connection, parameters = (input_size × output_size) + output_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your solution here\n",
    "\n",
    "# Network architecture\n",
    "layer_sizes = [10, 20, 15, 5]\n",
    "\n",
    "# TODO: Calculate parameters for each layer\n",
    "# Layer 1: Input (10) -> Hidden 1 (20)\n",
    "layer1_weights = None  # Replace with calculation\n",
    "layer1_biases = None   # Replace with calculation\n",
    "layer1_total = None    # Replace with calculation\n",
    "\n",
    "# Layer 2: Hidden 1 (20) -> Hidden 2 (15)\n",
    "layer2_weights = None\n",
    "layer2_biases = None\n",
    "layer2_total = None\n",
    "\n",
    "# Layer 3: Hidden 2 (15) -> Output (5)\n",
    "layer3_weights = None\n",
    "layer3_biases = None\n",
    "layer3_total = None\n",
    "\n",
    "# TODO: Calculate totals\n",
    "total_weights = None\n",
    "total_biases = None\n",
    "total_parameters = None\n",
    "\n",
    "print(\"Parameter Counts:\")\n",
    "print(f\"Total weights: {total_weights}\")\n",
    "print(f\"Total biases: {total_biases}\")\n",
    "print(f\"Total parameters: {total_parameters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to Exercise 2\n",
    "\n",
    "layer_sizes = [10, 20, 15, 5]\n",
    "\n",
    "print(\"Calculating Parameters for Each Layer:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Layer 1: Input (10) -> Hidden 1 (20)\n",
    "layer1_weights = layer_sizes[0] * layer_sizes[1]\n",
    "layer1_biases = layer_sizes[1]\n",
    "layer1_total = layer1_weights + layer1_biases\n",
    "print(f\"Layer 1 (10 → 20):\")\n",
    "print(f\"  Weights: 10 × 20 = {layer1_weights}\")\n",
    "print(f\"  Biases: {layer1_biases}\")\n",
    "print(f\"  Total: {layer1_total}\")\n",
    "\n",
    "# Layer 2: Hidden 1 (20) -> Hidden 2 (15)\n",
    "layer2_weights = layer_sizes[1] * layer_sizes[2]\n",
    "layer2_biases = layer_sizes[2]\n",
    "layer2_total = layer2_weights + layer2_biases\n",
    "print(f\"\\nLayer 2 (20 → 15):\")\n",
    "print(f\"  Weights: 20 × 15 = {layer2_weights}\")\n",
    "print(f\"  Biases: {layer2_biases}\")\n",
    "print(f\"  Total: {layer2_total}\")\n",
    "\n",
    "# Layer 3: Hidden 2 (15) -> Output (5)\n",
    "layer3_weights = layer_sizes[2] * layer_sizes[3]\n",
    "layer3_biases = layer_sizes[3]\n",
    "layer3_total = layer3_weights + layer3_biases\n",
    "print(f\"\\nLayer 3 (15 → 5):\")\n",
    "print(f\"  Weights: 15 × 5 = {layer3_weights}\")\n",
    "print(f\"  Biases: {layer3_biases}\")\n",
    "print(f\"  Total: {layer3_total}\")\n",
    "\n",
    "# Calculate totals\n",
    "total_weights = layer1_weights + layer2_weights + layer3_weights\n",
    "total_biases = layer1_biases + layer2_biases + layer3_biases\n",
    "total_parameters = total_weights + total_biases\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Total Weights: {total_weights:,}\")\n",
    "print(f\"Total Biases: {total_biases}\")\n",
    "print(f\"Total Trainable Parameters: {total_parameters:,}\")\n",
    "\n",
    "# Verify using the visualization function\n",
    "print(\"\\nVerification using our visualization function:\")\n",
    "visualize_network_architecture(layer_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Deep Learning Application Research\n",
    "\n",
    "Research and describe ONE real-world deep learning application that interests you. Include:\n",
    "\n",
    "1. **Application name and domain**\n",
    "2. **Problem it solves**\n",
    "3. **Type of neural network used** (if known)\n",
    "4. **Impact or results achieved**\n",
    "5. **Why this application interests you**\n",
    "\n",
    "Write your answer in the markdown cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answer to Exercise 3:**\n",
    "\n",
    "*(Double-click to edit this cell and write your response)*\n",
    "\n",
    "1. Application name and domain:\n",
    "\n",
    "2. Problem it solves:\n",
    "\n",
    "3. Type of neural network used:\n",
    "\n",
    "4. Impact or results:\n",
    "\n",
    "5. Why it interests me:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "Congratulations! You've completed the introduction to neural networks and deep learning. Let's recap what we covered:\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Biological Inspiration**\n",
    "   - Neural networks mimic brain neurons\n",
    "   - Artificial neurons use weights, biases, and activation functions\n",
    "   - Mathematical model: $y = f(\\sum w_i x_i + b)$\n",
    "\n",
    "2. **Historical Evolution**\n",
    "   - 1958: Perceptron (single-layer learning)\n",
    "   - 1986: Backpropagation (multi-layer training)\n",
    "   - 2012: AlexNet (deep learning revolution)\n",
    "   - 2020s: Foundation models (GPT, DALL-E, etc.)\n",
    "\n",
    "3. **Applications**\n",
    "   - Computer Vision: Image recognition, object detection, generation\n",
    "   - NLP: Translation, chatbots, text generation\n",
    "   - Healthcare: Disease detection, drug discovery\n",
    "   - Many other domains transforming industries\n",
    "\n",
    "4. **Mathematical Foundations**\n",
    "   - Vectors and matrices represent data and parameters\n",
    "   - Dot products compute weighted sums\n",
    "   - Gradients enable learning through optimization\n",
    "   - Chain rule powers backpropagation\n",
    "\n",
    "5. **Network Architecture**\n",
    "   - Input layer → Hidden layers → Output layer\n",
    "   - Parameters = weights + biases (learnable)\n",
    "   - Depth (layers) and width (neurons) define capacity\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In the upcoming notebooks, we'll dive deeper into:\n",
    "\n",
    "- **Module 01**: Perceptrons and activation functions in detail\n",
    "- **Module 02**: Backpropagation and gradient descent algorithms\n",
    "- **Module 03**: Building neural networks from scratch with NumPy\n",
    "- **Module 04**: Introduction to TensorFlow and Keras\n",
    "- **Module 05+**: Advanced architectures (CNNs, RNNs, Transformers)\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "**Books:**\n",
    "- \"Deep Learning\" by Goodfellow, Bengio, and Courville (free online)\n",
    "- \"Neural Networks and Deep Learning\" by Michael Nielsen (free online)\n",
    "- \"Hands-On Machine Learning\" by Aurélien Géron\n",
    "\n",
    "**Online Courses:**\n",
    "- Andrew Ng's Deep Learning Specialization (Coursera)\n",
    "- Fast.ai Practical Deep Learning\n",
    "- MIT 6.S191: Introduction to Deep Learning\n",
    "\n",
    "**Interactive Resources:**\n",
    "- TensorFlow Playground (visual neural network experimentation)\n",
    "- Distill.pub (beautiful visual explanations)\n",
    "- Papers with Code (latest research with implementations)\n",
    "\n",
    "---\n",
    "\n",
    "**Ready to continue?** Proceed to **Module 01: Perceptrons and Activation Functions** to start building your first neural network components!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
