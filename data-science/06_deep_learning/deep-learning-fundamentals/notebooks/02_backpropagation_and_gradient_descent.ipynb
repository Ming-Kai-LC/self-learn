{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 02: Backpropagation and Gradient Descent\n",
    "\n",
    "**Difficulty**: ⭐⭐⭐ (Advanced)\n",
    "**Estimated Time**: 60-75 minutes\n",
    "**Prerequisites**: \n",
    "- Module 00: Introduction to Neural Networks\n",
    "- Module 01: Perceptrons and Activation Functions\n",
    "- Solid understanding of calculus (derivatives, chain rule)\n",
    "- Matrix multiplication and linear algebra\n",
    "- Python and NumPy\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. **Execute** forward propagation step-by-step through a neural network\n",
    "2. **Calculate** loss using different cost functions (MSE, Binary Cross-Entropy)\n",
    "3. **Explain** the gradient descent optimization algorithm and its variants\n",
    "4. **Apply** the chain rule to compute gradients in backpropagation\n",
    "5. **Implement** backpropagation manually for a simple neural network\n",
    "6. **Visualize** optimization landscapes and learning rate effects\n",
    "7. **Compute** gradients by hand for multi-layer networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Forward Propagation\n",
    "\n",
    "### What is Forward Propagation?\n",
    "\n",
    "Forward propagation is the process of passing input data through the network to generate predictions. Data flows from input → hidden layers → output.\n",
    "\n",
    "### Single Neuron Forward Pass\n",
    "\n",
    "For a single neuron:\n",
    "\n",
    "1. **Weighted sum (pre-activation)**:\n",
    "   $$z = \\mathbf{w}^T \\mathbf{x} + b = \\sum_{i=1}^{n} w_i x_i + b$$\n",
    "\n",
    "2. **Activation**:\n",
    "   $$a = f(z)$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{x}$ = input vector\n",
    "- $\\mathbf{w}$ = weight vector\n",
    "- $b$ = bias\n",
    "- $z$ = pre-activation (linear combination)\n",
    "- $f$ = activation function\n",
    "- $a$ = activation (output)\n",
    "\n",
    "### Multi-Layer Network\n",
    "\n",
    "For a network with layers $L$:\n",
    "\n",
    "$$z^{[l]} = W^{[l]} a^{[l-1]} + b^{[l]}$$\n",
    "$$a^{[l]} = f^{[l]}(z^{[l]})$$\n",
    "\n",
    "Where:\n",
    "- $l$ = layer index (1 to $L$)\n",
    "- $a^{[0]} = \\mathbf{x}$ (input)\n",
    "- $a^{[L]} = \\hat{y}$ (output/prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Sigmoid activation function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))  # Clip to prevent overflow\n",
    "\n",
    "def relu(z):\n",
    "    \"\"\"ReLU activation function.\"\"\"\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def forward_propagation_example():\n",
    "    \"\"\"\n",
    "    Demonstrate forward propagation through a 2-layer network.\n",
    "    \n",
    "    Architecture:\n",
    "    - Input: 3 features\n",
    "    - Hidden: 4 neurons (ReLU)\n",
    "    - Output: 1 neuron (Sigmoid)\n",
    "    \"\"\"\n",
    "    print(\"FORWARD PROPAGATION EXAMPLE\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Input\n",
    "    x = np.array([[2.0, 3.0, -1.0]]).T  # Shape: (3, 1)\n",
    "    print(f\"\\nInput x (3 features):\\n{x.T}\")\n",
    "    \n",
    "    # Layer 1: Input (3) -> Hidden (4)\n",
    "    W1 = np.random.randn(4, 3) * 0.5  # Shape: (4, 3)\n",
    "    b1 = np.zeros((4, 1))              # Shape: (4, 1)\n",
    "    \n",
    "    print(f\"\\nLayer 1 weights W1 shape: {W1.shape}\")\n",
    "    print(f\"Layer 1 biases b1 shape: {b1.shape}\")\n",
    "    \n",
    "    # Forward through layer 1\n",
    "    z1 = np.dot(W1, x) + b1\n",
    "    a1 = relu(z1)\n",
    "    \n",
    "    print(f\"\\nLayer 1 pre-activation z1:\\n{z1.T}\")\n",
    "    print(f\"Layer 1 activation a1 (after ReLU):\\n{a1.T}\")\n",
    "    \n",
    "    # Layer 2: Hidden (4) -> Output (1)\n",
    "    W2 = np.random.randn(1, 4) * 0.5   # Shape: (1, 4)\n",
    "    b2 = np.zeros((1, 1))               # Shape: (1, 1)\n",
    "    \n",
    "    print(f\"\\nLayer 2 weights W2 shape: {W2.shape}\")\n",
    "    print(f\"Layer 2 biases b2 shape: {b2.shape}\")\n",
    "    \n",
    "    # Forward through layer 2\n",
    "    z2 = np.dot(W2, a1) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "    \n",
    "    print(f\"\\nLayer 2 pre-activation z2: {z2[0, 0]:.4f}\")\n",
    "    print(f\"Layer 2 activation a2 (after Sigmoid): {a2[0, 0]:.4f}\")\n",
    "    \n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(f\"FINAL PREDICTION: {a2[0, 0]:.4f}\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "    \n",
    "    return {'x': x, 'W1': W1, 'b1': b1, 'z1': z1, 'a1': a1,\n",
    "            'W2': W2, 'b2': b2, 'z2': z2, 'a2': a2}\n",
    "\n",
    "# Run example\n",
    "forward_cache = forward_propagation_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_forward_propagation():\n",
    "    \"\"\"\n",
    "    Visualize the flow of information in forward propagation.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    # Define layer positions\n",
    "    layer_x = [0.15, 0.5, 0.85]\n",
    "    layer_sizes = [3, 4, 1]\n",
    "    layer_names = ['Input\\n(3 features)', 'Hidden\\n(4 neurons, ReLU)', \n",
    "                   'Output\\n(1 neuron, Sigmoid)']\n",
    "    \n",
    "    # Draw neurons\n",
    "    for layer_idx, (x_pos, size, name) in enumerate(zip(layer_x, layer_sizes, layer_names)):\n",
    "        y_positions = np.linspace(0.2, 0.8, size)\n",
    "        \n",
    "        # Choose color\n",
    "        if layer_idx == 0:\n",
    "            color = 'lightblue'\n",
    "        elif layer_idx == len(layer_x) - 1:\n",
    "            color = 'lightgreen'\n",
    "        else:\n",
    "            color = 'coral'\n",
    "        \n",
    "        for y_pos in y_positions:\n",
    "            circle = plt.Circle((x_pos, y_pos), 0.04, color=color, \n",
    "                              ec='black', linewidth=2, zorder=3)\n",
    "            ax.add_patch(circle)\n",
    "            \n",
    "            # Draw connections to next layer\n",
    "            if layer_idx < len(layer_x) - 1:\n",
    "                next_x = layer_x[layer_idx + 1]\n",
    "                next_y_positions = np.linspace(0.2, 0.8, layer_sizes[layer_idx + 1])\n",
    "                \n",
    "                for next_y in next_y_positions:\n",
    "                    ax.plot([x_pos + 0.04, next_x - 0.04], [y_pos, next_y],\n",
    "                           'gray', alpha=0.3, linewidth=1, zorder=1)\n",
    "        \n",
    "        # Add layer label\n",
    "        ax.text(x_pos, 0.05, name, ha='center', va='top', \n",
    "               fontsize=11, weight='bold')\n",
    "    \n",
    "    # Add annotations\n",
    "    ax.annotate('', xy=(0.32, 0.5), xytext=(0.24, 0.5),\n",
    "               arrowprops=dict(arrowstyle='->', lw=2, color='blue'))\n",
    "    ax.text(0.28, 0.55, r'$z^{[1]} = W^{[1]}x + b^{[1]}$', \n",
    "           ha='center', fontsize=10, style='italic')\n",
    "    ax.text(0.28, 0.45, r'$a^{[1]} = ReLU(z^{[1]})$', \n",
    "           ha='center', fontsize=10, style='italic')\n",
    "    \n",
    "    ax.annotate('', xy=(0.67, 0.5), xytext=(0.59, 0.5),\n",
    "               arrowprops=dict(arrowstyle='->', lw=2, color='blue'))\n",
    "    ax.text(0.63, 0.55, r'$z^{[2]} = W^{[2]}a^{[1]} + b^{[2]}$', \n",
    "           ha='center', fontsize=10, style='italic')\n",
    "    ax.text(0.63, 0.45, r'$a^{[2]} = \\sigma(z^{[2]}) = \\hat{y}$', \n",
    "           ha='center', fontsize=10, style='italic')\n",
    "    \n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.axis('off')\n",
    "    ax.set_title('Forward Propagation Flow', fontsize=15, weight='bold', pad=20)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_forward_propagation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loss Functions\n",
    "\n",
    "The **loss function** (or cost function) measures how well the network's predictions match the true labels. Our goal is to **minimize** this loss.\n",
    "\n",
    "### 3.1 Mean Squared Error (MSE)\n",
    "\n",
    "Used for **regression** problems:\n",
    "\n",
    "$$L(y, \\hat{y}) = \\frac{1}{2}(y - \\hat{y})^2$$\n",
    "\n",
    "For $m$ samples:\n",
    "\n",
    "$$J = \\frac{1}{m} \\sum_{i=1}^{m} \\frac{1}{2}(y^{(i)} - \\hat{y}^{(i)})^2$$\n",
    "\n",
    "**Derivative** (needed for backpropagation):\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\hat{y}} = -(y - \\hat{y}) = \\hat{y} - y$$\n",
    "\n",
    "### 3.2 Binary Cross-Entropy Loss\n",
    "\n",
    "Used for **binary classification**:\n",
    "\n",
    "$$L(y, \\hat{y}) = -[y \\log(\\hat{y}) + (1-y) \\log(1-\\hat{y})]$$\n",
    "\n",
    "For $m$ samples:\n",
    "\n",
    "$$J = -\\frac{1}{m} \\sum_{i=1}^{m} [y^{(i)} \\log(\\hat{y}^{(i)}) + (1-y^{(i)}) \\log(1-\\hat{y}^{(i)})]$$\n",
    "\n",
    "**Derivative** (with sigmoid activation):\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial z} = \\hat{y} - y$$\n",
    "\n",
    "(This simplification is why sigmoid + binary cross-entropy work so well together!)\n",
    "\n",
    "### 3.3 Categorical Cross-Entropy Loss\n",
    "\n",
    "Used for **multi-class classification**:\n",
    "\n",
    "$$L(\\mathbf{y}, \\hat{\\mathbf{y}}) = -\\sum_{c=1}^{C} y_c \\log(\\hat{y}_c)$$\n",
    "\n",
    "Where $C$ is the number of classes, and $\\mathbf{y}$ is one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFunctions:\n",
    "    \"\"\"\n",
    "    Common loss functions and their derivatives.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def mse(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Mean Squared Error loss.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        y_true : array-like\n",
    "            True labels\n",
    "        y_pred : array-like\n",
    "            Predicted values\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        loss : float\n",
    "            Average MSE across samples\n",
    "        \"\"\"\n",
    "        return np.mean(0.5 * (y_true - y_pred) ** 2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def mse_derivative(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Derivative of MSE with respect to predictions.\n",
    "        \"\"\"\n",
    "        return y_pred - y_true\n",
    "    \n",
    "    @staticmethod\n",
    "    def binary_cross_entropy(y_true, y_pred, epsilon=1e-15):\n",
    "        \"\"\"\n",
    "        Binary Cross-Entropy loss.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        y_true : array-like\n",
    "            True binary labels (0 or 1)\n",
    "        y_pred : array-like\n",
    "            Predicted probabilities (0 to 1)\n",
    "        epsilon : float\n",
    "            Small value to prevent log(0)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        loss : float\n",
    "            Average BCE across samples\n",
    "        \"\"\"\n",
    "        # Clip predictions to prevent log(0)\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        return -np.mean(y_true * np.log(y_pred) + \n",
    "                       (1 - y_true) * np.log(1 - y_pred))\n",
    "    \n",
    "    @staticmethod\n",
    "    def binary_cross_entropy_derivative(y_true, y_pred, epsilon=1e-15):\n",
    "        \"\"\"\n",
    "        Derivative of BCE with respect to pre-activation (z) when using sigmoid.\n",
    "        This simplifies to: y_pred - y_true\n",
    "        \"\"\"\n",
    "        return y_pred - y_true\n",
    "\n",
    "# Demonstrate loss functions\n",
    "loss = LossFunctions()\n",
    "\n",
    "# Example predictions and true values\n",
    "y_true = np.array([0, 1, 1, 0, 1])\n",
    "y_pred = np.array([0.1, 0.9, 0.8, 0.2, 0.7])\n",
    "\n",
    "print(\"LOSS FUNCTION EXAMPLES\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nTrue labels: {y_true}\")\n",
    "print(f\"Predictions: {y_pred}\")\n",
    "print(f\"\\nBinary Cross-Entropy Loss: {loss.binary_cross_entropy(y_true, y_pred):.4f}\")\n",
    "print(f\"Mean Squared Error Loss: {loss.mse(y_true, y_pred):.4f}\")\n",
    "\n",
    "# Show why BCE is better for classification\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Why Binary Cross-Entropy for Classification?\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Case 1: Confident and correct\n",
    "y_true_1 = np.array([1])\n",
    "y_pred_1 = np.array([0.99])\n",
    "print(f\"\\nCase 1: True=1, Pred=0.99 (confident, correct)\")\n",
    "print(f\"  BCE: {loss.binary_cross_entropy(y_true_1, y_pred_1):.4f} (low penalty)\")\n",
    "print(f\"  MSE: {loss.mse(y_true_1, y_pred_1):.4f}\")\n",
    "\n",
    "# Case 2: Confident but wrong\n",
    "y_true_2 = np.array([1])\n",
    "y_pred_2 = np.array([0.01])\n",
    "print(f\"\\nCase 2: True=1, Pred=0.01 (confident, wrong)\")\n",
    "print(f\"  BCE: {loss.binary_cross_entropy(y_true_2, y_pred_2):.4f} (high penalty!)\")\n",
    "print(f\"  MSE: {loss.mse(y_true_2, y_pred_2):.4f}\")\n",
    "\n",
    "print(\"\\nBCE penalizes confident wrong predictions more heavily!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize loss functions\n",
    "\n",
    "def plot_loss_functions():\n",
    "    \"\"\"\n",
    "    Visualize how different loss functions behave.\n",
    "    \"\"\"\n",
    "    y_true = 1  # True label = 1\n",
    "    y_pred_range = np.linspace(0.01, 0.99, 1000)\n",
    "    \n",
    "    # Calculate losses for different predictions\n",
    "    bce_losses = [loss.binary_cross_entropy(np.array([y_true]), \n",
    "                                            np.array([pred])) \n",
    "                 for pred in y_pred_range]\n",
    "    mse_losses = [loss.mse(np.array([y_true]), np.array([pred])) \n",
    "                 for pred in y_pred_range]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Plot 1: Loss comparison\n",
    "    ax1.plot(y_pred_range, bce_losses, label='Binary Cross-Entropy', \n",
    "            linewidth=2.5, color='blue')\n",
    "    ax1.plot(y_pred_range, mse_losses, label='Mean Squared Error', \n",
    "            linewidth=2.5, color='red', linestyle='--')\n",
    "    ax1.axvline(x=1.0, color='green', linestyle=':', linewidth=2, \n",
    "               alpha=0.7, label='True Value')\n",
    "    ax1.set_xlabel('Predicted Value', fontsize=12, weight='bold')\n",
    "    ax1.set_ylabel('Loss', fontsize=12, weight='bold')\n",
    "    ax1.set_title('Loss Functions (True Label = 1)', fontsize=13, weight='bold')\n",
    "    ax1.legend(fontsize=11)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Loss derivatives (gradients)\n",
    "    bce_grads = [loss.binary_cross_entropy_derivative(np.array([y_true]), \n",
    "                                                      np.array([pred]))[0] \n",
    "                for pred in y_pred_range]\n",
    "    mse_grads = [loss.mse_derivative(np.array([y_true]), np.array([pred]))[0] \n",
    "                for pred in y_pred_range]\n",
    "    \n",
    "    ax2.plot(y_pred_range, bce_grads, label='BCE Gradient', \n",
    "            linewidth=2.5, color='blue')\n",
    "    ax2.plot(y_pred_range, mse_grads, label='MSE Gradient', \n",
    "            linewidth=2.5, color='red', linestyle='--')\n",
    "    ax2.axhline(y=0, color='black', linewidth=1, alpha=0.3)\n",
    "    ax2.axvline(x=1.0, color='green', linestyle=':', linewidth=2, \n",
    "               alpha=0.7, label='True Value')\n",
    "    ax2.set_xlabel('Predicted Value', fontsize=12, weight='bold')\n",
    "    ax2.set_ylabel('Gradient', fontsize=12, weight='bold')\n",
    "    ax2.set_title('Loss Gradients', fontsize=13, weight='bold')\n",
    "    ax2.legend(fontsize=11)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_loss_functions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Gradient Descent\n",
    "\n",
    "### What is Gradient Descent?\n",
    "\n",
    "Gradient descent is an **optimization algorithm** that iteratively adjusts parameters to minimize the loss function.\n",
    "\n",
    "### The Intuition\n",
    "\n",
    "Imagine you're on a mountain (loss landscape) and want to reach the lowest valley (minimum loss). You:\n",
    "1. Look around to find the steepest downward direction (gradient)\n",
    "2. Take a step in that direction (parameter update)\n",
    "3. Repeat until you reach the bottom (convergence)\n",
    "\n",
    "### Mathematical Update Rule\n",
    "\n",
    "For each parameter $\\theta$ (weight or bias):\n",
    "\n",
    "$$\\theta_{\\text{new}} = \\theta_{\\text{old}} - \\alpha \\frac{\\partial J}{\\partial \\theta}$$\n",
    "\n",
    "Where:\n",
    "- $\\alpha$ = learning rate (step size)\n",
    "- $\\frac{\\partial J}{\\partial \\theta}$ = gradient (direction of steepest ascent)\n",
    "- We subtract because we want to go downhill (minimize)\n",
    "\n",
    "### Learning Rate Effects\n",
    "\n",
    "- **Too small**: Slow convergence, may get stuck\n",
    "- **Too large**: Overshooting, divergence, oscillations\n",
    "- **Just right**: Fast, stable convergence\n",
    "\n",
    "### Variants\n",
    "\n",
    "1. **Batch Gradient Descent**: Use all training samples for each update\n",
    "   - Pro: Stable, converges to minimum\n",
    "   - Con: Slow for large datasets\n",
    "\n",
    "2. **Stochastic Gradient Descent (SGD)**: Use one sample at a time\n",
    "   - Pro: Fast updates, can escape local minima\n",
    "   - Con: Noisy, unstable\n",
    "\n",
    "3. **Mini-batch Gradient Descent**: Use small batches (e.g., 32, 64, 128)\n",
    "   - Pro: Balance between batch and SGD\n",
    "   - Con: Most commonly used in practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_gradient_descent_1d():\n",
    "    \"\"\"\n",
    "    Visualize gradient descent on a simple 1D function.\n",
    "    Function: f(x) = (x - 3)^2 + 1\n",
    "    \"\"\"\n",
    "    def f(x):\n",
    "        \"\"\"Simple quadratic function.\"\"\"\n",
    "        return (x - 3) ** 2 + 1\n",
    "    \n",
    "    def df(x):\n",
    "        \"\"\"Derivative of f.\"\"\"\n",
    "        return 2 * (x - 3)\n",
    "    \n",
    "    # Test different learning rates\n",
    "    learning_rates = [0.1, 0.5, 1.1]\n",
    "    colors = ['blue', 'green', 'red']\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "    \n",
    "    for idx, (lr, color) in enumerate(zip(learning_rates, colors)):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Plot function\n",
    "        x_range = np.linspace(-2, 8, 1000)\n",
    "        ax.plot(x_range, f(x_range), 'k-', linewidth=2, label='f(x)')\n",
    "        \n",
    "        # Gradient descent\n",
    "        x = 0.0  # Starting point\n",
    "        trajectory_x = [x]\n",
    "        trajectory_y = [f(x)]\n",
    "        \n",
    "        for iteration in range(10):\n",
    "            gradient = df(x)\n",
    "            x = x - lr * gradient\n",
    "            trajectory_x.append(x)\n",
    "            trajectory_y.append(f(x))\n",
    "            \n",
    "            # Stop if diverging\n",
    "            if abs(x) > 10:\n",
    "                break\n",
    "        \n",
    "        # Plot trajectory\n",
    "        ax.plot(trajectory_x, trajectory_y, 'o-', color=color, \n",
    "               markersize=8, linewidth=2, label=f'Trajectory (lr={lr})')\n",
    "        \n",
    "        # Mark start and end\n",
    "        ax.scatter(trajectory_x[0], trajectory_y[0], s=200, c='green', \n",
    "                  marker='*', edgecolors='black', linewidth=2, \n",
    "                  zorder=5, label='Start')\n",
    "        if len(trajectory_x) > 1:\n",
    "            ax.scatter(trajectory_x[-1], trajectory_y[-1], s=200, c='red', \n",
    "                      marker='X', edgecolors='black', linewidth=2, \n",
    "                      zorder=5, label='End')\n",
    "        \n",
    "        ax.set_xlabel('x', fontsize=12, weight='bold')\n",
    "        ax.set_ylabel('f(x)', fontsize=12, weight='bold')\n",
    "        ax.set_title(f'Learning Rate = {lr}', fontsize=13, weight='bold')\n",
    "        ax.legend(fontsize=9, loc='upper right')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_ylim(0, 30)\n",
    "    \n",
    "    plt.suptitle('Effect of Learning Rate on Gradient Descent', \n",
    "                fontsize=15, weight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Observations:\")\n",
    "    print(\"  lr=0.1 (blue): Slow but stable convergence\")\n",
    "    print(\"  lr=0.5 (green): Fast convergence to minimum\")\n",
    "    print(\"  lr=1.1 (red): Overshooting, diverges!\")\n",
    "\n",
    "visualize_gradient_descent_1d()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Backpropagation\n",
    "\n",
    "### What is Backpropagation?\n",
    "\n",
    "**Backpropagation** (backward propagation of errors) is the algorithm for computing gradients in neural networks. It uses the **chain rule** from calculus to efficiently compute $\\frac{\\partial J}{\\partial \\theta}$ for all parameters.\n",
    "\n",
    "### The Chain Rule\n",
    "\n",
    "If we have $z = f(y)$ and $y = g(x)$, then:\n",
    "\n",
    "$$\\frac{dz}{dx} = \\frac{dz}{dy} \\cdot \\frac{dy}{dx}$$\n",
    "\n",
    "### Backpropagation Steps\n",
    "\n",
    "For a 2-layer network:\n",
    "\n",
    "**Forward Pass** (already computed):\n",
    "1. $z^{[1]} = W^{[1]} x + b^{[1]}$\n",
    "2. $a^{[1]} = f^{[1]}(z^{[1]})$\n",
    "3. $z^{[2]} = W^{[2]} a^{[1]} + b^{[2]}$\n",
    "4. $a^{[2]} = f^{[2]}(z^{[2]}) = \\hat{y}$\n",
    "5. $J = L(y, \\hat{y})$\n",
    "\n",
    "**Backward Pass** (compute gradients):\n",
    "\n",
    "Starting from the output:\n",
    "\n",
    "1. **Output layer gradient**:\n",
    "   $$\\frac{\\partial J}{\\partial z^{[2]}} = \\frac{\\partial J}{\\partial a^{[2]}} \\cdot \\frac{\\partial a^{[2]}}{\\partial z^{[2]}}$$\n",
    "\n",
    "2. **Output layer parameter gradients**:\n",
    "   $$\\frac{\\partial J}{\\partial W^{[2]}} = \\frac{\\partial J}{\\partial z^{[2]}} \\cdot (a^{[1]})^T$$\n",
    "   $$\\frac{\\partial J}{\\partial b^{[2]}} = \\frac{\\partial J}{\\partial z^{[2]}}$$\n",
    "\n",
    "3. **Hidden layer gradient** (chain rule!):\n",
    "   $$\\frac{\\partial J}{\\partial a^{[1]}} = (W^{[2]})^T \\cdot \\frac{\\partial J}{\\partial z^{[2]}}$$\n",
    "   $$\\frac{\\partial J}{\\partial z^{[1]}} = \\frac{\\partial J}{\\partial a^{[1]}} \\cdot \\frac{\\partial a^{[1]}}{\\partial z^{[1]}}$$\n",
    "\n",
    "4. **Hidden layer parameter gradients**:\n",
    "   $$\\frac{\\partial J}{\\partial W^{[1]}} = \\frac{\\partial J}{\\partial z^{[1]}} \\cdot x^T$$\n",
    "   $$\\frac{\\partial J}{\\partial b^{[1]}} = \\frac{\\partial J}{\\partial z^{[1]}}$$\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "- Gradients flow **backwards** through the network\n",
    "- Each layer's gradient depends on the **next layer's gradient** (chain rule)\n",
    "- We **reuse** computations from the forward pass\n",
    "- This is why it's called \"back\" propagation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_backpropagation_example():\n",
    "    \"\"\"\n",
    "    Detailed backpropagation example with manual gradient calculations.\n",
    "    \n",
    "    Network: 2 inputs -> 2 hidden (sigmoid) -> 1 output (sigmoid)\n",
    "    Loss: Binary cross-entropy\n",
    "    \"\"\"\n",
    "    print(\"MANUAL BACKPROPAGATION EXAMPLE\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Simple network setup\n",
    "    x = np.array([[1.0], [2.0]])    # Input (2, 1)\n",
    "    y = np.array([[1.0]])            # True label (1, 1)\n",
    "    \n",
    "    # Layer 1: 2 inputs -> 2 hidden\n",
    "    W1 = np.array([[0.5, -0.3],\n",
    "                   [0.2, 0.8]])      # (2, 2)\n",
    "    b1 = np.array([[0.1], [0.0]])    # (2, 1)\n",
    "    \n",
    "    # Layer 2: 2 hidden -> 1 output\n",
    "    W2 = np.array([[0.6, -0.4]])     # (1, 2)\n",
    "    b2 = np.array([[0.2]])           # (1, 1)\n",
    "    \n",
    "    print(\"\\n--- FORWARD PASS ---\")\n",
    "    print(f\"Input x:\\n{x.T}\")\n",
    "    print(f\"True label y: {y[0, 0]}\")\n",
    "    \n",
    "    # Forward pass - Layer 1\n",
    "    z1 = np.dot(W1, x) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    print(f\"\\nLayer 1:\")\n",
    "    print(f\"  z1 = W1 @ x + b1 = \\n{z1.T}\")\n",
    "    print(f\"  a1 = sigmoid(z1) = \\n{a1.T}\")\n",
    "    \n",
    "    # Forward pass - Layer 2\n",
    "    z2 = np.dot(W2, a1) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "    print(f\"\\nLayer 2:\")\n",
    "    print(f\"  z2 = W2 @ a1 + b2 = {z2[0, 0]:.4f}\")\n",
    "    print(f\"  a2 = sigmoid(z2) = {a2[0, 0]:.4f} (prediction)\")\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss_value = loss.binary_cross_entropy(y, a2)\n",
    "    print(f\"\\nLoss: {loss_value:.4f}\")\n",
    "    \n",
    "    print(\"\\n--- BACKWARD PASS ---\")\n",
    "    \n",
    "    # Backward pass - Output layer\n",
    "    # For BCE + sigmoid: dL/dz2 = a2 - y\n",
    "    dz2 = a2 - y\n",
    "    print(f\"\\nOutput layer gradients:\")\n",
    "    print(f\"  dL/dz2 = a2 - y = {dz2[0, 0]:.4f}\")\n",
    "    \n",
    "    # Gradients for W2 and b2\n",
    "    dW2 = np.dot(dz2, a1.T)\n",
    "    db2 = dz2\n",
    "    print(f\"  dL/dW2 = dz2 @ a1.T =\\n{dW2}\")\n",
    "    print(f\"  dL/db2 = dz2 = {db2[0, 0]:.4f}\")\n",
    "    \n",
    "    # Backward pass - Hidden layer\n",
    "    # Chain rule: dL/da1 = W2.T @ dz2\n",
    "    da1 = np.dot(W2.T, dz2)\n",
    "    print(f\"\\nHidden layer gradients:\")\n",
    "    print(f\"  dL/da1 = W2.T @ dz2 =\\n{da1.T}\")\n",
    "    \n",
    "    # dL/dz1 = dL/da1 * sigmoid'(z1)\n",
    "    # sigmoid'(z) = sigmoid(z) * (1 - sigmoid(z)) = a1 * (1 - a1)\n",
    "    dz1 = da1 * a1 * (1 - a1)\n",
    "    print(f\"  dL/dz1 = da1 * a1 * (1-a1) =\\n{dz1.T}\")\n",
    "    \n",
    "    # Gradients for W1 and b1\n",
    "    dW1 = np.dot(dz1, x.T)\n",
    "    db1 = dz1\n",
    "    print(f\"  dL/dW1 = dz1 @ x.T =\\n{dW1}\")\n",
    "    print(f\"  dL/db1 = dz1 =\\n{db1.T}\")\n",
    "    \n",
    "    print(\"\\n--- PARAMETER UPDATES (lr=0.1) ---\")\n",
    "    lr = 0.1\n",
    "    \n",
    "    W2_new = W2 - lr * dW2\n",
    "    b2_new = b2 - lr * db2\n",
    "    W1_new = W1 - lr * dW1\n",
    "    b1_new = b1 - lr * db1\n",
    "    \n",
    "    print(f\"\\nW2: {W2} -> {W2_new}\")\n",
    "    print(f\"b2: {b2[0, 0]:.4f} -> {b2_new[0, 0]:.4f}\")\n",
    "    print(f\"W1:\\n{W1}\\n->\\n{W1_new}\")\n",
    "    print(f\"b1: {b1.T} -> {b1_new.T}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"Gradients computed successfully via backpropagation!\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "manual_backpropagation_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Manual Gradient Calculation\n",
    "\n",
    "Given a simple network with:\n",
    "- 1 input: $x = 2.0$\n",
    "- 1 weight: $w = 0.5$\n",
    "- 1 bias: $b = 0.3$\n",
    "- Activation: Sigmoid\n",
    "- True label: $y = 1$\n",
    "- Loss: Binary Cross-Entropy\n",
    "\n",
    "**Calculate by hand**:\n",
    "1. Forward pass: $z$, $a$ (prediction), and loss $L$\n",
    "2. Backward pass: $\\frac{\\partial L}{\\partial w}$ and $\\frac{\\partial L}{\\partial b}$\n",
    "3. New parameters after one gradient descent step with $\\alpha = 0.1$\n",
    "\n",
    "Show your work step by step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your solution here\n",
    "\n",
    "# Given values\n",
    "x = 2.0\n",
    "w = 0.5\n",
    "b = 0.3\n",
    "y = 1.0\n",
    "lr = 0.1\n",
    "\n",
    "print(\"EXERCISE 1: MANUAL GRADIENT CALCULATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Given: x={x}, w={w}, b={b}, y={y}, learning_rate={lr}\")\n",
    "print(\"\\n--- YOUR CALCULATIONS ---\\n\")\n",
    "\n",
    "# TODO: Step 1 - Forward pass\n",
    "# Calculate z = w*x + b\n",
    "z = None  # Your calculation\n",
    "\n",
    "# Calculate a = sigmoid(z)\n",
    "a = None  # Your calculation\n",
    "\n",
    "# Calculate loss L = -[y*log(a) + (1-y)*log(1-a)]\n",
    "L = None  # Your calculation\n",
    "\n",
    "print(\"Step 1 - Forward Pass:\")\n",
    "print(f\"  z = w*x + b = {w}*{x} + {b} = {z}\")\n",
    "print(f\"  a = sigmoid(z) = ?\")\n",
    "print(f\"  L = BCE(y, a) = ?\")\n",
    "\n",
    "# TODO: Step 2 - Backward pass\n",
    "# For sigmoid + BCE, dL/dz = a - y\n",
    "dL_dz = None  # Your calculation\n",
    "\n",
    "# dL/dw = dL/dz * dz/dw = dL/dz * x\n",
    "dL_dw = None  # Your calculation\n",
    "\n",
    "# dL/db = dL/dz * dz/db = dL/dz * 1 = dL/dz\n",
    "dL_db = None  # Your calculation\n",
    "\n",
    "print(\"\\nStep 2 - Backward Pass:\")\n",
    "print(f\"  dL/dz = a - y = ?\")\n",
    "print(f\"  dL/dw = dL/dz * x = ?\")\n",
    "print(f\"  dL/db = dL/dz = ?\")\n",
    "\n",
    "# TODO: Step 3 - Parameter updates\n",
    "# w_new = w - lr * dL/dw\n",
    "w_new = None  # Your calculation\n",
    "\n",
    "# b_new = b - lr * dL/db\n",
    "b_new = None  # Your calculation\n",
    "\n",
    "print(\"\\nStep 3 - Parameter Updates:\")\n",
    "print(f\"  w_new = w - lr*dL/dw = {w} - {lr}*? = ?\")\n",
    "print(f\"  b_new = b - lr*dL/db = {b} - {lr}*? = ?\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to Exercise 1\n",
    "\n",
    "x = 2.0\n",
    "w = 0.5\n",
    "b = 0.3\n",
    "y = 1.0\n",
    "lr = 0.1\n",
    "\n",
    "print(\"EXERCISE 1: SOLUTION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Given: x={x}, w={w}, b={b}, y={y}, learning_rate={lr}\")\n",
    "\n",
    "# Step 1: Forward pass\n",
    "z = w * x + b\n",
    "a = sigmoid(np.array([z]))[0]\n",
    "L = loss.binary_cross_entropy(np.array([y]), np.array([a]))\n",
    "\n",
    "print(\"\\nStep 1 - Forward Pass:\")\n",
    "print(f\"  z = w*x + b = {w}*{x} + {b} = {z}\")\n",
    "print(f\"  a = sigmoid({z}) = 1/(1+e^(-{z})) = {a:.6f}\")\n",
    "print(f\"  L = -[{y}*log({a:.6f}) + {1-y}*log({1-a:.6f})]\")\n",
    "print(f\"  L = {L:.6f}\")\n",
    "\n",
    "# Step 2: Backward pass\n",
    "dL_dz = a - y\n",
    "dL_dw = dL_dz * x\n",
    "dL_db = dL_dz\n",
    "\n",
    "print(\"\\nStep 2 - Backward Pass:\")\n",
    "print(f\"  dL/dz = a - y = {a:.6f} - {y} = {dL_dz:.6f}\")\n",
    "print(f\"  dL/dw = dL/dz * x = {dL_dz:.6f} * {x} = {dL_dw:.6f}\")\n",
    "print(f\"  dL/db = dL/dz = {dL_db:.6f}\")\n",
    "\n",
    "# Step 3: Parameter updates\n",
    "w_new = w - lr * dL_dw\n",
    "b_new = b - lr * dL_db\n",
    "\n",
    "print(\"\\nStep 3 - Parameter Updates (lr=0.1):\")\n",
    "print(f\"  w_new = {w} - {lr}*{dL_dw:.6f} = {w_new:.6f}\")\n",
    "print(f\"  b_new = {b} - {lr}*{dL_db:.6f} = {b_new:.6f}\")\n",
    "\n",
    "# Verify with new forward pass\n",
    "z_new = w_new * x + b_new\n",
    "a_new = sigmoid(np.array([z_new]))[0]\n",
    "L_new = loss.binary_cross_entropy(np.array([y]), np.array([a_new]))\n",
    "\n",
    "print(\"\\nVerification - Forward pass with new parameters:\")\n",
    "print(f\"  New prediction: {a_new:.6f} (closer to {y}!)\")\n",
    "print(f\"  New loss: {L_new:.6f} (lower than {L:.6f}!)\")\n",
    "print(f\"  Loss decreased by: {L - L_new:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"✓ Gradients correctly computed and parameters updated!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Implement Simple Gradient Descent\n",
    "\n",
    "Implement gradient descent to find the minimum of the function:\n",
    "\n",
    "$$f(x) = x^2 - 4x + 7$$\n",
    "\n",
    "**Tasks**:\n",
    "1. Implement the function and its derivative\n",
    "2. Run gradient descent for 20 iterations with learning rate 0.1\n",
    "3. Start from $x_0 = 0$\n",
    "4. Plot the trajectory\n",
    "5. Find the minimum value\n",
    "\n",
    "**Hint**: The derivative is $f'(x) = 2x - 4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your solution here\n",
    "\n",
    "def f(x):\n",
    "    \"\"\"Function: f(x) = x^2 - 4x + 7\"\"\"\n",
    "    # TODO: Implement\n",
    "    pass\n",
    "\n",
    "def df(x):\n",
    "    \"\"\"Derivative: f'(x) = 2x - 4\"\"\"\n",
    "    # TODO: Implement\n",
    "    pass\n",
    "\n",
    "# TODO: Initialize\n",
    "x = 0.0  # Starting point\n",
    "lr = 0.1  # Learning rate\n",
    "n_iterations = 20\n",
    "\n",
    "# TODO: Track trajectory\n",
    "trajectory_x = []\n",
    "trajectory_y = []\n",
    "\n",
    "# TODO: Run gradient descent\n",
    "for i in range(n_iterations):\n",
    "    # Calculate gradient\n",
    "    # Update x\n",
    "    # Record trajectory\n",
    "    pass\n",
    "\n",
    "# TODO: Plot results\n",
    "# - Plot the function f(x)\n",
    "# - Plot the trajectory\n",
    "# - Mark the minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to Exercise 2\n",
    "\n",
    "def f(x):\n",
    "    \"\"\"Function: f(x) = x^2 - 4x + 7\"\"\"\n",
    "    return x**2 - 4*x + 7\n",
    "\n",
    "def df(x):\n",
    "    \"\"\"Derivative: f'(x) = 2x - 4\"\"\"\n",
    "    return 2*x - 4\n",
    "\n",
    "# Initialize\n",
    "x = 0.0\n",
    "lr = 0.1\n",
    "n_iterations = 20\n",
    "\n",
    "trajectory_x = [x]\n",
    "trajectory_y = [f(x)]\n",
    "\n",
    "print(\"GRADIENT DESCENT ON f(x) = x^2 - 4x + 7\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Starting point: x = {x}\")\n",
    "print(f\"Learning rate: {lr}\")\n",
    "print(f\"\\nIteration | x | f(x) | gradient\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Run gradient descent\n",
    "for i in range(n_iterations):\n",
    "    gradient = df(x)\n",
    "    x = x - lr * gradient\n",
    "    \n",
    "    trajectory_x.append(x)\n",
    "    trajectory_y.append(f(x))\n",
    "    \n",
    "    if i < 5 or i == n_iterations - 1:  # Print first 5 and last\n",
    "        print(f\"{i+1:9d} | {x:8.4f} | {f(x):8.4f} | {gradient:8.4f}\")\n",
    "    elif i == 5:\n",
    "        print(\"      ... | ... | ... | ...\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nFinal x: {x:.6f}\")\n",
    "print(f\"Minimum value f(x): {f(x):.6f}\")\n",
    "print(f\"\\nAnalytical minimum: x = 2, f(2) = 3\")\n",
    "print(f\"Our solution: x = {x:.6f}, f(x) = {f(x):.6f}\")\n",
    "print(f\"Error: {abs(x - 2):.6f}\")\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Function and trajectory\n",
    "x_range = np.linspace(-1, 5, 1000)\n",
    "ax1.plot(x_range, f(x_range), 'k-', linewidth=2, label='f(x)')\n",
    "ax1.plot(trajectory_x, trajectory_y, 'ro-', markersize=6, \n",
    "        linewidth=2, alpha=0.7, label='Gradient Descent')\n",
    "ax1.scatter(trajectory_x[0], trajectory_y[0], s=200, c='green', \n",
    "           marker='*', edgecolors='black', linewidth=2, zorder=5, label='Start')\n",
    "ax1.scatter(trajectory_x[-1], trajectory_y[-1], s=200, c='red', \n",
    "           marker='X', edgecolors='black', linewidth=2, zorder=5, label='End')\n",
    "ax1.scatter(2, 3, s=200, c='yellow', marker='D', \n",
    "           edgecolors='black', linewidth=2, zorder=5, label='True Minimum')\n",
    "ax1.set_xlabel('x', fontsize=12, weight='bold')\n",
    "ax1.set_ylabel('f(x)', fontsize=12, weight='bold')\n",
    "ax1.set_title('Gradient Descent Trajectory', fontsize=13, weight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Convergence\n",
    "ax2.plot(range(len(trajectory_y)), trajectory_y, 'b-o', \n",
    "        linewidth=2, markersize=6)\n",
    "ax2.axhline(y=3, color='green', linestyle='--', linewidth=2, \n",
    "           label='True Minimum (f=3)')\n",
    "ax2.set_xlabel('Iteration', fontsize=12, weight='bold')\n",
    "ax2.set_ylabel('f(x)', fontsize=12, weight='bold')\n",
    "ax2.set_title('Convergence Over Time', fontsize=13, weight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Analyze Learning Rate Impact\n",
    "\n",
    "Use the same function from Exercise 2 and test different learning rates:\n",
    "- Try: [0.01, 0.1, 0.5, 0.9, 1.1]\n",
    "- Run for 30 iterations each\n",
    "- Plot all trajectories on the same graph\n",
    "- Determine which learning rate converges fastest\n",
    "- Identify which rates diverge or oscillate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Your solution here\n",
    "\n",
    "# TODO: Test multiple learning rates\n",
    "learning_rates = [0.01, 0.1, 0.5, 0.9, 1.1]\n",
    "\n",
    "# TODO: For each learning rate:\n",
    "#   - Run gradient descent\n",
    "#   - Track trajectory\n",
    "#   - Count iterations to convergence\n",
    "\n",
    "# TODO: Plot all trajectories\n",
    "# TODO: Create comparison table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to Exercise 3\n",
    "\n",
    "learning_rates = [0.01, 0.1, 0.5, 0.9, 1.1]\n",
    "colors = ['blue', 'green', 'orange', 'red', 'purple']\n",
    "n_iterations = 30\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Plot function\n",
    "x_range = np.linspace(-2, 6, 1000)\n",
    "plt.plot(x_range, f(x_range), 'k-', linewidth=2.5, label='f(x)', alpha=0.5)\n",
    "\n",
    "results = []\n",
    "\n",
    "for lr, color in zip(learning_rates, colors):\n",
    "    x = 0.0  # Reset starting point\n",
    "    trajectory_x = [x]\n",
    "    trajectory_y = [f(x)]\n",
    "    \n",
    "    converged = False\n",
    "    converged_iter = n_iterations\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        gradient = df(x)\n",
    "        x = x - lr * gradient\n",
    "        trajectory_x.append(x)\n",
    "        trajectory_y.append(f(x))\n",
    "        \n",
    "        # Check convergence (within 0.01 of minimum)\n",
    "        if not converged and abs(f(x) - 3.0) < 0.01:\n",
    "            converged = True\n",
    "            converged_iter = i + 1\n",
    "        \n",
    "        # Check divergence\n",
    "        if abs(x) > 20:\n",
    "            converged_iter = -1  # Diverged\n",
    "            break\n",
    "    \n",
    "    # Plot trajectory\n",
    "    plt.plot(trajectory_x, trajectory_y, 'o-', color=color, \n",
    "            markersize=4, linewidth=2, alpha=0.7, label=f'lr={lr}')\n",
    "    \n",
    "    # Store results\n",
    "    final_x = trajectory_x[-1]\n",
    "    final_fx = trajectory_y[-1]\n",
    "    results.append({\n",
    "        'lr': lr,\n",
    "        'final_x': final_x,\n",
    "        'final_f': final_fx,\n",
    "        'converged_iter': converged_iter,\n",
    "        'status': 'Converged' if converged_iter > 0 else 'Diverged'\n",
    "    })\n",
    "\n",
    "# Mark true minimum\n",
    "plt.scatter(2, 3, s=300, c='yellow', marker='*', \n",
    "           edgecolors='black', linewidth=2, zorder=5, label='True Minimum')\n",
    "\n",
    "plt.xlabel('x', fontsize=13, weight='bold')\n",
    "plt.ylabel('f(x)', fontsize=13, weight='bold')\n",
    "plt.title('Impact of Learning Rate on Gradient Descent', \n",
    "         fontsize=15, weight='bold')\n",
    "plt.legend(fontsize=11, loc='upper right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(-2, 6)\n",
    "plt.ylim(0, 30)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary table\n",
    "print(\"\\nLEARNING RATE COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'LR':<6} | {'Final x':<10} | {'Final f(x)':<12} | {'Converged':<10} | {'Status':<10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for r in results:\n",
    "    conv_str = f\"{r['converged_iter']} iters\" if r['converged_iter'] > 0 else \"No\"\n",
    "    print(f\"{r['lr']:<6.2f} | {r['final_x']:<10.4f} | {r['final_f']:<12.4f} | \"\n",
    "          f\"{conv_str:<10} | {r['status']:<10}\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nObservations:\")\n",
    "print(\"  - lr=0.01: Very slow convergence\")\n",
    "print(\"  - lr=0.1: Good balance - fast and stable\")\n",
    "print(\"  - lr=0.5: Very fast convergence\")\n",
    "print(\"  - lr=0.9: Near critical point - oscillations\")\n",
    "print(\"  - lr=1.1: Too large - diverges!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "Congratulations! You've mastered the core algorithms that power neural network training. Let's recap:\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Forward Propagation**\n",
    "   - Data flows input → hidden → output\n",
    "   - Each layer: $z = Wx + b$, then $a = f(z)$\n",
    "   - Final output is the prediction $\\hat{y}$\n",
    "\n",
    "2. **Loss Functions**\n",
    "   - **MSE**: For regression, $L = \\frac{1}{2}(y - \\hat{y})^2$\n",
    "   - **Binary Cross-Entropy**: For classification, $L = -[y\\log(\\hat{y}) + (1-y)\\log(1-\\hat{y})]$\n",
    "   - Measures prediction error\n",
    "\n",
    "3. **Gradient Descent**\n",
    "   - Optimization algorithm: $\\theta \\leftarrow \\theta - \\alpha \\nabla J$\n",
    "   - Learning rate $\\alpha$ controls step size\n",
    "   - Too small = slow, too large = divergence\n",
    "\n",
    "4. **Backpropagation**\n",
    "   - Efficiently computes gradients using chain rule\n",
    "   - Flows backward through network\n",
    "   - Each layer's gradient depends on next layer\n",
    "   - Enables training of deep networks\n",
    "\n",
    "5. **The Chain Rule**\n",
    "   - Foundation of backpropagation\n",
    "   - $\\frac{dz}{dx} = \\frac{dz}{dy} \\cdot \\frac{dy}{dx}$\n",
    "   - Allows gradient propagation through layers\n",
    "\n",
    "### Important Insights\n",
    "\n",
    "- **Why Backprop is Efficient**: Reuses forward pass computations, avoids redundant calculations\n",
    "- **Gradient Flow**: Gradients can vanish (sigmoid/tanh) or explode (deep networks)\n",
    "- **Learning Rate**: Critical hyperparameter affecting convergence speed and stability\n",
    "- **BCE + Sigmoid**: Perfect pair - simple gradient $\\hat{y} - y$\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In **Module 03: Building Neural Networks with NumPy**, we'll:\n",
    "- Implement a complete multi-layer perceptron from scratch\n",
    "- Build forward and backward propagation classes\n",
    "- Train on real datasets (XOR, circles, MNIST)\n",
    "- Compare with scikit-learn's MLPClassifier\n",
    "- Understand every detail of how neural networks work\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "**Classic Papers**:\n",
    "- Rumelhart et al. (1986): \"Learning representations by back-propagating errors\"\n",
    "- LeCun et al. (1998): \"Efficient BackProp\"\n",
    "\n",
    "**Tutorials**:\n",
    "- 3Blue1Brown: \"Backpropagation calculus\" (YouTube)\n",
    "- Andrej Karpathy: \"micrograd\" (minimal backprop implementation)\n",
    "\n",
    "**Interactive**:\n",
    "- TensorFlow Playground: See gradient descent in action\n",
    "- Distill.pub: \"Momentum\" and optimization visualizations\n",
    "\n",
    "---\n",
    "\n",
    "**Ready to build your own neural network from scratch?** Continue to **Module 03: Building Neural Networks with NumPy**!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
