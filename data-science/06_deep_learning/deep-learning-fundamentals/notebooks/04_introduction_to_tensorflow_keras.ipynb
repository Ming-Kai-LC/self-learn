{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 04: Introduction to TensorFlow and Keras\n",
    "\n",
    "**Difficulty**: ⭐⭐ (Intermediate)\n",
    "**Estimated Time**: 45-60 minutes\n",
    "**Prerequisites**: \n",
    "- Module 00: Introduction to Neural Networks\n",
    "- Module 01: Perceptrons and Activation Functions\n",
    "- Module 02: Backpropagation and Gradient Descent\n",
    "- Module 03: Building Neural Networks with NumPy\n",
    "- Python programming and NumPy\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. **Understand** the TensorFlow ecosystem and its components\n",
    "2. **Work** with TensorFlow tensors and operations\n",
    "3. **Build** neural networks using Keras Sequential API\n",
    "4. **Design** complex architectures with Keras Functional API\n",
    "5. **Train** models on real datasets (MNIST handwritten digits)\n",
    "6. **Evaluate** model performance with metrics and visualization\n",
    "7. **Save** and load trained models for deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist, fashion_mnist\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "\n",
    "# Scikit-learn for utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. TensorFlow Basics\n",
    "\n",
    "### What is TensorFlow?\n",
    "\n",
    "**TensorFlow** is an open-source deep learning framework developed by Google. It provides:\n",
    "- Efficient tensor operations (CPU and GPU)\n",
    "- Automatic differentiation (autograd)\n",
    "- High-level APIs (Keras)\n",
    "- Production deployment tools (TensorFlow Serving, TensorFlow Lite)\n",
    "- Distributed training support\n",
    "\n",
    "### Tensors\n",
    "\n",
    "A **tensor** is a multi-dimensional array (generalization of matrices):\n",
    "- **Scalar** (rank-0 tensor): Single number\n",
    "- **Vector** (rank-1 tensor): 1D array\n",
    "- **Matrix** (rank-2 tensor): 2D array\n",
    "- **3D+ Tensor** (rank-3+ tensor): Multi-dimensional arrays\n",
    "\n",
    "### Keras\n",
    "\n",
    "**Keras** is a high-level neural networks API that runs on top of TensorFlow. It provides:\n",
    "- Simple, intuitive interface\n",
    "- Modular and composable\n",
    "- Easy prototyping\n",
    "- Support for CNNs, RNNs, and combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with TensorFlow tensors\n",
    "\n",
    "print(\"TENSORFLOW TENSOR BASICS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create tensors\n",
    "scalar = tf.constant(42)\n",
    "vector = tf.constant([1, 2, 3, 4])\n",
    "matrix = tf.constant([[1, 2], [3, 4], [5, 6]])\n",
    "tensor_3d = tf.random.normal(shape=(2, 3, 4))\n",
    "\n",
    "print(\"\\n1. TENSOR CREATION\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"Scalar (rank-0): {scalar.numpy()}\")\n",
    "print(f\"  Shape: {scalar.shape}, Dtype: {scalar.dtype}\")\n",
    "\n",
    "print(f\"\\nVector (rank-1): {vector.numpy()}\")\n",
    "print(f\"  Shape: {vector.shape}, Dtype: {vector.dtype}\")\n",
    "\n",
    "print(f\"\\nMatrix (rank-2):\\n{matrix.numpy()}\")\n",
    "print(f\"  Shape: {matrix.shape}, Dtype: {matrix.dtype}\")\n",
    "\n",
    "print(f\"\\n3D Tensor (rank-3): Shape {tensor_3d.shape}\")\n",
    "\n",
    "# Tensor operations\n",
    "print(\"\\n2. TENSOR OPERATIONS\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "a = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
    "b = tf.constant([[5.0, 6.0], [7.0, 8.0]])\n",
    "\n",
    "print(f\"Tensor a:\\n{a.numpy()}\")\n",
    "print(f\"\\nTensor b:\\n{b.numpy()}\")\n",
    "\n",
    "# Element-wise operations\n",
    "add = tf.add(a, b)\n",
    "multiply = tf.multiply(a, b)\n",
    "\n",
    "print(f\"\\nElement-wise addition:\\n{add.numpy()}\")\n",
    "print(f\"\\nElement-wise multiplication:\\n{multiply.numpy()}\")\n",
    "\n",
    "# Matrix multiplication\n",
    "matmul = tf.matmul(a, b)\n",
    "print(f\"\\nMatrix multiplication (a @ b):\\n{matmul.numpy()}\")\n",
    "\n",
    "# Shape manipulation\n",
    "reshaped = tf.reshape(a, [1, 4])\n",
    "print(f\"\\nReshaped a from {a.shape} to {reshaped.shape}:\\n{reshaped.numpy()}\")\n",
    "\n",
    "# Useful functions\n",
    "print(\"\\n3. USEFUL FUNCTIONS\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "x = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "print(f\"Tensor x:\\n{x.numpy()}\")\n",
    "print(f\"\\nMean: {tf.reduce_mean(x).numpy():.2f}\")\n",
    "print(f\"Sum: {tf.reduce_sum(x).numpy():.2f}\")\n",
    "print(f\"Max: {tf.reduce_max(x).numpy():.2f}\")\n",
    "print(f\"Argmax (axis=1): {tf.argmax(x, axis=1).numpy()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Explore MNIST Dataset\n",
    "\n",
    "**MNIST** (Modified National Institute of Standards and Technology) is a classic dataset of handwritten digits (0-9):\n",
    "- 60,000 training images\n",
    "- 10,000 test images\n",
    "- Each image is 28×28 pixels, grayscale\n",
    "- 10 classes (digits 0-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "print(\"Loading MNIST dataset...\")\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "print(\"\\nDataset Information:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Test samples: {X_test.shape[0]}\")\n",
    "print(f\"Image shape: {X_train.shape[1:]}\")\n",
    "print(f\"Pixel value range: {X_train.min()} to {X_train.max()}\")\n",
    "print(f\"\\nClasses: {np.unique(y_train)}\")\n",
    "print(f\"Class distribution (train): {np.bincount(y_train)}\")\n",
    "\n",
    "# Visualize sample images\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(10):\n",
    "    # Find first occurrence of each digit\n",
    "    idx = np.where(y_train == i)[0][0]\n",
    "    axes[i].imshow(X_train[idx], cmap='gray')\n",
    "    axes[i].set_title(f'Digit: {i}', fontsize=12, weight='bold')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('MNIST Sample Images (One Per Class)', fontsize=15, weight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "print(\"Preprocessing data...\")\n",
    "\n",
    "# Normalize pixel values to [0, 1]\n",
    "X_train_normalized = X_train.astype('float32') / 255.0\n",
    "X_test_normalized = X_test.astype('float32') / 255.0\n",
    "\n",
    "# Flatten images for fully-connected network\n",
    "# Shape: (num_samples, 28, 28) -> (num_samples, 784)\n",
    "X_train_flat = X_train_normalized.reshape(-1, 28 * 28)\n",
    "X_test_flat = X_test_normalized.reshape(-1, 28 * 28)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "# Example: 3 -> [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "y_train_onehot = to_categorical(y_train, num_classes=10)\n",
    "y_test_onehot = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "print(\"\\nPreprocessing complete!\")\n",
    "print(f\"Flattened shape: {X_train_flat.shape}\")\n",
    "print(f\"One-hot labels shape: {y_train_onehot.shape}\")\n",
    "print(f\"\\nExample label conversion:\")\n",
    "print(f\"  Original: {y_train[0]}\")\n",
    "print(f\"  One-hot: {y_train_onehot[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Keras Sequential API\n",
    "\n",
    "The **Sequential API** is the simplest way to build models in Keras. It's used for linear stacks of layers.\n",
    "\n",
    "### Building a Model\n",
    "\n",
    "Two ways to create a Sequential model:\n",
    "\n",
    "**Method 1: Pass layers to constructor**\n",
    "```python\n",
    "model = models.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(784,)),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "```\n",
    "\n",
    "**Method 2: Use .add() method**\n",
    "```python\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(128, activation='relu', input_shape=(784,)))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a simple neural network using Sequential API\n",
    "\n",
    "print(\"BUILDING MODEL WITH SEQUENTIAL API\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create model\n",
    "model_sequential = models.Sequential([\n",
    "    # Input layer (implicit) + First hidden layer\n",
    "    layers.Dense(128, activation='relu', input_shape=(784,), name='hidden_1'),\n",
    "    \n",
    "    # Second hidden layer\n",
    "    layers.Dense(64, activation='relu', name='hidden_2'),\n",
    "    \n",
    "    # Output layer (10 classes)\n",
    "    layers.Dense(10, activation='softmax', name='output')\n",
    "])\n",
    "\n",
    "print(\"\\nModel created successfully!\")\n",
    "print(\"\\nModel Architecture:\")\n",
    "model_sequential.summary()\n",
    "\n",
    "# Compile the model\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"COMPILING MODEL\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "model_sequential.compile(\n",
    "    optimizer='adam',                    # Optimizer: Adam (adaptive learning rate)\n",
    "    loss='categorical_crossentropy',     # Loss: Cross-entropy for multi-class\n",
    "    metrics=['accuracy']                 # Track accuracy during training\n",
    ")\n",
    "\n",
    "print(\"\\nModel compiled!\")\n",
    "print(\"Optimizer: Adam\")\n",
    "print(\"Loss: Categorical Cross-Entropy\")\n",
    "print(\"Metrics: Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "print(\"TRAINING MODEL\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "history = model_sequential.fit(\n",
    "    X_train_flat,           # Training data\n",
    "    y_train_onehot,         # Training labels\n",
    "    epochs=10,              # Number of epochs\n",
    "    batch_size=128,         # Batch size\n",
    "    validation_split=0.2,   # Use 20% of training data for validation\n",
    "    verbose=1               # Show progress\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Training complete!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "\n",
    "def plot_training_history(history, title=\"Training History\"):\n",
    "    \"\"\"\n",
    "    Plot training and validation loss/accuracy.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    history : keras History object\n",
    "        Training history from model.fit()\n",
    "    title : str\n",
    "        Plot title\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Plot loss\n",
    "    ax1.plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "    ax1.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "    ax1.set_xlabel('Epoch', fontsize=12, weight='bold')\n",
    "    ax1.set_ylabel('Loss', fontsize=12, weight='bold')\n",
    "    ax1.set_title('Model Loss', fontsize=13, weight='bold')\n",
    "    ax1.legend(fontsize=11)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot accuracy\n",
    "    ax2.plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "    ax2.plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "    ax2.set_xlabel('Epoch', fontsize=12, weight='bold')\n",
    "    ax2.set_ylabel('Accuracy', fontsize=12, weight='bold')\n",
    "    ax2.set_title('Model Accuracy', fontsize=13, weight='bold')\n",
    "    ax2.legend(fontsize=11)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(title, fontsize=15, weight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(history, \"MNIST Training - Sequential Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "\n",
    "print(\"EVALUATING ON TEST SET\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "test_loss, test_accuracy = model_sequential.evaluate(\n",
    "    X_test_flat, \n",
    "    y_test_onehot,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model_sequential.predict(X_test_flat[:10], verbose=0)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = y_test[:10]\n",
    "\n",
    "print(\"\\nSample Predictions:\")\n",
    "print(\"-\" * 70)\n",
    "print(\"True | Predicted | Confidence\")\n",
    "print(\"-\" * 70)\n",
    "for i in range(10):\n",
    "    confidence = predictions[i][predicted_classes[i]]\n",
    "    correct = \"✓\" if predicted_classes[i] == true_classes[i] else \"✗\"\n",
    "    print(f\" {true_classes[i]}   |     {predicted_classes[i]}     | {confidence:.4f}   {correct}\")\n",
    "print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Keras Functional API\n",
    "\n",
    "The **Functional API** is more flexible than Sequential. It allows:\n",
    "- Multi-input and multi-output models\n",
    "- Shared layers\n",
    "- Residual connections\n",
    "- Complex architectures\n",
    "\n",
    "### Syntax\n",
    "\n",
    "```python\n",
    "# Define input\n",
    "inputs = layers.Input(shape=(784,))\n",
    "\n",
    "# Build layers (connect them like a graph)\n",
    "x = layers.Dense(128, activation='relu')(inputs)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "outputs = layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "# Create model\n",
    "model = models.Model(inputs=inputs, outputs=outputs)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model using Functional API\n",
    "\n",
    "print(\"BUILDING MODEL WITH FUNCTIONAL API\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Define input\n",
    "inputs = layers.Input(shape=(784,), name='input')\n",
    "\n",
    "# Build network\n",
    "x = layers.Dense(256, activation='relu', name='hidden_1')(inputs)\n",
    "x = layers.Dropout(0.3, name='dropout_1')(x)  # Dropout for regularization\n",
    "x = layers.Dense(128, activation='relu', name='hidden_2')(x)\n",
    "x = layers.Dropout(0.3, name='dropout_2')(x)\n",
    "x = layers.Dense(64, activation='relu', name='hidden_3')(x)\n",
    "outputs = layers.Dense(10, activation='softmax', name='output')(x)\n",
    "\n",
    "# Create model\n",
    "model_functional = models.Model(inputs=inputs, outputs=outputs, name='mnist_functional')\n",
    "\n",
    "print(\"\\nModel created!\")\n",
    "print(\"\\nModel Architecture:\")\n",
    "model_functional.summary()\n",
    "\n",
    "# Compile\n",
    "model_functional.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"\\nModel compiled and ready for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train functional model\n",
    "\n",
    "print(\"TRAINING FUNCTIONAL MODEL\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "history_functional = model_functional.fit(\n",
    "    X_train_flat,\n",
    "    y_train_onehot,\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "test_loss_func, test_accuracy_func = model_functional.evaluate(\n",
    "    X_test_flat, \n",
    "    y_test_onehot,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RESULTS COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Sequential Model:  {test_accuracy * 100:.2f}%\")\n",
    "print(f\"Functional Model:  {test_accuracy_func * 100:.2f}%\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Plot history\n",
    "plot_training_history(history_functional, \"MNIST Training - Functional Model (with Dropout)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation and Visualization\n",
    "\n",
    "Let's do a comprehensive evaluation of our trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for entire test set\n",
    "y_pred_proba = model_functional.predict(X_test_flat, verbose=0)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "           xticklabels=range(10), yticklabels=range(10),\n",
    "           cbar_kws={'label': 'Count'})\n",
    "plt.xlabel('Predicted Label', fontsize=13, weight='bold')\n",
    "plt.ylabel('True Label', fontsize=13, weight='bold')\n",
    "plt.title('Confusion Matrix - MNIST Test Set', fontsize=15, weight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Per-class accuracy\n",
    "print(\"\\nPER-CLASS ACCURACY\")\n",
    "print(\"=\" * 70)\n",
    "for digit in range(10):\n",
    "    mask = y_test == digit\n",
    "    accuracy = np.mean(y_pred[mask] == digit)\n",
    "    print(f\"Digit {digit}: {accuracy * 100:.2f}%\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions (correct and incorrect)\n",
    "\n",
    "def visualize_predictions(X, y_true, y_pred, num_samples=10, incorrect_only=False):\n",
    "    \"\"\"\n",
    "    Visualize model predictions.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : ndarray\n",
    "        Images\n",
    "    y_true : ndarray\n",
    "        True labels\n",
    "    y_pred : ndarray\n",
    "        Predicted labels\n",
    "    num_samples : int\n",
    "        Number of samples to show\n",
    "    incorrect_only : bool\n",
    "        Show only incorrect predictions\n",
    "    \"\"\"\n",
    "    if incorrect_only:\n",
    "        # Find incorrect predictions\n",
    "        incorrect_mask = y_pred != y_true\n",
    "        indices = np.where(incorrect_mask)[0][:num_samples]\n",
    "        title = 'Incorrect Predictions'\n",
    "    else:\n",
    "        indices = np.random.choice(len(X), num_samples, replace=False)\n",
    "        title = 'Random Predictions'\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 5, figsize=(14, 6))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        axes[i].imshow(X[idx].reshape(28, 28), cmap='gray')\n",
    "        \n",
    "        true_label = y_true[idx]\n",
    "        pred_label = y_pred[idx]\n",
    "        \n",
    "        color = 'green' if pred_label == true_label else 'red'\n",
    "        axes[i].set_title(f'True: {true_label}\\nPred: {pred_label}', \n",
    "                         fontsize=11, weight='bold', color=color)\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle(title, fontsize=15, weight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Show random predictions\n",
    "visualize_predictions(X_test_normalized, y_test, y_pred, num_samples=10)\n",
    "\n",
    "# Show incorrect predictions\n",
    "visualize_predictions(X_test_normalized, y_test, y_pred, \n",
    "                     num_samples=10, incorrect_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Saving and Loading Models\n",
    "\n",
    "After training, we need to save models for later use or deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "# Create temporary directory for saving models\n",
    "save_dir = tempfile.mkdtemp()\n",
    "print(f\"Saving models to: {save_dir}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SAVING MODELS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Method 1: Save entire model (architecture + weights + optimizer state)\n",
    "model_path = os.path.join(save_dir, 'mnist_model.h5')\n",
    "model_functional.save(model_path)\n",
    "print(f\"\\n✓ Full model saved to: {model_path}\")\n",
    "print(f\"  File size: {os.path.getsize(model_path) / 1e6:.2f} MB\")\n",
    "\n",
    "# Method 2: Save only weights\n",
    "weights_path = os.path.join(save_dir, 'model_weights.h5')\n",
    "model_functional.save_weights(weights_path)\n",
    "print(f\"\\n✓ Weights saved to: {weights_path}\")\n",
    "print(f\"  File size: {os.path.getsize(weights_path) / 1e6:.2f} MB\")\n",
    "\n",
    "# Method 3: Save architecture as JSON\n",
    "architecture_path = os.path.join(save_dir, 'model_architecture.json')\n",
    "with open(architecture_path, 'w') as f:\n",
    "    f.write(model_functional.to_json())\n",
    "print(f\"\\n✓ Architecture saved to: {architecture_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"LOADING MODELS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load full model\n",
    "loaded_model = models.load_model(model_path)\n",
    "print(\"\\n✓ Full model loaded successfully\")\n",
    "\n",
    "# Verify it works\n",
    "test_loss_loaded, test_accuracy_loaded = loaded_model.evaluate(\n",
    "    X_test_flat, y_test_onehot, verbose=0\n",
    ")\n",
    "\n",
    "print(\"\\nVerification:\")\n",
    "print(f\"  Original model accuracy: {test_accuracy_func * 100:.2f}%\")\n",
    "print(f\"  Loaded model accuracy:   {test_accuracy_loaded * 100:.2f}%\")\n",
    "\n",
    "if abs(test_accuracy_func - test_accuracy_loaded) < 1e-6:\n",
    "    print(\"  ✓ Models are identical!\")\n",
    "else:\n",
    "    print(\"  ⚠️ Warning: Models differ!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Build and Train on Fashion-MNIST\n",
    "\n",
    "**Fashion-MNIST** is a drop-in replacement for MNIST with clothing images instead of digits:\n",
    "- Same size (28×28 grayscale)\n",
    "- 10 classes: T-shirt, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Ankle boot\n",
    "- More challenging than MNIST\n",
    "\n",
    "**Tasks**:\n",
    "1. Load Fashion-MNIST dataset\n",
    "2. Preprocess the data\n",
    "3. Build a neural network (your choice of architecture)\n",
    "4. Train for at least 10 epochs\n",
    "5. Achieve >85% test accuracy\n",
    "6. Visualize some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your solution here\n",
    "\n",
    "# TODO: Load Fashion-MNIST\n",
    "# (X_train_fashion, y_train_fashion), (X_test_fashion, y_test_fashion) = fashion_mnist.load_data()\n",
    "\n",
    "# TODO: Preprocess data (normalize, flatten, one-hot encode)\n",
    "\n",
    "# TODO: Build model (Sequential or Functional API)\n",
    "\n",
    "# TODO: Compile and train\n",
    "\n",
    "# TODO: Evaluate and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to Exercise 1\n",
    "\n",
    "# Class names for Fashion-MNIST\n",
    "fashion_class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "                      'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# Load data\n",
    "print(\"Loading Fashion-MNIST...\")\n",
    "(X_train_fashion, y_train_fashion), (X_test_fashion, y_test_fashion) = fashion_mnist.load_data()\n",
    "\n",
    "# Preprocess\n",
    "X_train_fashion_norm = X_train_fashion.astype('float32') / 255.0\n",
    "X_test_fashion_norm = X_test_fashion.astype('float32') / 255.0\n",
    "X_train_fashion_flat = X_train_fashion_norm.reshape(-1, 784)\n",
    "X_test_fashion_flat = X_test_fashion_norm.reshape(-1, 784)\n",
    "y_train_fashion_oh = to_categorical(y_train_fashion, 10)\n",
    "y_test_fashion_oh = to_categorical(y_test_fashion, 10)\n",
    "\n",
    "print(f\"Dataset loaded: {X_train_fashion.shape[0]} training samples\")\n",
    "\n",
    "# Build model\n",
    "model_fashion = models.Sequential([\n",
    "    layers.Dense(512, activation='relu', input_shape=(784,)),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model_fashion.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"\\nModel architecture:\")\n",
    "model_fashion.summary()\n",
    "\n",
    "# Train\n",
    "print(\"\\nTraining...\")\n",
    "history_fashion = model_fashion.fit(\n",
    "    X_train_fashion_flat,\n",
    "    y_train_fashion_oh,\n",
    "    epochs=15,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "test_loss_fashion, test_acc_fashion = model_fashion.evaluate(\n",
    "    X_test_fashion_flat, y_test_fashion_oh, verbose=0\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"Fashion-MNIST Test Accuracy: {test_acc_fashion * 100:.2f}%\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if test_acc_fashion >= 0.85:\n",
    "    print(\"✓ Target achieved: >85% accuracy!\")\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(history_fashion, \"Fashion-MNIST Training\")\n",
    "\n",
    "# Visualize predictions\n",
    "y_pred_fashion = np.argmax(model_fashion.predict(X_test_fashion_flat, verbose=0), axis=1)\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(14, 6))\n",
    "axes = axes.ravel()\n",
    "indices = np.random.choice(len(X_test_fashion), 10, replace=False)\n",
    "\n",
    "for i, idx in enumerate(indices):\n",
    "    axes[i].imshow(X_test_fashion_norm[idx], cmap='gray')\n",
    "    true_label = fashion_class_names[y_test_fashion[idx]]\n",
    "    pred_label = fashion_class_names[y_pred_fashion[idx]]\n",
    "    color = 'green' if y_pred_fashion[idx] == y_test_fashion[idx] else 'red'\n",
    "    axes[i].set_title(f'True: {true_label}\\nPred: {pred_label}', \n",
    "                     fontsize=9, weight='bold', color=color)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Fashion-MNIST Predictions', fontsize=15, weight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Implement Early Stopping\n",
    "\n",
    "**Early stopping** is a regularization technique that stops training when validation loss stops improving.\n",
    "\n",
    "**Tasks**:\n",
    "1. Research Keras callbacks (look up `EarlyStopping`)\n",
    "2. Train a model on MNIST with early stopping\n",
    "3. Set `patience=3` (stop if no improvement for 3 epochs)\n",
    "4. Monitor validation loss\n",
    "5. Compare total epochs with and without early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your solution here\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# TODO: Create EarlyStopping callback\n",
    "# early_stop = EarlyStopping(...)\n",
    "\n",
    "# TODO: Build and compile model\n",
    "\n",
    "# TODO: Train with callbacks=[early_stop]\n",
    "\n",
    "# TODO: Check how many epochs it trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to Exercise 2\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "print(\"IMPLEMENTING EARLY STOPPING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create callback\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',      # Monitor validation loss\n",
    "    patience=3,              # Stop if no improvement for 3 epochs\n",
    "    restore_best_weights=True,  # Restore weights from best epoch\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Build model\n",
    "model_early = models.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(784,)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model_early.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train with early stopping\n",
    "print(\"\\nTraining with early stopping (max 50 epochs)...\")\n",
    "history_early = model_early.fit(\n",
    "    X_train_flat,\n",
    "    y_train_onehot,\n",
    "    epochs=50,  # Set high number\n",
    "    batch_size=128,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "actual_epochs = len(history_early.history['loss'])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"Early stopping triggered!\")\n",
    "print(f\"  Trained for: {actual_epochs} epochs (out of max 50)\")\n",
    "print(f\"  Best validation loss achieved at epoch: {actual_epochs - 3}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Evaluate\n",
    "test_loss_early, test_acc_early = model_early.evaluate(\n",
    "    X_test_flat, y_test_onehot, verbose=0\n",
    ")\n",
    "\n",
    "print(f\"\\nTest Accuracy: {test_acc_early * 100:.2f}%\")\n",
    "\n",
    "# Plot\n",
    "plot_training_history(history_early, \"Training with Early Stopping\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Compare Optimizers\n",
    "\n",
    "Test different optimizers on MNIST and compare their performance:\n",
    "\n",
    "**Optimizers to test**:\n",
    "1. SGD (Stochastic Gradient Descent)\n",
    "2. RMSprop\n",
    "3. Adam\n",
    "4. AdamW (Adam with weight decay)\n",
    "\n",
    "**Tasks**:\n",
    "1. Build identical models for each optimizer\n",
    "2. Train for 10 epochs each\n",
    "3. Track training time\n",
    "4. Compare final accuracy and convergence speed\n",
    "5. Plot loss curves on same graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Your solution here\n",
    "\n",
    "# TODO: Test different optimizers\n",
    "# optimizers = ['sgd', 'rmsprop', 'adam', ...]\n",
    "\n",
    "# TODO: For each optimizer:\n",
    "#   - Build model\n",
    "#   - Compile with optimizer\n",
    "#   - Train and time\n",
    "#   - Record results\n",
    "\n",
    "# TODO: Compare and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to Exercise 3\n",
    "\n",
    "import time\n",
    "\n",
    "print(\"COMPARING OPTIMIZERS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "optimizers = [\n",
    "    ('SGD', keras.optimizers.SGD(learning_rate=0.01)),\n",
    "    ('RMSprop', keras.optimizers.RMSprop(learning_rate=0.001)),\n",
    "    ('Adam', keras.optimizers.Adam(learning_rate=0.001)),\n",
    "]\n",
    "\n",
    "results = []\n",
    "histories = {}\n",
    "\n",
    "for name, optimizer in optimizers:\n",
    "    print(f\"\\nTesting: {name}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Build model\n",
    "    model = models.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=(784,)),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Train and time\n",
    "    start_time = time.time()\n",
    "    history = model.fit(\n",
    "        X_train_flat,\n",
    "        y_train_onehot,\n",
    "        epochs=10,\n",
    "        batch_size=128,\n",
    "        validation_split=0.2,\n",
    "        verbose=0\n",
    "    )\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Evaluate\n",
    "    test_loss, test_acc = model.evaluate(X_test_flat, y_test_onehot, verbose=0)\n",
    "    \n",
    "    results.append({\n",
    "        'name': name,\n",
    "        'test_acc': test_acc,\n",
    "        'time': training_time,\n",
    "        'final_val_loss': history.history['val_loss'][-1]\n",
    "    })\n",
    "    \n",
    "    histories[name] = history\n",
    "    \n",
    "    print(f\"Test Accuracy: {test_acc * 100:.2f}%\")\n",
    "    print(f\"Training Time: {training_time:.2f}s\")\n",
    "\n",
    "# Summary table\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"OPTIMIZER COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Optimizer':<12} | {'Test Acc':<10} | {'Val Loss':<10} | {'Time (s)':<10}\")\n",
    "print(\"-\" * 80)\n",
    "for r in results:\n",
    "    print(f\"{r['name']:<12} | {r['test_acc']*100:>9.2f}% | {r['final_val_loss']:>9.4f} | {r['time']:>9.2f}s\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Plot comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for name, history in histories.items():\n",
    "    ax1.plot(history.history['loss'], label=f'{name} (train)', linewidth=2)\n",
    "    ax2.plot(history.history['val_loss'], label=f'{name} (val)', linewidth=2)\n",
    "\n",
    "ax1.set_xlabel('Epoch', fontsize=12, weight='bold')\n",
    "ax1.set_ylabel('Loss', fontsize=12, weight='bold')\n",
    "ax1.set_title('Training Loss', fontsize=13, weight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.set_xlabel('Epoch', fontsize=12, weight='bold')\n",
    "ax2.set_ylabel('Loss', fontsize=12, weight='bold')\n",
    "ax2.set_title('Validation Loss', fontsize=13, weight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Optimizer Comparison', fontsize=15, weight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "best = max(results, key=lambda x: x['test_acc'])\n",
    "fastest = min(results, key=lambda x: x['time'])\n",
    "\n",
    "print(f\"\\nBest Accuracy: {best['name']} ({best['test_acc']*100:.2f}%)\")\n",
    "print(f\"Fastest Training: {fastest['name']} ({fastest['time']:.2f}s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "Congratulations! You've learned to use TensorFlow and Keras, the industry-standard tools for deep learning. This is a major milestone in your deep learning journey!\n",
    "\n",
    "### Key Accomplishments\n",
    "\n",
    "1. **TensorFlow Fundamentals**\n",
    "   - Understood tensors and operations\n",
    "   - Learned automatic differentiation\n",
    "   - Explored the TensorFlow ecosystem\n",
    "\n",
    "2. **Keras APIs**\n",
    "   - **Sequential API**: Simple, linear architectures\n",
    "   - **Functional API**: Complex, flexible architectures\n",
    "   - Model compilation and training\n",
    "\n",
    "3. **Real-World Application**\n",
    "   - Trained on MNIST (98%+ accuracy)\n",
    "   - Handled Fashion-MNIST (85%+ accuracy)\n",
    "   - Production-ready workflows\n",
    "\n",
    "4. **Best Practices**\n",
    "   - Data preprocessing and normalization\n",
    "   - Model evaluation and metrics\n",
    "   - Saving/loading models\n",
    "   - Using callbacks (early stopping)\n",
    "\n",
    "### Technical Insights\n",
    "\n",
    "**Sequential vs Functional API**:\n",
    "- Sequential: Simple, one input → one output, linear stack\n",
    "- Functional: Flexible, multiple inputs/outputs, complex graphs\n",
    "\n",
    "**Model Compilation**:\n",
    "```python\n",
    "model.compile(\n",
    "    optimizer='adam',                # How to update weights\n",
    "    loss='categorical_crossentropy', # What to minimize\n",
    "    metrics=['accuracy']             # What to track\n",
    ")\n",
    "```\n",
    "\n",
    "**Training**:\n",
    "```python\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=10,                # Number of full passes through data\n",
    "    batch_size=128,           # Samples per gradient update\n",
    "    validation_split=0.2      # Portion for validation\n",
    ")\n",
    "```\n",
    "\n",
    "### Important Concepts\n",
    "\n",
    "- **Epochs**: Complete pass through training data\n",
    "- **Batch Size**: Number of samples processed before weight update\n",
    "- **Validation Split**: Data held out for monitoring overfitting\n",
    "- **Callbacks**: Functions called during training (early stopping, checkpoints)\n",
    "- **One-Hot Encoding**: Converting class labels to binary vectors\n",
    "- **Dropout**: Regularization by randomly dropping neurons\n",
    "\n",
    "### Comparison: NumPy vs Keras\n",
    "\n",
    "| Aspect | NumPy (Module 03) | Keras (Module 04) |\n",
    "|--------|------------------|------------------|\n",
    "| **Code Length** | ~200 lines | ~10 lines |\n",
    "| **Training Speed** | Slow (CPU only) | Fast (GPU support) |\n",
    "| **Debugging** | Full control | Higher level |\n",
    "| **Learning Value** | High (understand internals) | High (practical skills) |\n",
    "| **Production Use** | Research/Education | Industry Standard |\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "Continue your deep learning journey:\n",
    "\n",
    "**Next Modules** (Advanced Topics):\n",
    "- **Module 05**: Convolutional Neural Networks (CNNs)\n",
    "- **Module 06**: Recurrent Neural Networks (RNNs)\n",
    "- **Module 07**: Advanced Architectures (ResNet, Transformers)\n",
    "- **Module 08**: Transfer Learning and Fine-Tuning\n",
    "- **Module 09**: Model Optimization and Deployment\n",
    "\n",
    "**Skills to Develop**:\n",
    "- Hyperparameter tuning\n",
    "- Data augmentation\n",
    "- Regularization techniques\n",
    "- Custom layers and losses\n",
    "- Distributed training\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "**Official Documentation**:\n",
    "- TensorFlow Guide: https://www.tensorflow.org/guide\n",
    "- Keras API Reference: https://keras.io/api/\n",
    "- TensorFlow Tutorials: https://www.tensorflow.org/tutorials\n",
    "\n",
    "**Books**:\n",
    "- \"Deep Learning with Python\" by François Chollet (Keras creator)\n",
    "- \"Hands-On Machine Learning\" by Aurélien Géron\n",
    "\n",
    "**Courses**:\n",
    "- TensorFlow in Practice Specialization (Coursera)\n",
    "- Fast.ai Practical Deep Learning\n",
    "- DeepLearning.AI TensorFlow Developer Certificate\n",
    "\n",
    "**Practice**:\n",
    "- Kaggle competitions and datasets\n",
    "- Build projects: Image classifier, chatbot, recommendation system\n",
    "- Contribute to open-source ML projects\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You've completed the Deep Learning Fundamentals module. You now have:\n",
    "- Theoretical understanding (Modules 00-02)\n",
    "- Implementation skills (Module 03)\n",
    "- Professional tools expertise (Module 04)\n",
    "\n",
    "You're ready to build real-world deep learning applications!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
