{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 01: Perceptrons and Activation Functions\n",
    "\n",
    "**Difficulty**: ⭐⭐ (Intermediate)\n",
    "**Estimated Time**: 45-60 minutes\n",
    "**Prerequisites**: \n",
    "- Module 00: Introduction to Neural Networks\n",
    "- Linear algebra (vectors, matrices, dot products)\n",
    "- Basic calculus (derivatives)\n",
    "- Python and NumPy\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. **Implement** a single perceptron from scratch using NumPy\n",
    "2. **Explain** the concept of linear separability and its limitations\n",
    "3. **Compare** different activation functions (Sigmoid, Tanh, ReLU, LeakyReLU, GELU, Swish)\n",
    "4. **Calculate** derivatives of activation functions for backpropagation\n",
    "5. **Visualize** activation function behaviors and their properties\n",
    "6. **Choose** appropriate activation functions for different scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification, make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Perceptron Model\n",
    "\n",
    "### 2.1 What is a Perceptron?\n",
    "\n",
    "A **perceptron** is the simplest form of a neural network, consisting of a single neuron. Invented by Frank Rosenblatt in 1958, it was one of the first algorithms capable of learning from data.\n",
    "\n",
    "### Mathematical Model\n",
    "\n",
    "Given input vector $\\mathbf{x} = [x_1, x_2, ..., x_n]$:\n",
    "\n",
    "1. **Weighted Sum (Pre-activation)**:\n",
    "   $$z = \\sum_{i=1}^{n} w_i x_i + b = \\mathbf{w}^T \\mathbf{x} + b$$\n",
    "\n",
    "2. **Activation Function**:\n",
    "   $$y = f(z)$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{w}$ = weight vector (learnable)\n",
    "- $b$ = bias term (learnable)\n",
    "- $f$ = activation function\n",
    "- $y$ = output/prediction\n",
    "\n",
    "### Original Perceptron (Step Function)\n",
    "\n",
    "The original perceptron used a step activation:\n",
    "\n",
    "$$f(z) = \\begin{cases} 1 & \\text{if } z \\geq 0 \\\\ 0 & \\text{if } z < 0 \\end{cases}$$\n",
    "\n",
    "This creates a **linear decision boundary** that divides the input space into two regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    \"\"\"\n",
    "    Simple perceptron implementation with step activation function.\n",
    "    \n",
    "    This is the original perceptron algorithm that can learn linearly\n",
    "    separable patterns through iterative weight updates.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_features, learning_rate=0.01, n_iterations=100):\n",
    "        \"\"\"\n",
    "        Initialize perceptron with random weights.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_features : int\n",
    "            Number of input features\n",
    "        learning_rate : float\n",
    "            Step size for weight updates\n",
    "        n_iterations : int\n",
    "            Number of training epochs\n",
    "        \"\"\"\n",
    "        self.lr = learning_rate\n",
    "        self.n_iter = n_iterations\n",
    "        \n",
    "        # Initialize weights and bias to small random values\n",
    "        self.weights = np.random.randn(n_features) * 0.01\n",
    "        self.bias = 0.0\n",
    "        \n",
    "        # Track training history\n",
    "        self.errors = []\n",
    "    \n",
    "    def activation(self, z):\n",
    "        \"\"\"\n",
    "        Step activation function.\n",
    "        Returns 1 if z >= 0, else 0.\n",
    "        \"\"\"\n",
    "        return np.where(z >= 0, 1, 0)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions on input data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Input data\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        predictions : array, shape (n_samples,)\n",
    "            Binary predictions (0 or 1)\n",
    "        \"\"\"\n",
    "        # Compute weighted sum\n",
    "        z = np.dot(X, self.weights) + self.bias\n",
    "        # Apply activation\n",
    "        return self.activation(z)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the perceptron using the perceptron learning rule.\n",
    "        \n",
    "        Update rule: w = w + lr * (y_true - y_pred) * x\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data\n",
    "        y : array-like, shape (n_samples,)\n",
    "            Target labels (0 or 1)\n",
    "        \"\"\"\n",
    "        for iteration in range(self.n_iter):\n",
    "            errors = 0\n",
    "            \n",
    "            # Iterate through each training sample\n",
    "            for xi, target in zip(X, y):\n",
    "                # Make prediction\n",
    "                prediction = self.predict(xi.reshape(1, -1))[0]\n",
    "                \n",
    "                # Calculate error\n",
    "                error = target - prediction\n",
    "                \n",
    "                # Update weights and bias if there's an error\n",
    "                if error != 0:\n",
    "                    self.weights += self.lr * error * xi\n",
    "                    self.bias += self.lr * error\n",
    "                    errors += 1\n",
    "            \n",
    "            # Record number of errors in this epoch\n",
    "            self.errors.append(errors)\n",
    "            \n",
    "            # Early stopping if no errors\n",
    "            if errors == 0:\n",
    "                print(f\"Converged after {iteration + 1} iterations\")\n",
    "                break\n",
    "        \n",
    "        return self\n",
    "\n",
    "# Test the perceptron on a simple dataset\n",
    "print(\"Creating a linearly separable dataset...\")\n",
    "X, y = make_blobs(n_samples=100, centers=2, n_features=2, \n",
    "                  center_box=(-5, 5), random_state=42)\n",
    "\n",
    "# Train perceptron\n",
    "perceptron = Perceptron(n_features=2, learning_rate=0.1, n_iterations=50)\n",
    "perceptron.fit(X, y)\n",
    "\n",
    "# Calculate accuracy\n",
    "predictions = perceptron.predict(X)\n",
    "accuracy = np.mean(predictions == y)\n",
    "print(f\"\\nTraining Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Final weights: {perceptron.weights}\")\n",
    "print(f\"Final bias: {perceptron.bias:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_perceptron_decision_boundary(perceptron, X, y):\n",
    "    \"\"\"\n",
    "    Visualize the perceptron's decision boundary and training data.\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Plot 1: Decision boundary\n",
    "    # Create mesh grid\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                         np.linspace(y_min, y_max, 200))\n",
    "    \n",
    "    # Predict for each point in mesh\n",
    "    Z = perceptron.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot decision boundary\n",
    "    ax1.contourf(xx, yy, Z, alpha=0.3, levels=1, cmap='RdYlBu')\n",
    "    ax1.scatter(X[:, 0], X[:, 1], c=y, edgecolors='black', \n",
    "               cmap='RdYlBu', s=100, linewidth=1.5)\n",
    "    \n",
    "    # Draw the decision boundary line\n",
    "    # Decision boundary: w1*x1 + w2*x2 + b = 0\n",
    "    # Solve for x2: x2 = -(w1*x1 + b) / w2\n",
    "    w1, w2 = perceptron.weights\n",
    "    if w2 != 0:\n",
    "        x_boundary = np.array([x_min, x_max])\n",
    "        y_boundary = -(w1 * x_boundary + perceptron.bias) / w2\n",
    "        ax1.plot(x_boundary, y_boundary, 'k--', linewidth=2, \n",
    "                label='Decision Boundary')\n",
    "    \n",
    "    ax1.set_xlabel('Feature 1', fontsize=12, weight='bold')\n",
    "    ax1.set_ylabel('Feature 2', fontsize=12, weight='bold')\n",
    "    ax1.set_title('Perceptron Decision Boundary', fontsize=13, weight='bold')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Training errors over time\n",
    "    ax2.plot(range(1, len(perceptron.errors) + 1), perceptron.errors, \n",
    "            marker='o', linewidth=2, markersize=6)\n",
    "    ax2.set_xlabel('Epoch', fontsize=12, weight='bold')\n",
    "    ax2.set_ylabel('Number of Errors', fontsize=12, weight='bold')\n",
    "    ax2.set_title('Training Progress', fontsize=13, weight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize results\n",
    "plot_perceptron_decision_boundary(perceptron, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Linear Separability and Limitations\n",
    "\n",
    "### What is Linear Separability?\n",
    "\n",
    "A dataset is **linearly separable** if a straight line (in 2D), plane (in 3D), or hyperplane (in higher dimensions) can perfectly separate the two classes.\n",
    "\n",
    "### The XOR Problem\n",
    "\n",
    "The famous limitation of single-layer perceptrons is their inability to solve the **XOR (exclusive OR)** problem:\n",
    "\n",
    "| X₁ | X₂ | XOR Output |\n",
    "|----|----|------------|\n",
    "| 0  | 0  | 0          |\n",
    "| 0  | 1  | 1          |\n",
    "| 1  | 0  | 1          |\n",
    "| 1  | 1  | 0          |\n",
    "\n",
    "XOR is **not linearly separable** - you cannot draw a single straight line to separate the classes. This limitation was highlighted by Minsky and Papert (1969), leading to the first \"AI winter.\"\n",
    "\n",
    "**Solution**: Multi-layer networks with non-linear activations can solve XOR and other non-linearly separable problems!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the XOR problem\n",
    "print(\"Attempting to learn XOR with a single perceptron...\\n\")\n",
    "\n",
    "# XOR dataset\n",
    "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_xor = np.array([0, 1, 1, 0])\n",
    "\n",
    "# Try to train perceptron on XOR\n",
    "perceptron_xor = Perceptron(n_features=2, learning_rate=0.1, n_iterations=100)\n",
    "perceptron_xor.fit(X_xor, y_xor)\n",
    "\n",
    "# Evaluate\n",
    "predictions = perceptron_xor.predict(X_xor)\n",
    "accuracy = np.mean(predictions == y_xor)\n",
    "\n",
    "print(f\"\\nXOR Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"\\nPredictions vs Actual:\")\n",
    "for i in range(len(X_xor)):\n",
    "    print(f\"Input: {X_xor[i]} | Predicted: {predictions[i]} | Actual: {y_xor[i]} | \"\n",
    "          f\"{'✓' if predictions[i] == y_xor[i] else '✗'}\")\n",
    "\n",
    "# Visualize why it fails\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Plot XOR points\n",
    "colors = ['red' if label == 0 else 'blue' for label in y_xor]\n",
    "ax.scatter(X_xor[:, 0], X_xor[:, 1], c=colors, s=300, \n",
    "          edgecolors='black', linewidth=2, alpha=0.7)\n",
    "\n",
    "# Add labels\n",
    "for i, (x, y, label) in enumerate(zip(X_xor[:, 0], X_xor[:, 1], y_xor)):\n",
    "    ax.annotate(f'({int(x)},{int(y)})\\nClass {label}', \n",
    "               xy=(x, y), xytext=(10, 10), textcoords='offset points',\n",
    "               fontsize=11, weight='bold')\n",
    "\n",
    "ax.set_xlim(-0.5, 1.5)\n",
    "ax.set_ylim(-0.5, 1.5)\n",
    "ax.set_xlabel('X₁', fontsize=13, weight='bold')\n",
    "ax.set_ylabel('X₂', fontsize=13, weight='bold')\n",
    "ax.set_title('XOR Problem: Not Linearly Separable', fontsize=14, weight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.text(0.5, -0.3, 'No single line can separate red from blue!', \n",
    "       ha='center', fontsize=12, style='italic', color='darkred')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Activation Functions\n",
    "\n",
    "Modern neural networks use **differentiable** activation functions instead of the step function. This allows gradient-based optimization (backpropagation).\n",
    "\n",
    "### Why Activation Functions?\n",
    "\n",
    "1. **Introduce Non-linearity**: Without activations, deep networks would just be linear transformations\n",
    "2. **Enable Complex Patterns**: Non-linear activations allow learning of complex decision boundaries\n",
    "3. **Gradient Flow**: Differentiable activations enable backpropagation\n",
    "\n",
    "### Common Activation Functions\n",
    "\n",
    "Let's implement and visualize the most important activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunctions:\n",
    "    \"\"\"\n",
    "    Collection of common activation functions and their derivatives.\n",
    "    Each function is implemented as both forward and backward (derivative).\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "        \"\"\"\n",
    "        Sigmoid (Logistic) activation: σ(z) = 1 / (1 + e^(-z))\n",
    "        \n",
    "        Properties:\n",
    "        - Range: (0, 1)\n",
    "        - S-shaped curve\n",
    "        - Often used in output layer for binary classification\n",
    "        - Problem: Vanishing gradients for large |z|\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid_derivative(z):\n",
    "        \"\"\"\n",
    "        Derivative: σ'(z) = σ(z) * (1 - σ(z))\n",
    "        \"\"\"\n",
    "        s = ActivationFunctions.sigmoid(z)\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh(z):\n",
    "        \"\"\"\n",
    "        Hyperbolic Tangent: tanh(z) = (e^z - e^(-z)) / (e^z + e^(-z))\n",
    "        \n",
    "        Properties:\n",
    "        - Range: (-1, 1)\n",
    "        - Zero-centered (better than sigmoid)\n",
    "        - Still suffers from vanishing gradients\n",
    "        \"\"\"\n",
    "        return np.tanh(z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh_derivative(z):\n",
    "        \"\"\"\n",
    "        Derivative: tanh'(z) = 1 - tanh²(z)\n",
    "        \"\"\"\n",
    "        return 1 - np.tanh(z) ** 2\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu(z):\n",
    "        \"\"\"\n",
    "        Rectified Linear Unit: ReLU(z) = max(0, z)\n",
    "        \n",
    "        Properties:\n",
    "        - Range: [0, ∞)\n",
    "        - Most popular in hidden layers\n",
    "        - Computationally efficient\n",
    "        - Helps with vanishing gradient problem\n",
    "        - Problem: \"Dying ReLU\" (neurons can stop learning)\n",
    "        \"\"\"\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu_derivative(z):\n",
    "        \"\"\"\n",
    "        Derivative: ReLU'(z) = 1 if z > 0, else 0\n",
    "        \"\"\"\n",
    "        return np.where(z > 0, 1, 0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def leaky_relu(z, alpha=0.01):\n",
    "        \"\"\"\n",
    "        Leaky ReLU: max(αz, z)\n",
    "        \n",
    "        Properties:\n",
    "        - Allows small gradient when z < 0\n",
    "        - Prevents dying ReLU problem\n",
    "        - α typically set to 0.01\n",
    "        \"\"\"\n",
    "        return np.where(z > 0, z, alpha * z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def leaky_relu_derivative(z, alpha=0.01):\n",
    "        \"\"\"\n",
    "        Derivative: 1 if z > 0, else α\n",
    "        \"\"\"\n",
    "        return np.where(z > 0, 1, alpha)\n",
    "    \n",
    "    @staticmethod\n",
    "    def gelu(z):\n",
    "        \"\"\"\n",
    "        Gaussian Error Linear Unit (GELU)\n",
    "        Approximation: GELU(z) ≈ 0.5z(1 + tanh[√(2/π)(z + 0.044715z³)])\n",
    "        \n",
    "        Properties:\n",
    "        - Smooth approximation to ReLU\n",
    "        - Used in transformers (BERT, GPT)\n",
    "        - Better performance in many tasks\n",
    "        \"\"\"\n",
    "        return 0.5 * z * (1 + np.tanh(np.sqrt(2 / np.pi) * \n",
    "                                       (z + 0.044715 * z**3)))\n",
    "    \n",
    "    @staticmethod\n",
    "    def swish(z, beta=1.0):\n",
    "        \"\"\"\n",
    "        Swish (also called SiLU): z * sigmoid(βz)\n",
    "        \n",
    "        Properties:\n",
    "        - Smooth, non-monotonic\n",
    "        - Self-gated activation\n",
    "        - Used in EfficientNet and other modern architectures\n",
    "        \"\"\"\n",
    "        return z * ActivationFunctions.sigmoid(beta * z)\n",
    "\n",
    "# Create instance for easy access\n",
    "act = ActivationFunctions()\n",
    "\n",
    "print(\"Activation functions loaded successfully!\")\n",
    "print(\"\\nAvailable functions:\")\n",
    "print(\"  - Sigmoid\")\n",
    "print(\"  - Tanh\")\n",
    "print(\"  - ReLU\")\n",
    "print(\"  - Leaky ReLU\")\n",
    "print(\"  - GELU\")\n",
    "print(\"  - Swish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize all activation functions\n",
    "\n",
    "z = np.linspace(-5, 5, 1000)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Define activations to plot\n",
    "activations = [\n",
    "    ('Sigmoid', act.sigmoid, act.sigmoid_derivative),\n",
    "    ('Tanh', act.tanh, act.tanh_derivative),\n",
    "    ('ReLU', act.relu, act.relu_derivative),\n",
    "    ('Leaky ReLU', act.leaky_relu, act.leaky_relu_derivative),\n",
    "    ('GELU', act.gelu, None),\n",
    "    ('Swish', act.swish, None)\n",
    "]\n",
    "\n",
    "for idx, (name, func, deriv_func) in enumerate(activations):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Plot activation function\n",
    "    y = func(z)\n",
    "    ax.plot(z, y, linewidth=2.5, label=f'{name}', color='blue')\n",
    "    \n",
    "    # Plot derivative if available\n",
    "    if deriv_func is not None:\n",
    "        dy = deriv_func(z)\n",
    "        ax.plot(z, dy, linewidth=2, linestyle='--', \n",
    "               label=f\"{name}'\", color='red', alpha=0.7)\n",
    "    \n",
    "    # Styling\n",
    "    ax.axhline(y=0, color='black', linewidth=0.8, alpha=0.3)\n",
    "    ax.axvline(x=0, color='black', linewidth=0.8, alpha=0.3)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlabel('z (input)', fontsize=11, weight='bold')\n",
    "    ax.set_ylabel('f(z) (output)', fontsize=11, weight='bold')\n",
    "    ax.set_title(name, fontsize=13, weight='bold')\n",
    "    ax.legend(loc='best', fontsize=10)\n",
    "    ax.set_xlim(-5, 5)\n",
    "\n",
    "plt.suptitle('Activation Functions and Their Derivatives', \n",
    "            fontsize=16, weight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Choosing Activation Functions\n",
    "\n",
    "### Decision Guide\n",
    "\n",
    "**Hidden Layers:**\n",
    "- **ReLU** (default choice): Fast, works well in most cases\n",
    "- **Leaky ReLU**: When you suspect dying ReLU problem\n",
    "- **GELU/Swish**: For transformers or when you want smooth non-linearity\n",
    "- **Tanh**: When you need zero-centered outputs\n",
    "\n",
    "**Output Layer:**\n",
    "- **Sigmoid**: Binary classification (probability output 0-1)\n",
    "- **Softmax**: Multi-class classification (probability distribution)\n",
    "- **Linear (no activation)**: Regression problems\n",
    "- **Tanh**: When output should be in range (-1, 1)\n",
    "\n",
    "### Comparison Table\n",
    "\n",
    "| Activation | Range | Advantages | Disadvantages | Use Cases |\n",
    "|-----------|-------|------------|---------------|----------|\n",
    "| **Sigmoid** | (0, 1) | Smooth, interpretable as probability | Vanishing gradients, not zero-centered | Output layer (binary) |\n",
    "| **Tanh** | (-1, 1) | Zero-centered | Vanishing gradients | Hidden layers (when zero-centered needed) |\n",
    "| **ReLU** | [0, ∞) | Fast, no vanishing gradient | Dying ReLU, not zero-centered | Hidden layers (default) |\n",
    "| **Leaky ReLU** | (-∞, ∞) | Fixes dying ReLU | Small negative slope may not help much | Hidden layers (alternative to ReLU) |\n",
    "| **GELU** | (-∞, ∞) | Smooth, state-of-the-art performance | Computationally expensive | Transformers, modern architectures |\n",
    "| **Swish** | (-∞, ∞) | Self-gated, smooth | Computationally expensive | Modern CNNs, when performance matters |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare gradient flow for different activations\n",
    "\n",
    "def compare_gradients():\n",
    "    \"\"\"\n",
    "    Visualize how gradients behave for different activation functions.\n",
    "    This is crucial for understanding the vanishing/exploding gradient problem.\n",
    "    \"\"\"\n",
    "    z_range = np.linspace(-6, 6, 1000)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot 1: Gradients comparison\n",
    "    ax1.plot(z_range, act.sigmoid_derivative(z_range), \n",
    "            label='Sigmoid', linewidth=2.5)\n",
    "    ax1.plot(z_range, act.tanh_derivative(z_range), \n",
    "            label='Tanh', linewidth=2.5)\n",
    "    ax1.plot(z_range, act.relu_derivative(z_range), \n",
    "            label='ReLU', linewidth=2.5)\n",
    "    ax1.plot(z_range, act.leaky_relu_derivative(z_range), \n",
    "            label='Leaky ReLU', linewidth=2.5)\n",
    "    \n",
    "    ax1.axhline(y=1, color='green', linestyle=':', linewidth=2, \n",
    "               alpha=0.5, label='Ideal (gradient = 1)')\n",
    "    ax1.axhline(y=0, color='black', linewidth=1, alpha=0.3)\n",
    "    ax1.axvline(x=0, color='black', linewidth=1, alpha=0.3)\n",
    "    \n",
    "    ax1.set_xlabel('z (pre-activation)', fontsize=12, weight='bold')\n",
    "    ax1.set_ylabel('Gradient (df/dz)', fontsize=12, weight='bold')\n",
    "    ax1.set_title('Gradient Flow Comparison', fontsize=14, weight='bold')\n",
    "    ax1.legend(loc='best', fontsize=11)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_ylim(-0.1, 1.2)\n",
    "    \n",
    "    # Plot 2: Vanishing gradient problem\n",
    "    # Simulate gradient through multiple layers\n",
    "    n_layers = 10\n",
    "    z_test = np.array([4.0])  # Large activation value\n",
    "    \n",
    "    sigmoid_grads = [1.0]\n",
    "    tanh_grads = [1.0]\n",
    "    relu_grads = [1.0]\n",
    "    \n",
    "    for layer in range(n_layers):\n",
    "        # Multiply gradients (chain rule)\n",
    "        sigmoid_grads.append(sigmoid_grads[-1] * \n",
    "                           act.sigmoid_derivative(z_test)[0])\n",
    "        tanh_grads.append(tanh_grads[-1] * \n",
    "                        act.tanh_derivative(z_test)[0])\n",
    "        relu_grads.append(relu_grads[-1] * \n",
    "                        act.relu_derivative(z_test)[0])\n",
    "    \n",
    "    layers = range(n_layers + 1)\n",
    "    ax2.semilogy(layers, sigmoid_grads, 'o-', label='Sigmoid', \n",
    "                linewidth=2.5, markersize=6)\n",
    "    ax2.semilogy(layers, tanh_grads, 's-', label='Tanh', \n",
    "                linewidth=2.5, markersize=6)\n",
    "    ax2.semilogy(layers, relu_grads, '^-', label='ReLU', \n",
    "                linewidth=2.5, markersize=6)\n",
    "    \n",
    "    ax2.set_xlabel('Layer Depth', fontsize=12, weight='bold')\n",
    "    ax2.set_ylabel('Gradient Magnitude (log scale)', fontsize=12, weight='bold')\n",
    "    ax2.set_title('Vanishing Gradient Problem\\n(z=4.0, propagating backward)', \n",
    "                 fontsize=14, weight='bold')\n",
    "    ax2.legend(loc='best', fontsize=11)\n",
    "    ax2.grid(True, alpha=0.3, which='both')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Gradient Analysis:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"After {n_layers} layers (with z=4.0):\")\n",
    "    print(f\"  Sigmoid gradient: {sigmoid_grads[-1]:.2e} (vanishing!)\")\n",
    "    print(f\"  Tanh gradient: {tanh_grads[-1]:.2e} (vanishing!)\")\n",
    "    print(f\"  ReLU gradient: {relu_grads[-1]:.2e} (preserved!)\")\n",
    "    print(\"\\nThis demonstrates why ReLU helps with deep networks!\")\n",
    "\n",
    "compare_gradients()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Implement Custom Activation\n",
    "\n",
    "Implement the **ELU (Exponential Linear Unit)** activation function:\n",
    "\n",
    "$$\\text{ELU}(z) = \\begin{cases} z & \\text{if } z > 0 \\\\ \\alpha(e^z - 1) & \\text{if } z \\leq 0 \\end{cases}$$\n",
    "\n",
    "Where $\\alpha$ is typically set to 1.0.\n",
    "\n",
    "**Properties**:\n",
    "- Smooth function\n",
    "- Negative values push mean activation closer to zero\n",
    "- Helps with vanishing gradient\n",
    "\n",
    "Implement both the function and its derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your solution here\n",
    "\n",
    "def elu(z, alpha=1.0):\n",
    "    \"\"\"\n",
    "    Implement ELU activation function.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    z : array-like\n",
    "        Input values\n",
    "    alpha : float\n",
    "        Scaling parameter (default=1.0)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    output : array-like\n",
    "        ELU(z)\n",
    "    \"\"\"\n",
    "    # TODO: Implement ELU\n",
    "    pass\n",
    "\n",
    "def elu_derivative(z, alpha=1.0):\n",
    "    \"\"\"\n",
    "    Derivative of ELU.\n",
    "    \n",
    "    Hint: \n",
    "    - For z > 0: derivative is 1\n",
    "    - For z <= 0: derivative is ELU(z) + alpha\n",
    "    \"\"\"\n",
    "    # TODO: Implement ELU derivative\n",
    "    pass\n",
    "\n",
    "# Test your implementation\n",
    "z_test = np.array([-2, -1, 0, 1, 2])\n",
    "print(\"Test values:\", z_test)\n",
    "print(\"ELU output:\", elu(z_test))\n",
    "print(\"ELU derivative:\", elu_derivative(z_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to Exercise 1\n",
    "\n",
    "def elu(z, alpha=1.0):\n",
    "    \"\"\"ELU activation function.\"\"\"\n",
    "    return np.where(z > 0, z, alpha * (np.exp(z) - 1))\n",
    "\n",
    "def elu_derivative(z, alpha=1.0):\n",
    "    \"\"\"Derivative of ELU.\"\"\"\n",
    "    return np.where(z > 0, 1, elu(z, alpha) + alpha)\n",
    "\n",
    "# Test and visualize\n",
    "z_range = np.linspace(-5, 5, 1000)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot ELU vs ReLU\n",
    "ax1.plot(z_range, elu(z_range), label='ELU', linewidth=2.5)\n",
    "ax1.plot(z_range, act.relu(z_range), label='ReLU', \n",
    "        linewidth=2.5, linestyle='--', alpha=0.7)\n",
    "ax1.axhline(y=0, color='black', linewidth=0.8, alpha=0.3)\n",
    "ax1.axvline(x=0, color='black', linewidth=0.8, alpha=0.3)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xlabel('z', fontsize=12, weight='bold')\n",
    "ax1.set_ylabel('f(z)', fontsize=12, weight='bold')\n",
    "ax1.set_title('ELU vs ReLU', fontsize=13, weight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "\n",
    "# Plot derivatives\n",
    "ax2.plot(z_range, elu_derivative(z_range), label=\"ELU'\", linewidth=2.5)\n",
    "ax2.plot(z_range, act.relu_derivative(z_range), label=\"ReLU'\", \n",
    "        linewidth=2.5, linestyle='--', alpha=0.7)\n",
    "ax2.axhline(y=0, color='black', linewidth=0.8, alpha=0.3)\n",
    "ax2.axvline(x=0, color='black', linewidth=0.8, alpha=0.3)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xlabel('z', fontsize=12, weight='bold')\n",
    "ax2.set_ylabel(\"f'(z)\", fontsize=12, weight='bold')\n",
    "ax2.set_title('Derivatives', fontsize=13, weight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey difference: ELU has smooth negative values, while ReLU is zero.\")\n",
    "print(\"This helps push mean activations closer to zero!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Perceptron on Real Data\n",
    "\n",
    "Train a perceptron on a linearly separable subset of the Iris dataset.\n",
    "\n",
    "**Tasks**:\n",
    "1. Create a binary classification problem (select 2 classes from Iris)\n",
    "2. Use only 2 features for visualization\n",
    "3. Train the perceptron\n",
    "4. Visualize the decision boundary\n",
    "5. Calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your solution here\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X_full = iris.data\n",
    "y_full = iris.target\n",
    "\n",
    "# TODO: Select only classes 0 and 1 (setosa and versicolor)\n",
    "# Hint: Use boolean indexing with (y_full == 0) | (y_full == 1)\n",
    "\n",
    "# TODO: Select only features 2 and 3 (petal length and petal width)\n",
    "\n",
    "# TODO: Standardize features (important for perceptron!)\n",
    "# Use StandardScaler from sklearn\n",
    "\n",
    "# TODO: Train perceptron\n",
    "\n",
    "# TODO: Calculate and print accuracy\n",
    "\n",
    "# TODO: Visualize decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to Exercise 2\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load and prepare data\n",
    "iris = load_iris()\n",
    "X_full = iris.data\n",
    "y_full = iris.target\n",
    "\n",
    "# Select only setosa (0) and versicolor (1)\n",
    "mask = (y_full == 0) | (y_full == 1)\n",
    "X = X_full[mask][:, 2:]  # Petal length and width\n",
    "y = y_full[mask]\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Class distribution: {np.bincount(y)}\")\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train perceptron\n",
    "print(\"\\nTraining perceptron...\")\n",
    "perceptron_iris = Perceptron(n_features=2, learning_rate=0.01, n_iterations=50)\n",
    "perceptron_iris.fit(X_scaled, y)\n",
    "\n",
    "# Evaluate\n",
    "predictions = perceptron_iris.predict(X_scaled)\n",
    "accuracy = np.mean(predictions == y)\n",
    "print(f\"\\nAccuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Visualize\n",
    "plot_perceptron_decision_boundary(perceptron_iris, X_scaled, y)\n",
    "\n",
    "# Add feature names\n",
    "plt.figure(figsize=(10, 6))\n",
    "for class_val in [0, 1]:\n",
    "    mask = y == class_val\n",
    "    plt.scatter(X_scaled[mask, 0], X_scaled[mask, 1], \n",
    "               label=iris.target_names[class_val],\n",
    "               s=100, edgecolors='black', linewidth=1.5, alpha=0.7)\n",
    "\n",
    "# Draw decision boundary\n",
    "x_min, x_max = X_scaled[:, 0].min() - 1, X_scaled[:, 0].max() + 1\n",
    "if perceptron_iris.weights[1] != 0:\n",
    "    x_boundary = np.array([x_min, x_max])\n",
    "    y_boundary = -(perceptron_iris.weights[0] * x_boundary + \n",
    "                   perceptron_iris.bias) / perceptron_iris.weights[1]\n",
    "    plt.plot(x_boundary, y_boundary, 'k--', linewidth=2, \n",
    "            label='Decision Boundary')\n",
    "\n",
    "plt.xlabel('Petal Length (standardized)', fontsize=12, weight='bold')\n",
    "plt.ylabel('Petal Width (standardized)', fontsize=12, weight='bold')\n",
    "plt.title(f'Perceptron on Iris Dataset (Accuracy: {accuracy*100:.1f}%)', \n",
    "         fontsize=14, weight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Activation Function Selection\n",
    "\n",
    "For each scenario below, choose the most appropriate activation function and explain why:\n",
    "\n",
    "1. **Hidden layer in a deep CNN for image classification (100 layers deep)**\n",
    "2. **Output layer for predicting house prices (regression)**\n",
    "3. **Output layer for binary classification (spam detection)**\n",
    "4. **Hidden layer in a transformer model for language understanding**\n",
    "5. **Output layer for multi-class classification (10 classes)**\n",
    "\n",
    "Write your answers in the markdown cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Answers to Exercise 3:**\n",
    "\n",
    "*(Double-click to edit)*\n",
    "\n",
    "1. **Deep CNN hidden layer**:\n",
    "   - Activation: \n",
    "   - Reason:\n",
    "\n",
    "2. **House price regression output**:\n",
    "   - Activation:\n",
    "   - Reason:\n",
    "\n",
    "3. **Binary classification output**:\n",
    "   - Activation:\n",
    "   - Reason:\n",
    "\n",
    "4. **Transformer hidden layer**:\n",
    "   - Activation:\n",
    "   - Reason:\n",
    "\n",
    "5. **Multi-class classification output**:\n",
    "   - Activation:\n",
    "   - Reason:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solutions to Exercise 3\n",
    "\n",
    "print(\"ACTIVATION FUNCTION SELECTION - SOLUTIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n1. Deep CNN Hidden Layer (100 layers):\")\n",
    "print(\"   Activation: ReLU or Leaky ReLU\")\n",
    "print(\"   Reason: Prevents vanishing gradients in very deep networks.\")\n",
    "print(\"           Fast computation. Leaky ReLU prevents dying ReLU problem.\")\n",
    "print(\"           Proven to work well in deep CNNs like ResNet, VGG.\")\n",
    "\n",
    "print(\"\\n2. House Price Regression Output:\")\n",
    "print(\"   Activation: None (Linear)\")\n",
    "print(\"   Reason: Regression requires predicting continuous values without\")\n",
    "print(\"           bounds. Linear activation allows any real number output.\")\n",
    "print(\"           Prices can range from low to very high values.\")\n",
    "\n",
    "print(\"\\n3. Binary Classification Output (Spam Detection):\")\n",
    "print(\"   Activation: Sigmoid\")\n",
    "print(\"   Reason: Outputs probability between 0 and 1.\")\n",
    "print(\"           Perfect for binary classification.\")\n",
    "print(\"           Can interpret as P(spam) - threshold at 0.5.\")\n",
    "print(\"           Works with binary cross-entropy loss.\")\n",
    "\n",
    "print(\"\\n4. Transformer Hidden Layer:\")\n",
    "print(\"   Activation: GELU\")\n",
    "print(\"   Reason: Standard in transformers (BERT, GPT use GELU).\")\n",
    "print(\"           Smooth approximation to ReLU with better performance.\")\n",
    "print(\"           Empirically shown to work better in attention-based models.\")\n",
    "print(\"           Allows probabilistic interpretation of neuron activation.\")\n",
    "\n",
    "print(\"\\n5. Multi-class Classification Output (10 classes):\")\n",
    "print(\"   Activation: Softmax\")\n",
    "print(\"   Reason: Converts logits to probability distribution over classes.\")\n",
    "print(\"           Ensures outputs sum to 1.0.\")\n",
    "print(\"           Works with categorical cross-entropy loss.\")\n",
    "print(\"           Provides interpretable class probabilities.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "Excellent work! You've completed the perceptron and activation functions module. Let's recap:\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Perceptron**\n",
    "   - Simplest neural network (single neuron)\n",
    "   - Linear model: $y = f(\\mathbf{w}^T\\mathbf{x} + b)$\n",
    "   - Can learn linearly separable patterns\n",
    "   - Cannot solve XOR (not linearly separable)\n",
    "\n",
    "2. **Linear Separability**\n",
    "   - Determines if single perceptron can solve problem\n",
    "   - XOR is classic example of non-linear problem\n",
    "   - Multi-layer networks needed for complex patterns\n",
    "\n",
    "3. **Activation Functions**\n",
    "   - Introduce non-linearity to networks\n",
    "   - Enable learning of complex patterns\n",
    "   - Must be differentiable for backpropagation\n",
    "\n",
    "4. **Common Activations**\n",
    "   - **Sigmoid**: (0, 1), good for probabilities, vanishing gradients\n",
    "   - **Tanh**: (-1, 1), zero-centered, still vanishing gradients\n",
    "   - **ReLU**: [0, ∞), default choice, fast, prevents vanishing gradients\n",
    "   - **Leaky ReLU**: Prevents dying ReLU problem\n",
    "   - **GELU**: Smooth, used in transformers\n",
    "   - **Swish**: Self-gated, used in modern architectures\n",
    "\n",
    "5. **Selection Guide**\n",
    "   - Hidden layers: ReLU (default), GELU (transformers)\n",
    "   - Binary output: Sigmoid\n",
    "   - Multi-class output: Softmax\n",
    "   - Regression output: Linear (no activation)\n",
    "\n",
    "### Important Insights\n",
    "\n",
    "- **Vanishing Gradients**: Sigmoid/Tanh derivatives become very small for large |z|, making deep networks hard to train\n",
    "- **ReLU Advantage**: Constant gradient of 1 for positive inputs prevents vanishing gradients\n",
    "- **Dying ReLU**: Neurons can \"die\" if they always output 0; Leaky ReLU helps\n",
    "- **Modern Trends**: GELU and Swish gaining popularity for better performance\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In **Module 02: Backpropagation and Gradient Descent**, we'll learn:\n",
    "- How neural networks actually learn (optimization)\n",
    "- Forward and backward propagation in detail\n",
    "- Computing gradients using the chain rule\n",
    "- Loss functions and their derivatives\n",
    "- Gradient descent variants\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "**Papers**:\n",
    "- Rosenblatt (1958): \"The Perceptron: A Probabilistic Model\"\n",
    "- Glorot et al. (2011): \"Deep Sparse Rectifier Neural Networks\" (ReLU)\n",
    "- Hendrycks & Gimpel (2016): \"Gaussian Error Linear Units (GELUs)\"\n",
    "\n",
    "**Interactive**:\n",
    "- TensorFlow Playground: Visualize activation effects\n",
    "- Distill.pub: \"Activation Atlas\" for understanding activations\n",
    "\n",
    "---\n",
    "\n",
    "**Ready to learn how neural networks optimize?** Continue to **Module 02: Backpropagation and Gradient Descent**!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
