{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 13: PyTorch Introduction (Comparison with TensorFlow)\n",
    "\n",
    "**Difficulty**: ⭐⭐ (Intermediate)\n",
    "\n",
    "**Estimated Time**: 45-60 minutes\n",
    "\n",
    "**Prerequisites**: \n",
    "- [Module 04: Introduction to TensorFlow/Keras](04_introduction_to_tensorflow_keras.ipynb)\n",
    "- [Module 05: Feed-Forward Neural Networks with Keras](05_feedforward_neural_networks_keras.ipynb)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Understand PyTorch tensors and operations\n",
    "2. Use automatic differentiation with PyTorch's autograd\n",
    "3. Build neural networks using nn.Module\n",
    "4. Implement training loops in PyTorch\n",
    "5. Compare PyTorch and TensorFlow/Keras workflows\n",
    "6. Decide when to use each framework based on 2025 best practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to PyTorch\n",
    "\n",
    "**PyTorch** is a deep learning framework developed by Meta (Facebook). It's known for:\n",
    "- **Dynamic computation graphs**: Build graphs on-the-fly\n",
    "- **Pythonic API**: Feels natural to Python developers\n",
    "- **Research-friendly**: Easy to experiment and debug\n",
    "- **Strong ecosystem**: Wide adoption in research community\n",
    "\n",
    "### PyTorch vs TensorFlow (2025 Perspective)\n",
    "\n",
    "| Aspect | PyTorch | TensorFlow/Keras |\n",
    "|--------|---------|------------------|\n",
    "| **Learning Curve** | Steeper initially | Easier with Keras high-level API |\n",
    "| **Flexibility** | Very flexible, explicit | More abstraction, less boilerplate |\n",
    "| **Research** | Dominant in research (70%+ papers) | Growing research adoption |\n",
    "| **Production** | Improving (TorchServe, TorchScript) | Mature (TF Serving, TFLite) |\n",
    "| **Debugging** | Easy (native Python debugging) | Improved with eager execution |\n",
    "| **Community** | Strong research community | Strong production community |\n",
    "\n",
    "### When to Use Each (2025 Guidelines)\n",
    "\n",
    "**Use PyTorch when**:\n",
    "- Research and experimentation\n",
    "- Custom architectures and training loops\n",
    "- Need maximum flexibility\n",
    "- Working with research papers (most use PyTorch)\n",
    "\n",
    "**Use TensorFlow/Keras when**:\n",
    "- Production deployment at scale\n",
    "- Mobile/edge deployment (TFLite)\n",
    "- Quick prototyping with standard architectures\n",
    "- Need extensive ecosystem (TF Extended, TF Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# TensorFlow for comparison\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Standard libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from time import time\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Plotting configuration\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PyTorch Tensors\n",
    "\n",
    "**Tensors** are the fundamental data structure in PyTorch, similar to NumPy arrays but with GPU support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Creating tensors\n",
    "print(\"=\" * 60)\n",
    "print(\"Creating Tensors\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# From Python list\n",
    "tensor_from_list = torch.tensor([1, 2, 3, 4])\n",
    "print(f\"From list: {tensor_from_list}\")\n",
    "\n",
    "# From NumPy array\n",
    "numpy_array = np.array([[1, 2], [3, 4]])\n",
    "tensor_from_numpy = torch.from_numpy(numpy_array)\n",
    "print(f\"From NumPy:\\n{tensor_from_numpy}\")\n",
    "\n",
    "# Special tensors\n",
    "zeros = torch.zeros(2, 3)\n",
    "ones = torch.ones(2, 3)\n",
    "random = torch.randn(2, 3)  # Random normal distribution\n",
    "\n",
    "print(f\"\\nZeros:\\n{zeros}\")\n",
    "print(f\"\\nOnes:\\n{ones}\")\n",
    "print(f\"\\nRandom (normal):\\n{random}\")\n",
    "\n",
    "# Tensor properties\n",
    "print(f\"\\nShape: {random.shape}\")\n",
    "print(f\"Data type: {random.dtype}\")\n",
    "print(f\"Device: {random.device}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Tensor operations\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Tensor Operations\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "a = torch.tensor([[1., 2.], [3., 4.]])\n",
    "b = torch.tensor([[5., 6.], [7., 8.]])\n",
    "\n",
    "# Element-wise operations\n",
    "print(f\"Addition:\\n{a + b}\")\n",
    "print(f\"\\nMultiplication (element-wise):\\n{a * b}\")\n",
    "\n",
    "# Matrix multiplication\n",
    "print(f\"\\nMatrix multiplication:\\n{torch.matmul(a, b)}\")\n",
    "# Or using @ operator\n",
    "print(f\"\\nUsing @ operator:\\n{a @ b}\")\n",
    "\n",
    "# Reshaping\n",
    "x = torch.randn(4, 3)\n",
    "print(f\"\\nOriginal shape: {x.shape}\")\n",
    "print(f\"Reshaped to (2, 6): {x.view(2, 6).shape}\")\n",
    "print(f\"Flattened: {x.view(-1).shape}\")  # -1 infers dimension"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# PyTorch vs NumPy comparison\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PyTorch vs NumPy\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# NumPy way\n",
    "numpy_array = np.random.randn(3, 3)\n",
    "print(f\"NumPy array:\\n{numpy_array}\")\n",
    "\n",
    "# PyTorch way\n",
    "pytorch_tensor = torch.randn(3, 3)\n",
    "print(f\"\\nPyTorch tensor:\\n{pytorch_tensor}\")\n",
    "\n",
    "# Converting between PyTorch and NumPy\n",
    "tensor = torch.tensor([1, 2, 3])\n",
    "numpy_from_tensor = tensor.numpy()\n",
    "print(f\"\\nTensor to NumPy: {numpy_from_tensor}\")\n",
    "\n",
    "array = np.array([4, 5, 6])\n",
    "tensor_from_numpy = torch.from_numpy(array)\n",
    "print(f\"NumPy to Tensor: {tensor_from_numpy}\")\n",
    "\n",
    "print(\"\\n⚠️  Note: Conversions share memory! Modifying one affects the other.\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Automatic Differentiation with Autograd\n",
    "\n",
    "**Autograd** is PyTorch's automatic differentiation engine. It tracks operations on tensors and computes gradients automatically.\n",
    "\n",
    "**Key Concept**: Set `requires_grad=True` to track computations for backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Simple autograd example\n",
    "print(\"Simple Autograd Example\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a tensor with gradient tracking\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "print(f\"x = {x}\")\n",
    "\n",
    "# Perform operations\n",
    "y = x ** 2 + 3 * x + 1\n",
    "print(f\"y = x² + 3x + 1 = {y}\")\n",
    "\n",
    "# Compute gradient dy/dx\n",
    "y.backward()  # Compute gradients\n",
    "\n",
    "print(f\"\\nGradient dy/dx = 2x + 3 = {x.grad}\")\n",
    "print(f\"Analytical result at x=2: {2*2 + 3} ✓\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# More complex example with multiple variables\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Autograd with Multiple Variables\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create tensors\n",
    "a = torch.tensor([3.0], requires_grad=True)\n",
    "b = torch.tensor([4.0], requires_grad=True)\n",
    "\n",
    "# Complex operation\n",
    "c = a * b\n",
    "d = c + a ** 2\n",
    "e = torch.sigmoid(d)  # Sigmoid activation\n",
    "\n",
    "print(f\"a = {a.item()}, b = {b.item()}\")\n",
    "print(f\"c = a * b = {c.item()}\")\n",
    "print(f\"d = c + a² = {d.item()}\")\n",
    "print(f\"e = sigmoid(d) = {e.item()}\")\n",
    "\n",
    "# Backpropagate\n",
    "e.backward()\n",
    "\n",
    "print(f\"\\nGradients:\")\n",
    "print(f\"de/da = {a.grad.item():.4f}\")\n",
    "print(f\"de/db = {b.grad.item():.4f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Gradient accumulation and zeroing\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Gradient Accumulation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "x = torch.tensor([1.0], requires_grad=True)\n",
    "\n",
    "# First computation\n",
    "y1 = x ** 2\n",
    "y1.backward()\n",
    "print(f\"After first backward: x.grad = {x.grad}\")\n",
    "\n",
    "# Second computation (gradients accumulate!)\n",
    "y2 = x ** 3\n",
    "y2.backward()\n",
    "print(f\"After second backward (accumulated): x.grad = {x.grad}\")\n",
    "\n",
    "# Zero gradients before next computation\n",
    "x.grad.zero_()\n",
    "print(f\"After zeroing: x.grad = {x.grad}\")\n",
    "\n",
    "print(\"\\n⚠️  Important: Always zero gradients before backward pass in training!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Building Neural Networks with nn.Module\n",
    "\n",
    "PyTorch uses **nn.Module** as the base class for all neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define a simple neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple feed-forward neural network.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        \n",
    "        # Define layers\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor\n",
    "        \n",
    "        Returns:\n",
    "            Output tensor\n",
    "        \"\"\"\n",
    "        # First hidden layer\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Second hidden layer\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Output layer (no activation, will use CrossEntropyLoss)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create model instance\n",
    "model_pytorch = SimpleNN(input_size=784, hidden_size=128, output_size=10)\n",
    "\n",
    "print(\"PyTorch Model Architecture:\")\n",
    "print(model_pytorch)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model_pytorch.parameters())\n",
    "trainable_params = sum(p.numel() for p in model_pytorch.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Equivalent Keras model for comparison\n",
    "def create_keras_model():\n",
    "    \"\"\"Create equivalent model in Keras.\"\"\"\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.InputLayer(input_shape=(784,)),\n",
    "        keras.layers.Dense(128, activation='relu'),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(128, activation='relu'),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model_keras = create_keras_model()\n",
    "\n",
    "print(\"\\nKeras Model Architecture:\")\n",
    "model_keras.summary()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Key Differences:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"PyTorch:\")\n",
    "print(\"  - Explicit forward() method\")\n",
    "print(\"  - Activations typically in forward(), not layer definition\")\n",
    "print(\"  - More control, more code\")\n",
    "print(\"\\nKeras:\")\n",
    "print(\"  - Sequential API is more concise\")\n",
    "print(\"  - Activations specified in layer definition\")\n",
    "print(\"  - Less code, less flexibility\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Loop in PyTorch\n",
    "\n",
    "Unlike Keras's `model.fit()`, PyTorch requires you to write explicit training loops.\n",
    "\n",
    "**Typical Training Loop**:\n",
    "1. Zero gradients\n",
    "2. Forward pass\n",
    "3. Compute loss\n",
    "4. Backward pass (compute gradients)\n",
    "5. Update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load and prepare MNIST data\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize and flatten\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_torch = torch.FloatTensor(X_train)\n",
    "y_train_torch = torch.LongTensor(y_train)\n",
    "X_test_torch = torch.FloatTensor(X_test)\n",
    "y_test_torch = torch.LongTensor(y_test)\n",
    "\n",
    "# Create PyTorch dataset and dataloader\n",
    "train_dataset = TensorDataset(X_train_torch, y_train_torch)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "print(f\"Training samples: {len(X_train_torch)}\")\n",
    "print(f\"Test samples: {len(X_test_torch)}\")\n",
    "print(f\"Batches per epoch: {len(train_loader)}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define training function\n",
    "def train_pytorch_model(model, train_loader, epochs=5, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Train PyTorch model.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        train_loader: DataLoader for training data\n",
    "        epochs: Number of epochs\n",
    "        learning_rate: Learning rate\n",
    "    \n",
    "    Returns:\n",
    "        Training history\n",
    "    \"\"\"\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    history = {'loss': [], 'accuracy': []}\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "        epoch_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            # 1. Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 2. Forward pass\n",
    "            outputs = model(data)\n",
    "            \n",
    "            # 3. Compute loss\n",
    "            loss = criterion(outputs, target)\n",
    "            \n",
    "            # 4. Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # 5. Update weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track metrics\n",
    "            epoch_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "        \n",
    "        # Calculate epoch metrics\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        accuracy = correct / total\n",
    "        \n",
    "        history['loss'].append(avg_loss)\n",
    "        history['accuracy'].append(accuracy)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Train PyTorch model\n",
    "print(\"Training PyTorch model...\\n\")\n",
    "pytorch_model = SimpleNN(784, 128, 10)\n",
    "pytorch_history = train_pytorch_model(pytorch_model, train_loader, epochs=5)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Train equivalent Keras model for comparison\n",
    "print(\"\\nTraining Keras model...\\n\")\n",
    "\n",
    "keras_model = create_keras_model()\n",
    "keras_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "keras_history = keras_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=5,\n",
    "    batch_size=128,\n",
    "    verbose=1\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compare training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss comparison\n",
    "axes[0].plot(pytorch_history['loss'], label='PyTorch', linewidth=2, marker='o')\n",
    "axes[0].plot(keras_history.history['loss'], label='Keras', linewidth=2, marker='s')\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('Training Loss Comparison', fontsize=13, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy comparison\n",
    "axes[1].plot(pytorch_history['accuracy'], label='PyTorch', linewidth=2, marker='o')\n",
    "axes[1].plot(keras_history.history['accuracy'], label='Keras', linewidth=2, marker='s')\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1].set_title('Training Accuracy Comparison', fontsize=13, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nBoth frameworks achieve similar results!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def evaluate_pytorch_model(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluate PyTorch model.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained PyTorch model\n",
    "        X_test: Test data\n",
    "        y_test: Test labels\n",
    "    \n",
    "    Returns:\n",
    "        Test accuracy\n",
    "    \"\"\"\n",
    "    model.eval()  # Set to evaluation mode (disables dropout, etc.)\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "        outputs = model(X_test)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct = (predicted == y_test).sum().item()\n",
    "        accuracy = correct / len(y_test)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# Evaluate PyTorch model\n",
    "pytorch_test_acc = evaluate_pytorch_model(pytorch_model, X_test_torch, y_test_torch)\n",
    "print(f\"PyTorch Test Accuracy: {pytorch_test_acc:.4f}\")\n",
    "\n",
    "# Evaluate Keras model\n",
    "keras_test_loss, keras_test_acc = keras_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Keras Test Accuracy: {keras_test_acc:.4f}\")\n",
    "\n",
    "print(\"\\n⚠️  Important PyTorch concepts:\")\n",
    "print(\"  - model.eval(): Switch to evaluation mode\")\n",
    "print(\"  - torch.no_grad(): Disable gradient tracking for inference\")\n",
    "print(\"  - model.train(): Switch back to training mode\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Saving and Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "# Create directory for models\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# PyTorch: Save entire model\n",
    "torch.save(pytorch_model, 'models/pytorch_model.pth')\n",
    "print(\"PyTorch model saved (entire model).\")\n",
    "\n",
    "# PyTorch: Save only state dict (recommended)\n",
    "torch.save(pytorch_model.state_dict(), 'models/pytorch_model_state.pth')\n",
    "print(\"PyTorch model state dict saved (recommended approach).\")\n",
    "\n",
    "# Loading in PyTorch\n",
    "loaded_model = SimpleNN(784, 128, 10)\n",
    "loaded_model.load_state_dict(torch.load('models/pytorch_model_state.pth'))\n",
    "loaded_model.eval()\n",
    "print(\"\\nPyTorch model loaded successfully.\")\n",
    "\n",
    "# Verify loaded model works\n",
    "loaded_acc = evaluate_pytorch_model(loaded_model, X_test_torch, y_test_torch)\n",
    "print(f\"Loaded model accuracy: {loaded_acc:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PyTorch vs Keras: Model Saving\")\n",
    "print(\"=\" * 60)\n",
    "print(\"PyTorch:\")\n",
    "print(\"  - torch.save(model.state_dict(), path): Save weights\")\n",
    "print(\"  - model.load_state_dict(torch.load(path)): Load weights\")\n",
    "print(\"  - Requires model architecture to be defined separately\")\n",
    "print(\"\\nKeras:\")\n",
    "print(\"  - model.save(path): Save entire model (architecture + weights)\")\n",
    "print(\"  - keras.models.load_model(path): Load complete model\")\n",
    "print(\"  - Easier but less flexible\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Converting Between Frameworks\n",
    "\n",
    "Sometimes you need to convert models between PyTorch and TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Simple weight transfer example\n",
    "def transfer_weights_pytorch_to_keras(pytorch_model, keras_model):\n",
    "    \"\"\"\n",
    "    Transfer weights from PyTorch to Keras model.\n",
    "    Note: This is a simplified example for demonstration.\n",
    "    Real conversion may require handling layer differences.\n",
    "    \"\"\"\n",
    "    # Get PyTorch weights\n",
    "    pytorch_weights = []\n",
    "    for param in pytorch_model.parameters():\n",
    "        pytorch_weights.append(param.detach().numpy())\n",
    "    \n",
    "    print(f\"PyTorch model has {len(pytorch_weights)} weight tensors\")\n",
    "    print(f\"Keras model has {len(keras_model.weights)} weight tensors\")\n",
    "    \n",
    "    # For demonstration, show weight shapes\n",
    "    print(\"\\nPyTorch weight shapes:\")\n",
    "    for i, w in enumerate(pytorch_weights):\n",
    "        print(f\"  {i}: {w.shape}\")\n",
    "    \n",
    "    print(\"\\nKeras weight shapes:\")\n",
    "    for i, w in enumerate(keras_model.weights):\n",
    "        print(f\"  {i}: {w.shape}\")\n",
    "    \n",
    "    print(\"\\n⚠️  Note: Full conversion requires careful alignment of layers!\")\n",
    "    print(\"Tools like ONNX can help with automatic conversion.\")\n",
    "\n",
    "transfer_weights_pytorch_to_keras(pytorch_model, keras_model)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Advanced PyTorch Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Custom loss function\n",
    "class CustomLoss(nn.Module):\n",
    "    \"\"\"Example custom loss function.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "    \n",
    "    def forward(self, predictions, targets):\n",
    "        # Example: MSE + L1 regularization\n",
    "        mse_loss = F.mse_loss(predictions, targets)\n",
    "        l1_reg = torch.abs(predictions).mean()\n",
    "        return mse_loss + 0.1 * l1_reg\n",
    "\n",
    "# Custom layer\n",
    "class CustomLayer(nn.Module):\n",
    "    \"\"\"Example custom layer.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(CustomLayer, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(input_size, output_size))\n",
    "        self.bias = nn.Parameter(torch.zeros(output_size))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Custom computation\n",
    "        return torch.matmul(x, self.weight) + self.bias\n",
    "\n",
    "# Learning rate scheduler\n",
    "model = SimpleNN(784, 128, 10)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Reduce LR when plateau\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min',\n",
    "    factor=0.5,\n",
    "    patience=2\n",
    ")\n",
    "\n",
    "print(\"Advanced PyTorch Features Demonstrated:\")\n",
    "print(\"  ✓ Custom loss function\")\n",
    "print(\"  ✓ Custom layer\")\n",
    "print(\"  ✓ Learning rate scheduler\")\n",
    "print(\"\\nThese are easier to implement in PyTorch than Keras!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Exercise 1: Implement a CNN in PyTorch\n",
    "\n",
    "**Task**: Create a Convolutional Neural Network for MNIST using PyTorch.\n",
    "\n",
    "**Requirements**:\n",
    "1. Define a CNN class with at least 2 convolutional layers\n",
    "2. Include MaxPooling and Dropout layers\n",
    "3. Implement the training loop\n",
    "4. Achieve >98% test accuracy\n",
    "5. Compare with Keras CNN implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# YOUR CODE HERE\n",
    "# Hint: Use nn.Conv2d for convolutional layers\n",
    "# Remember to reshape data: X_train.reshape(-1, 1, 28, 28)\n",
    "\n",
    "pass  # Replace with your implementation"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Exercise 2: Custom Training Loop with Validation\n",
    "\n",
    "**Task**: Extend the training loop to include validation and early stopping.\n",
    "\n",
    "**Requirements**:\n",
    "1. Split training data into train and validation sets\n",
    "2. Evaluate on validation set after each epoch\n",
    "3. Implement early stopping (stop if val loss doesn't improve for N epochs)\n",
    "4. Save the best model based on validation loss\n",
    "5. Plot training and validation curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# YOUR CODE HERE\n",
    "# Hint: Track validation loss and compare with best so far\n",
    "# Use torch.save() to save best model\n",
    "\n",
    "pass  # Replace with your implementation"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Exercise 3: Gradient Clipping and Monitoring\n",
    "\n",
    "**Task**: Implement gradient clipping and monitor gradient norms during training.\n",
    "\n",
    "**Requirements**:\n",
    "1. Create a deep network (5+ layers) prone to gradient issues\n",
    "2. Monitor gradient norms for each layer during training\n",
    "3. Implement gradient clipping using `torch.nn.utils.clip_grad_norm_`\n",
    "4. Compare training with and without gradient clipping\n",
    "5. Visualize gradient norms over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# YOUR CODE HERE\n",
    "# Hint: After loss.backward(), check gradients before optimizer.step()\n",
    "# Use: torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "\n",
    "pass  # Replace with your implementation"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Summary\n",
    "\n",
    "### Key Concepts Covered:\n",
    "\n",
    "1. **PyTorch Tensors**\n",
    "   - Similar to NumPy arrays with GPU support\n",
    "   - Easy conversion between NumPy and PyTorch\n",
    "   - Supports all standard mathematical operations\n",
    "\n",
    "2. **Automatic Differentiation (Autograd)**\n",
    "   - `requires_grad=True` enables gradient tracking\n",
    "   - `backward()` computes gradients\n",
    "   - Gradients accumulate (must zero them manually)\n",
    "\n",
    "3. **nn.Module**\n",
    "   - Base class for all neural networks\n",
    "   - Define layers in `__init__`, logic in `forward()`\n",
    "   - More explicit than Keras but more flexible\n",
    "\n",
    "4. **Training Loop**\n",
    "   - Explicit steps: zero_grad → forward → loss → backward → step\n",
    "   - More code than Keras but complete control\n",
    "   - Better for research and custom training procedures\n",
    "\n",
    "5. **PyTorch vs TensorFlow/Keras**\n",
    "   - PyTorch: Research-friendly, flexible, explicit\n",
    "   - Keras: Production-ready, simple, high-level\n",
    "   - Both can achieve same results\n",
    "\n",
    "### PyTorch Best Practices (2025):\n",
    "\n",
    "- **Use DataLoader** for efficient batching and shuffling\n",
    "- **Always call model.train()/model.eval()** appropriately\n",
    "- **Use torch.no_grad()** during inference\n",
    "- **Zero gradients** before each backward pass\n",
    "- **Save state_dict** instead of entire model\n",
    "- **Use GPU** when available: `model.to('cuda')`\n",
    "- **Profile code** with PyTorch Profiler for optimization\n",
    "\n",
    "### When to Use Each Framework (2025 Update):\n",
    "\n",
    "**Choose PyTorch for**:\n",
    "- Research and paper implementations\n",
    "- Custom architectures and training loops\n",
    "- Natural language processing (dominant in NLP)\n",
    "- Computer vision research\n",
    "- Learning deep learning fundamentals\n",
    "\n",
    "**Choose TensorFlow/Keras for**:\n",
    "- Production deployment at scale\n",
    "- Mobile and edge devices (TFLite)\n",
    "- JavaScript deployment (TensorFlow.js)\n",
    "- Quick prototyping with standard models\n",
    "- Integration with Google Cloud ecosystem\n",
    "\n",
    "**Good News**: You can learn both! Core concepts transfer easily.\n",
    "\n",
    "### Code Comparison Summary:\n",
    "\n",
    "| Task | PyTorch | Keras |\n",
    "|------|---------|-------|\n",
    "| **Model Definition** | Class + forward() | Sequential or Functional |\n",
    "| **Training** | Manual loop | model.fit() |\n",
    "| **Evaluation** | Manual with no_grad | model.evaluate() |\n",
    "| **Prediction** | model(x) | model.predict(x) |\n",
    "| **Saving** | torch.save(state_dict) | model.save() |\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "- [Module 14: Final Project - Deep Learning Pipeline](14_final_project_dl_pipeline.ipynb)\n",
    "- Advanced PyTorch: DataParallel, DistributedDataParallel, TorchScript\n",
    "- Framework-specific features: PyTorch Lightning, tf.data, etc.\n",
    "\n",
    "### Additional Resources:\n",
    "\n",
    "1. PyTorch Documentation: https://pytorch.org/docs/\n",
    "2. PyTorch Tutorials: https://pytorch.org/tutorials/\n",
    "3. \"Deep Learning with PyTorch\" (official book): https://pytorch.org/deep-learning-with-pytorch\n",
    "4. PyTorch Forums: https://discuss.pytorch.org/\n",
    "5. Papers With Code: https://paperswithcode.com/ (most use PyTorch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
