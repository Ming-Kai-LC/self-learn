{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 06: Optimizers (SGD, Adam, RMSprop)\n",
    "\n",
    "**Difficulty**: ⭐⭐⭐ (Advanced)\n",
    "\n",
    "**Estimated Time**: 60-75 minutes\n",
    "\n",
    "**Prerequisites**: \n",
    "- [Module 05: Feed-Forward Neural Networks with Keras](05_feedforward_neural_networks_keras.ipynb)\n",
    "- [Module 02: Backpropagation and Gradient Descent](02_backpropagation_and_gradient_descent.ipynb)\n",
    "- [Module 04: Introduction to TensorFlow and Keras](04_introduction_to_tensorflow_keras.ipynb)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Understand the differences between gradient descent variants (Batch, Mini-batch, Stochastic)\n",
    "2. Explain and implement momentum and Nesterov momentum for faster convergence\n",
    "3. Apply adaptive learning rate methods (AdaGrad, RMSprop, Adam)\n",
    "4. Compare optimizer performance on different problems\n",
    "5. Implement learning rate schedules to improve training\n",
    "6. Choose the appropriate optimizer for your specific task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "\n",
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.datasets import mnist, fashion_mnist\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, ReduceLROnPlateau\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding Gradient Descent Variants\n",
    "\n",
    "### The Three Main Variants:\n",
    "\n",
    "#### 1. **Batch Gradient Descent (BGD)**\n",
    "- Uses **entire** training dataset to compute gradients\n",
    "- Update rule: $\\theta_{t+1} = \\theta_t - \\eta \\nabla_\\theta J(\\theta; x^{(1:n)}, y^{(1:n)})$\n",
    "- **Pros**: Stable convergence, exact gradient\n",
    "- **Cons**: Slow for large datasets, memory intensive\n",
    "\n",
    "#### 2. **Stochastic Gradient Descent (SGD)**\n",
    "- Uses **single** random sample to compute gradients\n",
    "- Update rule: $\\theta_{t+1} = \\theta_t - \\eta \\nabla_\\theta J(\\theta; x^{(i)}, y^{(i)})$\n",
    "- **Pros**: Fast updates, can escape local minima\n",
    "- **Cons**: Noisy updates, high variance\n",
    "\n",
    "#### 3. **Mini-batch Gradient Descent (MBGD)** ⭐ Most Common\n",
    "- Uses **small batch** of samples (typically 32, 64, 128, 256)\n",
    "- Update rule: $\\theta_{t+1} = \\theta_t - \\eta \\nabla_\\theta J(\\theta; x^{(i:i+b)}, y^{(i:i+b)})$\n",
    "- **Pros**: Balance of speed and stability, GPU efficient\n",
    "- **Cons**: Requires batch size tuning\n",
    "\n",
    "### Visual Comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Simulate gradient descent paths on a simple 2D function\n",
    "def rosenbrock(x, y):\n",
    "    \"\"\"Rosenbrock function - classic optimization test function.\"\"\"\n",
    "    return (1 - x)**2 + 100 * (y - x**2)**2\n",
    "\n",
    "def rosenbrock_gradient(x, y):\n",
    "    \"\"\"Gradient of Rosenbrock function.\"\"\"\n",
    "    dx = -2 * (1 - x) - 400 * x * (y - x**2)\n",
    "    dy = 200 * (y - x**2)\n",
    "    return np.array([dx, dy])\n",
    "\n",
    "# Create contour plot\n",
    "x = np.linspace(-2, 2, 400)\n",
    "y = np.linspace(-1, 3, 400)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = rosenbrock(X, Y)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "contour = ax.contour(X, Y, Z, levels=np.logspace(-1, 3.5, 20), cmap='viridis', alpha=0.6)\n",
    "ax.clabel(contour, inline=True, fontsize=8)\n",
    "\n",
    "# Simulate different GD variants (simplified visualization)\n",
    "# Starting point\n",
    "start = np.array([-1.5, 2.5])\n",
    "\n",
    "# Batch GD: smooth path\n",
    "path_batch = [start]\n",
    "point = start.copy()\n",
    "lr = 0.001\n",
    "for _ in range(100):\n",
    "    grad = rosenbrock_gradient(point[0], point[1])\n",
    "    point = point - lr * grad\n",
    "    path_batch.append(point.copy())\n",
    "path_batch = np.array(path_batch)\n",
    "\n",
    "# SGD: noisy path (add random noise to gradients)\n",
    "path_sgd = [start]\n",
    "point = start.copy()\n",
    "for _ in range(100):\n",
    "    grad = rosenbrock_gradient(point[0], point[1])\n",
    "    # Add noise to simulate stochastic behavior\n",
    "    noisy_grad = grad + np.random.randn(2) * 50\n",
    "    point = point - lr * noisy_grad\n",
    "    path_sgd.append(point.copy())\n",
    "path_sgd = np.array(path_sgd)\n",
    "\n",
    "# Mini-batch: moderate noise\n",
    "path_minibatch = [start]\n",
    "point = start.copy()\n",
    "for _ in range(100):\n",
    "    grad = rosenbrock_gradient(point[0], point[1])\n",
    "    # Less noise than SGD\n",
    "    noisy_grad = grad + np.random.randn(2) * 20\n",
    "    point = point - lr * noisy_grad\n",
    "    path_minibatch.append(point.copy())\n",
    "path_minibatch = np.array(path_minibatch)\n",
    "\n",
    "# Plot paths\n",
    "ax.plot(path_batch[:, 0], path_batch[:, 1], 'b-', linewidth=2, label='Batch GD (smooth)', alpha=0.7)\n",
    "ax.plot(path_sgd[:, 0], path_sgd[:, 1], 'r-', linewidth=1, label='SGD (noisy)', alpha=0.6)\n",
    "ax.plot(path_minibatch[:, 0], path_minibatch[:, 1], 'g-', linewidth=1.5, \n",
    "        label='Mini-batch GD (balanced)', alpha=0.7)\n",
    "\n",
    "# Mark start and optimum\n",
    "ax.plot(start[0], start[1], 'ko', markersize=10, label='Start')\n",
    "ax.plot(1, 1, 'r*', markersize=20, label='Optimum (1, 1)')\n",
    "\n",
    "ax.set_xlabel('x', fontsize=12)\n",
    "ax.set_ylabel('y', fontsize=12)\n",
    "ax.set_title('Gradient Descent Variants Comparison', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.set_xlim(-2, 2)\n",
    "ax.set_ylim(-1, 3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observation:\")\n",
    "print(\"- Batch GD: Smooth but slow convergence\")\n",
    "print(\"- SGD: Fast but very noisy updates\")\n",
    "print(\"- Mini-batch GD: Good balance between speed and stability\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Momentum: Accelerating Gradient Descent\n",
    "\n",
    "### Problem with Standard GD:\n",
    "- Oscillates in steep dimensions\n",
    "- Slow progress in gentle dimensions\n",
    "- Can get stuck in saddle points\n",
    "\n",
    "### Momentum Solution:\n",
    "\n",
    "Accumulates a **velocity vector** in directions of persistent reduction:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "v_t &= \\beta v_{t-1} + \\nabla_\\theta J(\\theta) \\\\\n",
    "\\theta_{t+1} &= \\theta_t - \\eta v_t\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $v_t$ is the velocity (exponentially weighted average of gradients)\n",
    "- $\\beta$ is the momentum coefficient (typically 0.9)\n",
    "- $\\eta$ is the learning rate\n",
    "\n",
    "**Physical Analogy**: Ball rolling down a hill - builds momentum in consistent directions\n",
    "\n",
    "### Nesterov Momentum (NAG):\n",
    "\n",
    "**Lookahead** gradient - evaluates gradient at the anticipated position:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "v_t &= \\beta v_{t-1} + \\nabla_\\theta J(\\theta - \\eta \\beta v_{t-1}) \\\\\n",
    "\\theta_{t+1} &= \\theta_t - \\eta v_t\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Benefit**: More responsive corrections, prevents overshooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Demonstrate momentum on a simple optimization problem\n",
    "def compare_momentum_methods():\n",
    "    \"\"\"\n",
    "    Compare standard GD, momentum, and Nesterov momentum.\n",
    "    \"\"\"\n",
    "    # Simple quadratic function with different curvatures\n",
    "    def f(x, y):\n",
    "        return 0.5 * x**2 + 4.5 * y**2\n",
    "    \n",
    "    def grad_f(x, y):\n",
    "        return np.array([x, 9*y])\n",
    "    \n",
    "    # Parameters\n",
    "    start = np.array([10.0, 10.0])\n",
    "    lr = 0.1\n",
    "    beta = 0.9\n",
    "    steps = 50\n",
    "    \n",
    "    # Standard GD\n",
    "    path_gd = [start]\n",
    "    point = start.copy()\n",
    "    for _ in range(steps):\n",
    "        grad = grad_f(point[0], point[1])\n",
    "        point = point - lr * grad\n",
    "        path_gd.append(point.copy())\n",
    "    \n",
    "    # Momentum GD\n",
    "    path_momentum = [start]\n",
    "    point = start.copy()\n",
    "    velocity = np.zeros(2)\n",
    "    for _ in range(steps):\n",
    "        grad = grad_f(point[0], point[1])\n",
    "        velocity = beta * velocity + grad\n",
    "        point = point - lr * velocity\n",
    "        path_momentum.append(point.copy())\n",
    "    \n",
    "    # Nesterov Momentum\n",
    "    path_nesterov = [start]\n",
    "    point = start.copy()\n",
    "    velocity = np.zeros(2)\n",
    "    for _ in range(steps):\n",
    "        # Look ahead\n",
    "        lookahead = point - lr * beta * velocity\n",
    "        grad = grad_f(lookahead[0], lookahead[1])\n",
    "        velocity = beta * velocity + grad\n",
    "        point = point - lr * velocity\n",
    "        path_nesterov.append(point.copy())\n",
    "    \n",
    "    return np.array(path_gd), np.array(path_momentum), np.array(path_nesterov), f\n",
    "\n",
    "# Compare methods\n",
    "path_gd, path_momentum, path_nesterov, func = compare_momentum_methods()\n",
    "\n",
    "# Visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Contour plot\n",
    "x = np.linspace(-12, 12, 200)\n",
    "y = np.linspace(-12, 12, 200)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = func(X, Y)\n",
    "\n",
    "contour = ax1.contour(X, Y, Z, levels=20, cmap='viridis', alpha=0.6)\n",
    "ax1.plot(path_gd[:, 0], path_gd[:, 1], 'b-o', linewidth=2, markersize=4, \n",
    "         label='Standard GD', alpha=0.7)\n",
    "ax1.plot(path_momentum[:, 0], path_momentum[:, 1], 'r-o', linewidth=2, markersize=4,\n",
    "         label='Momentum', alpha=0.7)\n",
    "ax1.plot(path_nesterov[:, 0], path_nesterov[:, 1], 'g-o', linewidth=2, markersize=4,\n",
    "         label='Nesterov', alpha=0.7)\n",
    "ax1.plot(0, 0, 'r*', markersize=20, label='Optimum')\n",
    "ax1.set_xlabel('x', fontsize=12)\n",
    "ax1.set_ylabel('y', fontsize=12)\n",
    "ax1.set_title('Optimization Paths', fontsize=12, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Loss over iterations\n",
    "losses_gd = [func(p[0], p[1]) for p in path_gd]\n",
    "losses_momentum = [func(p[0], p[1]) for p in path_momentum]\n",
    "losses_nesterov = [func(p[0], p[1]) for p in path_nesterov]\n",
    "\n",
    "ax2.semilogy(losses_gd, 'b-', linewidth=2, label='Standard GD')\n",
    "ax2.semilogy(losses_momentum, 'r-', linewidth=2, label='Momentum')\n",
    "ax2.semilogy(losses_nesterov, 'g-', linewidth=2, label='Nesterov')\n",
    "ax2.set_xlabel('Iteration', fontsize=12)\n",
    "ax2.set_ylabel('Loss (log scale)', fontsize=12)\n",
    "ax2.set_title('Convergence Speed Comparison', fontsize=12, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Iterations to reach loss < 1.0:\")\n",
    "print(f\"  Standard GD: {np.argmax(np.array(losses_gd) < 1.0)}\")\n",
    "print(f\"  Momentum:    {np.argmax(np.array(losses_momentum) < 1.0)}\")\n",
    "print(f\"  Nesterov:    {np.argmax(np.array(losses_nesterov) < 1.0)}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Adaptive Learning Rate Methods\n",
    "\n",
    "### Problem with Fixed Learning Rates:\n",
    "- Too large: Overshooting, divergence\n",
    "- Too small: Slow convergence\n",
    "- Different parameters may need different learning rates\n",
    "\n",
    "### Solution: Adaptive Methods\n",
    "\n",
    "Automatically adjust learning rates for each parameter based on historical gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 AdaGrad (Adaptive Gradient)\n",
    "\n",
    "**Key Idea**: Scale learning rate inversely to the square root of sum of squared gradients\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "G_t &= G_{t-1} + (\\nabla_\\theta J(\\theta))^2 \\\\\n",
    "\\theta_{t+1} &= \\theta_t - \\frac{\\eta}{\\sqrt{G_t + \\epsilon}} \\nabla_\\theta J(\\theta)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $G_t$ accumulates squared gradients (element-wise)\n",
    "- $\\epsilon$ is a small constant for numerical stability (e.g., $10^{-8}$)\n",
    "\n",
    "**Pros**: \n",
    "- Good for sparse gradients\n",
    "- Automatically adjusts learning rate per parameter\n",
    "\n",
    "**Cons**: \n",
    "- Accumulation of squared gradients grows monotonically\n",
    "- Learning rate may become infinitesimally small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 RMSprop (Root Mean Square Propagation)\n",
    "\n",
    "**Key Idea**: Fix AdaGrad's diminishing learning rate by using exponentially weighted average\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "E[g^2]_t &= \\beta E[g^2]_{t-1} + (1-\\beta) (\\nabla_\\theta J(\\theta))^2 \\\\\n",
    "\\theta_{t+1} &= \\theta_t - \\frac{\\eta}{\\sqrt{E[g^2]_t + \\epsilon}} \\nabla_\\theta J(\\theta)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $E[g^2]_t$ is the exponentially weighted average of squared gradients\n",
    "- $\\beta$ is the decay rate (typically 0.9)\n",
    "\n",
    "**Pros**: \n",
    "- Fixes AdaGrad's aggressive learning rate decay\n",
    "- Works well on non-stationary problems\n",
    "\n",
    "**Recommended for**: RNNs, time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Adam (Adaptive Moment Estimation) ⭐ Most Popular\n",
    "\n",
    "**Key Idea**: Combine momentum (first moment) with RMSprop (second moment)\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "m_t &= \\beta_1 m_{t-1} + (1-\\beta_1) \\nabla_\\theta J(\\theta) \\quad \\text{(momentum)} \\\\\n",
    "v_t &= \\beta_2 v_{t-1} + (1-\\beta_2) (\\nabla_\\theta J(\\theta))^2 \\quad \\text{(RMSprop)} \\\\\n",
    "\\hat{m}_t &= \\frac{m_t}{1-\\beta_1^t} \\quad \\text{(bias correction)} \\\\\n",
    "\\hat{v}_t &= \\frac{v_t}{1-\\beta_2^t} \\quad \\text{(bias correction)} \\\\\n",
    "\\theta_{t+1} &= \\theta_t - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\hat{m}_t\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Default hyperparameters**:\n",
    "- $\\eta = 0.001$ (learning rate)\n",
    "- $\\beta_1 = 0.9$ (momentum decay)\n",
    "- $\\beta_2 = 0.999$ (RMSprop decay)\n",
    "- $\\epsilon = 10^{-8}$ (numerical stability)\n",
    "\n",
    "**Pros**: \n",
    "- Works well on most problems\n",
    "- Bias correction handles initial timesteps\n",
    "- Robust to hyperparameter choices\n",
    "\n",
    "**When to use**: Default choice for deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 AdamW (Adam with Weight Decay)\n",
    "\n",
    "**Key Idea**: Decouple weight decay from gradient-based update\n",
    "\n",
    "Standard Adam with L2 regularization is inconsistent. AdamW fixes this:\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\eta \\left( \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon} + \\lambda \\theta_t \\right)\n",
    "$$\n",
    "\n",
    "Where $\\lambda$ is the weight decay coefficient.\n",
    "\n",
    "**When to use**: Training transformers, large models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Optimizer Comparison on Fashion-MNIST\n",
    "\n",
    "Let's compare different optimizers empirically on a real task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load and preprocess Fashion-MNIST\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Normalize and flatten\n",
    "X_train_full = X_train_full.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "X_train_full_flat = X_train_full.reshape(-1, 784)\n",
    "X_test_flat = X_test.reshape(-1, 784)\n",
    "\n",
    "# Create validation split\n",
    "X_train = X_train_full_flat[:50000]\n",
    "X_valid = X_train_full_flat[50000:]\n",
    "y_train = y_train_full[:50000]\n",
    "y_valid = y_train_full[50000:]\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Validation set: {X_valid.shape}\")\n",
    "print(f\"Test set: {X_test_flat.shape}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define a standard architecture for fair comparison\n",
    "def create_model():\n",
    "    \"\"\"\n",
    "    Create a standard neural network for optimizer comparison.\n",
    "    Using same architecture ensures fair comparison.\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(784,)),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Test the model creation\n",
    "test_model = create_model()\n",
    "test_model.summary()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compare different optimizers\n",
    "optimizers_to_compare = {\n",
    "    'SGD': optimizers.SGD(learning_rate=0.01),\n",
    "    'SGD + Momentum': optimizers.SGD(learning_rate=0.01, momentum=0.9),\n",
    "    'SGD + Nesterov': optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True),\n",
    "    'RMSprop': optimizers.RMSprop(learning_rate=0.001),\n",
    "    'Adam': optimizers.Adam(learning_rate=0.001),\n",
    "    'AdamW': optimizers.AdamW(learning_rate=0.001, weight_decay=0.0001)\n",
    "}\n",
    "\n",
    "# Train each optimizer\n",
    "histories = {}\n",
    "epochs = 15\n",
    "\n",
    "for name, optimizer in optimizers_to_compare.items():\n",
    "    print(f\"\\nTraining with {name}...\")\n",
    "    \n",
    "    # Create fresh model\n",
    "    model = create_model()\n",
    "    \n",
    "    # Compile with specific optimizer\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=128,\n",
    "        validation_data=(X_valid, y_valid),\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    histories[name] = history\n",
    "    \n",
    "    # Print final metrics\n",
    "    final_val_acc = history.history['val_accuracy'][-1]\n",
    "    final_val_loss = history.history['val_loss'][-1]\n",
    "    print(f\"  Final Validation Accuracy: {final_val_acc:.4f}\")\n",
    "    print(f\"  Final Validation Loss:     {final_val_loss:.4f}\")\n",
    "\n",
    "print(\"\\nAll optimizers trained!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot validation loss\n",
    "for name, history in histories.items():\n",
    "    ax1.plot(history.history['val_loss'], linewidth=2, marker='o', label=name)\n",
    "\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Validation Loss', fontsize=12)\n",
    "ax1.set_title('Optimizer Comparison: Validation Loss', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot validation accuracy\n",
    "for name, history in histories.items():\n",
    "    ax2.plot(history.history['val_accuracy'], linewidth=2, marker='o', label=name)\n",
    "\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Validation Accuracy', fontsize=12)\n",
    "ax2.set_title('Optimizer Comparison: Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OPTIMIZER PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Optimizer':<20} {'Best Val Acc':<15} {'Final Val Acc':<15} {'Convergence Speed'}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for name, history in histories.items():\n",
    "    best_acc = max(history.history['val_accuracy'])\n",
    "    final_acc = history.history['val_accuracy'][-1]\n",
    "    # Epoch where accuracy reached 85% (if it did)\n",
    "    acc_array = np.array(history.history['val_accuracy'])\n",
    "    epoch_85 = np.argmax(acc_array >= 0.85) if any(acc_array >= 0.85) else epochs\n",
    "    \n",
    "    print(f\"{name:<20} {best_acc:<15.4f} {final_acc:<15.4f} {epoch_85+1}/{epochs}\")\n",
    "\n",
    "print(\"=\"*80)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Learning Rate Schedules\n",
    "\n",
    "### Why Use Learning Rate Schedules?\n",
    "\n",
    "- **Early training**: Need higher learning rate for fast progress\n",
    "- **Late training**: Need lower learning rate for fine-tuning\n",
    "- **Solution**: Gradually decrease learning rate over time\n",
    "\n",
    "### Common Schedules:\n",
    "\n",
    "#### 1. **Step Decay**\n",
    "$$\\eta_t = \\eta_0 \\times \\gamma^{\\lfloor t/k \\rfloor}$$\n",
    "\n",
    "Reduce learning rate by factor $\\gamma$ every $k$ epochs.\n",
    "\n",
    "#### 2. **Exponential Decay**\n",
    "$$\\eta_t = \\eta_0 \\times e^{-\\lambda t}$$\n",
    "\n",
    "Smooth exponential reduction.\n",
    "\n",
    "#### 3. **Cosine Annealing**\n",
    "$$\\eta_t = \\eta_{\\min} + \\frac{1}{2}(\\eta_{\\max} - \\eta_{\\min})\\left(1 + \\cos\\left(\\frac{t}{T}\\pi\\right)\\right)$$\n",
    "\n",
    "Gradually decrease following a cosine curve.\n",
    "\n",
    "#### 4. **ReduceLROnPlateau**\n",
    "Reduce learning rate when validation metric stops improving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize different learning rate schedules\n",
    "def step_decay(epoch, initial_lr=0.1, drop=0.5, epochs_drop=5):\n",
    "    \"\"\"Step decay schedule.\"\"\"\n",
    "    return initial_lr * (drop ** np.floor(epoch / epochs_drop))\n",
    "\n",
    "def exponential_decay(epoch, initial_lr=0.1, k=0.1):\n",
    "    \"\"\"Exponential decay schedule.\"\"\"\n",
    "    return initial_lr * np.exp(-k * epoch)\n",
    "\n",
    "def cosine_annealing(epoch, initial_lr=0.1, min_lr=0.001, T_max=30):\n",
    "    \"\"\"Cosine annealing schedule.\"\"\"\n",
    "    return min_lr + (initial_lr - min_lr) * 0.5 * (1 + np.cos(np.pi * epoch / T_max))\n",
    "\n",
    "# Generate schedules\n",
    "epochs_range = np.arange(0, 30)\n",
    "lr_step = [step_decay(e) for e in epochs_range]\n",
    "lr_exp = [exponential_decay(e) for e in epochs_range]\n",
    "lr_cosine = [cosine_annealing(e) for e in epochs_range]\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "ax.plot(epochs_range, lr_step, linewidth=2, marker='o', label='Step Decay')\n",
    "ax.plot(epochs_range, lr_exp, linewidth=2, marker='s', label='Exponential Decay')\n",
    "ax.plot(epochs_range, lr_cosine, linewidth=2, marker='^', label='Cosine Annealing')\n",
    "\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Learning Rate', fontsize=12)\n",
    "ax.set_title('Learning Rate Schedules Comparison', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Example: Train with cosine annealing schedule\n",
    "print(\"Training with Cosine Annealing Learning Rate Schedule...\\n\")\n",
    "\n",
    "# Create model\n",
    "model_scheduled = create_model()\n",
    "\n",
    "# Define cosine annealing callback\n",
    "def cosine_schedule(epoch):\n",
    "    \"\"\"Cosine annealing schedule for Keras callback.\"\"\"\n",
    "    initial_lr = 0.01\n",
    "    min_lr = 0.0001\n",
    "    T_max = 20\n",
    "    return min_lr + (initial_lr - min_lr) * 0.5 * (1 + np.cos(np.pi * epoch / T_max))\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(cosine_schedule, verbose=0)\n",
    "\n",
    "# Compile\n",
    "model_scheduled.compile(\n",
    "    optimizer=optimizers.SGD(learning_rate=0.01, momentum=0.9),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train with scheduler\n",
    "history_scheduled = model_scheduled.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=20,\n",
    "    batch_size=128,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    callbacks=[lr_scheduler],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"Training completed!\")\n",
    "print(f\"Final Validation Accuracy: {history_scheduled.history['val_accuracy'][-1]:.4f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compare with constant learning rate\n",
    "print(\"\\nTraining with Constant Learning Rate (for comparison)...\\n\")\n",
    "\n",
    "model_constant = create_model()\n",
    "model_constant.compile(\n",
    "    optimizer=optimizers.SGD(learning_rate=0.01, momentum=0.9),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history_constant = model_constant.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=20,\n",
    "    batch_size=128,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"Training completed!\")\n",
    "print(f\"Final Validation Accuracy: {history_constant.history['val_accuracy'][-1]:.4f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compare scheduled vs constant learning rate\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Validation accuracy comparison\n",
    "ax1.plot(history_constant.history['val_accuracy'], linewidth=2, \n",
    "         marker='o', label='Constant LR (0.01)')\n",
    "ax1.plot(history_scheduled.history['val_accuracy'], linewidth=2, \n",
    "         marker='s', label='Cosine Annealing LR')\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Validation Accuracy', fontsize=12)\n",
    "ax1.set_title('Learning Rate Schedule Impact', fontsize=12, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Show actual learning rate schedule\n",
    "lr_values = [cosine_schedule(e) for e in range(20)]\n",
    "ax2.plot(lr_values, linewidth=2, marker='o', color='green')\n",
    "ax2.axhline(y=0.01, color='red', linestyle='--', linewidth=2, label='Constant LR')\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Learning Rate', fontsize=12)\n",
    "ax2.set_title('Learning Rate Schedule', fontsize=12, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBest Validation Accuracy:\")\n",
    "print(f\"  Constant LR:        {max(history_constant.history['val_accuracy']):.4f}\")\n",
    "print(f\"  Cosine Annealing:   {max(history_scheduled.history['val_accuracy']):.4f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Optimizer Selection Guide\n",
    "\n",
    "### Decision Tree for Choosing Optimizers:\n",
    "\n",
    "```\n",
    "Start Here\n",
    "    |\n",
    "    |-- Need quick baseline? --> Adam (default choice)\n",
    "    |\n",
    "    |-- Training RNN/LSTM? --> RMSprop or Adam\n",
    "    |\n",
    "    |-- Training Transformer? --> AdamW\n",
    "    |\n",
    "    |-- Need best generalization? --> SGD + Momentum + LR Schedule\n",
    "    |\n",
    "    |-- Sparse gradients? --> Adam or AdaGrad\n",
    "    |\n",
    "    |-- Limited memory? --> SGD\n",
    "```\n",
    "\n",
    "### Hyperparameter Recommendations:\n",
    "\n",
    "| Optimizer | Learning Rate | Other Params | When to Use |\n",
    "|-----------|---------------|--------------|-------------|\n",
    "| **SGD** | 0.01 - 0.1 | momentum=0.9 | Best final performance with tuning |\n",
    "| **Adam** | 0.001 - 0.0001 | defaults are good | Default choice, fast convergence |\n",
    "| **RMSprop** | 0.001 | rho=0.9 | RNNs, non-stationary problems |\n",
    "| **AdamW** | 0.001 | weight_decay=0.01 | Transformers, large models |\n",
    "\n",
    "### Common Pitfalls:\n",
    "\n",
    "1. **Using same LR for different optimizers**: Adam needs ~10x smaller LR than SGD\n",
    "2. **Not tuning momentum**: Default 0.9 is good, but try 0.95 or 0.99 for some tasks\n",
    "3. **Forgetting LR schedules**: Can significantly improve SGD performance\n",
    "4. **Not monitoring gradients**: Check for vanishing/exploding gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "1. **Gradient Descent Variants**:\n",
    "   - Batch GD: Stable but slow\n",
    "   - SGD: Fast but noisy\n",
    "   - Mini-batch: Best balance (most common)\n",
    "\n",
    "2. **Momentum Methods**:\n",
    "   - Accelerate learning in consistent directions\n",
    "   - Dampen oscillations\n",
    "   - Nesterov: Look-ahead variant\n",
    "\n",
    "3. **Adaptive Learning Rates**:\n",
    "   - AdaGrad: Good for sparse data, but learning rate decays\n",
    "   - RMSprop: Fixes AdaGrad, good for RNNs\n",
    "   - Adam: Best of both worlds, default choice\n",
    "   - AdamW: Adam with proper weight decay\n",
    "\n",
    "4. **Learning Rate Schedules**:\n",
    "   - Step decay: Periodic drops\n",
    "   - Exponential: Smooth decay\n",
    "   - Cosine annealing: Gentle curve\n",
    "   - Adaptive: Based on validation metrics\n",
    "\n",
    "5. **Practical Guidelines**:\n",
    "   - Start with Adam for quick results\n",
    "   - Use SGD + momentum + schedule for best final performance\n",
    "   - Adjust learning rates appropriately for each optimizer\n",
    "   - Monitor training curves to diagnose issues\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "- **Module 07**: Regularization Techniques (Dropout, Batch Normalization)\n",
    "- **Module 08**: Loss Functions and Metrics\n",
    "- **Module 09**: Hyperparameter Tuning for Deep Learning\n",
    "\n",
    "### Additional Resources:\n",
    "\n",
    "- [Keras Optimizers Documentation](https://keras.io/api/optimizers/)\n",
    "- [An Overview of Gradient Descent Optimization Algorithms](https://arxiv.org/abs/1609.04747)\n",
    "- [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980)\n",
    "- [CS231n: Optimization](https://cs231n.github.io/neural-networks-3/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Implement Basic SGD with Momentum\n",
    "\n",
    "**Task**: Implement SGD with momentum from scratch (without using Keras optimizers).\n",
    "\n",
    "**Requirements**:\n",
    "- Create a simple 2D optimization problem (e.g., minimize $f(x,y) = x^2 + 10y^2$)\n",
    "- Implement the momentum update rule\n",
    "- Compare convergence with standard GD\n",
    "- Visualize the optimization path\n",
    "\n",
    "**Questions**:\n",
    "1. How does changing momentum ($\\beta$) affect convergence?\n",
    "2. What happens with momentum=0 vs momentum=0.9?\n",
    "\n",
    "```python\n",
    "# Your code here\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Exercise 1 Solution\n",
    "# Uncomment to reveal\n",
    "\n",
    "# def sgd_momentum(grad_fn, start, lr=0.1, beta=0.9, iterations=50):\n",
    "#     \"\"\"SGD with momentum implementation.\"\"\"\n",
    "#     path = [start]\n",
    "#     point = np.array(start, dtype=float)\n",
    "#     velocity = np.zeros_like(point)\n",
    "#     \n",
    "#     for _ in range(iterations):\n",
    "#         grad = grad_fn(point[0], point[1])\n",
    "#         velocity = beta * velocity + grad\n",
    "#         point = point - lr * velocity\n",
    "#         path.append(point.copy())\n",
    "#     \n",
    "#     return np.array(path)\n",
    "# \n",
    "# # Test function and gradient\n",
    "# def f(x, y):\n",
    "#     return x**2 + 10 * y**2\n",
    "# \n",
    "# def grad_f(x, y):\n",
    "#     return np.array([2*x, 20*y])\n",
    "# \n",
    "# # Compare different momentum values\n",
    "# start = np.array([5.0, 5.0])\n",
    "# path_no_momentum = sgd_momentum(grad_f, start, beta=0.0)\n",
    "# path_momentum = sgd_momentum(grad_f, start, beta=0.9)\n",
    "# \n",
    "# # Visualize\n",
    "# # (Add visualization code here)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Learning Rate Sensitivity Analysis\n",
    "\n",
    "**Task**: Investigate how different learning rates affect training with different optimizers.\n",
    "\n",
    "**Requirements**:\n",
    "- Train the same model with Adam using learning rates: [0.1, 0.01, 0.001, 0.0001]\n",
    "- Train the same model with SGD using learning rates: [1.0, 0.1, 0.01, 0.001]\n",
    "- Plot convergence curves for each\n",
    "- Identify optimal learning rate ranges\n",
    "\n",
    "**Questions**:\n",
    "1. Which optimizer is more sensitive to learning rate?\n",
    "2. What happens with too high learning rates?\n",
    "3. What are the signs of learning rate being too small?\n",
    "\n",
    "```python\n",
    "# Your code here\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Exercise 2 Solution\n",
    "# Uncomment to reveal\n",
    "\n",
    "# adam_lrs = [0.1, 0.01, 0.001, 0.0001]\n",
    "# sgd_lrs = [1.0, 0.1, 0.01, 0.001]\n",
    "# \n",
    "# # Train with different Adam learning rates\n",
    "# adam_histories = {}\n",
    "# for lr in adam_lrs:\n",
    "#     model = create_model()\n",
    "#     model.compile(optimizer=optimizers.Adam(learning_rate=lr),\n",
    "#                   loss='sparse_categorical_crossentropy',\n",
    "#                   metrics=['accuracy'])\n",
    "#     history = model.fit(X_train, y_train, epochs=10, \n",
    "#                        validation_data=(X_valid, y_valid), verbose=0)\n",
    "#     adam_histories[lr] = history\n",
    "# \n",
    "# # Repeat for SGD\n",
    "# # (Add SGD training code here)\n",
    "# \n",
    "# # Plot comparison\n",
    "# # (Add plotting code here)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Custom Learning Rate Schedule\n",
    "\n",
    "**Task**: Implement a custom \"warm-up\" learning rate schedule.\n",
    "\n",
    "**Warm-up Schedule**:\n",
    "- Linearly increase LR from 0 to max_lr over first `warmup_epochs`\n",
    "- Then apply cosine annealing for remaining epochs\n",
    "\n",
    "This is commonly used in transformer training.\n",
    "\n",
    "**Requirements**:\n",
    "- Implement the warm-up schedule function\n",
    "- Train a model using this schedule\n",
    "- Compare with standard cosine annealing\n",
    "- Visualize the learning rate curve\n",
    "\n",
    "```python\n",
    "# Your code here\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Exercise 3 Solution\n",
    "# Uncomment to reveal\n",
    "\n",
    "# def warmup_cosine_schedule(epoch, warmup_epochs=5, max_lr=0.01, min_lr=0.0001, total_epochs=30):\n",
    "#     \"\"\"Warm-up followed by cosine annealing.\"\"\"\n",
    "#     if epoch < warmup_epochs:\n",
    "#         # Linear warm-up\n",
    "#         return max_lr * (epoch + 1) / warmup_epochs\n",
    "#     else:\n",
    "#         # Cosine annealing\n",
    "#         progress = (epoch - warmup_epochs) / (total_epochs - warmup_epochs)\n",
    "#         return min_lr + (max_lr - min_lr) * 0.5 * (1 + np.cos(np.pi * progress))\n",
    "# \n",
    "# # Create callback\n",
    "# warmup_scheduler = LearningRateScheduler(warmup_cosine_schedule, verbose=0)\n",
    "# \n",
    "# # Train model\n",
    "# model_warmup = create_model()\n",
    "# model_warmup.compile(optimizer=optimizers.SGD(learning_rate=0.01, momentum=0.9),\n",
    "#                      loss='sparse_categorical_crossentropy',\n",
    "#                      metrics=['accuracy'])\n",
    "# \n",
    "# history_warmup = model_warmup.fit(X_train, y_train, epochs=30,\n",
    "#                                   validation_data=(X_valid, y_valid),\n",
    "#                                   callbacks=[warmup_scheduler], verbose=0)\n",
    "# \n",
    "# # Visualize schedule\n",
    "# lrs = [warmup_cosine_schedule(e) for e in range(30)]\n",
    "# plt.plot(lrs)\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Learning Rate')\n",
    "# plt.title('Warm-up + Cosine Annealing Schedule')\n",
    "# plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Optimizer Robustness Test\n",
    "\n",
    "**Task**: Test optimizer robustness to different batch sizes.\n",
    "\n",
    "**Requirements**:\n",
    "- Train with batch sizes: [16, 32, 64, 128, 256, 512]\n",
    "- Use both Adam and SGD+momentum\n",
    "- Keep other hyperparameters constant\n",
    "- Measure: final accuracy, training time, convergence speed\n",
    "\n",
    "**Questions**:\n",
    "1. Which optimizer is more sensitive to batch size?\n",
    "2. What's the trade-off between batch size and convergence?\n",
    "3. Is there an optimal batch size?\n",
    "\n",
    "```python\n",
    "# Your code here\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Exercise 4 Solution\n",
    "# Uncomment to reveal\n",
    "\n",
    "# import time\n",
    "# \n",
    "# batch_sizes = [16, 32, 64, 128, 256, 512]\n",
    "# results = {'Adam': {}, 'SGD': {}}\n",
    "# \n",
    "# for batch_size in batch_sizes:\n",
    "#     # Test Adam\n",
    "#     model = create_model()\n",
    "#     model.compile(optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "#                   loss='sparse_categorical_crossentropy',\n",
    "#                   metrics=['accuracy'])\n",
    "#     \n",
    "#     start_time = time.time()\n",
    "#     history = model.fit(X_train, y_train, epochs=10, batch_size=batch_size,\n",
    "#                        validation_data=(X_valid, y_valid), verbose=0)\n",
    "#     training_time = time.time() - start_time\n",
    "#     \n",
    "#     results['Adam'][batch_size] = {\n",
    "#         'accuracy': history.history['val_accuracy'][-1],\n",
    "#         'time': training_time\n",
    "#     }\n",
    "#     \n",
    "#     # Repeat for SGD\n",
    "#     # (Add SGD training code here)\n",
    "# \n",
    "# # Analyze and visualize results\n",
    "# # (Add analysis code here)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Congratulations!** You've completed Module 06. You now understand:\n",
    "- Different gradient descent variants and their trade-offs\n",
    "- How momentum accelerates convergence\n",
    "- Adaptive learning rate methods (AdaGrad, RMSprop, Adam)\n",
    "- Learning rate schedules for better training\n",
    "- How to choose the right optimizer for your task\n",
    "\n",
    "Continue to **Module 07: Regularization Techniques** to learn how to prevent overfitting!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
