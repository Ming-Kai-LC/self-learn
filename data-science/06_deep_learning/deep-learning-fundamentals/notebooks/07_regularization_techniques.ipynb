{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 07: Regularization Techniques (Dropout, Batch Normalization)\n",
    "\n",
    "**Difficulty**: ⭐⭐⭐ (Advanced)\n",
    "\n",
    "**Estimated Time**: 60-75 minutes\n",
    "\n",
    "**Prerequisites**: \n",
    "- [Module 05: Feed-Forward Neural Networks with Keras](05_feedforward_neural_networks_keras.ipynb)\n",
    "- [Module 06: Optimizers (SGD, Adam, RMSprop)](06_optimizers_sgd_adam_rmsprop.ipynb)\n",
    "- [Module 04: Introduction to TensorFlow and Keras](04_introduction_to_tensorflow_keras.ipynb)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Understand overfitting in deep learning and diagnose it effectively\n",
    "2. Apply L1 and L2 regularization to control model complexity\n",
    "3. Implement Dropout for reducing co-adaptation of neurons\n",
    "4. Use Batch Normalization to stabilize and accelerate training\n",
    "5. Apply Early Stopping to prevent overtraining\n",
    "6. Combine multiple regularization techniques effectively\n",
    "7. Understand when and how to use each regularization method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from tensorflow.keras.datasets import fashion_mnist, cifar10\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Sklearn for data splitting\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding Overfitting\n",
    "\n",
    "### What is Overfitting?\n",
    "\n",
    "**Overfitting** occurs when a model learns the training data **too well**, including noise and random fluctuations, at the expense of generalization to new data.\n",
    "\n",
    "### Visual Signature of Overfitting:\n",
    "\n",
    "```\n",
    "Loss/Accuracy Curves:\n",
    "\n",
    "Training Loss:     \\          (keeps decreasing)\n",
    "                    \\___\n",
    "\n",
    "Validation Loss:    \\  /      (starts increasing)\n",
    "                     \\/\n",
    "                      ^\n",
    "                      |\n",
    "                Overfitting begins here\n",
    "```\n",
    "\n",
    "### Causes of Overfitting:\n",
    "\n",
    "1. **Model too complex**: Too many parameters relative to data\n",
    "2. **Insufficient data**: Not enough samples to learn generalizable patterns\n",
    "3. **Training too long**: Model memorizes training examples\n",
    "4. **No regularization**: Nothing prevents memorization\n",
    "5. **Noisy labels**: Model learns incorrect patterns\n",
    "\n",
    "### Solutions (Regularization Techniques):\n",
    "\n",
    "1. **L1/L2 Regularization**: Penalize large weights\n",
    "2. **Dropout**: Randomly deactivate neurons during training\n",
    "3. **Batch Normalization**: Normalize activations\n",
    "4. **Early Stopping**: Stop training when validation performance degrades\n",
    "5. **Data Augmentation**: Artificially expand training data\n",
    "6. **Architecture simplification**: Use fewer layers/neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load Fashion-MNIST for demonstration\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Normalize and flatten\n",
    "X_train_full = X_train_full.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "X_train_full_flat = X_train_full.reshape(-1, 784)\n",
    "X_test_flat = X_test.reshape(-1, 784)\n",
    "\n",
    "# Create validation split\n",
    "X_train = X_train_full_flat[:50000]\n",
    "X_valid = X_train_full_flat[50000:]\n",
    "y_train = y_train_full[:50000]\n",
    "y_valid = y_train_full[50000:]\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Validation set: {X_valid.shape}\")\n",
    "print(f\"Test set: {X_test_flat.shape}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a deliberately overfitting model (too complex)\n",
    "def create_overfitting_model():\n",
    "    \"\"\"\n",
    "    Create an overly complex model to demonstrate overfitting.\n",
    "    Many parameters, deep architecture.\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(784,)),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ], name='overfitting_model')\n",
    "    return model\n",
    "\n",
    "# Train overfitting model\n",
    "print(\"Training overly complex model (will overfit)...\\n\")\n",
    "model_overfit = create_overfitting_model()\n",
    "model_overfit.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train with small dataset to induce overfitting\n",
    "# Use only 5000 samples\n",
    "X_train_small = X_train[:5000]\n",
    "y_train_small = y_train[:5000]\n",
    "\n",
    "history_overfit = model_overfit.fit(\n",
    "    X_train_small, y_train_small,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"Training completed!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize overfitting\n",
    "def plot_overfitting_analysis(history, title=\"Overfitting Analysis\"):\n",
    "    \"\"\"\n",
    "    Plot training history with overfitting annotations.\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Loss plot\n",
    "    epochs = range(1, len(history.history['loss']) + 1)\n",
    "    ax1.plot(epochs, history.history['loss'], 'b-', linewidth=2, label='Training Loss')\n",
    "    ax1.plot(epochs, history.history['val_loss'], 'r-', linewidth=2, label='Validation Loss')\n",
    "    \n",
    "    # Find overfitting point (where val_loss starts increasing)\n",
    "    val_loss = np.array(history.history['val_loss'])\n",
    "    # Simple heuristic: find first local minimum\n",
    "    best_epoch = np.argmin(val_loss)\n",
    "    \n",
    "    ax1.axvline(x=best_epoch+1, color='green', linestyle='--', linewidth=2, \n",
    "                label=f'Best Epoch ({best_epoch+1})')\n",
    "    ax1.fill_between(epochs[best_epoch:], 0, max(history.history['val_loss']), \n",
    "                      alpha=0.2, color='red', label='Overfitting Region')\n",
    "    \n",
    "    ax1.set_xlabel('Epoch', fontsize=12)\n",
    "    ax1.set_ylabel('Loss', fontsize=12)\n",
    "    ax1.set_title('Loss Over Time', fontsize=12, fontweight='bold')\n",
    "    ax1.legend(fontsize=10)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy plot\n",
    "    ax2.plot(epochs, history.history['accuracy'], 'b-', linewidth=2, label='Training Accuracy')\n",
    "    ax2.plot(epochs, history.history['val_accuracy'], 'r-', linewidth=2, label='Validation Accuracy')\n",
    "    ax2.axvline(x=best_epoch+1, color='green', linestyle='--', linewidth=2, \n",
    "                label=f'Best Epoch ({best_epoch+1})')\n",
    "    ax2.fill_between(epochs[best_epoch:], 0, 1, \n",
    "                      alpha=0.2, color='red', label='Overfitting Region')\n",
    "    \n",
    "    ax2.set_xlabel('Epoch', fontsize=12)\n",
    "    ax2.set_ylabel('Accuracy', fontsize=12)\n",
    "    ax2.set_title('Accuracy Over Time', fontsize=12, fontweight='bold')\n",
    "    ax2.legend(fontsize=10)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(title, fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"\\nOverfitting Statistics:\")\n",
    "    print(f\"  Best Epoch: {best_epoch + 1}\")\n",
    "    print(f\"  Best Validation Loss: {val_loss[best_epoch]:.4f}\")\n",
    "    print(f\"  Final Training Accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "    print(f\"  Final Validation Accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n",
    "    print(f\"  Accuracy Gap: {abs(history.history['accuracy'][-1] - history.history['val_accuracy'][-1]):.4f}\")\n",
    "\n",
    "plot_overfitting_analysis(history_overfit, \"Demonstration of Overfitting\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. L1 and L2 Regularization\n",
    "\n",
    "### Weight Regularization Concept:\n",
    "\n",
    "Add a penalty term to the loss function to discourage large weights.\n",
    "\n",
    "### L2 Regularization (Ridge, Weight Decay):\n",
    "\n",
    "$$\n",
    "L_{\\text{total}} = L_{\\text{original}} + \\lambda \\sum_{i} w_i^2\n",
    "$$\n",
    "\n",
    "- **Effect**: Penalizes large weights, prefers many small weights\n",
    "- **Gradient**: $\\frac{\\partial L}{\\partial w_i} = \\frac{\\partial L_{\\text{original}}}{\\partial w_i} + 2\\lambda w_i$\n",
    "- **Use case**: Default choice, works well in most scenarios\n",
    "\n",
    "### L1 Regularization (Lasso):\n",
    "\n",
    "$$\n",
    "L_{\\text{total}} = L_{\\text{original}} + \\lambda \\sum_{i} |w_i|\n",
    "$$\n",
    "\n",
    "- **Effect**: Drives weights to exactly zero, creates sparse models\n",
    "- **Gradient**: $\\frac{\\partial L}{\\partial w_i} = \\frac{\\partial L_{\\text{original}}}{\\partial w_i} + \\lambda \\cdot \\text{sign}(w_i)$\n",
    "- **Use case**: Feature selection, when you want sparse networks\n",
    "\n",
    "### Elastic Net (L1 + L2):\n",
    "\n",
    "$$\n",
    "L_{\\text{total}} = L_{\\text{original}} + \\lambda_1 \\sum_{i} |w_i| + \\lambda_2 \\sum_{i} w_i^2\n",
    "$$\n",
    "\n",
    "Combines benefits of both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Model with L2 regularization\n",
    "def create_l2_model(l2_lambda=0.01):\n",
    "    \"\"\"\n",
    "    Create model with L2 regularization.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    l2_lambda : float\n",
    "        L2 regularization strength (typically 0.001 to 0.1)\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(784,)),\n",
    "        layers.Dense(512, activation='relu', \n",
    "                    kernel_regularizer=regularizers.l2(l2_lambda)),\n",
    "        layers.Dense(512, activation='relu',\n",
    "                    kernel_regularizer=regularizers.l2(l2_lambda)),\n",
    "        layers.Dense(512, activation='relu',\n",
    "                    kernel_regularizer=regularizers.l2(l2_lambda)),\n",
    "        layers.Dense(512, activation='relu',\n",
    "                    kernel_regularizer=regularizers.l2(l2_lambda)),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ], name='l2_regularized')\n",
    "    return model\n",
    "\n",
    "# Model with L1 regularization\n",
    "def create_l1_model(l1_lambda=0.01):\n",
    "    \"\"\"\n",
    "    Create model with L1 regularization.\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(784,)),\n",
    "        layers.Dense(512, activation='relu',\n",
    "                    kernel_regularizer=regularizers.l1(l1_lambda)),\n",
    "        layers.Dense(512, activation='relu',\n",
    "                    kernel_regularizer=regularizers.l1(l1_lambda)),\n",
    "        layers.Dense(512, activation='relu',\n",
    "                    kernel_regularizer=regularizers.l1(l1_lambda)),\n",
    "        layers.Dense(512, activation='relu',\n",
    "                    kernel_regularizer=regularizers.l1(l1_lambda)),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ], name='l1_regularized')\n",
    "    return model\n",
    "\n",
    "# Train models with regularization\n",
    "print(\"Training models with L1/L2 regularization...\\n\")\n",
    "\n",
    "# L2 model\n",
    "model_l2 = create_l2_model(l2_lambda=0.001)\n",
    "model_l2.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "history_l2 = model_l2.fit(X_train_small, y_train_small, epochs=50, batch_size=32,\n",
    "                          validation_data=(X_valid, y_valid), verbose=0)\n",
    "print(\"L2 model trained!\")\n",
    "\n",
    "# L1 model\n",
    "model_l1 = create_l1_model(l1_lambda=0.0001)\n",
    "model_l1.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "history_l1 = model_l1.fit(X_train_small, y_train_small, epochs=50, batch_size=32,\n",
    "                          validation_data=(X_valid, y_valid), verbose=0)\n",
    "print(\"L1 model trained!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compare regularization methods\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Validation loss\n",
    "ax1.plot(history_overfit.history['val_loss'], linewidth=2, label='No Regularization', marker='o')\n",
    "ax1.plot(history_l2.history['val_loss'], linewidth=2, label='L2 Regularization', marker='s')\n",
    "ax1.plot(history_l1.history['val_loss'], linewidth=2, label='L1 Regularization', marker='^')\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Validation Loss', fontsize=12)\n",
    "ax1.set_title('Impact of L1/L2 Regularization on Validation Loss', fontsize=12, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Validation accuracy\n",
    "ax2.plot(history_overfit.history['val_accuracy'], linewidth=2, label='No Regularization', marker='o')\n",
    "ax2.plot(history_l2.history['val_accuracy'], linewidth=2, label='L2 Regularization', marker='s')\n",
    "ax2.plot(history_l1.history['val_accuracy'], linewidth=2, label='L1 Regularization', marker='^')\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Validation Accuracy', fontsize=12)\n",
    "ax2.set_title('Impact of L1/L2 Regularization on Validation Accuracy', fontsize=12, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nBest Validation Accuracy:\")\n",
    "print(f\"  No Regularization: {max(history_overfit.history['val_accuracy']):.4f}\")\n",
    "print(f\"  L2 Regularization: {max(history_l2.history['val_accuracy']):.4f}\")\n",
    "print(f\"  L1 Regularization: {max(history_l1.history['val_accuracy']):.4f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dropout: Preventing Co-Adaptation\n",
    "\n",
    "### Dropout Intuition:\n",
    "\n",
    "**Key Idea**: During training, randomly \"drop\" (set to zero) a fraction of neurons.\n",
    "\n",
    "### How Dropout Works:\n",
    "\n",
    "1. **Training**: Each neuron has probability $p$ of being dropped\n",
    "   - Remaining neurons scaled by $1/(1-p)$ to maintain expected sum\n",
    "   - Forces network to learn redundant representations\n",
    "\n",
    "2. **Inference**: Use all neurons (no dropout)\n",
    "   - Weights automatically scaled due to training scaling\n",
    "\n",
    "### Mathematical Formulation:\n",
    "\n",
    "During training:\n",
    "$$\n",
    "\\begin{align}\n",
    "r_i &\\sim \\text{Bernoulli}(1-p) \\quad \\text{(dropout mask)} \\\\\n",
    "h_i &= \\frac{r_i}{1-p} \\cdot f(\\mathbf{W}\\mathbf{x} + \\mathbf{b}) \\quad \\text{(scaled activation)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "### Why Dropout Works:\n",
    "\n",
    "1. **Ensemble Effect**: Training many \"thinned\" networks, averaging at test time\n",
    "2. **Breaks Co-Adaptation**: Neurons can't rely on specific other neurons\n",
    "3. **Implicit Regularization**: Similar to L2 for linear models\n",
    "\n",
    "### Typical Dropout Rates:\n",
    "- **Hidden layers**: 0.2 to 0.5 (20-50% dropped)\n",
    "- **Input layer**: 0.1 to 0.2 (10-20% dropped, if used at all)\n",
    "- **Output layer**: Never use dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Model with dropout\n",
    "def create_dropout_model(dropout_rate=0.5):\n",
    "    \"\"\"\n",
    "    Create model with dropout regularization.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dropout_rate : float\n",
    "        Fraction of neurons to drop (typically 0.2 to 0.5)\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(784,)),\n",
    "        \n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dropout(dropout_rate),  # Drop 50% of neurons\n",
    "        \n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        \n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        \n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        \n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ], name='dropout_model')\n",
    "    return model\n",
    "\n",
    "# Train with different dropout rates\n",
    "dropout_rates = [0.2, 0.3, 0.5]\n",
    "histories_dropout = {}\n",
    "\n",
    "print(\"Training models with different dropout rates...\\n\")\n",
    "\n",
    "for rate in dropout_rates:\n",
    "    print(f\"Training with dropout rate = {rate}...\")\n",
    "    model = create_dropout_model(dropout_rate=rate)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(X_train_small, y_train_small, epochs=50, batch_size=32,\n",
    "                       validation_data=(X_valid, y_valid), verbose=0)\n",
    "    histories_dropout[rate] = history\n",
    "    print(f\"  Best validation accuracy: {max(history.history['val_accuracy']):.4f}\")\n",
    "\n",
    "print(\"\\nAll dropout models trained!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compare dropout rates\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot no-dropout baseline\n",
    "ax1.plot(history_overfit.history['val_loss'], linewidth=2, \n",
    "         label='No Dropout', linestyle='--', alpha=0.7)\n",
    "ax2.plot(history_overfit.history['val_accuracy'], linewidth=2, \n",
    "         label='No Dropout', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Plot different dropout rates\n",
    "colors = ['green', 'blue', 'red']\n",
    "for (rate, history), color in zip(histories_dropout.items(), colors):\n",
    "    ax1.plot(history.history['val_loss'], linewidth=2, \n",
    "             label=f'Dropout {rate}', marker='o', color=color)\n",
    "    ax2.plot(history.history['val_accuracy'], linewidth=2, \n",
    "             label=f'Dropout {rate}', marker='o', color=color)\n",
    "\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Validation Loss', fontsize=12)\n",
    "ax1.set_title('Dropout Impact on Validation Loss', fontsize=12, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Validation Accuracy', fontsize=12)\n",
    "ax2.set_title('Dropout Impact on Validation Accuracy', fontsize=12, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Batch Normalization: Stabilizing Training\n",
    "\n",
    "### The Internal Covariate Shift Problem:\n",
    "\n",
    "During training, the distribution of layer inputs changes as parameters update.\n",
    "This slows down training and requires careful initialization and low learning rates.\n",
    "\n",
    "### Batch Normalization Solution:\n",
    "\n",
    "**Normalize** each mini-batch to have mean 0 and variance 1:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mu_B &= \\frac{1}{m} \\sum_{i=1}^m x_i \\quad \\text{(batch mean)} \\\\\n",
    "\\sigma_B^2 &= \\frac{1}{m} \\sum_{i=1}^m (x_i - \\mu_B)^2 \\quad \\text{(batch variance)} \\\\\n",
    "\\hat{x}_i &= \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}} \\quad \\text{(normalize)} \\\\\n",
    "y_i &= \\gamma \\hat{x}_i + \\beta \\quad \\text{(scale and shift)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\gamma$ and $\\beta$ are **learnable** parameters (scale and shift)\n",
    "- $\\epsilon$ is a small constant for numerical stability ($10^{-5}$)\n",
    "\n",
    "### Benefits of Batch Normalization:\n",
    "\n",
    "1. **Faster Training**: Can use higher learning rates\n",
    "2. **Better Generalization**: Acts as regularization\n",
    "3. **Less Sensitive to Initialization**: Normalizes activations\n",
    "4. **Reduces Need for Dropout**: Provides regularization effect\n",
    "\n",
    "### Where to Place BatchNorm:\n",
    "\n",
    "**Option 1** (Original paper): After activation\n",
    "```python\n",
    "Dense → Activation → BatchNorm → Dropout\n",
    "```\n",
    "\n",
    "**Option 2** (More common): Before activation\n",
    "```python\n",
    "Dense → BatchNorm → Activation → Dropout\n",
    "```\n",
    "\n",
    "Both work well in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Model with Batch Normalization\n",
    "def create_batchnorm_model():\n",
    "    \"\"\"\n",
    "    Create model with Batch Normalization.\n",
    "    Using BatchNorm before activation (more common approach).\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(784,)),\n",
    "        \n",
    "        layers.Dense(512),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        \n",
    "        layers.Dense(512),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        \n",
    "        layers.Dense(512),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        \n",
    "        layers.Dense(512),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        \n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ], name='batchnorm_model')\n",
    "    return model\n",
    "\n",
    "# Model with both Batch Normalization and Dropout\n",
    "def create_batchnorm_dropout_model(dropout_rate=0.3):\n",
    "    \"\"\"\n",
    "    Create model combining BatchNorm and Dropout.\n",
    "    Order: Dense → BatchNorm → Activation → Dropout\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(784,)),\n",
    "        \n",
    "        layers.Dense(512),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        \n",
    "        layers.Dense(512),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        \n",
    "        layers.Dense(512),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        \n",
    "        layers.Dense(512),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        \n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ], name='batchnorm_dropout_model')\n",
    "    return model\n",
    "\n",
    "# Train models\n",
    "print(\"Training BatchNorm models...\\n\")\n",
    "\n",
    "# BatchNorm only\n",
    "model_bn = create_batchnorm_model()\n",
    "model_bn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "history_bn = model_bn.fit(X_train_small, y_train_small, epochs=50, batch_size=32,\n",
    "                          validation_data=(X_valid, y_valid), verbose=0)\n",
    "print(\"BatchNorm model trained!\")\n",
    "print(f\"  Best validation accuracy: {max(history_bn.history['val_accuracy']):.4f}\")\n",
    "\n",
    "# BatchNorm + Dropout\n",
    "model_bn_drop = create_batchnorm_dropout_model(dropout_rate=0.3)\n",
    "model_bn_drop.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "history_bn_drop = model_bn_drop.fit(X_train_small, y_train_small, epochs=50, batch_size=32,\n",
    "                                     validation_data=(X_valid, y_valid), verbose=0)\n",
    "print(\"\\nBatchNorm + Dropout model trained!\")\n",
    "print(f\"  Best validation accuracy: {max(history_bn_drop.history['val_accuracy']):.4f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compare all regularization techniques\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Validation loss comparison\n",
    "ax1.plot(history_overfit.history['val_loss'], linewidth=2, label='No Regularization', alpha=0.7)\n",
    "ax1.plot(history_l2.history['val_loss'], linewidth=2, label='L2 Only')\n",
    "ax1.plot(histories_dropout[0.3].history['val_loss'], linewidth=2, label='Dropout Only')\n",
    "ax1.plot(history_bn.history['val_loss'], linewidth=2, label='BatchNorm Only')\n",
    "ax1.plot(history_bn_drop.history['val_loss'], linewidth=2, label='BatchNorm + Dropout', linewidth=3)\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Validation Loss', fontsize=12)\n",
    "ax1.set_title('Regularization Techniques Comparison: Loss', fontsize=12, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Validation accuracy comparison\n",
    "ax2.plot(history_overfit.history['val_accuracy'], linewidth=2, label='No Regularization', alpha=0.7)\n",
    "ax2.plot(history_l2.history['val_accuracy'], linewidth=2, label='L2 Only')\n",
    "ax2.plot(histories_dropout[0.3].history['val_accuracy'], linewidth=2, label='Dropout Only')\n",
    "ax2.plot(history_bn.history['val_accuracy'], linewidth=2, label='BatchNorm Only')\n",
    "ax2.plot(history_bn_drop.history['val_accuracy'], linewidth=2, label='BatchNorm + Dropout', linewidth=3)\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Validation Accuracy', fontsize=12)\n",
    "ax2.set_title('Regularization Techniques Comparison: Accuracy', fontsize=12, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"REGULARIZATION TECHNIQUES COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Technique':<30} {'Best Val Acc':<20} {'Final Val Acc':<20}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "results = [\n",
    "    ('No Regularization', history_overfit),\n",
    "    ('L2 Only', history_l2),\n",
    "    ('Dropout Only (0.3)', histories_dropout[0.3]),\n",
    "    ('BatchNorm Only', history_bn),\n",
    "    ('BatchNorm + Dropout', history_bn_drop)\n",
    "]\n",
    "\n",
    "for name, history in results:\n",
    "    best_acc = max(history.history['val_accuracy'])\n",
    "    final_acc = history.history['val_accuracy'][-1]\n",
    "    print(f\"{name:<30} {best_acc:<20.4f} {final_acc:<20.4f}\")\n",
    "\n",
    "print(\"=\"*80)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Early Stopping: Stop Before Overfitting\n",
    "\n",
    "### Concept:\n",
    "\n",
    "Monitor validation performance and **stop training** when it stops improving.\n",
    "\n",
    "### How It Works:\n",
    "\n",
    "1. Train the model while monitoring validation metric\n",
    "2. If validation metric doesn't improve for `patience` epochs, stop\n",
    "3. Optionally restore weights from best epoch\n",
    "\n",
    "### Parameters:\n",
    "\n",
    "- **monitor**: Metric to watch (`'val_loss'`, `'val_accuracy'`)\n",
    "- **patience**: How many epochs to wait for improvement\n",
    "- **mode**: `'min'` for loss, `'max'` for accuracy\n",
    "- **restore_best_weights**: Restore model to best epoch (recommended)\n",
    "\n",
    "### Benefits:\n",
    "\n",
    "- Automatically finds optimal training duration\n",
    "- Prevents overtraining\n",
    "- Saves computation time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Train with Early Stopping\n",
    "print(\"Training with Early Stopping...\\n\")\n",
    "\n",
    "# Create model\n",
    "model_early_stop = create_batchnorm_dropout_model(dropout_rate=0.3)\n",
    "model_early_stop.compile(optimizer='adam', \n",
    "                         loss='sparse_categorical_crossentropy', \n",
    "                         metrics=['accuracy'])\n",
    "\n",
    "# Define Early Stopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',          # Monitor validation loss\n",
    "    patience=10,                  # Wait 10 epochs for improvement\n",
    "    mode='min',                   # Lower is better for loss\n",
    "    restore_best_weights=True,    # Restore weights from best epoch\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train with callback\n",
    "history_early_stop = model_early_stop.fit(\n",
    "    X_train_small, y_train_small,\n",
    "    epochs=100,  # Set high, early stopping will terminate\n",
    "    batch_size=32,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining stopped at epoch {len(history_early_stop.history['loss'])}\")\n",
    "print(f\"Best validation accuracy: {max(history_early_stop.history['val_accuracy']):.4f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize Early Stopping\n",
    "plot_overfitting_analysis(history_early_stop, \"Early Stopping Example\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Combining Regularization Techniques\n",
    "\n",
    "### Best Practices for Combining Regularization:\n",
    "\n",
    "#### Common Combinations:\n",
    "\n",
    "1. **BatchNorm + Dropout + Early Stopping** (Most Common)\n",
    "   ```python\n",
    "   Dense → BatchNorm → ReLU → Dropout\n",
    "   ```\n",
    "   - BatchNorm: Faster training, some regularization\n",
    "   - Dropout: Additional regularization\n",
    "   - Early Stopping: Prevent overtraining\n",
    "\n",
    "2. **L2 + Dropout + Early Stopping**\n",
    "   - L2: Smooth weight regularization\n",
    "   - Dropout: Ensemble effect\n",
    "   - Early Stopping: Automatic duration\n",
    "\n",
    "3. **BatchNorm + L2 + Early Stopping**\n",
    "   - Good for very deep networks\n",
    "   - BatchNorm may reduce need for dropout\n",
    "\n",
    "### Guidelines:\n",
    "\n",
    "| Scenario | Recommended Regularization |\n",
    "|----------|---------------------------|\n",
    "| **Small dataset** | L2 + Dropout (high rate) + Early Stopping |\n",
    "| **Large dataset** | BatchNorm + Light Dropout + Early Stopping |\n",
    "| **Deep network** | BatchNorm + Dropout + Early Stopping |\n",
    "| **Need speed** | BatchNorm + Early Stopping |\n",
    "| **Need sparsity** | L1 + Dropout + Early Stopping |\n",
    "\n",
    "### Hyperparameter Tuning Order:\n",
    "\n",
    "1. Start with BatchNorm (almost always helps)\n",
    "2. Add moderate Dropout (0.2-0.3)\n",
    "3. Add Early Stopping (patience ~10-20 epochs)\n",
    "4. If still overfitting, increase Dropout or add L2\n",
    "5. If underfitting, reduce regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create production-ready model with all best practices\n",
    "def create_production_model():\n",
    "    \"\"\"\n",
    "    Production-ready model with modern best practices:\n",
    "    - Batch Normalization for stable training\n",
    "    - Dropout for regularization\n",
    "    - Will use Early Stopping during training\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(784,)),\n",
    "        \n",
    "        # Layer 1\n",
    "        layers.Dense(256),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        # Layer 2\n",
    "        layers.Dense(128),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        # Layer 3\n",
    "        layers.Dense(64),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        \n",
    "        # Output layer (no dropout, no batchnorm)\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ], name='production_model')\n",
    "    return model\n",
    "\n",
    "# Train on FULL dataset with all regularization techniques\n",
    "print(\"Training production model on full dataset...\\n\")\n",
    "\n",
    "model_prod = create_production_model()\n",
    "model_prod.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1),\n",
    "    ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, verbose=0)\n",
    "]\n",
    "\n",
    "# Train on FULL training set\n",
    "history_prod = model_prod.fit(\n",
    "    X_train, y_train,  # Full training set\n",
    "    epochs=100,\n",
    "    batch_size=128,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    callbacks=callbacks,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining completed at epoch {len(history_prod.history['loss'])}\")\n",
    "print(f\"Best validation accuracy: {max(history_prod.history['val_accuracy']):.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_accuracy = model_prod.evaluate(X_test_flat, y_test, verbose=0)\n",
    "print(f\"\\nTest set performance:\")\n",
    "print(f\"  Test Loss: {test_loss:.4f}\")\n",
    "print(f\"  Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot production model training\n",
    "plot_overfitting_analysis(history_prod, \"Production Model with All Regularization Techniques\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "1. **Overfitting**:\n",
    "   - Model learns training data too well\n",
    "   - Validation loss increases while training loss decreases\n",
    "   - Large gap between training and validation accuracy\n",
    "\n",
    "2. **L1/L2 Regularization**:\n",
    "   - L2: Penalizes large weights, most common\n",
    "   - L1: Creates sparse weights, feature selection\n",
    "   - Add regularization term to loss function\n",
    "\n",
    "3. **Dropout**:\n",
    "   - Randomly drop neurons during training\n",
    "   - Prevents co-adaptation\n",
    "   - Ensemble effect\n",
    "   - Typical rates: 0.2-0.5 for hidden layers\n",
    "\n",
    "4. **Batch Normalization**:\n",
    "   - Normalizes layer inputs\n",
    "   - Faster training, higher learning rates\n",
    "   - Regularization effect\n",
    "   - Place before or after activation\n",
    "\n",
    "5. **Early Stopping**:\n",
    "   - Monitor validation performance\n",
    "   - Stop when no improvement\n",
    "   - Restore best weights\n",
    "   - Patience typically 10-20 epochs\n",
    "\n",
    "6. **Combining Techniques**:\n",
    "   - BatchNorm + Dropout + Early Stopping (most common)\n",
    "   - Start with BatchNorm, add Dropout if needed\n",
    "   - Always use Early Stopping\n",
    "\n",
    "### Decision Guide:\n",
    "\n",
    "```\n",
    "Is your model overfitting?\n",
    "    |\n",
    "    |-- YES → Add regularization\n",
    "    |       |\n",
    "    |       |-- Try BatchNorm first (usually helps)\n",
    "    |       |-- Add Dropout (0.2-0.5)\n",
    "    |       |-- Add L2 if still overfitting\n",
    "    |       |-- Always use Early Stopping\n",
    "    |\n",
    "    |-- NO → Check if underfitting\n",
    "            |\n",
    "            |-- Reduce regularization\n",
    "            |-- Increase model capacity\n",
    "            |-- Train longer\n",
    "```\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "- **Module 08**: Loss Functions and Metrics\n",
    "- **Module 09**: Hyperparameter Tuning for Deep Learning\n",
    "\n",
    "### Additional Resources:\n",
    "\n",
    "- [Dropout: A Simple Way to Prevent Overfitting](https://jmlr.org/papers/v15/srivastava14a.html)\n",
    "- [Batch Normalization Paper](https://arxiv.org/abs/1502.03167)\n",
    "- [Keras Regularizers Documentation](https://keras.io/api/layers/regularizers/)\n",
    "- [CS231n: Regularization](https://cs231n.github.io/neural-networks-2/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Dropout Rate Sensitivity\n",
    "\n",
    "**Task**: Systematically test how dropout rate affects model performance.\n",
    "\n",
    "**Requirements**:\n",
    "- Test dropout rates: [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "- Use same architecture and training configuration\n",
    "- Plot validation accuracy vs dropout rate\n",
    "- Identify optimal dropout rate\n",
    "\n",
    "**Questions**:\n",
    "1. What happens with very high dropout (0.6-0.7)?\n",
    "2. Is there a \"sweet spot\" dropout rate?\n",
    "3. How does dropout affect training time?\n",
    "\n",
    "```python\n",
    "# Your code here\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Exercise 1 Solution\n",
    "# Uncomment to reveal\n",
    "\n",
    "# dropout_rates = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "# results = {}\n",
    "# \n",
    "# for rate in dropout_rates:\n",
    "#     model = create_dropout_model(dropout_rate=rate)\n",
    "#     model.compile(optimizer='adam',\n",
    "#                   loss='sparse_categorical_crossentropy',\n",
    "#                   metrics=['accuracy'])\n",
    "#     \n",
    "#     history = model.fit(X_train, y_train, epochs=20,\n",
    "#                        validation_data=(X_valid, y_valid),\n",
    "#                        verbose=0)\n",
    "#     \n",
    "#     results[rate] = {\n",
    "#         'best_val_acc': max(history.history['val_accuracy']),\n",
    "#         'final_val_acc': history.history['val_accuracy'][-1]\n",
    "#     }\n",
    "# \n",
    "# # Plot results\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# rates = list(results.keys())\n",
    "# best_accs = [results[r]['best_val_acc'] for r in rates]\n",
    "# plt.plot(rates, best_accs, marker='o', linewidth=2)\n",
    "# plt.xlabel('Dropout Rate')\n",
    "# plt.ylabel('Best Validation Accuracy')\n",
    "# plt.title('Dropout Rate vs Model Performance')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: BatchNorm Placement Comparison\n",
    "\n",
    "**Task**: Compare different BatchNorm placement strategies.\n",
    "\n",
    "**Requirements**:\n",
    "- Model 1: Dense → BatchNorm → Activation\n",
    "- Model 2: Dense → Activation → BatchNorm\n",
    "- Model 3: No BatchNorm (baseline)\n",
    "- Train all with same configuration\n",
    "- Compare convergence speed and final accuracy\n",
    "\n",
    "**Questions**:\n",
    "1. Which placement converges faster?\n",
    "2. Which achieves better final accuracy?\n",
    "3. Is there a significant difference?\n",
    "\n",
    "```python\n",
    "# Your code here\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Exercise 2 Solution\n",
    "# Uncomment to reveal\n",
    "\n",
    "# def create_bn_before_activation():\n",
    "#     model = models.Sequential([\n",
    "#         layers.Input(shape=(784,)),\n",
    "#         layers.Dense(256),\n",
    "#         layers.BatchNormalization(),\n",
    "#         layers.Activation('relu'),\n",
    "#         layers.Dense(128),\n",
    "#         layers.BatchNormalization(),\n",
    "#         layers.Activation('relu'),\n",
    "#         layers.Dense(10, activation='softmax')\n",
    "#     ])\n",
    "#     return model\n",
    "# \n",
    "# def create_bn_after_activation():\n",
    "#     model = models.Sequential([\n",
    "#         layers.Input(shape=(784,)),\n",
    "#         layers.Dense(256, activation='relu'),\n",
    "#         layers.BatchNormalization(),\n",
    "#         layers.Dense(128, activation='relu'),\n",
    "#         layers.BatchNormalization(),\n",
    "#         layers.Dense(10, activation='softmax')\n",
    "#     ])\n",
    "#     return model\n",
    "# \n",
    "# # Train and compare\n",
    "# # (Add training code here)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Regularization for CIFAR-10\n",
    "\n",
    "**Task**: Apply regularization techniques to improve CIFAR-10 classification.\n",
    "\n",
    "**Requirements**:\n",
    "- Load CIFAR-10 dataset\n",
    "- Create model with BatchNorm + Dropout + L2\n",
    "- Use Early Stopping\n",
    "- Achieve >55% validation accuracy\n",
    "- Compare with unregularized baseline\n",
    "\n",
    "**Questions**:\n",
    "1. Which regularization technique helped most?\n",
    "2. How much did regularization improve generalization?\n",
    "3. Did training take longer with regularization?\n",
    "\n",
    "```python\n",
    "# Your code here\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Exercise 3 Solution\n",
    "# Uncomment to reveal\n",
    "\n",
    "# # Load CIFAR-10\n",
    "# (X_cifar_train, y_cifar_train), (X_cifar_test, y_cifar_test) = cifar10.load_data()\n",
    "# \n",
    "# # Preprocess\n",
    "# X_cifar_train = X_cifar_train.astype('float32') / 255.0\n",
    "# X_cifar_test = X_cifar_test.astype('float32') / 255.0\n",
    "# X_cifar_train_flat = X_cifar_train.reshape(-1, 3072)\n",
    "# X_cifar_test_flat = X_cifar_test.reshape(-1, 3072)\n",
    "# y_cifar_train = y_cifar_train.flatten()\n",
    "# \n",
    "# # Split validation\n",
    "# X_c_train, X_c_valid, y_c_train, y_c_valid = train_test_split(\n",
    "#     X_cifar_train_flat, y_cifar_train, test_size=0.2, random_state=42\n",
    "# )\n",
    "# \n",
    "# # Create regularized model\n",
    "# model_cifar = models.Sequential([\n",
    "#     layers.Input(shape=(3072,)),\n",
    "#     layers.Dense(512, kernel_regularizer=regularizers.l2(0.001)),\n",
    "#     layers.BatchNormalization(),\n",
    "#     layers.Activation('relu'),\n",
    "#     layers.Dropout(0.4),\n",
    "#     # ... add more layers\n",
    "# ])\n",
    "# \n",
    "# # Compile and train\n",
    "# # (Add training code here)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Congratulations!** You've completed Module 07. You now understand:\n",
    "- How to diagnose and prevent overfitting\n",
    "- L1/L2 regularization for weight control\n",
    "- Dropout for ensemble-like regularization\n",
    "- Batch Normalization for stable training\n",
    "- Early Stopping to prevent overtraining\n",
    "- How to combine regularization techniques effectively\n",
    "\n",
    "Continue to **Module 08: Loss Functions and Metrics** to learn about different training objectives!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
