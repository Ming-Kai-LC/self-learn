{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 09: Hyperparameter Tuning for Deep Learning\n",
    "\n",
    "**Difficulty**: ⭐⭐⭐ (Advanced)\n",
    "\n",
    "**Estimated Time**: 60-75 minutes\n",
    "\n",
    "**Prerequisites**: \n",
    "- [Module 05: Feed-Forward Neural Networks with Keras](05_feedforward_neural_networks_keras.ipynb)\n",
    "- [Module 06: Optimizers](06_optimizers_sgd_adam_rmsprop.ipynb)\n",
    "- [Module 07: Regularization Techniques](07_regularization_techniques.ipynb)\n",
    "- [Module 08: Loss Functions and Metrics](08_loss_functions_and_metrics.ipynb)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Understand the importance of different hyperparameters and their impact on model performance\n",
    "2. Implement grid search and random search for hyperparameter optimization\n",
    "3. Use Keras Tuner for automated hyperparameter tuning\n",
    "4. Apply learning rate finder techniques to identify optimal learning rates\n",
    "5. Design effective hyperparameter search strategies for neural networks\n",
    "6. Track and compare experiments systematically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "We'll import all necessary libraries for hyperparameter tuning experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Deep learning libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "# Scikit-learn utilities\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Plotting configuration\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data\n",
    "\n",
    "We'll use Fashion-MNIST for our experiments. This dataset is large enough to show performance differences but small enough for quick iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load Fashion-MNIST dataset\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Normalize pixel values to [0, 1] range\n",
    "X_train_full = X_train_full.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "# Flatten images for fully connected network\n",
    "X_train_full = X_train_full.reshape(-1, 28 * 28)\n",
    "X_test = X_test.reshape(-1, 28 * 28)\n",
    "\n",
    "# Create validation split (20% of training data)\n",
    "validation_split = int(0.8 * len(X_train_full))\n",
    "X_train = X_train_full[:validation_split]\n",
    "y_train = y_train_full[:validation_split]\n",
    "X_val = X_train_full[validation_split:]\n",
    "y_val = y_train_full[validation_split:]\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Validation set shape: {X_val.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "print(f\"Number of classes: {len(np.unique(y_train))}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hyperparameter Importance Ranking\n",
    "\n",
    "Not all hyperparameters are equally important. Based on empirical research and practice, here's a typical importance ranking:\n",
    "\n",
    "### Critical Hyperparameters (High Impact)\n",
    "1. **Learning Rate**: Most important single hyperparameter\n",
    "2. **Network Architecture**: Number of layers and units per layer\n",
    "3. **Batch Size**: Affects training speed and generalization\n",
    "\n",
    "### Important Hyperparameters (Medium Impact)\n",
    "4. **Optimizer Type**: Adam vs SGD vs RMSprop\n",
    "5. **Regularization Strength**: L2 penalty, dropout rate\n",
    "6. **Activation Functions**: ReLU vs LeakyReLU vs others\n",
    "\n",
    "### Secondary Hyperparameters (Lower Impact)\n",
    "7. **Learning Rate Schedule**: Step decay, exponential decay\n",
    "8. **Batch Normalization**: Position and parameters\n",
    "9. **Initialization Method**: He vs Xavier initialization\n",
    "\n",
    "**Best Practice**: Start tuning from top to bottom. Don't waste time on minor hyperparameters if you haven't optimized the critical ones!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Manual Search Strategy\n",
    "\n",
    "Before using automated tools, let's understand manual hyperparameter search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def create_model(n_hidden=1, n_neurons=30, learning_rate=0.001, dropout_rate=0.0):\n",
    "    \"\"\"\n",
    "    Create a configurable neural network model.\n",
    "    \n",
    "    Args:\n",
    "        n_hidden: Number of hidden layers\n",
    "        n_neurons: Number of neurons per hidden layer\n",
    "        learning_rate: Learning rate for optimizer\n",
    "        dropout_rate: Dropout rate for regularization\n",
    "    \n",
    "    Returns:\n",
    "        Compiled Keras model\n",
    "    \"\"\"\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.InputLayer(input_shape=(784,)))\n",
    "    \n",
    "    # Add hidden layers\n",
    "    for _ in range(n_hidden):\n",
    "        model.add(layers.Dense(n_neurons, activation='relu'))\n",
    "        if dropout_rate > 0:\n",
    "            model.add(layers.Dropout(dropout_rate))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "    \n",
    "    # Compile model\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Test the model creation function\n",
    "test_model = create_model(n_hidden=2, n_neurons=64, learning_rate=0.001)\n",
    "print(\"Model created successfully!\")\n",
    "test_model.summary()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Grid Search\n",
    "\n",
    "Grid search tests all possible combinations of hyperparameters. It's exhaustive but can be slow.\n",
    "\n",
    "**When to use**: Small search spaces, critical hyperparameters only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define hyperparameter grid (keep it small for speed)\n",
    "param_grid = {\n",
    "    'n_hidden': [1, 2],\n",
    "    'n_neurons': [32, 64],\n",
    "    'learning_rate': [0.01, 0.001]\n",
    "}\n",
    "\n",
    "# Generate all combinations\n",
    "grid = ParameterGrid(param_grid)\n",
    "print(f\"Total combinations to test: {len(grid)}\")\n",
    "\n",
    "# Store results\n",
    "grid_search_results = []\n",
    "\n",
    "# Test each combination\n",
    "for idx, params in enumerate(grid):\n",
    "    print(f\"\\nTesting combination {idx + 1}/{len(grid)}: {params}\")\n",
    "    \n",
    "    # Create and train model\n",
    "    model = create_model(**params)\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=5,  # Short training for demo\n",
    "        batch_size=128,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Record best validation accuracy\n",
    "    best_val_acc = max(history.history['val_accuracy'])\n",
    "    \n",
    "    result = params.copy()\n",
    "    result['val_accuracy'] = best_val_acc\n",
    "    grid_search_results.append(result)\n",
    "    \n",
    "    print(f\"  Best validation accuracy: {best_val_acc:.4f}\")\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "grid_results_df = pd.DataFrame(grid_search_results)\n",
    "grid_results_df = grid_results_df.sort_values('val_accuracy', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Grid Search Results (sorted by validation accuracy):\")\n",
    "print(\"=\"*60)\n",
    "print(grid_results_df.to_string(index=False))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Random Search\n",
    "\n",
    "Random search samples random combinations from hyperparameter distributions. Research shows it often outperforms grid search!\n",
    "\n",
    "**Why Random Search Works Better**:\n",
    "- Explores more diverse hyperparameter values\n",
    "- More efficient when some hyperparameters don't matter much\n",
    "- Can set a time budget instead of exhaustive search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define hyperparameter distributions\n",
    "n_iterations = 10  # Number of random combinations to try\n",
    "\n",
    "random_search_results = []\n",
    "\n",
    "for idx in range(n_iterations):\n",
    "    # Sample random hyperparameters\n",
    "    params = {\n",
    "        'n_hidden': np.random.choice([1, 2, 3]),\n",
    "        'n_neurons': np.random.choice([32, 64, 128]),\n",
    "        'learning_rate': 10 ** np.random.uniform(-4, -2),  # Log-uniform sampling\n",
    "        'dropout_rate': np.random.uniform(0.0, 0.5)\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nIteration {idx + 1}/{n_iterations}:\")\n",
    "    print(f\"  Parameters: {params}\")\n",
    "    \n",
    "    # Create and train model\n",
    "    model = create_model(**params)\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=5,\n",
    "        batch_size=128,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    best_val_acc = max(history.history['val_accuracy'])\n",
    "    \n",
    "    result = params.copy()\n",
    "    result['val_accuracy'] = best_val_acc\n",
    "    random_search_results.append(result)\n",
    "    \n",
    "    print(f\"  Validation accuracy: {best_val_acc:.4f}\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "random_results_df = pd.DataFrame(random_search_results)\n",
    "random_results_df = random_results_df.sort_values('val_accuracy', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Random Search Results (top 5):\")\n",
    "print(\"=\"*60)\n",
    "print(random_results_df.head().to_string(index=False))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Keras Tuner for Automated Hyperparameter Tuning\n",
    "\n",
    "Keras Tuner provides several advanced search algorithms including Bayesian Optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install Keras Tuner if needed (uncomment if not installed)\n",
    "# !pip install keras-tuner -q\n",
    "\n",
    "try:\n",
    "    import keras_tuner as kt\n",
    "    print(f\"Keras Tuner version: {kt.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"Keras Tuner not installed. Skipping this section.\")\n",
    "    print(\"To install: pip install keras-tuner\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define model builder for Keras Tuner\n",
    "def build_tuner_model(hp):\n",
    "    \"\"\"\n",
    "    Model builder with hyperparameter search space.\n",
    "    \n",
    "    Args:\n",
    "        hp: HyperParameters object from Keras Tuner\n",
    "    \n",
    "    Returns:\n",
    "        Compiled Keras model\n",
    "    \"\"\"\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.InputLayer(input_shape=(784,)))\n",
    "    \n",
    "    # Tune number of hidden layers (1-3)\n",
    "    for i in range(hp.Int('n_hidden', 1, 3)):\n",
    "        # Tune number of neurons per layer (32-128, step of 32)\n",
    "        model.add(layers.Dense(\n",
    "            units=hp.Int(f'units_{i}', min_value=32, max_value=128, step=32),\n",
    "            activation='relu'\n",
    "        ))\n",
    "        \n",
    "        # Tune dropout rate (0.0-0.5)\n",
    "        model.add(layers.Dropout(\n",
    "            rate=hp.Float('dropout', min_value=0.0, max_value=0.5, step=0.1)\n",
    "        ))\n",
    "    \n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "    \n",
    "    # Tune learning rate (log scale)\n",
    "    learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"Model builder function defined.\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Note: This cell demonstrates Keras Tuner usage but is commented out\n",
    "# to avoid long training times. Uncomment to run actual hyperparameter search.\n",
    "\n",
    "# Initialize Bayesian Optimization tuner\n",
    "# tuner = kt.BayesianOptimization(\n",
    "#     build_tuner_model,\n",
    "#     objective='val_accuracy',\n",
    "#     max_trials=10,  # Number of configurations to try\n",
    "#     executions_per_trial=1,  # Train each config once\n",
    "#     directory='tuner_results',\n",
    "#     project_name='fashion_mnist_tuning'\n",
    "# )\n",
    "\n",
    "# Print search space summary\n",
    "# tuner.search_space_summary()\n",
    "\n",
    "# Run the search\n",
    "# tuner.search(\n",
    "#     X_train, y_train,\n",
    "#     validation_data=(X_val, y_val),\n",
    "#     epochs=10,\n",
    "#     batch_size=128,\n",
    "#     callbacks=[keras.callbacks.EarlyStopping(patience=3)]\n",
    "# )\n",
    "\n",
    "# Get best hyperparameters\n",
    "# best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "# print(\"Best hyperparameters:\")\n",
    "# print(f\"  Hidden layers: {best_hps.get('n_hidden')}\")\n",
    "# print(f\"  Neurons (layer 0): {best_hps.get('units_0')}\")\n",
    "# print(f\"  Dropout rate: {best_hps.get('dropout')}\")\n",
    "# print(f\"  Learning rate: {best_hps.get('learning_rate')}\")\n",
    "\n",
    "print(\"Keras Tuner example code defined (commented out for speed).\")\n",
    "print(\"Uncomment to run Bayesian Optimization search.\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Learning Rate Finder\n",
    "\n",
    "The Learning Rate Finder technique helps identify good learning rate ranges by training with exponentially increasing learning rates.\n",
    "\n",
    "**How it works**:\n",
    "1. Start with very small learning rate (e.g., 1e-7)\n",
    "2. Train for a few batches, exponentially increasing LR\n",
    "3. Plot loss vs learning rate\n",
    "4. Choose LR where loss decreases fastest (steepest slope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class LearningRateFinder(keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Callback to find optimal learning rate range.\n",
    "    Exponentially increases learning rate and records loss.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, min_lr=1e-7, max_lr=10, steps=100):\n",
    "        super().__init__()\n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.steps = steps\n",
    "        self.learning_rates = []\n",
    "        self.losses = []\n",
    "        self.best_loss = float('inf')\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        # Calculate multiplication factor\n",
    "        self.lr_mult = (self.max_lr / self.min_lr) ** (1 / self.steps)\n",
    "        self.current_lr = self.min_lr\n",
    "        keras.backend.set_value(self.model.optimizer.lr, self.current_lr)\n",
    "        \n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        # Record current learning rate and loss\n",
    "        loss = logs.get('loss')\n",
    "        self.learning_rates.append(self.current_lr)\n",
    "        self.losses.append(loss)\n",
    "        \n",
    "        # Stop if loss is exploding\n",
    "        if loss > 4 * self.best_loss:\n",
    "            self.model.stop_training = True\n",
    "            return\n",
    "        \n",
    "        # Update best loss\n",
    "        if loss < self.best_loss:\n",
    "            self.best_loss = loss\n",
    "        \n",
    "        # Increase learning rate\n",
    "        self.current_lr *= self.lr_mult\n",
    "        keras.backend.set_value(self.model.optimizer.lr, self.current_lr)\n",
    "\n",
    "# Create model for LR finding\n",
    "lr_model = create_model(n_hidden=2, n_neurons=64, learning_rate=1e-7)\n",
    "\n",
    "# Create LR finder callback\n",
    "lr_finder = LearningRateFinder(min_lr=1e-7, max_lr=1, steps=100)\n",
    "\n",
    "# Run LR finder\n",
    "print(\"Running Learning Rate Finder...\")\n",
    "lr_model.fit(\n",
    "    X_train[:10000], y_train[:10000],  # Use subset for speed\n",
    "    batch_size=128,\n",
    "    epochs=1,\n",
    "    callbacks=[lr_finder],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(f\"Tested {len(lr_finder.learning_rates)} learning rates.\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot learning rate vs loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(lr_finder.learning_rates, lr_finder.losses)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Learning Rate (log scale)', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Learning Rate Finder', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Find learning rate with steepest descent\n",
    "# (largest negative gradient)\n",
    "gradients = np.gradient(lr_finder.losses)\n",
    "steepest_idx = np.argmin(gradients)\n",
    "optimal_lr = lr_finder.learning_rates[steepest_idx]\n",
    "\n",
    "plt.axvline(optimal_lr, color='red', linestyle='--', linewidth=2, \n",
    "            label=f'Suggested LR: {optimal_lr:.2e}')\n",
    "plt.legend(fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nSuggested optimal learning rate: {optimal_lr:.2e}\")\n",
    "print(f\"Recommended range: {optimal_lr/10:.2e} to {optimal_lr:.2e}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Experiment Tracking Best Practices\n",
    "\n",
    "When tuning hyperparameters, systematic experiment tracking is crucial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class ExperimentTracker:\n",
    "    \"\"\"\n",
    "    Simple experiment tracker for hyperparameter tuning.\n",
    "    Records hyperparameters, metrics, and training history.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.experiments = []\n",
    "    \n",
    "    def log_experiment(self, name, hyperparameters, metrics, notes=\"\"):\n",
    "        \"\"\"\n",
    "        Log an experiment with its results.\n",
    "        \n",
    "        Args:\n",
    "            name: Experiment name/ID\n",
    "            hyperparameters: Dict of hyperparameters\n",
    "            metrics: Dict of evaluation metrics\n",
    "            notes: Optional notes about the experiment\n",
    "        \"\"\"\n",
    "        experiment = {\n",
    "            'name': name,\n",
    "            'hyperparameters': hyperparameters,\n",
    "            'metrics': metrics,\n",
    "            'notes': notes,\n",
    "            'timestamp': pd.Timestamp.now()\n",
    "        }\n",
    "        self.experiments.append(experiment)\n",
    "    \n",
    "    def get_best_experiment(self, metric='val_accuracy', mode='max'):\n",
    "        \"\"\"\n",
    "        Get the best experiment based on a metric.\n",
    "        \"\"\"\n",
    "        if not self.experiments:\n",
    "            return None\n",
    "        \n",
    "        if mode == 'max':\n",
    "            best = max(self.experiments, key=lambda x: x['metrics'].get(metric, -float('inf')))\n",
    "        else:\n",
    "            best = min(self.experiments, key=lambda x: x['metrics'].get(metric, float('inf')))\n",
    "        \n",
    "        return best\n",
    "    \n",
    "    def to_dataframe(self):\n",
    "        \"\"\"\n",
    "        Convert experiments to DataFrame for easy analysis.\n",
    "        \"\"\"\n",
    "        if not self.experiments:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        rows = []\n",
    "        for exp in self.experiments:\n",
    "            row = {'name': exp['name']}\n",
    "            row.update(exp['hyperparameters'])\n",
    "            row.update(exp['metrics'])\n",
    "            rows.append(row)\n",
    "        \n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "# Example usage\n",
    "tracker = ExperimentTracker()\n",
    "\n",
    "# Log some experiments\n",
    "tracker.log_experiment(\n",
    "    name='exp_001',\n",
    "    hyperparameters={'n_hidden': 2, 'n_neurons': 64, 'learning_rate': 0.001},\n",
    "    metrics={'val_accuracy': 0.87, 'val_loss': 0.42}\n",
    ")\n",
    "\n",
    "tracker.log_experiment(\n",
    "    name='exp_002',\n",
    "    hyperparameters={'n_hidden': 3, 'n_neurons': 128, 'learning_rate': 0.0001},\n",
    "    metrics={'val_accuracy': 0.89, 'val_loss': 0.38}\n",
    ")\n",
    "\n",
    "# View all experiments\n",
    "print(\"All Experiments:\")\n",
    "print(tracker.to_dataframe())\n",
    "\n",
    "# Get best experiment\n",
    "best = tracker.get_best_experiment()\n",
    "print(f\"\\nBest Experiment: {best['name']}\")\n",
    "print(f\"Hyperparameters: {best['hyperparameters']}\")\n",
    "print(f\"Metrics: {best['metrics']}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Hyperparameter Tuning Strategy Guide\n",
    "\n",
    "### Recommended Tuning Process:\n",
    "\n",
    "**Step 1: Find Good Learning Rate (Most Critical)**\n",
    "- Use Learning Rate Finder\n",
    "- Test suggested LR with simple model\n",
    "- Typical range: 1e-4 to 1e-2\n",
    "\n",
    "**Step 2: Tune Architecture**\n",
    "- Start simple: 1-2 hidden layers\n",
    "- Gradually increase if underfitting\n",
    "- Use random search for layer count and neuron count\n",
    "\n",
    "**Step 3: Tune Batch Size**\n",
    "- Larger batches (128-512): More stable, faster on GPU\n",
    "- Smaller batches (16-64): Better generalization\n",
    "- Balance based on dataset size and hardware\n",
    "\n",
    "**Step 4: Add Regularization**\n",
    "- Only if overfitting occurs\n",
    "- Try dropout: 0.2-0.5\n",
    "- Try L2: 0.001-0.01\n",
    "\n",
    "**Step 5: Fine-tune Optimizer**\n",
    "- Adam works well for most cases\n",
    "- SGD+momentum for very large datasets\n",
    "- RMSprop for RNNs\n",
    "\n",
    "### Common Pitfalls to Avoid:\n",
    "\n",
    "1. **Tuning too many hyperparameters at once**\n",
    "   - Solution: Tune one at a time or in small groups\n",
    "\n",
    "2. **Not using validation set properly**\n",
    "   - Solution: Keep test set completely separate until final evaluation\n",
    "\n",
    "3. **Training for too few epochs during search**\n",
    "   - Solution: Use early stopping, ensure convergence\n",
    "\n",
    "4. **Ignoring training time**\n",
    "   - Solution: Consider efficiency vs performance trade-off\n",
    "\n",
    "5. **Overfitting to validation set**\n",
    "   - Solution: Use cross-validation or hold out final test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Exercise 1: Compare Grid vs Random Search\n",
    "\n",
    "**Task**: Implement and compare grid search and random search with the same computational budget.\n",
    "\n",
    "**Requirements**:\n",
    "1. Define a hyperparameter space with at least 3 hyperparameters\n",
    "2. Run grid search with 8 combinations\n",
    "3. Run random search with 8 iterations\n",
    "4. Compare the best results from each method\n",
    "5. Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# YOUR CODE HERE\n",
    "# Hint: Use the techniques from sections 5 and 6\n",
    "# Store results in dictionaries and compare\n",
    "\n",
    "pass  # Replace with your implementation"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Exercise 2: Learning Rate Schedule Comparison\n",
    "\n",
    "**Task**: Compare different learning rate schedules and their impact on training.\n",
    "\n",
    "**Requirements**:\n",
    "1. Implement constant, step decay, and exponential decay schedules\n",
    "2. Train the same model with each schedule\n",
    "3. Plot training curves for comparison\n",
    "4. Analyze which schedule works best and why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# YOUR CODE HERE\n",
    "# Hint: Use keras.optimizers.schedules or callbacks.LearningRateScheduler\n",
    "# Train for at least 15 epochs to see differences\n",
    "\n",
    "pass  # Replace with your implementation"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Exercise 3: Build an Experiment Tracker Dashboard\n",
    "\n",
    "**Task**: Extend the ExperimentTracker class to create a visualization dashboard.\n",
    "\n",
    "**Requirements**:\n",
    "1. Add a method to plot experiment comparison (bar charts or scatter plots)\n",
    "2. Add a method to show hyperparameter importance (which HP varies most in top experiments?)\n",
    "3. Add a method to save/load experiments to/from CSV\n",
    "4. Run 10+ experiments and use your dashboard to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# YOUR CODE HERE\n",
    "# Hint: Extend the ExperimentTracker class from section 9\n",
    "# Use matplotlib/seaborn for visualizations\n",
    "# Consider using pandas for data manipulation\n",
    "\n",
    "pass  # Replace with your implementation"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Summary\n",
    "\n",
    "### Key Concepts Covered:\n",
    "\n",
    "1. **Hyperparameter Importance Ranking**\n",
    "   - Learning rate is the most critical hyperparameter\n",
    "   - Focus on high-impact hyperparameters first\n",
    "\n",
    "2. **Search Strategies**\n",
    "   - Grid search: Exhaustive but slow\n",
    "   - Random search: Often more efficient\n",
    "   - Bayesian optimization: Most sophisticated\n",
    "\n",
    "3. **Learning Rate Finder**\n",
    "   - Quickly identifies good LR ranges\n",
    "   - Exponentially increases LR during short training run\n",
    "\n",
    "4. **Keras Tuner**\n",
    "   - Automates hyperparameter search\n",
    "   - Supports multiple search algorithms\n",
    "\n",
    "5. **Experiment Tracking**\n",
    "   - Systematic logging is essential\n",
    "   - Track hyperparameters, metrics, and notes\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "- Start with learning rate tuning\n",
    "- Use random search for initial exploration\n",
    "- Apply Bayesian optimization for refinement\n",
    "- Always use a separate validation set\n",
    "- Track all experiments systematically\n",
    "- Consider training time in your budget\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "- [Module 10: Transfer Learning Concepts](10_transfer_learning_concepts.ipynb)\n",
    "- Advanced topics: Neural Architecture Search (NAS), Multi-objective optimization\n",
    "\n",
    "### Additional Resources:\n",
    "\n",
    "1. \"Random Search for Hyper-Parameter Optimization\" (Bergstra & Bengio, 2012)\n",
    "2. Keras Tuner documentation: https://keras.io/keras_tuner/\n",
    "3. \"Cyclical Learning Rates for Training Neural Networks\" (Smith, 2017)\n",
    "4. Fast.ai's learning rate finder approach"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
