{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 05: Feed-Forward Neural Networks with Keras\n",
    "\n",
    "**Difficulty**: ⭐⭐ (Intermediate)\n",
    "\n",
    "**Estimated Time**: 45-60 minutes\n",
    "\n",
    "**Prerequisites**: \n",
    "- [Module 04: Introduction to TensorFlow and Keras](04_introduction_to_tensorflow_keras.ipynb)\n",
    "- [Module 02: Backpropagation and Gradient Descent](02_backpropagation_and_gradient_descent.ipynb)\n",
    "- [Module 01: Perceptrons and Activation Functions](01_perceptrons_and_activation_functions.ipynb)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Design deep neural network architectures with multiple hidden layers\n",
    "2. Understand and apply principles for hidden layer design (depth vs width tradeoffs)\n",
    "3. Build and train deep networks on realistic datasets (Fashion-MNIST and CIFAR-10)\n",
    "4. Monitor training progress and detect overfitting through validation strategies\n",
    "5. Implement effective validation strategies to ensure model generalization\n",
    "6. Interpret training/validation curves to diagnose model performance issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "Let's import all the necessary libraries for building deep neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import fashion_mnist, cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Scikit-learn for evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding Deep Neural Networks\n",
    "\n",
    "### What Makes a Network \"Deep\"?\n",
    "\n",
    "A **deep neural network** is characterized by having multiple hidden layers between the input and output layers. The term \"deep\" refers to the depth of the network (number of layers), not the width (number of neurons per layer).\n",
    "\n",
    "### Architecture Components:\n",
    "\n",
    "1. **Input Layer**: Receives raw features (e.g., pixel values)\n",
    "2. **Hidden Layers**: Extract hierarchical features\n",
    "   - Early layers: Low-level features (edges, textures)\n",
    "   - Middle layers: Mid-level features (shapes, patterns)\n",
    "   - Deep layers: High-level features (object parts)\n",
    "3. **Output Layer**: Produces final predictions\n",
    "\n",
    "### Mathematical Representation:\n",
    "\n",
    "For a network with $L$ layers:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{h}^{(1)} &= \\sigma(\\mathbf{W}^{(1)} \\mathbf{x} + \\mathbf{b}^{(1)}) \\\\\n",
    "\\mathbf{h}^{(2)} &= \\sigma(\\mathbf{W}^{(2)} \\mathbf{h}^{(1)} + \\mathbf{b}^{(2)}) \\\\\n",
    "&\\vdots \\\\\n",
    "\\mathbf{h}^{(L-1)} &= \\sigma(\\mathbf{W}^{(L-1)} \\mathbf{h}^{(L-2)} + \\mathbf{b}^{(L-1)}) \\\\\n",
    "\\hat{\\mathbf{y}} &= \\text{softmax}(\\mathbf{W}^{(L)} \\mathbf{h}^{(L-1)} + \\mathbf{b}^{(L)})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{h}^{(i)}$ is the activation of layer $i$\n",
    "- $\\mathbf{W}^{(i)}$ and $\\mathbf{b}^{(i)}$ are weights and biases for layer $i$\n",
    "- $\\sigma$ is the activation function (e.g., ReLU)\n",
    "- $\\hat{\\mathbf{y}}$ is the predicted output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Depth vs Width: Design Principles\n",
    "\n",
    "### The Depth vs Width Tradeoff\n",
    "\n",
    "When designing neural networks, you must decide:\n",
    "- **Depth**: How many layers?\n",
    "- **Width**: How many neurons per layer?\n",
    "\n",
    "### General Guidelines:\n",
    "\n",
    "| Aspect | Deeper Networks (More Layers) | Wider Networks (More Neurons) |\n",
    "|--------|-------------------------------|-------------------------------|\n",
    "| **Representation Power** | Can learn hierarchical features | Can learn complex single-level patterns |\n",
    "| **Parameters** | Fewer parameters for same capacity | More parameters needed |\n",
    "| **Training** | Can be harder to train (vanishing gradients) | Generally easier to train |\n",
    "| **Generalization** | Often better (hierarchical inductive bias) | May overfit without regularization |\n",
    "| **Computation** | More sequential operations | More parallel operations |\n",
    "\n",
    "### Rule of Thumb:\n",
    "- Start with **2-3 hidden layers** of moderate width (64-256 neurons)\n",
    "- For complex tasks (image classification), prefer **deeper networks**\n",
    "- For simpler tasks (tabular data), wider shallow networks may suffice\n",
    "- Use validation performance to guide architecture choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Helper function to visualize network architecture\n",
    "def visualize_architecture(layer_sizes, title=\"Network Architecture\"):\n",
    "    \"\"\"\n",
    "    Visualize neural network architecture.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    layer_sizes : list\n",
    "        Number of neurons in each layer [input, hidden1, hidden2, ..., output]\n",
    "    title : str\n",
    "        Plot title\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Calculate positions\n",
    "    n_layers = len(layer_sizes)\n",
    "    x_positions = np.linspace(0, 10, n_layers)\n",
    "    \n",
    "    # Draw each layer\n",
    "    for i, (x, size) in enumerate(zip(x_positions, layer_sizes)):\n",
    "        # Calculate y positions for neurons in this layer\n",
    "        y_positions = np.linspace(0, 10, size)\n",
    "        \n",
    "        # Draw neurons\n",
    "        for y in y_positions:\n",
    "            circle = plt.Circle((x, y), 0.2, color='skyblue', ec='black', zorder=3)\n",
    "            ax.add_patch(circle)\n",
    "        \n",
    "        # Add layer label\n",
    "        if i == 0:\n",
    "            label = f\"Input\\n({size})\"\n",
    "        elif i == n_layers - 1:\n",
    "            label = f\"Output\\n({size})\"\n",
    "        else:\n",
    "            label = f\"Hidden {i}\\n({size})\"\n",
    "        ax.text(x, -1, label, ha='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    ax.set_xlim(-1, 11)\n",
    "    ax.set_ylim(-2, 11)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example: Compare deep vs wide networks\n",
    "print(\"Deep Network (more layers, fewer neurons per layer):\")\n",
    "visualize_architecture([784, 64, 64, 64, 10], \"Deep Network: 3 Hidden Layers\")\n",
    "\n",
    "print(\"\\nWide Network (fewer layers, more neurons per layer):\")\n",
    "visualize_architecture([784, 256, 10], \"Wide Network: 1 Hidden Layer\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Loading and Preprocessing Fashion-MNIST\n",
    "\n",
    "**Fashion-MNIST** is a dataset of 70,000 grayscale images (28×28 pixels) of clothing items in 10 categories:\n",
    "- T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Ankle boot\n",
    "\n",
    "It's a drop-in replacement for MNIST, but more challenging and realistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load Fashion-MNIST dataset\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Class names for Fashion-MNIST\n",
    "class_names = [\n",
    "    'T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "    'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'\n",
    "]\n",
    "\n",
    "print(f\"Training data shape: {X_train_full.shape}\")\n",
    "print(f\"Training labels shape: {y_train_full.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "print(f\"Test labels shape: {y_test.shape}\")\n",
    "print(f\"\\nNumber of classes: {len(class_names)}\")\n",
    "print(f\"Pixel value range: [{X_train_full.min()}, {X_train_full.max()}]\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize sample images\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(10):\n",
    "    axes[i].imshow(X_train_full[i], cmap='gray')\n",
    "    axes[i].set_title(f\"{class_names[y_train_full[i]]}\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle(\"Fashion-MNIST Sample Images\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Preprocessing steps\n",
    "# 1. Normalize pixel values to [0, 1] range\n",
    "X_train_full = X_train_full.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "# 2. Flatten images from 28x28 to 784-dimensional vectors\n",
    "# This is needed for fully-connected (Dense) layers\n",
    "X_train_full_flat = X_train_full.reshape(-1, 28 * 28)\n",
    "X_test_flat = X_test.reshape(-1, 28 * 28)\n",
    "\n",
    "# 3. Create validation set (last 10,000 samples)\n",
    "X_train, X_valid = X_train_full_flat[:50000], X_train_full_flat[50000:]\n",
    "y_train, y_valid = y_train_full[:50000], y_train_full[50000:]\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Validation set: {X_valid.shape}\")\n",
    "print(f\"Test set: {X_test_flat.shape}\")\n",
    "print(f\"\\nPixel values after normalization: [{X_train.min():.2f}, {X_train.max():.2f}]\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Building Deep Neural Networks\n",
    "\n",
    "### Architecture Design Principles:\n",
    "\n",
    "1. **Start Simple**: Begin with 2-3 hidden layers\n",
    "2. **Layer Size Progression**: Common patterns:\n",
    "   - Decreasing: 512 → 256 → 128 (funnel architecture)\n",
    "   - Constant: 256 → 256 → 256 (uniform architecture)\n",
    "   - Increasing: 128 → 256 → 512 (expansion architecture)\n",
    "3. **Activation Functions**: Use ReLU for hidden layers, softmax for output\n",
    "4. **Output Layer**: Must match number of classes (10 for Fashion-MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Model 1: Simple Deep Network (3 hidden layers, decreasing size)\n",
    "def create_simple_deep_model():\n",
    "    \"\"\"\n",
    "    Create a simple deep neural network with 3 hidden layers.\n",
    "    Uses decreasing layer sizes (funnel architecture).\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        # Input layer - explicitly define input shape\n",
    "        layers.Input(shape=(784,)),\n",
    "        \n",
    "        # Hidden layer 1: 256 neurons\n",
    "        layers.Dense(256, activation='relu', name='hidden_1'),\n",
    "        \n",
    "        # Hidden layer 2: 128 neurons\n",
    "        layers.Dense(128, activation='relu', name='hidden_2'),\n",
    "        \n",
    "        # Hidden layer 3: 64 neurons\n",
    "        layers.Dense(64, activation='relu', name='hidden_3'),\n",
    "        \n",
    "        # Output layer: 10 classes (softmax for multi-class classification)\n",
    "        layers.Dense(10, activation='softmax', name='output')\n",
    "    ], name='simple_deep_model')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create and display model architecture\n",
    "model_simple = create_simple_deep_model()\n",
    "model_simple.summary()\n",
    "\n",
    "# Calculate total parameters\n",
    "total_params = model_simple.count_params()\n",
    "print(f\"\\nTotal trainable parameters: {total_params:,}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Parameter Count\n",
    "\n",
    "For each Dense layer, the number of parameters is:\n",
    "$$\\text{parameters} = (\\text{input\\_size} + 1) \\times \\text{output\\_size}$$\n",
    "\n",
    "The \"+1\" accounts for the bias term for each neuron.\n",
    "\n",
    "Example for first hidden layer:\n",
    "- Input: 784 features\n",
    "- Output: 256 neurons\n",
    "- Parameters: $(784 + 1) \\times 256 = 200,960$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compile the model\n",
    "model_simple.compile(\n",
    "    optimizer='adam',  # Adam optimizer (adaptive learning rate)\n",
    "    loss='sparse_categorical_crossentropy',  # For integer labels\n",
    "    metrics=['accuracy']  # Track accuracy during training\n",
    ")\n",
    "\n",
    "print(\"Model compiled successfully!\")\n",
    "print(f\"Optimizer: Adam\")\n",
    "print(f\"Loss function: Sparse Categorical Crossentropy\")\n",
    "print(f\"Metrics: Accuracy\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training and Monitoring Progress\n",
    "\n",
    "### Best Practices for Training:\n",
    "\n",
    "1. **Use Validation Data**: Monitor performance on unseen data\n",
    "2. **Track Metrics**: Loss and accuracy for both training and validation\n",
    "3. **Watch for Overfitting**: Validation loss increasing while training loss decreases\n",
    "4. **Be Patient**: Deep networks may take many epochs to converge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Train the model with validation monitoring\n",
    "print(\"Training simple deep model...\")\n",
    "history_simple = model_simple.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=20,\n",
    "    batch_size=128,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    verbose=1  # Show progress bar\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Helper function to plot training history\n",
    "def plot_training_history(history, title=\"Training History\"):\n",
    "    \"\"\"\n",
    "    Plot training and validation loss/accuracy curves.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    history : keras.callbacks.History\n",
    "        Training history object returned by model.fit()\n",
    "    title : str\n",
    "        Main title for the plot\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Plot loss\n",
    "    ax1.plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "    ax1.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "    ax1.set_xlabel('Epoch', fontsize=12)\n",
    "    ax1.set_ylabel('Loss', fontsize=12)\n",
    "    ax1.set_title('Model Loss', fontsize=12, fontweight='bold')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot accuracy\n",
    "    ax2.plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "    ax2.plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "    ax2.set_xlabel('Epoch', fontsize=12)\n",
    "    ax2.set_ylabel('Accuracy', fontsize=12)\n",
    "    ax2.set_title('Model Accuracy', fontsize=12, fontweight='bold')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(title, fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(history_simple, \"Simple Deep Model Training History\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Detecting Overfitting\n",
    "\n",
    "### Signs of Overfitting:\n",
    "\n",
    "1. **Training accuracy >> Validation accuracy**: Large gap indicates overfitting\n",
    "2. **Validation loss increases**: While training loss continues to decrease\n",
    "3. **Early plateau**: Validation metrics stop improving early\n",
    "\n",
    "### Common Causes:\n",
    "- Model too complex for the dataset\n",
    "- Insufficient training data\n",
    "- Training for too many epochs\n",
    "- Lack of regularization\n",
    "\n",
    "### Solutions (covered in later modules):\n",
    "- Dropout (Module 07)\n",
    "- Batch Normalization (Module 07)\n",
    "- L1/L2 Regularization (Module 07)\n",
    "- Early Stopping (Module 07)\n",
    "- Data Augmentation (Module 07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to analyze overfitting\n",
    "def analyze_overfitting(history):\n",
    "    \"\"\"\n",
    "    Analyze training history to detect overfitting.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    history : keras.callbacks.History\n",
    "        Training history object\n",
    "    \"\"\"\n",
    "    final_epoch = len(history.history['loss']) - 1\n",
    "    \n",
    "    train_loss = history.history['loss'][-1]\n",
    "    val_loss = history.history['val_loss'][-1]\n",
    "    train_acc = history.history['accuracy'][-1]\n",
    "    val_acc = history.history['val_accuracy'][-1]\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"OVERFITTING ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nFinal Epoch: {final_epoch + 1}\")\n",
    "    print(f\"\\nTraining Loss:      {train_loss:.4f}\")\n",
    "    print(f\"Validation Loss:    {val_loss:.4f}\")\n",
    "    print(f\"Loss Gap:           {abs(val_loss - train_loss):.4f}\")\n",
    "    \n",
    "    print(f\"\\nTraining Accuracy:  {train_acc:.4f} ({train_acc*100:.2f}%)\")\n",
    "    print(f\"Validation Accuracy: {val_acc:.4f} ({val_acc*100:.2f}%)\")\n",
    "    print(f\"Accuracy Gap:       {abs(train_acc - val_acc):.4f} ({abs(train_acc - val_acc)*100:.2f}%)\")\n",
    "    \n",
    "    # Overfitting diagnosis\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"DIAGNOSIS:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if val_loss > train_loss * 1.1 and abs(train_acc - val_acc) > 0.05:\n",
    "        print(\"⚠️  OVERFITTING DETECTED\")\n",
    "        print(\"   - Validation loss significantly higher than training loss\")\n",
    "        print(\"   - Consider: regularization, dropout, or early stopping\")\n",
    "    elif abs(train_acc - val_acc) < 0.02:\n",
    "        print(\"✅ GOOD GENERALIZATION\")\n",
    "        print(\"   - Training and validation metrics are close\")\n",
    "        print(\"   - Model generalizes well to unseen data\")\n",
    "    else:\n",
    "        print(\"⚠️  MILD OVERFITTING\")\n",
    "        print(\"   - Some gap between training and validation\")\n",
    "        print(\"   - Consider monitoring for more epochs\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "\n",
    "# Analyze the simple model\n",
    "analyze_overfitting(history_simple)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comparing Different Architectures\n",
    "\n",
    "Let's compare three different architectures:\n",
    "1. **Shallow Wide**: 1 hidden layer with many neurons\n",
    "2. **Deep Narrow**: Many hidden layers with fewer neurons\n",
    "3. **Balanced**: Moderate depth and width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Architecture 1: Shallow Wide (1 hidden layer, 512 neurons)\n",
    "model_shallow = models.Sequential([\n",
    "    layers.Input(shape=(784,)),\n",
    "    layers.Dense(512, activation='relu', name='hidden_wide'),\n",
    "    layers.Dense(10, activation='softmax', name='output')\n",
    "], name='shallow_wide')\n",
    "\n",
    "# Architecture 2: Deep Narrow (5 hidden layers, 64 neurons each)\n",
    "model_deep = models.Sequential([\n",
    "    layers.Input(shape=(784,)),\n",
    "    layers.Dense(64, activation='relu', name='hidden_1'),\n",
    "    layers.Dense(64, activation='relu', name='hidden_2'),\n",
    "    layers.Dense(64, activation='relu', name='hidden_3'),\n",
    "    layers.Dense(64, activation='relu', name='hidden_4'),\n",
    "    layers.Dense(64, activation='relu', name='hidden_5'),\n",
    "    layers.Dense(10, activation='softmax', name='output')\n",
    "], name='deep_narrow')\n",
    "\n",
    "# Architecture 3: Balanced (3 hidden layers, 128 neurons each)\n",
    "model_balanced = models.Sequential([\n",
    "    layers.Input(shape=(784,)),\n",
    "    layers.Dense(128, activation='relu', name='hidden_1'),\n",
    "    layers.Dense(128, activation='relu', name='hidden_2'),\n",
    "    layers.Dense(128, activation='relu', name='hidden_3'),\n",
    "    layers.Dense(10, activation='softmax', name='output')\n",
    "], name='balanced')\n",
    "\n",
    "# Compare parameter counts\n",
    "print(\"Architecture Comparison:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Shallow Wide:  {model_shallow.count_params():>10,} parameters\")\n",
    "print(f\"Deep Narrow:   {model_deep.count_params():>10,} parameters\")\n",
    "print(f\"Balanced:      {model_balanced.count_params():>10,} parameters\")\n",
    "print(\"=\" * 60)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compile all models with same configuration\n",
    "for model in [model_shallow, model_deep, model_balanced]:\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "# Train all models (fewer epochs for comparison)\n",
    "histories = {}\n",
    "epochs = 10\n",
    "\n",
    "for name, model in [('Shallow Wide', model_shallow), \n",
    "                     ('Deep Narrow', model_deep),\n",
    "                     ('Balanced', model_balanced)]:\n",
    "    print(f\"\\nTraining {name} model...\")\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=128,\n",
    "        validation_data=(X_valid, y_valid),\n",
    "        verbose=0  # Suppress output for cleaner comparison\n",
    "    )\n",
    "    histories[name] = history\n",
    "    \n",
    "    # Print final metrics\n",
    "    final_train_acc = history.history['accuracy'][-1]\n",
    "    final_val_acc = history.history['val_accuracy'][-1]\n",
    "    print(f\"  Final Training Accuracy:   {final_train_acc:.4f}\")\n",
    "    print(f\"  Final Validation Accuracy: {final_val_acc:.4f}\")\n",
    "\n",
    "print(\"\\nAll models trained!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compare validation accuracy across architectures\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "for name, history in histories.items():\n",
    "    ax.plot(history.history['val_accuracy'], label=name, linewidth=2, marker='o')\n",
    "\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Validation Accuracy', fontsize=12)\n",
    "ax.set_title('Architecture Comparison: Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nBest Final Validation Accuracy:\")\n",
    "for name, history in histories.items():\n",
    "    best_val_acc = max(history.history['val_accuracy'])\n",
    "    print(f\"  {name:15s}: {best_val_acc:.4f} ({best_val_acc*100:.2f}%)\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training on CIFAR-10 (Color Images)\n",
    "\n",
    "**CIFAR-10** consists of 60,000 color images (32×32 pixels, RGB) in 10 classes:\n",
    "- Airplane, Automobile, Bird, Cat, Deer, Dog, Frog, Horse, Ship, Truck\n",
    "\n",
    "This is more challenging than Fashion-MNIST due to:\n",
    "- Color images (3 channels)\n",
    "- More complex visual patterns\n",
    "- Greater intra-class variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load CIFAR-10 dataset\n",
    "# Note: We'll use a subset for faster training on CPU\n",
    "(X_cifar_train, y_cifar_train), (X_cifar_test, y_cifar_test) = cifar10.load_data()\n",
    "\n",
    "# CIFAR-10 class names\n",
    "cifar_classes = [\n",
    "    'Airplane', 'Automobile', 'Bird', 'Cat', 'Deer',\n",
    "    'Dog', 'Frog', 'Horse', 'Ship', 'Truck'\n",
    "]\n",
    "\n",
    "# Use subset for faster training (10,000 training samples)\n",
    "subset_size = 10000\n",
    "X_cifar_train = X_cifar_train[:subset_size]\n",
    "y_cifar_train = y_cifar_train[:subset_size]\n",
    "\n",
    "print(f\"CIFAR-10 Training subset: {X_cifar_train.shape}\")\n",
    "print(f\"CIFAR-10 Test set: {X_cifar_test.shape}\")\n",
    "print(f\"Image shape: {X_cifar_train[0].shape} (height, width, channels)\")\n",
    "print(f\"Pixel value range: [{X_cifar_train.min()}, {X_cifar_train.max()}]\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize CIFAR-10 samples\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(10):\n",
    "    axes[i].imshow(X_cifar_train[i])\n",
    "    axes[i].set_title(f\"{cifar_classes[y_cifar_train[i][0]]}\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle(\"CIFAR-10 Sample Images\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Preprocess CIFAR-10 data\n",
    "# 1. Normalize to [0, 1]\n",
    "X_cifar_train = X_cifar_train.astype('float32') / 255.0\n",
    "X_cifar_test = X_cifar_test.astype('float32') / 255.0\n",
    "\n",
    "# 2. Flatten images: 32x32x3 = 3072 features\n",
    "X_cifar_train_flat = X_cifar_train.reshape(-1, 32 * 32 * 3)\n",
    "X_cifar_test_flat = X_cifar_test.reshape(-1, 32 * 32 * 3)\n",
    "\n",
    "# 3. Flatten labels (they're 2D for some reason)\n",
    "y_cifar_train = y_cifar_train.flatten()\n",
    "y_cifar_test = y_cifar_test.flatten()\n",
    "\n",
    "# 4. Create validation set\n",
    "split_idx = int(0.8 * len(X_cifar_train_flat))\n",
    "X_cifar_train_final = X_cifar_train_flat[:split_idx]\n",
    "X_cifar_valid = X_cifar_train_flat[split_idx:]\n",
    "y_cifar_train_final = y_cifar_train[:split_idx]\n",
    "y_cifar_valid = y_cifar_train[split_idx:]\n",
    "\n",
    "print(f\"CIFAR-10 Training set: {X_cifar_train_final.shape}\")\n",
    "print(f\"CIFAR-10 Validation set: {X_cifar_valid.shape}\")\n",
    "print(f\"CIFAR-10 Test set: {X_cifar_test_flat.shape}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Build model for CIFAR-10\n",
    "# Deeper network needed for color images\n",
    "model_cifar = models.Sequential([\n",
    "    layers.Input(shape=(3072,)),  # 32x32x3 = 3072\n",
    "    layers.Dense(512, activation='relu', name='hidden_1'),\n",
    "    layers.Dense(256, activation='relu', name='hidden_2'),\n",
    "    layers.Dense(128, activation='relu', name='hidden_3'),\n",
    "    layers.Dense(64, activation='relu', name='hidden_4'),\n",
    "    layers.Dense(10, activation='softmax', name='output')\n",
    "], name='cifar10_model')\n",
    "\n",
    "model_cifar.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model_cifar.summary()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Train on CIFAR-10\n",
    "print(\"Training on CIFAR-10...\")\n",
    "print(\"Note: This is more challenging than Fashion-MNIST!\")\n",
    "\n",
    "history_cifar = model_cifar.fit(\n",
    "    X_cifar_train_final, y_cifar_train_final,\n",
    "    epochs=15,\n",
    "    batch_size=128,\n",
    "    validation_data=(X_cifar_valid, y_cifar_valid),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nCIFAR-10 training completed!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot CIFAR-10 training history\n",
    "plot_training_history(history_cifar, \"CIFAR-10 Model Training History\")\n",
    "\n",
    "# Analyze overfitting\n",
    "analyze_overfitting(history_cifar)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Evaluation and Testing\n",
    "\n",
    "After training, we evaluate on the **test set** - data the model has never seen.\n",
    "This gives us the true estimate of generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Evaluate Fashion-MNIST model\n",
    "test_loss, test_accuracy = model_simple.evaluate(X_test_flat, y_test, verbose=0)\n",
    "\n",
    "print(\"Fashion-MNIST Test Results:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Test Loss:     {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model_simple.predict(X_test_flat, verbose=0)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Visualize some predictions\n",
    "fig, axes = plt.subplots(2, 5, figsize=(14, 6))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(10):\n",
    "    idx = np.random.randint(0, len(X_test))\n",
    "    axes[i].imshow(X_test[idx], cmap='gray')\n",
    "    \n",
    "    true_label = class_names[y_test[idx]]\n",
    "    pred_label = class_names[y_pred_classes[idx]]\n",
    "    confidence = y_pred[idx][y_pred_classes[idx]]\n",
    "    \n",
    "    # Color code: green if correct, red if wrong\n",
    "    color = 'green' if y_test[idx] == y_pred_classes[idx] else 'red'\n",
    "    \n",
    "    axes[i].set_title(f\"True: {true_label}\\nPred: {pred_label}\\nConf: {confidence:.2f}\",\n",
    "                      fontsize=9, color=color)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle(\"Fashion-MNIST Predictions (Green=Correct, Red=Wrong)\", \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary\n",
    "\n",
    "### Key Concepts Learned:\n",
    "\n",
    "1. **Deep Neural Networks**: Multiple hidden layers extract hierarchical features\n",
    "\n",
    "2. **Depth vs Width**:\n",
    "   - Deeper networks: Better hierarchical representation, fewer parameters\n",
    "   - Wider networks: More parallel computation, may need regularization\n",
    "\n",
    "3. **Architecture Design**:\n",
    "   - Start simple (2-3 hidden layers)\n",
    "   - Common patterns: Funnel (decreasing), Uniform, Expansion\n",
    "   - Use ReLU for hidden layers, softmax for output\n",
    "\n",
    "4. **Training Best Practices**:\n",
    "   - Always use validation data\n",
    "   - Monitor both loss and accuracy\n",
    "   - Watch for overfitting (validation metrics diverging)\n",
    "\n",
    "5. **Overfitting Detection**:\n",
    "   - Training accuracy >> Validation accuracy\n",
    "   - Validation loss increases over time\n",
    "   - Solutions: Regularization (covered in Module 07)\n",
    "\n",
    "6. **Dataset Complexity**:\n",
    "   - Fashion-MNIST: Grayscale, simpler patterns\n",
    "   - CIFAR-10: Color, more complex, needs deeper networks\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In the next modules, we'll learn:\n",
    "- **Module 06**: Optimizers (SGD, Adam, RMSprop) - How to train efficiently\n",
    "- **Module 07**: Regularization (Dropout, Batch Norm) - How to prevent overfitting\n",
    "- **Module 08**: Loss Functions and Metrics - Choosing the right objectives\n",
    "- **Module 09**: Hyperparameter Tuning - Finding optimal configurations\n",
    "\n",
    "### Additional Resources:\n",
    "\n",
    "- [Keras Sequential Model Guide](https://keras.io/guides/sequential_model/)\n",
    "- [Fashion-MNIST Dataset](https://github.com/zalandoresearch/fashion-mnist)\n",
    "- [CIFAR-10 Dataset](https://www.cs.toronto.edu/~kriz/cifar.html)\n",
    "- [Deep Learning Book - Chapter 6](https://www.deeplearningbook.org/contents/mlp.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Exercises\n",
    "\n",
    "Test your understanding with these exercises!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Design Your Own Architecture\n",
    "\n",
    "**Task**: Create a neural network architecture with:\n",
    "- 4 hidden layers\n",
    "- Use an \"expansion\" pattern (increasing neuron count): 64 → 128 → 256 → 512\n",
    "- Train on Fashion-MNIST for 10 epochs\n",
    "- Compare its performance to the simple model\n",
    "\n",
    "**Questions**:\n",
    "1. How many parameters does your model have?\n",
    "2. Does it perform better or worse than the simple model?\n",
    "3. Does it show signs of overfitting?\n",
    "\n",
    "```python\n",
    "# Your code here\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Exercise 1 Solution\n",
    "# Uncomment to reveal\n",
    "\n",
    "# model_expansion = models.Sequential([\n",
    "#     layers.Input(shape=(784,)),\n",
    "#     layers.Dense(64, activation='relu', name='hidden_1'),\n",
    "#     layers.Dense(128, activation='relu', name='hidden_2'),\n",
    "#     layers.Dense(256, activation='relu', name='hidden_3'),\n",
    "#     layers.Dense(512, activation='relu', name='hidden_4'),\n",
    "#     layers.Dense(10, activation='softmax', name='output')\n",
    "# ], name='expansion_model')\n",
    "# \n",
    "# model_expansion.compile(\n",
    "#     optimizer='adam',\n",
    "#     loss='sparse_categorical_crossentropy',\n",
    "#     metrics=['accuracy']\n",
    "# )\n",
    "# \n",
    "# print(f\"Total parameters: {model_expansion.count_params():,}\")\n",
    "# \n",
    "# history_expansion = model_expansion.fit(\n",
    "#     X_train, y_train,\n",
    "#     epochs=10,\n",
    "#     batch_size=128,\n",
    "#     validation_data=(X_valid, y_valid),\n",
    "#     verbose=1\n",
    "# )\n",
    "# \n",
    "# plot_training_history(history_expansion, \"Expansion Model\")\n",
    "# analyze_overfitting(history_expansion)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Optimal Depth Experiment\n",
    "\n",
    "**Task**: Experiment with different depths while keeping total parameters similar:\n",
    "- Create 3 models: 2 layers (wide), 4 layers (medium), 6 layers (narrow)\n",
    "- Adjust neuron counts to keep total parameters around 200,000\n",
    "- Train each for 10 epochs and compare validation accuracy\n",
    "\n",
    "**Questions**:\n",
    "1. Which depth performs best?\n",
    "2. How does training time differ?\n",
    "3. Which shows the least overfitting?\n",
    "\n",
    "```python\n",
    "# Your code here\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Exercise 2 Solution\n",
    "# Uncomment to reveal\n",
    "\n",
    "# # Hint: Use count_params() to check parameter count\n",
    "# # Adjust layer sizes to get similar counts\n",
    "# \n",
    "# model_2layers = models.Sequential([\n",
    "#     layers.Input(shape=(784,)),\n",
    "#     layers.Dense(256, activation='relu'),\n",
    "#     layers.Dense(256, activation='relu'),\n",
    "#     layers.Dense(10, activation='softmax')\n",
    "# ])\n",
    "# \n",
    "# model_4layers = models.Sequential([\n",
    "#     layers.Input(shape=(784,)),\n",
    "#     layers.Dense(128, activation='relu'),\n",
    "#     layers.Dense(128, activation='relu'),\n",
    "#     layers.Dense(128, activation='relu'),\n",
    "#     layers.Dense(128, activation='relu'),\n",
    "#     layers.Dense(10, activation='softmax')\n",
    "# ])\n",
    "# \n",
    "# model_6layers = models.Sequential([\n",
    "#     layers.Input(shape=(784,)),\n",
    "#     layers.Dense(96, activation='relu'),\n",
    "#     layers.Dense(96, activation='relu'),\n",
    "#     layers.Dense(96, activation='relu'),\n",
    "#     layers.Dense(96, activation='relu'),\n",
    "#     layers.Dense(96, activation='relu'),\n",
    "#     layers.Dense(96, activation='relu'),\n",
    "#     layers.Dense(10, activation='softmax')\n",
    "# ])\n",
    "# \n",
    "# for name, model in [('2 Layers', model_2layers),\n",
    "#                     ('4 Layers', model_4layers),\n",
    "#                     ('6 Layers', model_6layers)]:\n",
    "#     print(f\"{name}: {model.count_params():,} parameters\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: CIFAR-10 Improvement Challenge\n",
    "\n",
    "**Task**: Try to improve the CIFAR-10 model performance:\n",
    "- Experiment with different architectures\n",
    "- Try different batch sizes (32, 64, 256)\n",
    "- Train for more epochs (20-30)\n",
    "- Analyze what works and what doesn't\n",
    "\n",
    "**Goal**: Achieve >50% validation accuracy\n",
    "\n",
    "**Questions**:\n",
    "1. What architecture worked best?\n",
    "2. Did more epochs help or hurt?\n",
    "3. How does batch size affect training?\n",
    "\n",
    "```python\n",
    "# Your code here\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Exercise 3 Solution\n",
    "# Uncomment to reveal\n",
    "\n",
    "# # Try a deeper architecture with more neurons\n",
    "# model_improved = models.Sequential([\n",
    "#     layers.Input(shape=(3072,)),\n",
    "#     layers.Dense(1024, activation='relu'),\n",
    "#     layers.Dense(512, activation='relu'),\n",
    "#     layers.Dense(256, activation='relu'),\n",
    "#     layers.Dense(128, activation='relu'),\n",
    "#     layers.Dense(10, activation='softmax')\n",
    "# ])\n",
    "# \n",
    "# model_improved.compile(\n",
    "#     optimizer='adam',\n",
    "#     loss='sparse_categorical_crossentropy',\n",
    "#     metrics=['accuracy']\n",
    "# )\n",
    "# \n",
    "# history_improved = model_improved.fit(\n",
    "#     X_cifar_train_final, y_cifar_train_final,\n",
    "#     epochs=25,\n",
    "#     batch_size=64,  # Smaller batch size\n",
    "#     validation_data=(X_cifar_valid, y_cifar_valid),\n",
    "#     verbose=1\n",
    "# )\n",
    "# \n",
    "# plot_training_history(history_improved, \"Improved CIFAR-10 Model\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Validation Strategy Comparison\n",
    "\n",
    "**Task**: Compare different validation split ratios on Fashion-MNIST:\n",
    "- 70% train / 30% validation\n",
    "- 80% train / 20% validation  \n",
    "- 90% train / 10% validation\n",
    "\n",
    "Use the same architecture and training configuration for fair comparison.\n",
    "\n",
    "**Questions**:\n",
    "1. How does validation set size affect the reliability of validation metrics?\n",
    "2. Which split gives the best final test accuracy?\n",
    "3. What are the tradeoffs between more training data vs more validation data?\n",
    "\n",
    "```python\n",
    "# Your code here\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Exercise 4 Solution\n",
    "# Uncomment to reveal\n",
    "\n",
    "# results = {}\n",
    "# \n",
    "# for val_ratio in [0.3, 0.2, 0.1]:\n",
    "#     train_ratio = 1 - val_ratio\n",
    "#     split_point = int(len(X_train_full_flat) * train_ratio)\n",
    "#     \n",
    "#     X_tr = X_train_full_flat[:split_point]\n",
    "#     X_val = X_train_full_flat[split_point:]\n",
    "#     y_tr = y_train_full[:split_point]\n",
    "#     y_val = y_train_full[split_point:]\n",
    "#     \n",
    "#     model = create_simple_deep_model()\n",
    "#     model.compile(optimizer='adam',\n",
    "#                   loss='sparse_categorical_crossentropy',\n",
    "#                   metrics=['accuracy'])\n",
    "#     \n",
    "#     history = model.fit(X_tr, y_tr,\n",
    "#                        epochs=15,\n",
    "#                        batch_size=128,\n",
    "#                        validation_data=(X_val, y_val),\n",
    "#                        verbose=0)\n",
    "#     \n",
    "#     test_acc = model.evaluate(X_test_flat, y_test, verbose=0)[1]\n",
    "#     results[f\"{int(train_ratio*100)}/{int(val_ratio*100)}\"] = {\n",
    "#         'val_acc': history.history['val_accuracy'][-1],\n",
    "#         'test_acc': test_acc\n",
    "#     }\n",
    "# \n",
    "# for split, metrics in results.items():\n",
    "#     print(f\"Split {split}: Val={metrics['val_acc']:.4f}, Test={metrics['test_acc']:.4f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Congratulations!** You've completed Module 05. You now understand how to:\n",
    "- Design deep neural network architectures\n",
    "- Balance depth vs width tradeoffs\n",
    "- Train models on realistic datasets\n",
    "- Monitor training progress\n",
    "- Detect and diagnose overfitting\n",
    "\n",
    "Continue to **Module 06: Optimizers (SGD, Adam, RMSprop)** to learn how to train deep networks more efficiently!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
