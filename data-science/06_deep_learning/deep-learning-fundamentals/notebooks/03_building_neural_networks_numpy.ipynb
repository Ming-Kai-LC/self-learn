{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 03: Building Neural Networks with NumPy (From Scratch)\n",
    "\n",
    "**Difficulty**: â­â­â­ (Advanced)\n",
    "**Estimated Time**: 60-75 minutes\n",
    "**Prerequisites**: \n",
    "- Module 00: Introduction to Neural Networks\n",
    "- Module 01: Perceptrons and Activation Functions\n",
    "- Module 02: Backpropagation and Gradient Descent\n",
    "- Strong Python and NumPy skills\n",
    "- Understanding of object-oriented programming\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. **Implement** a complete multi-layer perceptron (MLP) from scratch using only NumPy\n",
    "2. **Design** modular neural network classes with forward and backward methods\n",
    "3. **Train** neural networks on non-linearly separable datasets (XOR, circles)\n",
    "4. **Debug** neural network training by monitoring loss and gradients\n",
    "5. **Compare** your implementation with scikit-learn's MLPClassifier\n",
    "6. **Understand** every component: initialization, forward pass, backward pass, optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Activation Functions Module\n",
    "\n",
    "First, let's create a reusable module for activation functions and their derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation:\n",
    "    \"\"\"\n",
    "    Collection of activation functions and their derivatives.\n",
    "    Each activation is a class method that can be called without instantiation.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "        \"\"\"Sigmoid activation: 1 / (1 + e^(-z))\"\"\"\n",
    "        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid_derivative(z):\n",
    "        \"\"\"Derivative of sigmoid: sigmoid(z) * (1 - sigmoid(z))\"\"\"\n",
    "        s = Activation.sigmoid(z)\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu(z):\n",
    "        \"\"\"ReLU activation: max(0, z)\"\"\"\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu_derivative(z):\n",
    "        \"\"\"Derivative of ReLU: 1 if z > 0, else 0\"\"\"\n",
    "        return (z > 0).astype(float)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh(z):\n",
    "        \"\"\"Tanh activation\"\"\"\n",
    "        return np.tanh(z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh_derivative(z):\n",
    "        \"\"\"Derivative of tanh: 1 - tanh^2(z)\"\"\"\n",
    "        return 1 - np.tanh(z) ** 2\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(z):\n",
    "        \"\"\"\n",
    "        Softmax activation for multi-class classification.\n",
    "        Numerically stable implementation.\n",
    "        \"\"\"\n",
    "        exp_z = np.exp(z - np.max(z, axis=0, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=0, keepdims=True)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_activation(name):\n",
    "        \"\"\"Get activation function by name.\"\"\"\n",
    "        activations = {\n",
    "            'sigmoid': (Activation.sigmoid, Activation.sigmoid_derivative),\n",
    "            'relu': (Activation.relu, Activation.relu_derivative),\n",
    "            'tanh': (Activation.tanh, Activation.tanh_derivative)\n",
    "        }\n",
    "        if name not in activations:\n",
    "            raise ValueError(f\"Unknown activation: {name}. Choose from {list(activations.keys())}\")\n",
    "        return activations[name]\n",
    "\n",
    "print(\"Activation functions module loaded.\")\n",
    "print(\"Available: sigmoid, relu, tanh, softmax\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Neural Network Layer Class\n",
    "\n",
    "Let's build a fully-connected (dense) layer class that handles:\n",
    "- Weight initialization\n",
    "- Forward propagation\n",
    "- Backward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "    \"\"\"\n",
    "    Fully-connected neural network layer.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_size : int\n",
    "        Number of input features\n",
    "    output_size : int\n",
    "        Number of neurons in this layer\n",
    "    activation : str\n",
    "        Activation function name ('sigmoid', 'relu', 'tanh')\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, output_size, activation='relu'):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.activation_name = activation\n",
    "        \n",
    "        # Get activation function and its derivative\n",
    "        self.activation, self.activation_derivative = Activation.get_activation(activation)\n",
    "        \n",
    "        # Initialize weights using He initialization (good for ReLU)\n",
    "        # Weights shape: (output_size, input_size)\n",
    "        self.W = np.random.randn(output_size, input_size) * np.sqrt(2.0 / input_size)\n",
    "        \n",
    "        # Initialize biases to zero\n",
    "        # Biases shape: (output_size, 1)\n",
    "        self.b = np.zeros((output_size, 1))\n",
    "        \n",
    "        # Cache for backward pass\n",
    "        self.cache = {}\n",
    "    \n",
    "    def forward(self, A_prev):\n",
    "        \"\"\"\n",
    "        Forward propagation through the layer.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        A_prev : ndarray, shape (input_size, m)\n",
    "            Activations from previous layer (or input)\n",
    "            m = number of samples\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        A : ndarray, shape (output_size, m)\n",
    "            Activations of this layer\n",
    "        \"\"\"\n",
    "        # Linear transformation: Z = W @ A_prev + b\n",
    "        Z = np.dot(self.W, A_prev) + self.b\n",
    "        \n",
    "        # Apply activation function\n",
    "        A = self.activation(Z)\n",
    "        \n",
    "        # Cache values needed for backward pass\n",
    "        self.cache = {\n",
    "            'A_prev': A_prev,\n",
    "            'Z': Z,\n",
    "            'A': A\n",
    "        }\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Backward propagation through the layer.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        dA : ndarray, shape (output_size, m)\n",
    "            Gradient of loss with respect to this layer's activation\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dA_prev : ndarray, shape (input_size, m)\n",
    "            Gradient to pass to previous layer\n",
    "        \"\"\"\n",
    "        # Retrieve cached values\n",
    "        A_prev = self.cache['A_prev']\n",
    "        Z = self.cache['Z']\n",
    "        m = A_prev.shape[1]  # Number of samples\n",
    "        \n",
    "        # Compute dZ: gradient of loss w.r.t. pre-activation\n",
    "        # dZ = dA * activation'(Z)\n",
    "        dZ = dA * self.activation_derivative(Z)\n",
    "        \n",
    "        # Compute gradients for parameters\n",
    "        # dW = (1/m) * dZ @ A_prev.T\n",
    "        self.dW = (1 / m) * np.dot(dZ, A_prev.T)\n",
    "        \n",
    "        # db = (1/m) * sum of dZ across samples\n",
    "        self.db = (1 / m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "        \n",
    "        # Compute gradient for previous layer\n",
    "        # dA_prev = W.T @ dZ\n",
    "        dA_prev = np.dot(self.W.T, dZ)\n",
    "        \n",
    "        return dA_prev\n",
    "    \n",
    "    def update_parameters(self, learning_rate):\n",
    "        \"\"\"\n",
    "        Update parameters using gradient descent.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        learning_rate : float\n",
    "            Learning rate for gradient descent\n",
    "        \"\"\"\n",
    "        self.W -= learning_rate * self.dW\n",
    "        self.b -= learning_rate * self.db\n",
    "\n",
    "print(\"DenseLayer class defined.\")\n",
    "print(\"Features: forward propagation, backpropagation, parameter updates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Complete Neural Network Class\n",
    "\n",
    "Now let's build a complete neural network that can have multiple layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    Multi-layer neural network for binary classification.\n",
    "    \n",
    "    Example usage:\n",
    "    ```python\n",
    "    nn = NeuralNetwork()\n",
    "    nn.add_layer(2, 4, 'relu')      # Input: 2, Hidden: 4 neurons\n",
    "    nn.add_layer(4, 4, 'relu')      # Hidden: 4 neurons\n",
    "    nn.add_layer(4, 1, 'sigmoid')   # Output: 1 neuron\n",
    "    nn.compile()\n",
    "    nn.fit(X_train, y_train, epochs=1000, learning_rate=0.01)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.compiled = False\n",
    "        self.history = {\n",
    "            'loss': [],\n",
    "            'accuracy': []\n",
    "        }\n",
    "    \n",
    "    def add_layer(self, input_size, output_size, activation='relu'):\n",
    "        \"\"\"\n",
    "        Add a dense layer to the network.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_size : int\n",
    "            Number of input features to this layer\n",
    "        output_size : int\n",
    "            Number of neurons in this layer\n",
    "        activation : str\n",
    "            Activation function\n",
    "        \"\"\"\n",
    "        layer = DenseLayer(input_size, output_size, activation)\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def compile(self):\n",
    "        \"\"\"Compile the network (validates architecture).\"\"\"\n",
    "        if len(self.layers) == 0:\n",
    "            raise ValueError(\"No layers added! Use add_layer() first.\")\n",
    "        \n",
    "        self.compiled = True\n",
    "        print(\"Network compiled successfully!\")\n",
    "        print(f\"Architecture: {self.get_architecture()}\")\n",
    "    \n",
    "    def get_architecture(self):\n",
    "        \"\"\"Get a string representation of network architecture.\"\"\"\n",
    "        arch = []\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            arch.append(f\"Layer {i+1}: {layer.input_size} â†’ {layer.output_size} ({layer.activation_name})\")\n",
    "        return \"\\n  \".join(arch)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward propagation through entire network.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : ndarray, shape (n_features, m)\n",
    "            Input data\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        A : ndarray\n",
    "            Final activations (predictions)\n",
    "        \"\"\"\n",
    "        A = X\n",
    "        for layer in self.layers:\n",
    "            A = layer.forward(A)\n",
    "        return A\n",
    "    \n",
    "    def compute_loss(self, Y_true, Y_pred):\n",
    "        \"\"\"\n",
    "        Compute binary cross-entropy loss.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        Y_true : ndarray, shape (1, m)\n",
    "            True labels\n",
    "        Y_pred : ndarray, shape (1, m)\n",
    "            Predicted probabilities\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        loss : float\n",
    "            Binary cross-entropy loss\n",
    "        \"\"\"\n",
    "        m = Y_true.shape[1]\n",
    "        epsilon = 1e-15  # Prevent log(0)\n",
    "        \n",
    "        Y_pred = np.clip(Y_pred, epsilon, 1 - epsilon)\n",
    "        loss = -(1/m) * np.sum(Y_true * np.log(Y_pred) + \n",
    "                              (1 - Y_true) * np.log(1 - Y_pred))\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, Y_true, Y_pred):\n",
    "        \"\"\"\n",
    "        Backward propagation through entire network.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        Y_true : ndarray, shape (1, m)\n",
    "            True labels\n",
    "        Y_pred : ndarray, shape (1, m)\n",
    "            Predicted probabilities\n",
    "        \"\"\"\n",
    "        # Gradient of loss w.r.t. final activation\n",
    "        # For binary cross-entropy + sigmoid: dA = Y_pred - Y_true\n",
    "        epsilon = 1e-15\n",
    "        Y_pred = np.clip(Y_pred, epsilon, 1 - epsilon)\n",
    "        dA = -(Y_true / Y_pred) + (1 - Y_true) / (1 - Y_pred)\n",
    "        \n",
    "        # Backward through all layers\n",
    "        for layer in reversed(self.layers):\n",
    "            dA = layer.backward(dA)\n",
    "    \n",
    "    def update_parameters(self, learning_rate):\n",
    "        \"\"\"Update all layer parameters.\"\"\"\n",
    "        for layer in self.layers:\n",
    "            layer.update_parameters(learning_rate)\n",
    "    \n",
    "    def fit(self, X, Y, epochs=1000, learning_rate=0.01, verbose=True):\n",
    "        \"\"\"\n",
    "        Train the neural network.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : ndarray, shape (n_features, m)\n",
    "            Training data\n",
    "        Y : ndarray, shape (1, m)\n",
    "            Training labels\n",
    "        epochs : int\n",
    "            Number of training iterations\n",
    "        learning_rate : float\n",
    "            Learning rate for gradient descent\n",
    "        verbose : bool\n",
    "            Whether to print progress\n",
    "        \"\"\"\n",
    "        if not self.compiled:\n",
    "            raise ValueError(\"Network not compiled! Call compile() first.\")\n",
    "        \n",
    "        self.history['loss'] = []\n",
    "        self.history['accuracy'] = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Forward propagation\n",
    "            Y_pred = self.forward(X)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = self.compute_loss(Y, Y_pred)\n",
    "            \n",
    "            # Compute accuracy\n",
    "            predictions = (Y_pred > 0.5).astype(int)\n",
    "            accuracy = np.mean(predictions == Y)\n",
    "            \n",
    "            # Store history\n",
    "            self.history['loss'].append(loss)\n",
    "            self.history['accuracy'].append(accuracy)\n",
    "            \n",
    "            # Backward propagation\n",
    "            self.backward(Y, Y_pred)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.update_parameters(learning_rate)\n",
    "            \n",
    "            # Print progress\n",
    "            if verbose and (epoch % 100 == 0 or epoch == epochs - 1):\n",
    "                print(f\"Epoch {epoch:4d}/{epochs}: Loss = {loss:.4f}, \"\n",
    "                      f\"Accuracy = {accuracy * 100:.2f}%\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions on new data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : ndarray, shape (n_features, m)\n",
    "            Input data\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        predictions : ndarray, shape (1, m)\n",
    "            Binary predictions (0 or 1)\n",
    "        \"\"\"\n",
    "        Y_pred = self.forward(X)\n",
    "        return (Y_pred > 0.5).astype(int)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Get prediction probabilities.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : ndarray, shape (n_features, m)\n",
    "            Input data\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        probabilities : ndarray, shape (1, m)\n",
    "            Predicted probabilities\n",
    "        \"\"\"\n",
    "        return self.forward(X)\n",
    "\n",
    "print(\"NeuralNetwork class defined.\")\n",
    "print(\"Features: add_layer, compile, fit, predict, predict_proba\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test on XOR Problem\n",
    "\n",
    "Let's test our network on the classic XOR problem - a problem that single-layer perceptrons cannot solve!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"TESTING ON XOR PROBLEM\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create XOR dataset\n",
    "X_xor = np.array([[0, 0, 1, 1],\n",
    "                  [0, 1, 0, 1]])  # Shape: (2, 4)\n",
    "Y_xor = np.array([[0, 1, 1, 0]])  # Shape: (1, 4)\n",
    "\n",
    "print(\"\\nXOR Dataset:\")\n",
    "print(\"Inputs:\")\n",
    "print(X_xor.T)\n",
    "print(\"\\nOutputs:\")\n",
    "print(Y_xor.T)\n",
    "\n",
    "# Build network\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Building Neural Network...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "nn_xor = NeuralNetwork()\n",
    "nn_xor.add_layer(2, 4, 'relu')      # Input: 2, Hidden: 4\n",
    "nn_xor.add_layer(4, 1, 'sigmoid')   # Hidden: 4, Output: 1\n",
    "nn_xor.compile()\n",
    "\n",
    "# Train\n",
    "print(\"\\nTraining...\")\n",
    "nn_xor.fit(X_xor, Y_xor, epochs=2000, learning_rate=0.1, verbose=True)\n",
    "\n",
    "# Test\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "predictions = nn_xor.predict(X_xor)\n",
    "probabilities = nn_xor.predict_proba(X_xor)\n",
    "\n",
    "print(\"\\nInput | True | Predicted | Probability | Correct\")\n",
    "print(\"-\" * 60)\n",
    "for i in range(4):\n",
    "    x1, x2 = X_xor[:, i]\n",
    "    y_true = Y_xor[0, i]\n",
    "    y_pred = predictions[0, i]\n",
    "    prob = probabilities[0, i]\n",
    "    correct = \"âœ“\" if y_pred == y_true else \"âœ—\"\n",
    "    print(f\"[{int(x1)}, {int(x2)}]  |  {int(y_true)}   |     {int(y_pred)}     |   {prob:.4f}    | {correct}\")\n",
    "\n",
    "accuracy = np.mean(predictions == Y_xor)\n",
    "print(\"-\" * 60)\n",
    "print(f\"Final Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if accuracy == 1.0:\n",
    "    print(\"\\nðŸŽ‰ SUCCESS! XOR problem solved perfectly!\")\n",
    "    print(\"This proves that multi-layer networks can solve non-linear problems!\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ Network didn't fully converge. Try:\")\n",
    "    print(\"  - Training for more epochs\")\n",
    "    print(\"  - Adjusting learning rate\")\n",
    "    print(\"  - Re-running (different initialization)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history for XOR\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot loss\n",
    "ax1.plot(nn_xor.history['loss'], linewidth=2, color='blue')\n",
    "ax1.set_xlabel('Epoch', fontsize=12, weight='bold')\n",
    "ax1.set_ylabel('Loss (Binary Cross-Entropy)', fontsize=12, weight='bold')\n",
    "ax1.set_title('Training Loss', fontsize=13, weight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot accuracy\n",
    "ax2.plot(nn_xor.history['accuracy'], linewidth=2, color='green')\n",
    "ax2.axhline(y=1.0, color='red', linestyle='--', linewidth=2, alpha=0.7, label='Perfect Accuracy')\n",
    "ax2.set_xlabel('Epoch', fontsize=12, weight='bold')\n",
    "ax2.set_ylabel('Accuracy', fontsize=12, weight='bold')\n",
    "ax2.set_title('Training Accuracy', fontsize=13, weight='bold')\n",
    "ax2.set_ylim(0, 1.1)\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('XOR Problem - Training Progress', fontsize=15, weight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test on Circles Dataset\n",
    "\n",
    "Now let's tackle a more challenging non-linear problem: concentric circles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate circles dataset\n",
    "X_circles, y_circles = make_circles(n_samples=1000, noise=0.1, factor=0.5, random_state=42)\n",
    "\n",
    "# Reshape for our network (features as rows)\n",
    "X_circles = X_circles.T  # Shape: (2, 1000)\n",
    "y_circles = y_circles.reshape(1, -1)  # Shape: (1, 1000)\n",
    "\n",
    "print(\"Circles Dataset:\")\n",
    "print(f\"  Shape: {X_circles.shape}\")\n",
    "print(f\"  Classes: {np.unique(y_circles)}\")\n",
    "print(f\"  Class distribution: {np.bincount(y_circles[0])}\")\n",
    "\n",
    "# Visualize dataset\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(X_circles[0, y_circles[0] == 0], X_circles[1, y_circles[0] == 0],\n",
    "           c='blue', s=30, alpha=0.6, edgecolors='k', linewidth=0.5, label='Class 0')\n",
    "plt.scatter(X_circles[0, y_circles[0] == 1], X_circles[1, y_circles[0] == 1],\n",
    "           c='red', s=30, alpha=0.6, edgecolors='k', linewidth=0.5, label='Class 1')\n",
    "plt.xlabel('Feature 1', fontsize=12, weight='bold')\n",
    "plt.ylabel('Feature 2', fontsize=12, weight='bold')\n",
    "plt.title('Circles Dataset (Non-linearly Separable)', fontsize=14, weight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axis('equal')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train network on circles\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"BUILDING NETWORK FOR CIRCLES DATASET\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "nn_circles = NeuralNetwork()\n",
    "nn_circles.add_layer(2, 16, 'relu')     # Input: 2, Hidden: 16\n",
    "nn_circles.add_layer(16, 8, 'relu')     # Hidden: 8\n",
    "nn_circles.add_layer(8, 1, 'sigmoid')   # Output: 1\n",
    "nn_circles.compile()\n",
    "\n",
    "print(\"\\nTraining...\")\n",
    "nn_circles.fit(X_circles, y_circles, epochs=1000, learning_rate=0.1, verbose=True)\n",
    "\n",
    "# Evaluate\n",
    "predictions = nn_circles.predict(X_circles)\n",
    "accuracy = np.mean(predictions == y_circles)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"FINAL ACCURACY: {accuracy * 100:.2f}%\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, X, y, title=\"Decision Boundary\"):\n",
    "    \"\"\"\n",
    "    Plot decision boundary for a binary classifier.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : NeuralNetwork or sklearn model\n",
    "        Trained model with predict method\n",
    "    X : ndarray, shape (2, m)\n",
    "        Input data (2 features)\n",
    "    y : ndarray, shape (1, m) or (m,)\n",
    "        True labels\n",
    "    title : str\n",
    "        Plot title\n",
    "    \"\"\"\n",
    "    # Create mesh grid\n",
    "    h = 0.02  # Step size\n",
    "    x_min, x_max = X[0].min() - 0.5, X[0].max() + 0.5\n",
    "    y_min, y_max = X[1].min() - 0.5, X[1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                        np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Predict for each point in mesh\n",
    "    mesh_input = np.c_[xx.ravel(), yy.ravel()].T  # Shape: (2, n_points)\n",
    "    \n",
    "    if isinstance(model, NeuralNetwork):\n",
    "        Z = model.predict(mesh_input)\n",
    "        Z = Z.reshape(xx.shape)\n",
    "    else:  # sklearn model\n",
    "        Z = model.predict(mesh_input.T)\n",
    "        Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot decision boundary\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, levels=1, colors=['blue', 'red'])\n",
    "    plt.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2)\n",
    "    \n",
    "    # Flatten y if needed\n",
    "    y_flat = y.ravel() if len(y.shape) > 1 else y\n",
    "    \n",
    "    # Plot data points\n",
    "    plt.scatter(X[0, y_flat == 0], X[1, y_flat == 0],\n",
    "               c='blue', s=30, alpha=0.6, edgecolors='k', \n",
    "               linewidth=0.5, label='Class 0')\n",
    "    plt.scatter(X[0, y_flat == 1], X[1, y_flat == 1],\n",
    "               c='red', s=30, alpha=0.6, edgecolors='k', \n",
    "               linewidth=0.5, label='Class 1')\n",
    "    \n",
    "    plt.xlabel('Feature 1', fontsize=12, weight='bold')\n",
    "    plt.ylabel('Feature 2', fontsize=12, weight='bold')\n",
    "    plt.title(title, fontsize=14, weight='bold')\n",
    "    plt.legend(fontsize=11)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot decision boundary\n",
    "plot_decision_boundary(nn_circles, X_circles, y_circles,\n",
    "                      title=f\"Our Neural Network Decision Boundary (Acc: {accuracy*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparison with Scikit-Learn\n",
    "\n",
    "Let's compare our implementation with scikit-learn's MLPClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"COMPARISON WITH SCIKIT-LEARN MLPClassifier\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Prepare data for sklearn (expects features as columns)\n",
    "X_sklearn = X_circles.T\n",
    "y_sklearn = y_circles.ravel()\n",
    "\n",
    "# Build sklearn model with similar architecture\n",
    "mlp_sklearn = MLPClassifier(\n",
    "    hidden_layer_sizes=(16, 8),  # Same as our network\n",
    "    activation='relu',\n",
    "    solver='sgd',               # Stochastic gradient descent\n",
    "    learning_rate_init=0.1,     # Same learning rate\n",
    "    max_iter=1000,              # Same epochs\n",
    "    random_state=42,\n",
    "    batch_size=X_sklearn.shape[0]  # Full batch (like ours)\n",
    ")\n",
    "\n",
    "print(\"\\nTraining sklearn MLPClassifier...\")\n",
    "mlp_sklearn.fit(X_sklearn, y_sklearn)\n",
    "\n",
    "# Evaluate\n",
    "sklearn_accuracy = mlp_sklearn.score(X_sklearn, y_sklearn)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RESULTS COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Our Implementation:    {accuracy * 100:.2f}%\")\n",
    "print(f\"Sklearn MLPClassifier: {sklearn_accuracy * 100:.2f}%\")\n",
    "print(f\"Difference:            {abs(accuracy - sklearn_accuracy) * 100:.2f}%\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if abs(accuracy - sklearn_accuracy) < 0.05:\n",
    "    print(\"\\nâœ“ Great! Our implementation performs similarly to sklearn!\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ Note: Some difference is expected due to:\")\n",
    "    print(\"  - Different weight initializations\")\n",
    "    print(\"  - Numerical precision differences\")\n",
    "    print(\"  - Optimization details\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sklearn's decision boundary for comparison\n",
    "plot_decision_boundary(mlp_sklearn, X_circles, y_circles,\n",
    "                      title=f\"Sklearn MLPClassifier Decision Boundary (Acc: {sklearn_accuracy*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Implement Different Layer Sizes\n",
    "\n",
    "Experiment with different network architectures on the circles dataset:\n",
    "\n",
    "1. **Shallow and wide**: 2 â†’ 32 â†’ 1\n",
    "2. **Deep and narrow**: 2 â†’ 8 â†’ 8 â†’ 8 â†’ 1\n",
    "3. **Pyramid**: 2 â†’ 16 â†’ 8 â†’ 4 â†’ 1\n",
    "\n",
    "**Questions**:\n",
    "- Which architecture achieves the highest accuracy?\n",
    "- Which trains fastest?\n",
    "- Which generalizes best (test on a validation set)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your solution here\n",
    "\n",
    "# TODO: Split data into train/validation sets\n",
    "\n",
    "# TODO: Test architecture 1: Shallow and wide\n",
    "# nn1 = NeuralNetwork()\n",
    "# ...\n",
    "\n",
    "# TODO: Test architecture 2: Deep and narrow\n",
    "# nn2 = NeuralNetwork()\n",
    "# ...\n",
    "\n",
    "# TODO: Test architecture 3: Pyramid\n",
    "# nn3 = NeuralNetwork()\n",
    "# ...\n",
    "\n",
    "# TODO: Compare results in a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to Exercise 1\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_circles.T, y_circles.T, test_size=0.2, random_state=42\n",
    ")\n",
    "X_train, X_val = X_train.T, X_val.T\n",
    "y_train, y_val = y_train.T, y_val.T\n",
    "\n",
    "print(f\"Training set: {X_train.shape[1]} samples\")\n",
    "print(f\"Validation set: {X_val.shape[1]} samples\")\n",
    "\n",
    "architectures = [\n",
    "    ('Shallow & Wide', [(2, 32, 'relu'), (32, 1, 'sigmoid')]),\n",
    "    ('Deep & Narrow', [(2, 8, 'relu'), (8, 8, 'relu'), (8, 8, 'relu'), (8, 1, 'sigmoid')]),\n",
    "    ('Pyramid', [(2, 16, 'relu'), (16, 8, 'relu'), (8, 4, 'relu'), (4, 1, 'sigmoid')])\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, layers in architectures:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Testing: {name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Build network\n",
    "    nn = NeuralNetwork()\n",
    "    for layer_config in layers:\n",
    "        nn.add_layer(*layer_config)\n",
    "    nn.compile()\n",
    "    \n",
    "    # Train\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    nn.fit(X_train, y_train, epochs=1000, learning_rate=0.1, verbose=False)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Evaluate\n",
    "    train_acc = np.mean(nn.predict(X_train) == y_train)\n",
    "    val_acc = np.mean(nn.predict(X_val) == y_val)\n",
    "    \n",
    "    results.append({\n",
    "        'name': name,\n",
    "        'train_acc': train_acc,\n",
    "        'val_acc': val_acc,\n",
    "        'time': training_time\n",
    "    })\n",
    "    \n",
    "    print(f\"Training Accuracy: {train_acc*100:.2f}%\")\n",
    "    print(f\"Validation Accuracy: {val_acc*100:.2f}%\")\n",
    "    print(f\"Training Time: {training_time:.2f}s\")\n",
    "\n",
    "# Summary table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ARCHITECTURE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Architecture':<20} | {'Train Acc':<10} | {'Val Acc':<10} | {'Time (s)':<10}\")\n",
    "print(\"-\"*80)\n",
    "for r in results:\n",
    "    print(f\"{r['name']:<20} | {r['train_acc']*100:>9.2f}% | {r['val_acc']*100:>9.2f}% | {r['time']:>9.2f}s\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find best\n",
    "best_val = max(results, key=lambda x: x['val_acc'])\n",
    "fastest = min(results, key=lambda x: x['time'])\n",
    "\n",
    "print(f\"\\nBest Validation Accuracy: {best_val['name']} ({best_val['val_acc']*100:.2f}%)\")\n",
    "print(f\"Fastest Training: {fastest['name']} ({fastest['time']:.2f}s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Implement Moons Dataset\n",
    "\n",
    "Train your neural network on the \"moons\" dataset (another non-linear problem).\n",
    "\n",
    "**Tasks**:\n",
    "1. Generate moons dataset using `make_moons(n_samples=1000, noise=0.2)`\n",
    "2. Design an appropriate architecture\n",
    "3. Train the network\n",
    "4. Visualize the decision boundary\n",
    "5. Achieve >95% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your solution here\n",
    "\n",
    "# TODO: Generate moons dataset\n",
    "# X_moons, y_moons = make_moons(...)\n",
    "\n",
    "# TODO: Visualize the dataset\n",
    "\n",
    "# TODO: Build and train network\n",
    "\n",
    "# TODO: Evaluate and plot decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to Exercise 2\n",
    "\n",
    "# Generate dataset\n",
    "X_moons, y_moons = make_moons(n_samples=1000, noise=0.2, random_state=42)\n",
    "X_moons = X_moons.T\n",
    "y_moons = y_moons.reshape(1, -1)\n",
    "\n",
    "print(\"Moons Dataset Generated\")\n",
    "print(f\"Shape: {X_moons.shape}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(X_moons[0, y_moons[0] == 0], X_moons[1, y_moons[0] == 0],\n",
    "           c='blue', s=30, alpha=0.6, edgecolors='k', linewidth=0.5, label='Class 0')\n",
    "plt.scatter(X_moons[0, y_moons[0] == 1], X_moons[1, y_moons[0] == 1],\n",
    "           c='red', s=30, alpha=0.6, edgecolors='k', linewidth=0.5, label='Class 1')\n",
    "plt.xlabel('Feature 1', fontsize=12, weight='bold')\n",
    "plt.ylabel('Feature 2', fontsize=12, weight='bold')\n",
    "plt.title('Moons Dataset', fontsize=14, weight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Build network\n",
    "print(\"\\nBuilding Neural Network...\")\n",
    "nn_moons = NeuralNetwork()\n",
    "nn_moons.add_layer(2, 8, 'relu')\n",
    "nn_moons.add_layer(8, 8, 'relu')\n",
    "nn_moons.add_layer(8, 1, 'sigmoid')\n",
    "nn_moons.compile()\n",
    "\n",
    "# Train\n",
    "print(\"\\nTraining...\")\n",
    "nn_moons.fit(X_moons, y_moons, epochs=1000, learning_rate=0.1, verbose=True)\n",
    "\n",
    "# Evaluate\n",
    "predictions = nn_moons.predict(X_moons)\n",
    "accuracy = np.mean(predictions == y_moons)\n",
    "\n",
    "print(f\"\\nFinal Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "if accuracy >= 0.95:\n",
    "    print(\"âœ“ Target achieved: >95% accuracy!\")\n",
    "\n",
    "# Plot decision boundary\n",
    "plot_decision_boundary(nn_moons, X_moons, y_moons,\n",
    "                      title=f\"Moons Dataset - Decision Boundary (Acc: {accuracy*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Add Momentum to Optimizer\n",
    "\n",
    "Enhance the neural network by implementing momentum in the parameter updates.\n",
    "\n",
    "**Momentum Update Rule**:\n",
    "$$v_t = \\beta v_{t-1} + (1-\\beta) \\nabla J$$\n",
    "$$\\theta_t = \\theta_{t-1} - \\alpha v_t$$\n",
    "\n",
    "Where:\n",
    "- $v_t$ = velocity (exponentially weighted average of gradients)\n",
    "- $\\beta$ = momentum coefficient (typically 0.9)\n",
    "- $\\alpha$ = learning rate\n",
    "\n",
    "**Tasks**:\n",
    "1. Modify `DenseLayer.update_parameters()` to include momentum\n",
    "2. Test on circles dataset\n",
    "3. Compare convergence speed with and without momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Your solution here\n",
    "\n",
    "# TODO: Create DenseLayerWithMomentum class\n",
    "# Hint: Add v_W and v_b attributes for velocity\n",
    "\n",
    "# TODO: Test with and without momentum on circles dataset\n",
    "\n",
    "# TODO: Plot loss curves to compare convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "Congratulations! You've built a complete neural network from scratch using only NumPy. This is a significant achievement that gives you deep understanding of how neural networks actually work.\n",
    "\n",
    "### Key Accomplishments\n",
    "\n",
    "1. **Implemented Core Components**\n",
    "   - Activation functions (sigmoid, ReLU, tanh) with derivatives\n",
    "   - Dense layer with forward and backward propagation\n",
    "   - Complete neural network with multiple layers\n",
    "   - Training loop with loss computation and optimization\n",
    "\n",
    "2. **Solved Non-Linear Problems**\n",
    "   - XOR problem (impossible for single perceptron)\n",
    "   - Circles dataset (concentric patterns)\n",
    "   - Achieved high accuracy on complex decision boundaries\n",
    "\n",
    "3. **Validated Implementation**\n",
    "   - Compared with scikit-learn's MLPClassifier\n",
    "   - Results match professional implementations\n",
    "   - Understanding of every detail\n",
    "\n",
    "### Technical Insights\n",
    "\n",
    "- **Weight Initialization**: He initialization prevents vanishing/exploding gradients\n",
    "- **Batch Processing**: Matrix operations handle multiple samples efficiently\n",
    "- **Gradient Flow**: Chain rule propagates gradients backward through layers\n",
    "- **Loss Monitoring**: Tracking loss helps debug and tune training\n",
    "- **Architecture Design**: Depth and width affect learning capacity\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "**Forward Propagation**:\n",
    "```\n",
    "For each layer:\n",
    "  Z = W @ A_prev + b\n",
    "  A = activation(Z)\n",
    "```\n",
    "\n",
    "**Backward Propagation**:\n",
    "```\n",
    "For each layer (reversed):\n",
    "  dZ = dA * activation'(Z)\n",
    "  dW = (1/m) * dZ @ A_prev.T\n",
    "  db = (1/m) * sum(dZ)\n",
    "  dA_prev = W.T @ dZ\n",
    "```\n",
    "\n",
    "**Parameter Update**:\n",
    "```\n",
    "W = W - learning_rate * dW\n",
    "b = b - learning_rate * db\n",
    "```\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In **Module 04: Introduction to TensorFlow/Keras**, we'll:\n",
    "- Learn professional deep learning frameworks\n",
    "- Build networks much faster with high-level APIs\n",
    "- Access GPU acceleration for large-scale training\n",
    "- Train on real datasets like MNIST and Fashion-MNIST\n",
    "- Use advanced features: callbacks, model saving, visualization\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "Building neural networks from scratch gives you:\n",
    "- **Deep Understanding**: Know exactly what happens under the hood\n",
    "- **Debugging Skills**: Can diagnose and fix training issues\n",
    "- **Architecture Intuition**: Understand design trade-offs\n",
    "- **Research Capability**: Can implement novel architectures\n",
    "- **Interview Preparation**: Demonstrate mastery in technical interviews\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "**Code Examples**:\n",
    "- Andrej Karpathy: \"micrograd\" (minimal autograd engine)\n",
    "- Joel Grus: \"neural network from scratch\" (livecoding)\n",
    "\n",
    "**Theory**:\n",
    "- CS231n: \"Neural Networks Part 1-3\" (Stanford)\n",
    "- \"Neural Networks and Deep Learning\" by Michael Nielsen\n",
    "\n",
    "**Practice**:\n",
    "- Implement more activation functions (ELU, SELU, Mish)\n",
    "- Add regularization (L1, L2, dropout)\n",
    "- Implement different optimizers (Adam, RMSprop)\n",
    "- Build convolutional layers for images\n",
    "\n",
    "---\n",
    "\n",
    "**Ready to use professional tools?** Continue to **Module 04: Introduction to TensorFlow/Keras**!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
