{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 11: Debugging Neural Networks\n",
    "\n",
    "**Difficulty**: ⭐⭐⭐ (Advanced)\n",
    "\n",
    "**Estimated Time**: 60-75 minutes\n",
    "\n",
    "**Prerequisites**: \n",
    "- [Module 02: Backpropagation and Gradient Descent](02_backpropagation_and_gradient_descent.ipynb)\n",
    "- [Module 05: Feed-Forward Neural Networks with Keras](05_feedforward_neural_networks_keras.ipynb)\n",
    "- [Module 06: Optimizers](06_optimizers_sgd_adam_rmsprop.ipynb)\n",
    "- [Module 07: Regularization Techniques](07_regularization_techniques.ipynb)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Identify common neural network training problems and their symptoms\n",
    "2. Understand and detect vanishing and exploding gradients\n",
    "3. Implement gradient checking for debugging backpropagation\n",
    "4. Use proper weight initialization to prevent training issues\n",
    "5. Apply systematic debugging strategies to troubleshoot neural networks\n",
    "6. Visualize network internals (weights, activations, gradients) for diagnosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Deep learning libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from tensorflow.keras.datasets import mnist, fashion_mnist\n",
    "\n",
    "# For gradient checking\n",
    "from scipy.optimize import approx_fprime\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Plotting configuration\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Common Neural Network Problems\n",
    "\n",
    "### Problem Categories:\n",
    "\n",
    "1. **Not Learning At All**\n",
    "   - Loss stays constant or changes minimally\n",
    "   - Accuracy stuck at random chance\n",
    "\n",
    "2. **Learning Too Slowly**\n",
    "   - Loss decreases but very gradually\n",
    "   - Takes many epochs to converge\n",
    "\n",
    "3. **Training Instability**\n",
    "   - Loss oscillates wildly\n",
    "   - NaN or Inf values appear\n",
    "   - Model diverges instead of converging\n",
    "\n",
    "4. **Overfitting**\n",
    "   - Training accuracy high, validation accuracy low\n",
    "   - Large gap between training and validation loss\n",
    "\n",
    "5. **Underfitting**\n",
    "   - Both training and validation accuracy are low\n",
    "   - Model is too simple for the task\n",
    "\n",
    "Let's simulate and diagnose each of these problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Data for Debugging Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load Fashion-MNIST\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Normalize\n",
    "X_train_full = X_train_full.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "# Flatten images\n",
    "X_train_full = X_train_full.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "\n",
    "# Create validation split\n",
    "validation_split = int(0.9 * len(X_train_full))\n",
    "X_train = X_train_full[:validation_split]\n",
    "y_train = y_train_full[:validation_split]\n",
    "X_val = X_train_full[validation_split:]\n",
    "y_val = y_train_full[validation_split:]\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Problem 1: Model Not Learning (Dead Neurons)\n",
    "\n",
    "**Symptoms**: Loss barely changes, accuracy near random guess\n",
    "\n",
    "**Common Causes**:\n",
    "- Learning rate too small\n",
    "- Wrong activation function (e.g., saturating activations)\n",
    "- Dead ReLU neurons (always output 0)\n",
    "- Input not normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a poorly initialized model that won't learn well\n",
    "def create_dead_model():\n",
    "    \"\"\"Model with issues that prevent learning.\"\"\"\n",
    "    model = keras.Sequential([\n",
    "        layers.InputLayer(input_shape=(784,)),\n",
    "        layers.Dense(128, activation='relu', \n",
    "                    kernel_initializer=keras.initializers.Constant(-1.0)),  # Bad init!\n",
    "        layers.Dense(64, activation='relu',\n",
    "                    kernel_initializer=keras.initializers.Constant(-1.0)),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Very small learning rate\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.SGD(learning_rate=1e-7),  # Too small!\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Train the problematic model\n",
    "dead_model = create_dead_model()\n",
    "print(\"Training model with dead neurons...\")\n",
    "dead_history = dead_model.fit(\n",
    "    X_train[:5000], y_train[:5000],\n",
    "    validation_split=0.2,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(f\"Final training accuracy: {dead_history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"Expected random guess: {1/10:.4f}\")\n",
    "print(\"Notice: Model barely learns!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Diagnose: Check activation outputs\n",
    "def check_activation_statistics(model, X_sample):\n",
    "    \"\"\"\n",
    "    Check what percentage of neurons are active (non-zero).\n",
    "    \"\"\"\n",
    "    layer_outputs = []\n",
    "    \n",
    "    # Create intermediate model to extract layer outputs\n",
    "    for i, layer in enumerate(model.layers[:-1]):  # Exclude output layer\n",
    "        intermediate_model = keras.Model(\n",
    "            inputs=model.input,\n",
    "            outputs=layer.output\n",
    "        )\n",
    "        output = intermediate_model.predict(X_sample, verbose=0)\n",
    "        layer_outputs.append(output)\n",
    "    \n",
    "    # Analyze activations\n",
    "    print(\"Activation Statistics:\")\n",
    "    print(\"=\" * 50)\n",
    "    for i, activations in enumerate(layer_outputs):\n",
    "        dead_neurons = np.sum(activations == 0, axis=0)\n",
    "        dead_percentage = (dead_neurons / len(X_sample)) * 100\n",
    "        avg_dead = np.mean(dead_percentage)\n",
    "        \n",
    "        print(f\"Layer {i}: {avg_dead:.1f}% dead neurons (average)\")\n",
    "        if avg_dead > 50:\n",
    "            print(f\"  ⚠️  WARNING: Over 50% neurons are dead!\")\n",
    "\n",
    "# Check the dead model\n",
    "check_activation_statistics(dead_model, X_val[:100])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Problem 2: Vanishing Gradients\n",
    "\n",
    "**Vanishing Gradients** occur when gradients become extremely small as they backpropagate through layers.\n",
    "\n",
    "**Mathematical Intuition**:\n",
    "\n",
    "During backpropagation, gradients are multiplied across layers:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w_1} = \\frac{\\partial L}{\\partial w_n} \\cdot \\frac{\\partial w_n}{\\partial w_{n-1}} \\cdots \\frac{\\partial w_2}{\\partial w_1}$$\n",
    "\n",
    "If each gradient term is < 1, the product becomes exponentially smaller.\n",
    "\n",
    "**Common Causes**:\n",
    "- Deep networks with sigmoid/tanh activations\n",
    "- Poor weight initialization\n",
    "- Very deep networks without skip connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create deep network with vanishing gradient problem\n",
    "def create_vanishing_gradient_model():\n",
    "    \"\"\"Deep network with sigmoid activations (prone to vanishing gradients).\"\"\"\n",
    "    model = keras.Sequential([\n",
    "        layers.InputLayer(input_shape=(784,))\n",
    "    ])\n",
    "    \n",
    "    # Add many layers with sigmoid activation\n",
    "    for _ in range(10):\n",
    "        model.add(layers.Dense(50, activation='sigmoid'))  # Sigmoid saturates!\n",
    "    \n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "vanishing_model = create_vanishing_gradient_model()\n",
    "print(\"Deep network with sigmoid activations:\")\n",
    "print(f\"Total layers: {len(vanishing_model.layers)}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Custom callback to monitor gradient norms\n",
    "class GradientMonitor(keras.callbacks.Callback):\n",
    "    \"\"\"Monitor gradient magnitudes during training.\"\"\"\n",
    "    \n",
    "    def __init__(self, X_sample, y_sample):\n",
    "        super().__init__()\n",
    "        self.X_sample = X_sample\n",
    "        self.y_sample = y_sample\n",
    "        self.gradient_norms = []\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Compute gradients\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.model(self.X_sample, training=True)\n",
    "            loss = self.model.compiled_loss(self.y_sample, predictions)\n",
    "        \n",
    "        gradients = tape.gradient(loss, self.model.trainable_weights)\n",
    "        \n",
    "        # Calculate gradient norms for each layer\n",
    "        norms = []\n",
    "        for grad in gradients:\n",
    "            if grad is not None:\n",
    "                norms.append(tf.norm(grad).numpy())\n",
    "        \n",
    "        self.gradient_norms.append(norms)\n",
    "\n",
    "# Train with gradient monitoring\n",
    "gradient_monitor = GradientMonitor(X_train[:100], y_train[:100])\n",
    "\n",
    "print(\"Training model with vanishing gradient problem...\")\n",
    "vanishing_history = vanishing_model.fit(\n",
    "    X_train[:5000], y_train[:5000],\n",
    "    validation_split=0.2,\n",
    "    epochs=5,\n",
    "    batch_size=32,\n",
    "    callbacks=[gradient_monitor],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(f\"Final training accuracy: {vanishing_history.history['accuracy'][-1]:.4f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize gradient norms across layers\n",
    "final_gradients = gradient_monitor.gradient_norms[-1]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(len(final_gradients)), final_gradients, 'o-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Layer Index', fontsize=12)\n",
    "plt.ylabel('Gradient Norm', fontsize=12)\n",
    "plt.title('Gradient Norms Across Layers (Vanishing Gradient Problem)', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.yscale('log')  # Log scale to see vanishing effect\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=1e-5, color='red', linestyle='--', label='Very Small Gradient')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nGradient Analysis:\")\n",
    "print(f\"First layer gradient norm: {final_gradients[0]:.2e}\")\n",
    "print(f\"Last layer gradient norm: {final_gradients[-1]:.2e}\")\n",
    "if final_gradients[0] < 1e-5:\n",
    "    print(\"⚠️  WARNING: Vanishing gradients detected in early layers!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Problem 3: Exploding Gradients\n",
    "\n",
    "**Exploding Gradients** occur when gradients become extremely large, causing weight updates to overshoot.\n",
    "\n",
    "**Symptoms**:\n",
    "- Loss becomes NaN or Inf\n",
    "- Weights grow exponentially\n",
    "- Model diverges instead of converging\n",
    "\n",
    "**Common Causes**:\n",
    "- Learning rate too high\n",
    "- Poor weight initialization (weights too large)\n",
    "- Unstable network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create model with exploding gradient problem\n",
    "def create_exploding_gradient_model():\n",
    "    \"\"\"Model with large initialization and high learning rate.\"\"\"\n",
    "    model = keras.Sequential([\n",
    "        layers.InputLayer(input_shape=(784,)),\n",
    "        # Bad initialization: weights too large\n",
    "        layers.Dense(128, activation='relu',\n",
    "                    kernel_initializer=keras.initializers.RandomNormal(mean=0, stddev=10)),\n",
    "        layers.Dense(64, activation='relu',\n",
    "                    kernel_initializer=keras.initializers.RandomNormal(mean=0, stddev=10)),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Very high learning rate\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.SGD(learning_rate=10.0),  # Too high!\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "exploding_model = create_exploding_gradient_model()\n",
    "\n",
    "# Custom callback to detect NaN\n",
    "class NaNTerminator(keras.callbacks.Callback):\n",
    "    \"\"\"Stop training if NaN is detected.\"\"\"\n",
    "    \n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        loss = logs.get('loss')\n",
    "        if loss is not None and (np.isnan(loss) or np.isinf(loss)):\n",
    "            print(f\"\\n⚠️  NaN/Inf detected at batch {batch}! Stopping training.\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "print(\"Training model with exploding gradient problem...\")\n",
    "try:\n",
    "    exploding_history = exploding_model.fit(\n",
    "        X_train[:1000], y_train[:1000],\n",
    "        epochs=5,\n",
    "        batch_size=32,\n",
    "        callbacks=[NaNTerminator()],\n",
    "        verbose=0\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Training failed: {e}\")\n",
    "\n",
    "print(\"\\nNotice: Model likely diverged or produced NaN values!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Solution: Proper Weight Initialization\n",
    "\n",
    "**Weight Initialization Methods**:\n",
    "\n",
    "1. **Xavier/Glorot Initialization** (for sigmoid/tanh):\n",
    "   $$W \\sim \\mathcal{N}\\left(0, \\sqrt{\\frac{2}{n_{in} + n_{out}}}\\right)$$\n",
    "\n",
    "2. **He Initialization** (for ReLU):\n",
    "   $$W \\sim \\mathcal{N}\\left(0, \\sqrt{\\frac{2}{n_{in}}}\\right)$$\n",
    "\n",
    "These ensure gradients neither vanish nor explode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compare different initializations\n",
    "def create_model_with_init(init_method):\n",
    "    \"\"\"Create model with specified initialization.\"\"\"\n",
    "    model = keras.Sequential([\n",
    "        layers.InputLayer(input_shape=(784,)),\n",
    "        layers.Dense(128, activation='relu', kernel_initializer=init_method),\n",
    "        layers.Dense(64, activation='relu', kernel_initializer=init_method),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Test different initializations\n",
    "initializers = {\n",
    "    'Random Normal': keras.initializers.RandomNormal(stddev=0.01),\n",
    "    'Xavier/Glorot': keras.initializers.GlorotUniform(),\n",
    "    'He': keras.initializers.HeNormal()  # Best for ReLU\n",
    "}\n",
    "\n",
    "init_results = {}\n",
    "\n",
    "for name, init in initializers.items():\n",
    "    print(f\"\\nTraining with {name} initialization...\")\n",
    "    model = create_model_with_init(init)\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train[:5000], y_train[:5000],\n",
    "        validation_split=0.2,\n",
    "        epochs=10,\n",
    "        batch_size=32,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    init_results[name] = history\n",
    "    print(f\"Final accuracy: {history.history['accuracy'][-1]:.4f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compare initialization methods\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "for name, history in init_results.items():\n",
    "    plt.plot(history.history['val_accuracy'], label=name, linewidth=2)\n",
    "\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Validation Accuracy', fontsize=12)\n",
    "plt.title('Impact of Weight Initialization on Training', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Insight: He initialization works best for ReLU activations!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Gradient Checking\n",
    "\n",
    "**Gradient Checking** verifies that your backpropagation implementation is correct by comparing analytical gradients with numerical gradients.\n",
    "\n",
    "**Numerical Gradient** (slow but accurate):\n",
    "$$\\frac{\\partial f}{\\partial \\theta} \\approx \\frac{f(\\theta + \\epsilon) - f(\\theta - \\epsilon)}{2\\epsilon}$$\n",
    "\n",
    "**Use Case**: Debugging custom layers or loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def gradient_check(model, X, y, epsilon=1e-7):\n",
    "    \"\"\"\n",
    "    Perform gradient checking on a model.\n",
    "    Compares analytical gradients (from backprop) with numerical gradients.\n",
    "    \n",
    "    Args:\n",
    "        model: Keras model\n",
    "        X: Input data\n",
    "        y: Labels\n",
    "        epsilon: Small value for numerical gradient computation\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with gradient comparison results\n",
    "    \"\"\"\n",
    "    # Get analytical gradients\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(X, training=True)\n",
    "        loss = model.compiled_loss(y, predictions)\n",
    "    \n",
    "    analytical_grads = tape.gradient(loss, model.trainable_weights)\n",
    "    \n",
    "    # Check first weight only (for efficiency)\n",
    "    weight = model.trainable_weights[0]\n",
    "    analytical_grad = analytical_grads[0].numpy().flatten()\n",
    "    \n",
    "    # Compute numerical gradient for a few random weights\n",
    "    num_checks = min(10, len(analytical_grad))  # Check 10 random weights\n",
    "    indices = np.random.choice(len(analytical_grad), num_checks, replace=False)\n",
    "    \n",
    "    differences = []\n",
    "    \n",
    "    for idx in indices:\n",
    "        # Compute f(theta + epsilon)\n",
    "        original_value = weight.numpy().flatten()[idx]\n",
    "        \n",
    "        # Perturb weight positively\n",
    "        weight_flat = weight.numpy().flatten()\n",
    "        weight_flat[idx] = original_value + epsilon\n",
    "        weight.assign(weight_flat.reshape(weight.shape))\n",
    "        \n",
    "        predictions_plus = model(X, training=False)\n",
    "        loss_plus = model.compiled_loss(y, predictions_plus).numpy()\n",
    "        \n",
    "        # Perturb weight negatively\n",
    "        weight_flat[idx] = original_value - epsilon\n",
    "        weight.assign(weight_flat.reshape(weight.shape))\n",
    "        \n",
    "        predictions_minus = model(X, training=False)\n",
    "        loss_minus = model.compiled_loss(y, predictions_minus).numpy()\n",
    "        \n",
    "        # Restore original weight\n",
    "        weight_flat[idx] = original_value\n",
    "        weight.assign(weight_flat.reshape(weight.shape))\n",
    "        \n",
    "        # Compute numerical gradient\n",
    "        numerical_grad = (loss_plus - loss_minus) / (2 * epsilon)\n",
    "        \n",
    "        # Compare\n",
    "        analytical_val = analytical_grad[idx]\n",
    "        difference = abs(numerical_grad - analytical_val)\n",
    "        relative_diff = difference / (abs(numerical_grad) + abs(analytical_val) + 1e-8)\n",
    "        \n",
    "        differences.append(relative_diff)\n",
    "    \n",
    "    avg_difference = np.mean(differences)\n",
    "    max_difference = np.max(differences)\n",
    "    \n",
    "    return {\n",
    "        'average_difference': avg_difference,\n",
    "        'max_difference': max_difference,\n",
    "        'all_differences': differences\n",
    "    }\n",
    "\n",
    "# Test gradient checking\n",
    "test_model = create_model_with_init('he_normal')\n",
    "X_sample = X_train[:10]\n",
    "y_sample = y_train[:10]\n",
    "\n",
    "print(\"Performing gradient check...\")\n",
    "results = gradient_check(test_model, X_sample, y_sample)\n",
    "\n",
    "print(\"\\nGradient Check Results:\")\n",
    "print(f\"Average relative difference: {results['average_difference']:.2e}\")\n",
    "print(f\"Max relative difference: {results['max_difference']:.2e}\")\n",
    "\n",
    "if results['average_difference'] < 1e-5:\n",
    "    print(\"✅ Gradients are correct!\")\n",
    "elif results['average_difference'] < 1e-3:\n",
    "    print(\"⚠️  Gradients are approximately correct (acceptable)\")\n",
    "else:\n",
    "    print(\"❌ Gradient computation may have errors!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Debugging Checklist\n",
    "\n",
    "When your neural network isn't working, follow this systematic checklist:\n",
    "\n",
    "### Step 1: Start Simple\n",
    "- [ ] Try overfitting on a single batch (should get 100% accuracy)\n",
    "- [ ] If can't overfit single batch → implementation bug\n",
    "\n",
    "### Step 2: Check Data\n",
    "- [ ] Visualize input data\n",
    "- [ ] Check for correct normalization\n",
    "- [ ] Verify labels match inputs\n",
    "- [ ] Check for class imbalance\n",
    "\n",
    "### Step 3: Check Model Architecture\n",
    "- [ ] Verify input/output shapes\n",
    "- [ ] Check activation functions\n",
    "- [ ] Ensure sufficient capacity\n",
    "\n",
    "### Step 4: Check Initialization and Learning Rate\n",
    "- [ ] Use proper initialization (He for ReLU)\n",
    "- [ ] Start with standard learning rate (1e-3 for Adam)\n",
    "- [ ] Use learning rate finder if needed\n",
    "\n",
    "### Step 5: Monitor Training\n",
    "- [ ] Plot loss curves\n",
    "- [ ] Check gradient norms\n",
    "- [ ] Monitor activation statistics\n",
    "- [ ] Look for NaN/Inf values\n",
    "\n",
    "### Step 6: Regularization (only if overfitting)\n",
    "- [ ] Add dropout\n",
    "- [ ] Add L2 regularization\n",
    "- [ ] Use data augmentation\n",
    "- [ ] Reduce model capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Implement \"overfit single batch\" test\n",
    "def overfit_single_batch_test(model, X_batch, y_batch, epochs=100):\n",
    "    \"\"\"\n",
    "    Test if model can overfit a single batch.\n",
    "    This verifies the model has learning capacity and backprop works.\n",
    "    \n",
    "    Returns:\n",
    "        True if model successfully overfits, False otherwise\n",
    "    \"\"\"\n",
    "    print(\"Testing if model can overfit single batch...\")\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_batch, y_batch,\n",
    "        epochs=epochs,\n",
    "        batch_size=len(X_batch),\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    final_acc = history.history['accuracy'][-1]\n",
    "    \n",
    "    print(f\"Final accuracy on single batch: {final_acc:.4f}\")\n",
    "    \n",
    "    if final_acc > 0.95:\n",
    "        print(\"✅ Model can learn! Backprop and optimization are working.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"❌ Model cannot overfit single batch. Check:\")\n",
    "        print(\"   - Model architecture (sufficient capacity?)\")\n",
    "        print(\"   - Learning rate (too small?)\")\n",
    "        print(\"   - Loss function (correct for task?)\")\n",
    "        print(\"   - Implementation bugs\")\n",
    "        return False\n",
    "\n",
    "# Test on a good model\n",
    "good_model = create_model_with_init('he_normal')\n",
    "single_batch_X = X_train[:32]\n",
    "single_batch_y = y_train[:32]\n",
    "\n",
    "overfit_single_batch_test(good_model, single_batch_X, single_batch_y)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualization Tools for Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def visualize_weights_and_activations(model, X_sample):\n",
    "    \"\"\"\n",
    "    Visualize weight distributions and activation distributions.\n",
    "    Helps identify dead neurons, saturation, etc.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "    \n",
    "    # Plot weight distributions for first 3 layers\n",
    "    for i in range(min(3, len(model.layers))):\n",
    "        layer = model.layers[i]\n",
    "        if hasattr(layer, 'kernel'):\n",
    "            weights = layer.kernel.numpy().flatten()\n",
    "            \n",
    "            axes[0, i].hist(weights, bins=50, alpha=0.7, edgecolor='black')\n",
    "            axes[0, i].set_title(f'Layer {i} Weights', fontweight='bold')\n",
    "            axes[0, i].set_xlabel('Weight Value')\n",
    "            axes[0, i].set_ylabel('Frequency')\n",
    "            axes[0, i].axvline(0, color='red', linestyle='--', alpha=0.5)\n",
    "            axes[0, i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot activation distributions\n",
    "    for i in range(min(3, len(model.layers))):\n",
    "        layer = model.layers[i]\n",
    "        intermediate_model = keras.Model(\n",
    "            inputs=model.input,\n",
    "            outputs=layer.output\n",
    "        )\n",
    "        activations = intermediate_model.predict(X_sample, verbose=0).flatten()\n",
    "        \n",
    "        axes[1, i].hist(activations, bins=50, alpha=0.7, edgecolor='black')\n",
    "        axes[1, i].set_title(f'Layer {i} Activations', fontweight='bold')\n",
    "        axes[1, i].set_xlabel('Activation Value')\n",
    "        axes[1, i].set_ylabel('Frequency')\n",
    "        axes[1, i].axvline(0, color='red', linestyle='--', alpha=0.5)\n",
    "        axes[1, i].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Check for dead neurons\n",
    "        dead_percentage = (np.sum(activations == 0) / len(activations)) * 100\n",
    "        axes[1, i].text(0.95, 0.95, f'{dead_percentage:.1f}% dead',\n",
    "                       transform=axes[1, i].transAxes,\n",
    "                       verticalalignment='top',\n",
    "                       horizontalalignment='right',\n",
    "                       bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    plt.suptitle('Weight and Activation Distributions', \n",
    "                 fontsize=14, fontweight='bold', y=1.00)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Train a model and visualize\n",
    "debug_model = create_model_with_init('he_normal')\n",
    "debug_model.fit(X_train[:1000], y_train[:1000], epochs=5, batch_size=32, verbose=0)\n",
    "\n",
    "visualize_weights_and_activations(debug_model, X_val[:100])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Exercise 1: Diagnose and Fix a Broken Model\n",
    "\n",
    "**Task**: You're given a model that isn't learning. Diagnose the problem and fix it.\n",
    "\n",
    "**Requirements**:\n",
    "1. Train the broken model and observe the symptoms\n",
    "2. Use debugging techniques to identify the root cause\n",
    "3. Fix the model and verify it learns properly\n",
    "4. Document what was wrong and how you fixed it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Broken model for you to fix\n",
    "def create_broken_model():\n",
    "    \"\"\"This model has multiple issues. Can you find and fix them all?\"\"\"\n",
    "    model = keras.Sequential([\n",
    "        layers.InputLayer(input_shape=(784,)),\n",
    "        layers.Dense(1000, activation='sigmoid'),\n",
    "        layers.Dense(1000, activation='sigmoid'),\n",
    "        layers.Dense(1000, activation='sigmoid'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.SGD(learning_rate=0.00001),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# 1. Train the broken model and observe issues\n",
    "# 2. Apply debugging techniques (gradient monitoring, activation checking, etc.)\n",
    "# 3. Identify the problems\n",
    "# 4. Create a fixed version\n",
    "# 5. Compare performance\n",
    "\n",
    "pass  # Replace with your implementation"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Exercise 2: Implement Learning Rate Warmup\n",
    "\n",
    "**Task**: Implement learning rate warmup to prevent early training instability.\n",
    "\n",
    "**Concept**: Start with very small learning rate and gradually increase it over first few epochs.\n",
    "\n",
    "**Requirements**:\n",
    "1. Create a custom callback that implements LR warmup\n",
    "2. Warmup should gradually increase LR from 0 to target LR over N epochs\n",
    "3. After warmup, use constant or decaying LR\n",
    "4. Compare training with and without warmup\n",
    "5. Show when warmup helps most (large models, large batch sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# YOUR CODE HERE\n",
    "# Hint: Create a custom callback that modifies learning rate\n",
    "# class LRWarmup(keras.callbacks.Callback):\n",
    "#     def on_epoch_begin(self, epoch, logs=None):\n",
    "#         # Implement warmup logic here\n",
    "\n",
    "pass  # Replace with your implementation"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Exercise 3: Build a Comprehensive Training Monitor\n",
    "\n",
    "**Task**: Create a training monitor that tracks multiple debugging metrics.\n",
    "\n",
    "**Requirements**:\n",
    "1. Monitor gradient norms for each layer\n",
    "2. Track percentage of dead neurons (activation = 0)\n",
    "3. Record weight statistics (mean, std, min, max)\n",
    "4. Detect potential issues (vanishing/exploding gradients, dead neurons)\n",
    "5. Create a visualization dashboard showing all metrics\n",
    "6. Issue warnings when problems are detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# YOUR CODE HERE\n",
    "# Hint: Extend the GradientMonitor callback\n",
    "# Add methods to track multiple metrics\n",
    "# Create a comprehensive plotting function\n",
    "\n",
    "pass  # Replace with your implementation"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Summary\n",
    "\n",
    "### Key Concepts Covered:\n",
    "\n",
    "1. **Common Neural Network Problems**\n",
    "   - Not learning: dead neurons, wrong settings\n",
    "   - Learning too slowly: poor initialization, small LR\n",
    "   - Training instability: exploding gradients, high LR\n",
    "   - Overfitting/Underfitting: capacity issues\n",
    "\n",
    "2. **Gradient Problems**\n",
    "   - Vanishing gradients: common in deep networks with sigmoid\n",
    "   - Exploding gradients: caused by large weights or high LR\n",
    "   - Solutions: proper initialization, ReLU, gradient clipping\n",
    "\n",
    "3. **Weight Initialization**\n",
    "   - He initialization for ReLU: $W \\sim \\mathcal{N}(0, \\sqrt{2/n_{in}})$\n",
    "   - Xavier/Glorot for sigmoid/tanh\n",
    "   - Critical for training stability\n",
    "\n",
    "4. **Gradient Checking**\n",
    "   - Numerical vs analytical gradients\n",
    "   - Useful for debugging custom implementations\n",
    "   - Expensive but valuable for validation\n",
    "\n",
    "5. **Systematic Debugging**\n",
    "   - Overfit single batch test\n",
    "   - Check data, architecture, initialization\n",
    "   - Monitor gradients and activations\n",
    "   - Use visualization tools\n",
    "\n",
    "### Debugging Best Practices:\n",
    "\n",
    "- **Start simple**: Verify model can overfit small data\n",
    "- **Monitor everything**: Gradients, weights, activations\n",
    "- **Visualize**: Plots reveal patterns humans miss\n",
    "- **Use proper defaults**: He init + ReLU + Adam\n",
    "- **Test incrementally**: Add complexity gradually\n",
    "- **Keep logs**: Track experiments systematically\n",
    "\n",
    "### Quick Reference: Common Fixes\n",
    "\n",
    "| Problem | Likely Cause | Solution |\n",
    "|---------|-------------|----------|\n",
    "| Not learning | LR too small, dead neurons | Increase LR, check activation |\n",
    "| NaN loss | LR too large, exploding gradients | Reduce LR, gradient clipping |\n",
    "| Slow convergence | Poor initialization, bad architecture | Use He init, add capacity |\n",
    "| Vanishing gradients | Deep + sigmoid, bad init | Use ReLU, proper init |\n",
    "| Dead neurons | Bad init, high LR | He init, reduce LR |\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "- [Module 12: Model Interpretation and Visualization](12_model_interpretation_visualization.ipynb)\n",
    "- Advanced debugging: TensorBoard, profiling, distributed training issues\n",
    "\n",
    "### Additional Resources:\n",
    "\n",
    "1. \"Delving Deep into Rectifiers\" (He et al., 2015) - He initialization paper\n",
    "2. \"Understanding the difficulty of training deep feedforward neural networks\" (Glorot & Bengio, 2010)\n",
    "3. Andrej Karpathy's \"Recipe for Training Neural Networks\": http://karpathy.github.io/2019/04/25/recipe/\n",
    "4. TensorFlow Debugger documentation: https://www.tensorflow.org/guide/debugger"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
