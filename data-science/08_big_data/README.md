# Big Data

Distributed data processing at scale with Apache Spark.

## Overview

According to the DataScience_SelfLearnPath.md: **"Apache Spark with PySpark is the industry standard (38.7% of data engineer roles require it)."** Big data technologies become essential when datasets exceed single-machine memory and distributed computing becomes necessary.

## Projects in This Domain

1. **big-data-pyspark** (50-60 hours) üöß *Placeholder*
   - Spark fundamentals and architecture
   - RDDs, DataFrames, and Datasets
   - Spark SQL for distributed queries
   - MLlib for machine learning at scale
   - Performance optimization

## Learning Path

**Recommended order:**
1. Complete big-data-pyspark modules
2. Practice on large-scale datasets
3. Deploy Spark on cloud platforms

**Time investment:** 8-10 weeks (Advanced phase)

## Prerequisites

- **data-engineering-fundamentals** (ETL concepts)
- **data-science-fundamentals** (Pandas - Spark DataFrame API is similar)
- **sql-fundamentals** (Spark SQL builds on SQL knowledge)
- **machine-learning-fundamentals** (for MLlib)

## What You'll Achieve

After completing these projects, you will:
- Process terabyte-scale datasets with Spark
- Write efficient distributed computations
- Use Spark SQL for large-scale analytics
- Train ML models on big data with MLlib
- Optimize Spark performance
- Deploy on Databricks, AWS EMR, or Azure Synapse
- Know when to use Spark vs Pandas

## Key Technologies

**Core**: Apache Spark, PySpark, Spark SQL
**Cloud Platforms**: Databricks, AWS EMR, Azure Synapse, GCP Dataproc
**Related**: Hadoop ecosystem (HDFS, Hive) - declining but still relevant

## When to Use Spark

‚úÖ **Use Spark when:**
- Data > 100GB (doesn't fit in RAM)
- Need distributed processing
- Complex ETL at scale
- Production data pipelines

‚ùå **Don't use Spark when:**
- Data < 10GB (Pandas is faster and simpler)
- Rapid prototyping
- Simple analyses

## Next Steps

Proceed to:
- **09_mlops_deployment** - Deploy Spark ML models
- **04_data_engineering/streaming** - Combine with Kafka/Flink
- **09_mlops_deployment/cloud-platforms** - Cloud-based big data

## Roadmap Alignment

**DataScience_SelfLearnPath.md**: Advanced Phase (Months 10-12)
- Part of Big Data ML Engineer track (9-10 months total)
- Critical for data engineering roles (38.7% requirement)
- Increasingly dominates over Hadoop ecosystem
- Essential for processing TB/PB-scale data

## Industry Statistics

- **38.7%** of data engineer roles require Spark
- **74.5%** need Azure, **49.5%** need AWS (often for Spark deployment)
- Databricks (managed Spark) growing rapidly
- Higher compensation for candidates with big data skills

## Learning Resources

- **DataCamp**: Introduction to PySpark (4 hours, 18.48M learners)
- **Databricks Academy**: Free Apache Spark Developer Learning Plan
- **Coursera**: Big Data Analysis with Apache Spark
- **Book**: "Learning Spark" 2nd Edition by Damji et al.

---

**Status**: üöß big-data-pyspark is a placeholder - needs development
