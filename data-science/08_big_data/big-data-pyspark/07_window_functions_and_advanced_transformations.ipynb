{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 07: Window Functions and Advanced Transformations\n",
    "\n",
    "**Difficulty**: â­â­\n",
    "\n",
    "**Estimated Time**: 85-100 minutes\n",
    "\n",
    "**Prerequisites**: \n",
    "- Module 00: Introduction to Big Data and Spark Ecosystem\n",
    "- Module 01: PySpark Setup and SparkSession\n",
    "- Module 03: DataFrames and Datasets\n",
    "- Module 05: DataFrame Operations\n",
    "- Module 06: Spark SQL and Temporary Views\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Understand window functions and when to use them\n",
    "2. Perform ranking operations (row_number, rank, dense_rank)\n",
    "3. Calculate running totals and moving averages\n",
    "4. Use lead() and lag() for accessing neighboring rows\n",
    "5. Create and use User Defined Functions (UDFs) for custom transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType\n",
    "from pyspark.sql.functions import (\n",
    "    col, lit, when,\n",
    "    row_number, rank, dense_rank, percent_rank, ntile,\n",
    "    lag, lead,\n",
    "    sum, avg, count, min, max,\n",
    "    round, concat, upper,\n",
    "    year, month, dayofmonth, datediff, current_date\n",
    ")\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "from datetime import date, timedelta\n",
    "import random\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Module 07: Window Functions and Advanced Transformations\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"âœ“ SparkSession created: {spark.sparkContext.appName}\")\n",
    "print(f\"  Spark version: {spark.version}\")\n",
    "print(f\"  Spark UI: {spark.sparkContext.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Sample DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample employees data\n",
    "employees_data = [\n",
    "    (1, \"Alice Johnson\", \"Engineering\", 75000, date(2020, 1, 15)),\n",
    "    (2, \"Bob Smith\", \"Sales\", 65000, date(2019, 6, 10)),\n",
    "    (3, \"Charlie Brown\", \"Engineering\", 80000, date(2021, 3, 22)),\n",
    "    (4, \"Diana Prince\", \"Marketing\", 70000, date(2020, 8, 5)),\n",
    "    (5, \"Eve Davis\", \"Engineering\", 85000, date(2018, 11, 30)),\n",
    "    (6, \"Frank Miller\", \"Sales\", 68000, date(2022, 2, 14)),\n",
    "    (7, \"Grace Lee\", \"Marketing\", 72000, date(2021, 9, 1)),\n",
    "    (8, \"Henry Wilson\", \"Engineering\", 78000, date(2019, 4, 20)),\n",
    "    (9, \"Ivy Chen\", \"Sales\", 71000, date(2020, 12, 10)),\n",
    "    (10, \"Jack Thompson\", \"Engineering\", 82000, date(2021, 7, 5)),\n",
    "    (11, \"Kelly Adams\", \"Sales\", 73000, date(2020, 3, 18)),\n",
    "    (12, \"Leo Martinez\", \"Marketing\", 69000, date(2022, 1, 20))\n",
    "]\n",
    "\n",
    "df_employees = spark.createDataFrame(\n",
    "    employees_data,\n",
    "    [\"emp_id\", \"name\", \"department\", \"salary\", \"hire_date\"]\n",
    ")\n",
    "\n",
    "print(\"Employees DataFrame:\")\n",
    "df_employees.show()\n",
    "\n",
    "# Sample sales data with dates\n",
    "sales_data = []\n",
    "base_date = date(2024, 1, 1)\n",
    "products = ['Laptop', 'Phone', 'Tablet']\n",
    "regions = ['North', 'South']\n",
    "\n",
    "for i in range(30):\n",
    "    sales_data.append((\n",
    "        i + 1,\n",
    "        base_date + timedelta(days=i),\n",
    "        random.choice(products),\n",
    "        random.choice(regions),\n",
    "        random.randint(1, 5),\n",
    "        round(random.uniform(500, 2000), 2)\n",
    "    ))\n",
    "\n",
    "df_sales = spark.createDataFrame(\n",
    "    sales_data,\n",
    "    [\"sale_id\", \"sale_date\", \"product\", \"region\", \"quantity\", \"revenue\"]\n",
    ")\n",
    "\n",
    "print(\"\\nSales DataFrame (first 10 rows):\")\n",
    "df_sales.show(10)\n",
    "\n",
    "# Sample stock prices\n",
    "stock_data = []\n",
    "for i in range(20):\n",
    "    stock_data.append((\n",
    "        base_date + timedelta(days=i),\n",
    "        \"TECH\",\n",
    "        round(100 + random.uniform(-10, 10), 2)\n",
    "    ))\n",
    "\n",
    "df_stocks = spark.createDataFrame(\n",
    "    stock_data,\n",
    "    [\"date\", \"symbol\", \"price\"]\n",
    ")\n",
    "\n",
    "print(\"\\nStock Prices (first 10 rows):\")\n",
    "df_stocks.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Window Functions\n",
    "\n",
    "### What are Window Functions?\n",
    "\n",
    "**Window functions** perform calculations across a set of rows (a \"window\") related to the current row.\n",
    "\n",
    "Unlike GROUP BY:\n",
    "- **Rows are NOT collapsed** - you keep all original rows\n",
    "- **Calculations across related rows** - e.g., running totals, rankings\n",
    "- **Flexible partitioning and ordering**\n",
    "\n",
    "### Window Function Components\n",
    "\n",
    "```python\n",
    "Window.partitionBy(\"column\")     # Divide data into partitions\n",
    "      .orderBy(\"column\")         # Order within each partition\n",
    "      .rowsBetween(start, end)   # Define window frame (optional)\n",
    "```\n",
    "\n",
    "### Visual Example\n",
    "\n",
    "```\n",
    "Original Data:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Department â”‚ Salary â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Sales      â”‚ 65000  â”‚\n",
    "â”‚ Sales      â”‚ 68000  â”‚\n",
    "â”‚ Sales      â”‚ 71000  â”‚\n",
    "â”‚ Engineeringâ”‚ 75000  â”‚\n",
    "â”‚ Engineeringâ”‚ 80000  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "With Window Function (row_number per department):\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Department â”‚ Salary â”‚ Rank â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Sales      â”‚ 65000  â”‚  1   â”‚  â† Window: Sales rows\n",
    "â”‚ Sales      â”‚ 68000  â”‚  2   â”‚  â†\n",
    "â”‚ Sales      â”‚ 71000  â”‚  3   â”‚  â†\n",
    "â”‚ Engineeringâ”‚ 75000  â”‚  1   â”‚  â† Window: Engineering rows\n",
    "â”‚ Engineeringâ”‚ 80000  â”‚  2   â”‚  â†\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ranking Functions\n",
    "\n",
    "### row_number(), rank(), dense_rank()\n",
    "\n",
    "| Function | Description | Example |\n",
    "|----------|-------------|----------|\n",
    "| `row_number()` | Sequential number, no gaps | 1, 2, 3, 4, 5 |\n",
    "| `rank()` | Rank with gaps for ties | 1, 2, 2, 4, 5 |\n",
    "| `dense_rank()` | Rank without gaps | 1, 2, 2, 3, 4 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking Employees by Salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define window: partition by department, order by salary descending\n",
    "window_spec = Window.partitionBy(\"department\").orderBy(col(\"salary\").desc())\n",
    "\n",
    "# Apply ranking functions\n",
    "df_ranked = df_employees.select(\n",
    "    col(\"name\"),\n",
    "    col(\"department\"),\n",
    "    col(\"salary\"),\n",
    "    row_number().over(window_spec).alias(\"row_num\"),\n",
    "    rank().over(window_spec).alias(\"rank\"),\n",
    "    dense_rank().over(window_spec).alias(\"dense_rank\")\n",
    ")\n",
    "\n",
    "print(\"Employee rankings by salary within department:\")\n",
    "df_ranked.orderBy(\"department\", \"row_num\").show()\n",
    "\n",
    "print(\"\\nğŸ’¡ Notice: row_number is always unique, rank has gaps, dense_rank has no gaps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Top N per Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find top 2 highest paid employees per department\n",
    "window_top = Window.partitionBy(\"department\").orderBy(col(\"salary\").desc())\n",
    "\n",
    "df_top2 = df_employees \\\n",
    "    .withColumn(\"rank\", row_number().over(window_top)) \\\n",
    "    .filter(col(\"rank\") <= 2) \\\n",
    "    .select(\"name\", \"department\", \"salary\", \"rank\")\n",
    "\n",
    "print(\"Top 2 highest paid per department:\")\n",
    "df_top2.orderBy(\"department\", \"rank\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percent Rank and Ntile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percent_rank: Relative rank (0 to 1)\n",
    "# ntile: Divide into N buckets\n",
    "\n",
    "window_all = Window.orderBy(col(\"salary\").desc())\n",
    "\n",
    "df_percentiles = df_employees.select(\n",
    "    col(\"name\"),\n",
    "    col(\"salary\"),\n",
    "    round(percent_rank().over(window_all), 2).alias(\"percent_rank\"),\n",
    "    ntile(4).over(window_all).alias(\"quartile\")\n",
    ")\n",
    "\n",
    "print(\"Salary percentiles and quartiles:\")\n",
    "df_percentiles.orderBy(\"salary\", ascending=False).show()\n",
    "\n",
    "print(\"\\nğŸ’¡ Quartile 1 = top 25%, Quartile 4 = bottom 25%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Aggregate Window Functions\n",
    "\n",
    "### Running Totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate running total of revenue by date\n",
    "window_running = Window.orderBy(\"sale_date\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "df_running_total = df_sales \\\n",
    "    .select(\n",
    "        col(\"sale_date\"),\n",
    "        col(\"product\"),\n",
    "        col(\"revenue\"),\n",
    "        round(sum(\"revenue\").over(window_running), 2).alias(\"running_total\")\n",
    "    ) \\\n",
    "    .orderBy(\"sale_date\")\n",
    "\n",
    "print(\"Running total of revenue:\")\n",
    "df_running_total.show(15)\n",
    "\n",
    "print(\"\\nğŸ’¡ Running total accumulates revenue from start to current row\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate 3-day moving average of stock prices\n",
    "# Window: current row + 2 preceding rows\n",
    "window_moving = Window.orderBy(\"date\").rowsBetween(-2, 0)\n",
    "\n",
    "df_moving_avg = df_stocks \\\n",
    "    .select(\n",
    "        col(\"date\"),\n",
    "        col(\"symbol\"),\n",
    "        col(\"price\"),\n",
    "        round(avg(\"price\").over(window_moving), 2).alias(\"moving_avg_3day\")\n",
    "    ) \\\n",
    "    .orderBy(\"date\")\n",
    "\n",
    "print(\"3-day moving average of stock price:\")\n",
    "df_moving_avg.show(10)\n",
    "\n",
    "print(\"\\nğŸ’¡ Moving average smooths out short-term fluctuations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cumulative Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple cumulative aggregations\n",
    "window_cumulative = Window \\\n",
    "    .partitionBy(\"product\") \\\n",
    "    .orderBy(\"sale_date\") \\\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "df_cumulative = df_sales \\\n",
    "    .select(\n",
    "        col(\"sale_date\"),\n",
    "        col(\"product\"),\n",
    "        col(\"revenue\"),\n",
    "        count(\"*\").over(window_cumulative).alias(\"cumulative_count\"),\n",
    "        round(sum(\"revenue\").over(window_cumulative), 2).alias(\"cumulative_revenue\"),\n",
    "        round(avg(\"revenue\").over(window_cumulative), 2).alias(\"cumulative_avg\")\n",
    "    ) \\\n",
    "    .orderBy(\"product\", \"sale_date\")\n",
    "\n",
    "print(\"Cumulative statistics by product:\")\n",
    "df_cumulative.filter(col(\"product\") == \"Laptop\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Lag and Lead Functions\n",
    "\n",
    "### What are lag() and lead()?\n",
    "\n",
    "- **lag()**: Access previous row value\n",
    "- **lead()**: Access next row value\n",
    "\n",
    "Useful for:\n",
    "- Comparing with previous/next period\n",
    "- Calculating change/growth rates\n",
    "- Finding consecutive patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate day-over-day price change\n",
    "window_ordered = Window.orderBy(\"date\")\n",
    "\n",
    "df_price_changes = df_stocks \\\n",
    "    .select(\n",
    "        col(\"date\"),\n",
    "        col(\"price\"),\n",
    "        lag(\"price\", 1).over(window_ordered).alias(\"prev_price\"),\n",
    "        lead(\"price\", 1).over(window_ordered).alias(\"next_price\")\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        \"day_change\",\n",
    "        round(col(\"price\") - col(\"prev_price\"), 2)\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        \"day_change_pct\",\n",
    "        round((col(\"price\") - col(\"prev_price\")) / col(\"prev_price\") * 100, 2)\n",
    "    )\n",
    "\n",
    "print(\"Stock price changes:\")\n",
    "df_price_changes.show(10)\n",
    "\n",
    "print(\"\\nğŸ’¡ First row has null prev_price (no previous data)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Consecutive Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find consecutive days with price increase\n",
    "window_price = Window.orderBy(\"date\")\n",
    "\n",
    "df_trends = df_stocks \\\n",
    "    .withColumn(\"prev_price\", lag(\"price\", 1).over(window_price)) \\\n",
    "    .withColumn(\n",
    "        \"is_increase\",\n",
    "        when(col(\"price\") > col(\"prev_price\"), 1).otherwise(0)\n",
    "    ) \\\n",
    "    .select(\"date\", \"price\", \"prev_price\", \"is_increase\")\n",
    "\n",
    "print(\"Price increase indicators:\")\n",
    "df_trends.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Window Frames\n",
    "\n",
    "### Understanding Window Frames\n",
    "\n",
    "Window frames define **which rows** to include in the calculation.\n",
    "\n",
    "**Common frames**:\n",
    "```python\n",
    "# From start to current row (cumulative)\n",
    ".rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "# Current row only\n",
    ".rowsBetween(Window.currentRow, Window.currentRow)\n",
    "\n",
    "# 3 rows: previous, current, next\n",
    ".rowsBetween(-1, 1)\n",
    "\n",
    "# Last 7 rows including current\n",
    ".rowsBetween(-6, 0)\n",
    "\n",
    "# All rows in partition\n",
    ".rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different window frames\n",
    "window_all = Window.orderBy(\"sale_date\") \\\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "\n",
    "window_cumulative = Window.orderBy(\"sale_date\") \\\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "window_rolling3 = Window.orderBy(\"sale_date\") \\\n",
    "    .rowsBetween(-2, 0)\n",
    "\n",
    "df_frames = df_sales \\\n",
    "    .select(\n",
    "        col(\"sale_date\"),\n",
    "        col(\"revenue\"),\n",
    "        round(avg(\"revenue\").over(window_all), 2).alias(\"overall_avg\"),\n",
    "        round(avg(\"revenue\").over(window_cumulative), 2).alias(\"cumulative_avg\"),\n",
    "        round(avg(\"revenue\").over(window_rolling3), 2).alias(\"rolling_3day_avg\")\n",
    "    ) \\\n",
    "    .orderBy(\"sale_date\")\n",
    "\n",
    "print(\"Comparing different window frames:\")\n",
    "df_frames.show(10)\n",
    "\n",
    "print(\"\"\"\n",
    "ğŸ’¡ Notice the differences:\n",
    "   - overall_avg: Same for all rows (entire dataset)\n",
    "   - cumulative_avg: Increases as more data accumulated\n",
    "   - rolling_3day_avg: Changes with recent 3 days\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. User Defined Functions (UDFs)\n",
    "\n",
    "### What are UDFs?\n",
    "\n",
    "**UDFs** let you define custom functions when built-in functions aren't enough.\n",
    "\n",
    "âš ï¸ **Warning**: UDFs are slower than built-in functions!\n",
    "- They break Catalyst optimization\n",
    "- Require serialization/deserialization\n",
    "- Use only when necessary\n",
    "\n",
    "### Creating a Simple UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Python function\n",
    "def categorize_salary(salary):\n",
    "    if salary >= 80000:\n",
    "        return \"Senior\"\n",
    "    elif salary >= 70000:\n",
    "        return \"Mid-Level\"\n",
    "    else:\n",
    "        return \"Junior\"\n",
    "\n",
    "# Register as UDF\n",
    "from pyspark.sql.functions import udf\n",
    "categorize_salary_udf = udf(categorize_salary, StringType())\n",
    "\n",
    "# Use UDF\n",
    "df_with_category = df_employees.select(\n",
    "    col(\"name\"),\n",
    "    col(\"salary\"),\n",
    "    categorize_salary_udf(col(\"salary\")).alias(\"salary_category\")\n",
    ")\n",
    "\n",
    "print(\"Using UDF to categorize salaries:\")\n",
    "df_with_category.show()\n",
    "\n",
    "print(\"\"\"\n",
    "ğŸ’¡ Better alternative: Use built-in when() function!\n",
    "   UDFs should be last resort.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better Alternative: Built-in Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same logic using built-in when() - MUCH FASTER!\n",
    "df_with_category_builtin = df_employees.select(\n",
    "    col(\"name\"),\n",
    "    col(\"salary\"),\n",
    "    when(col(\"salary\") >= 80000, \"Senior\")\n",
    "        .when(col(\"salary\") >= 70000, \"Mid-Level\")\n",
    "        .otherwise(\"Junior\")\n",
    "        .alias(\"salary_category\")\n",
    ")\n",
    "\n",
    "print(\"Same result using built-in when():\")\n",
    "df_with_category_builtin.show()\n",
    "\n",
    "print(\"âœ“ This is MUCH faster than UDF!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use UDFs\n",
    "\n",
    "âœ… **Use UDFs when**:\n",
    "- Complex business logic not expressible with built-in functions\n",
    "- Need to call external libraries\n",
    "- Integrating with existing Python code\n",
    "\n",
    "âŒ **Avoid UDFs when**:\n",
    "- Built-in functions can do the job\n",
    "- Performance is critical\n",
    "- Working with large datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UDF with Multiple Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDF with multiple inputs\n",
    "def format_employee_info(name, department, salary):\n",
    "    return f\"{name} ({department}) - ${salary:,.0f}\"\n",
    "\n",
    "format_info_udf = udf(format_employee_info, StringType())\n",
    "\n",
    "df_formatted = df_employees.select(\n",
    "    col(\"name\"),\n",
    "    format_info_udf(\n",
    "        col(\"name\"),\n",
    "        col(\"department\"),\n",
    "        col(\"salary\")\n",
    "    ).alias(\"formatted_info\")\n",
    ")\n",
    "\n",
    "print(\"Formatted employee info:\")\n",
    "df_formatted.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Sales Rankings\n",
    "\n",
    "Using the `df_sales` DataFrame:\n",
    "1. Rank sales by revenue within each product category\n",
    "2. Find the top 3 sales for each product\n",
    "3. Add a column showing what percentage of total product revenue each sale represents\n",
    "4. Display results sorted by product and rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Moving Average Analysis\n",
    "\n",
    "Using the `df_stocks` DataFrame:\n",
    "1. Calculate 5-day moving average of price\n",
    "2. Calculate 10-day moving average of price\n",
    "3. Add a column indicating if price is above/below the 5-day moving average\n",
    "4. Show dates where short-term trend (5-day) crosses above long-term trend (10-day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your code here\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Year-over-Year Comparison\n",
    "\n",
    "Create sample monthly sales data for 2023 and 2024, then:\n",
    "1. Calculate total revenue by month for each year\n",
    "2. Use lag() to get previous year's revenue for same month\n",
    "3. Calculate year-over-year growth percentage\n",
    "4. Identify months with highest growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Your code here\n",
    "\n",
    "# First create sample data for 2023 and 2024\n",
    "# Then calculate YoY growth\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Department Analytics\n",
    "\n",
    "Using the `df_employees` DataFrame:\n",
    "1. Calculate each employee's salary as percentage of department total\n",
    "2. Find how each employee's salary compares to department average (difference)\n",
    "3. Rank employees within department by hire date (earliest = 1)\n",
    "4. Add a column showing the salary of the next person hired in the same department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Your code here\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions\n",
    "\n",
    "### Exercise 1 Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 1: Sales Rankings\n",
    "\n",
    "# Window for ranking and percentage calculation\n",
    "window_rank = Window.partitionBy(\"product\").orderBy(col(\"revenue\").desc())\n",
    "window_total = Window.partitionBy(\"product\") \\\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "\n",
    "df_sales_ranked = df_sales \\\n",
    "    .withColumn(\"rank\", row_number().over(window_rank)) \\\n",
    "    .withColumn(\"product_total_revenue\", sum(\"revenue\").over(window_total)) \\\n",
    "    .withColumn(\n",
    "        \"pct_of_product_revenue\",\n",
    "        round((col(\"revenue\") / col(\"product_total_revenue\")) * 100, 2)\n",
    "    ) \\\n",
    "    .filter(col(\"rank\") <= 3) \\\n",
    "    .select(\n",
    "        \"product\",\n",
    "        \"sale_date\",\n",
    "        \"revenue\",\n",
    "        \"rank\",\n",
    "        \"pct_of_product_revenue\"\n",
    "    ) \\\n",
    "    .orderBy(\"product\", \"rank\")\n",
    "\n",
    "print(\"Top 3 sales by product with revenue percentage:\")\n",
    "df_sales_ranked.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 2: Moving Average Analysis\n",
    "\n",
    "window_5day = Window.orderBy(\"date\").rowsBetween(-4, 0)\n",
    "window_10day = Window.orderBy(\"date\").rowsBetween(-9, 0)\n",
    "\n",
    "df_moving_avgs = df_stocks \\\n",
    "    .withColumn(\"ma_5day\", round(avg(\"price\").over(window_5day), 2)) \\\n",
    "    .withColumn(\"ma_10day\", round(avg(\"price\").over(window_10day), 2)) \\\n",
    "    .withColumn(\n",
    "        \"above_5day_ma\",\n",
    "        when(col(\"price\") > col(\"ma_5day\"), \"Above\").otherwise(\"Below\")\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        \"prev_ma_5day\",\n",
    "        lag(\"ma_5day\", 1).over(Window.orderBy(\"date\"))\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        \"prev_ma_10day\",\n",
    "        lag(\"ma_10day\", 1).over(Window.orderBy(\"date\"))\n",
    "    )\n",
    "\n",
    "print(\"Stock price with moving averages:\")\n",
    "df_moving_avgs.select(\n",
    "    \"date\", \"price\", \"ma_5day\", \"ma_10day\", \"above_5day_ma\"\n",
    ").show(15)\n",
    "\n",
    "# Find crossover points (5-day crosses above 10-day)\n",
    "df_crossovers = df_moving_avgs.filter(\n",
    "    (col(\"ma_5day\") > col(\"ma_10day\")) &\n",
    "    (col(\"prev_ma_5day\") <= col(\"prev_ma_10day\"))\n",
    ")\n",
    "\n",
    "print(\"\\nCrossover points (5-day MA crosses above 10-day MA):\")\n",
    "df_crossovers.select(\"date\", \"price\", \"ma_5day\", \"ma_10day\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 3: Year-over-Year Comparison\n",
    "\n",
    "# Create sample monthly data\n",
    "monthly_data = []\n",
    "for year in [2023, 2024]:\n",
    "    for month in range(1, 7):  # Jan to Jun\n",
    "        revenue = round(random.uniform(10000, 50000), 2)\n",
    "        if year == 2024:\n",
    "            revenue *= 1.15  # 15% growth on average\n",
    "        monthly_data.append((year, month, revenue))\n",
    "\n",
    "df_monthly = spark.createDataFrame(\n",
    "    monthly_data,\n",
    "    [\"year\", \"month\", \"revenue\"]\n",
    ")\n",
    "\n",
    "# Calculate YoY growth\n",
    "window_yoy = Window.partitionBy(\"month\").orderBy(\"year\")\n",
    "\n",
    "df_yoy = df_monthly \\\n",
    "    .withColumn(\"prev_year_revenue\", lag(\"revenue\", 1).over(window_yoy)) \\\n",
    "    .withColumn(\n",
    "        \"yoy_growth\",\n",
    "        round(\n",
    "            ((col(\"revenue\") - col(\"prev_year_revenue\")) / col(\"prev_year_revenue\")) * 100,\n",
    "            2\n",
    "        )\n",
    "    ) \\\n",
    "    .orderBy(\"month\", \"year\")\n",
    "\n",
    "print(\"Year-over-Year revenue comparison:\")\n",
    "df_yoy.show()\n",
    "\n",
    "print(\"\\nMonths with highest growth:\")\n",
    "df_yoy.filter(col(\"yoy_growth\").isNotNull()) \\\n",
    "    .orderBy(col(\"yoy_growth\").desc()) \\\n",
    "    .show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4 Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 4: Department Analytics\n",
    "\n",
    "# Windows for different calculations\n",
    "window_dept = Window.partitionBy(\"department\") \\\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "window_hire = Window.partitionBy(\"department\").orderBy(\"hire_date\")\n",
    "\n",
    "df_dept_analytics = df_employees \\\n",
    "    .withColumn(\"dept_total_salary\", sum(\"salary\").over(window_dept)) \\\n",
    "    .withColumn(\"dept_avg_salary\", avg(\"salary\").over(window_dept)) \\\n",
    "    .withColumn(\n",
    "        \"pct_of_dept_salary\",\n",
    "        round((col(\"salary\") / col(\"dept_total_salary\")) * 100, 2)\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        \"diff_from_avg\",\n",
    "        round(col(\"salary\") - col(\"dept_avg_salary\"), 2)\n",
    "    ) \\\n",
    "    .withColumn(\"hire_rank\", row_number().over(window_hire)) \\\n",
    "    .withColumn(\"next_hire_salary\", lead(\"salary\", 1).over(window_hire)) \\\n",
    "    .select(\n",
    "        \"name\",\n",
    "        \"department\",\n",
    "        \"salary\",\n",
    "        \"pct_of_dept_salary\",\n",
    "        \"diff_from_avg\",\n",
    "        \"hire_rank\",\n",
    "        \"next_hire_salary\"\n",
    "    ) \\\n",
    "    .orderBy(\"department\", \"hire_rank\")\n",
    "\n",
    "print(\"Department analytics:\")\n",
    "df_dept_analytics.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts Covered\n",
    "\n",
    "âœ… **Window Functions**: Calculations across related rows without collapsing\n",
    "\n",
    "âœ… **Ranking**: row_number(), rank(), dense_rank(), percent_rank(), ntile()\n",
    "\n",
    "âœ… **Aggregates**: sum(), avg(), count() over windows\n",
    "\n",
    "âœ… **Navigation**: lag(), lead() for accessing neighboring rows\n",
    "\n",
    "âœ… **UDFs**: Custom functions (use sparingly!)\n",
    "\n",
    "### Window Function Components\n",
    "\n",
    "```python\n",
    "Window.partitionBy(\"col\")              # Divide into groups\n",
    "      .orderBy(\"col\")                  # Sort within groups\n",
    "      .rowsBetween(start, end)         # Define frame\n",
    "```\n",
    "\n",
    "### Common Window Frames\n",
    "\n",
    "| Frame | Description | Use Case |\n",
    "|-------|-------------|----------|\n",
    "| `(unboundedPreceding, currentRow)` | From start to current | Running totals |\n",
    "| `(-6, 0)` | Last 7 rows | 7-day moving average |\n",
    "| `(-1, 1)` | Previous, current, next | Smoothing |\n",
    "| `(unboundedPreceding, unboundedFollowing)` | All rows | Percentage of total |\n",
    "\n",
    "### Ranking Functions\n",
    "\n",
    "| Function | Behavior | Example |\n",
    "|----------|----------|----------|\n",
    "| `row_number()` | Sequential, no gaps | 1,2,3,4,5 |\n",
    "| `rank()` | Gaps for ties | 1,2,2,4,5 |\n",
    "| `dense_rank()` | No gaps | 1,2,2,3,4 |\n",
    "| `percent_rank()` | 0 to 1 scale | 0, 0.25, 0.5, 0.75, 1.0 |\n",
    "| `ntile(n)` | Split into n buckets | 1,1,2,2,3,3 |\n",
    "\n",
    "### Lag and Lead\n",
    "\n",
    "```python\n",
    "lag(\"col\", 1)   # Previous row value\n",
    "lead(\"col\", 1)  # Next row value\n",
    "lag(\"col\", 2)   # 2 rows back\n",
    "lead(\"col\", 3)  # 3 rows ahead\n",
    "```\n",
    "\n",
    "### UDF Best Practices\n",
    "\n",
    "âŒ **Avoid UDFs when possible**:\n",
    "- They break Catalyst optimization\n",
    "- Much slower than built-in functions\n",
    "- Require serialization overhead\n",
    "\n",
    "âœ… **Use built-in functions instead**:\n",
    "- `when()` for conditional logic\n",
    "- String functions: `concat()`, `upper()`, `lower()`, `substring()`\n",
    "- Math functions: `round()`, `abs()`, `sqrt()`\n",
    "- Date functions: `year()`, `month()`, `datediff()`\n",
    "\n",
    "âœ… **Only use UDFs when**:\n",
    "- Complex custom logic not in built-ins\n",
    "- Calling external libraries\n",
    "- Legacy Python code integration\n",
    "\n",
    "### Common Use Cases\n",
    "\n",
    "**Top N per Group**:\n",
    "```python\n",
    "Window.partitionBy(\"group\").orderBy(col(\"value\").desc())\n",
    "df.withColumn(\"rank\", row_number().over(window))\n",
    "  .filter(col(\"rank\") <= N)\n",
    "```\n",
    "\n",
    "**Running Total**:\n",
    "```python\n",
    "Window.orderBy(\"date\")\n",
    "      .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "df.withColumn(\"running_total\", sum(\"amount\").over(window))\n",
    "```\n",
    "\n",
    "**Moving Average**:\n",
    "```python\n",
    "Window.orderBy(\"date\").rowsBetween(-6, 0)\n",
    "df.withColumn(\"7day_avg\", avg(\"value\").over(window))\n",
    "```\n",
    "\n",
    "**Percentage of Total**:\n",
    "```python\n",
    "window_total = Window.partitionBy(\"group\")\n",
    "              .rowsBetween(unboundedPreceding, unboundedFollowing)\n",
    "df.withColumn(\"total\", sum(\"value\").over(window_total))\n",
    "  .withColumn(\"pct\", col(\"value\") / col(\"total\") * 100)\n",
    "```\n",
    "\n",
    "**Day-over-Day Change**:\n",
    "```python\n",
    "Window.orderBy(\"date\")\n",
    "df.withColumn(\"prev_value\", lag(\"value\", 1).over(window))\n",
    "  .withColumn(\"change\", col(\"value\") - col(\"prev_value\"))\n",
    "```\n",
    "\n",
    "### Performance Tips\n",
    "\n",
    "1. **Partition wisely**: More partitions = more parallelism\n",
    "2. **Order only when needed**: Ordering is expensive\n",
    "3. **Use specific frames**: Don't use entire partition if not needed\n",
    "4. **Avoid UDFs**: Use built-in functions whenever possible\n",
    "5. **Cache if reusing**: If same window used multiple times\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "Congratulations! You've completed the PySpark fundamentals. Next steps:\n",
    "\n",
    "- **Module 08**: Performance Optimization and Tuning\n",
    "- **Module 09**: Advanced Topics (Broadcast joins, caching, partitioning)\n",
    "- **Module 10**: Real-world Projects and Case Studies\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [Window Functions Guide](https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-window.html)\n",
    "- [Window Functions API](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/window.html)\n",
    "- [Built-in Functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "spark.stop()\n",
    "print(\"SparkSession stopped. âœ“\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
