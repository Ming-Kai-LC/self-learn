{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 00: Introduction to Big Data and Spark Ecosystem\n",
    "\n",
    "**Difficulty**: ⭐\n",
    "\n",
    "**Estimated Time**: 45-60 minutes\n",
    "\n",
    "**Prerequisites**: \n",
    "- Python fundamentals\n",
    "- Pandas basics (helpful for comparison)\n",
    "- SQL knowledge (helpful but not required)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Define what \"big data\" means and identify scenarios requiring distributed computing\n",
    "2. Explain the Apache Spark ecosystem and its core components\n",
    "3. Compare Spark vs Pandas and determine when to use each tool\n",
    "4. Understand the fundamentals of distributed computing architecture\n",
    "5. Identify real-world use cases where Spark excels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is Big Data?\n",
    "\n",
    "### The Three Vs of Big Data\n",
    "\n",
    "Big Data is commonly defined by three characteristics:\n",
    "\n",
    "1. **Volume**: The sheer amount of data (terabytes to petabytes)\n",
    "2. **Velocity**: The speed at which data is generated and processed\n",
    "3. **Variety**: Different types and formats of data (structured, semi-structured, unstructured)\n",
    "\n",
    "### When Do You Need Big Data Tools?\n",
    "\n",
    "You need distributed computing tools like Spark when:\n",
    "\n",
    "- **Data doesn't fit in memory**: Your dataset exceeds your computer's RAM (e.g., >16GB on typical laptops)\n",
    "- **Processing takes too long**: Single-machine processing would take hours or days\n",
    "- **Data is distributed**: Data already lives across multiple machines or locations\n",
    "- **Real-time processing**: You need to process streaming data in real-time\n",
    "\n",
    "### Example: Pandas vs Spark Threshold\n",
    "\n",
    "```\n",
    "Pandas (Single Machine):\n",
    "- Dataset: < 10GB\n",
    "- RAM: 16-32GB\n",
    "- Processing: Minutes\n",
    "- Use case: Exploratory analysis, prototyping\n",
    "\n",
    "Spark (Distributed):\n",
    "- Dataset: > 10GB to Petabytes\n",
    "- RAM: Distributed across cluster\n",
    "- Processing: Scales linearly with cluster size\n",
    "- Use case: Production pipelines, large-scale ML\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Apache Spark Ecosystem\n",
    "\n",
    "### What is Apache Spark?\n",
    "\n",
    "Apache Spark is an open-source, distributed computing system designed for:\n",
    "- Fast processing of large-scale data\n",
    "- Both batch and streaming workloads\n",
    "- In-memory computation (100x faster than Hadoop MapReduce)\n",
    "\n",
    "### Spark Core Components\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────┐\n",
    "│         Apache Spark Ecosystem          │\n",
    "├─────────────────────────────────────────┤\n",
    "│  Spark SQL  │  MLlib   │  GraphX  │ ... │\n",
    "│  (DataFrames)│ (ML)     │ (Graphs) │    │\n",
    "├─────────────────────────────────────────┤\n",
    "│         Spark Streaming                 │\n",
    "│      (Real-time Processing)             │\n",
    "├─────────────────────────────────────────┤\n",
    "│           Spark Core                    │\n",
    "│    (RDDs, Task Scheduling, Memory)      │\n",
    "├─────────────────────────────────────────┤\n",
    "│  Cluster Managers: YARN, Mesos, K8s     │\n",
    "└─────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "#### Key Components:\n",
    "\n",
    "1. **Spark Core**: Foundation with RDDs (Resilient Distributed Datasets)\n",
    "2. **Spark SQL**: Structured data processing with DataFrames\n",
    "3. **MLlib**: Scalable machine learning library\n",
    "4. **Spark Streaming**: Real-time data stream processing\n",
    "5. **GraphX**: Graph computation (social networks, recommendation systems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Distributed Computing Fundamentals\n",
    "\n",
    "### Master-Worker Architecture\n",
    "\n",
    "Spark uses a master-worker architecture:\n",
    "\n",
    "```\n",
    "        ┌─────────────┐\n",
    "        │   Driver    │  ← Your Python code runs here\n",
    "        │  (Master)   │  ← Coordinates the work\n",
    "        └──────┬──────┘\n",
    "               │\n",
    "       ┌───────┴───────┬───────┐\n",
    "       │               │       │\n",
    "   ┌───▼───┐      ┌───▼───┐  ┌▼──────┐\n",
    "   │Worker │      │Worker │  │Worker │\n",
    "   │(Exec) │      │(Exec) │  │(Exec) │\n",
    "   └───────┘      └───────┘  └───────┘\n",
    "   ↓ Data        ↓ Data      ↓ Data\n",
    "   Partition 1   Partition 2 Partition 3\n",
    "```\n",
    "\n",
    "**Driver (Master)**:\n",
    "- Runs your main program\n",
    "- Coordinates tasks\n",
    "- Collects results\n",
    "\n",
    "**Workers (Executors)**:\n",
    "- Execute tasks on data partitions\n",
    "- Store data in memory or disk\n",
    "- Return results to driver\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Partitioning**: Data is split across multiple machines\n",
    "2. **Parallel Processing**: Operations run simultaneously on each partition\n",
    "3. **Fault Tolerance**: If a worker fails, Spark can recompute lost data\n",
    "4. **Lazy Evaluation**: Operations are not executed until results are needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Spark vs Pandas: When to Use Each\n",
    "\n",
    "### Comparison Matrix\n",
    "\n",
    "| Aspect | Pandas | PySpark |\n",
    "|--------|--------|----------|\n",
    "| **Data Size** | < 10GB (fits in RAM) | > 10GB to Petabytes |\n",
    "| **Processing** | Single machine | Distributed cluster |\n",
    "| **Speed** | Fast for small data | Scales for large data |\n",
    "| **API** | Python-native, intuitive | Spark API (similar to Pandas) |\n",
    "| **Memory** | In-memory (limited by RAM) | Distributed memory + disk |\n",
    "| **Learning Curve** | Easy | Moderate |\n",
    "| **Use Case** | Exploration, prototyping | Production, big data |\n",
    "| **Execution** | Eager (immediate) | Lazy (optimized) |\n",
    "\n",
    "### Decision Tree: Which Tool to Use?\n",
    "\n",
    "```\n",
    "Does your data fit in memory (< 10GB)?\n",
    "├─ YES → Use Pandas\n",
    "│        (Faster development, easier debugging)\n",
    "│\n",
    "└─ NO → Does your data fit on one machine (< 100GB)?\n",
    "         ├─ YES → Consider:\n",
    "         │        - Dask (similar to Pandas, out-of-core)\n",
    "         │        - Spark on single node\n",
    "         │\n",
    "         └─ NO → Use Spark on cluster\n",
    "                 (Only option for 100GB+ datasets)\n",
    "```\n",
    "\n",
    "### Best Practice: Start with Pandas, Scale to Spark\n",
    "\n",
    "1. **Prototype with Pandas** on a small sample\n",
    "2. **Validate your logic** with fast iteration\n",
    "3. **Migrate to Spark** when you need to process the full dataset\n",
    "4. **Leverage Spark's Pandas API** for easier migration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Real-World Use Cases for Spark\n",
    "\n",
    "### Industry Applications\n",
    "\n",
    "1. **E-commerce**: \n",
    "   - Processing billions of transactions\n",
    "   - Real-time recommendation engines\n",
    "   - Customer behavior analysis\n",
    "\n",
    "2. **Finance**:\n",
    "   - Fraud detection on millions of transactions per second\n",
    "   - Risk analysis on historical market data\n",
    "   - High-frequency trading analytics\n",
    "\n",
    "3. **Healthcare**:\n",
    "   - Analyzing genomic data (terabytes per patient)\n",
    "   - Processing medical imaging at scale\n",
    "   - Real-time patient monitoring systems\n",
    "\n",
    "4. **Social Media**:\n",
    "   - Processing user activity logs (petabytes)\n",
    "   - Real-time trend detection\n",
    "   - Content recommendation systems\n",
    "\n",
    "5. **IoT (Internet of Things)**:\n",
    "   - Sensor data from millions of devices\n",
    "   - Predictive maintenance\n",
    "   - Real-time monitoring and alerts\n",
    "\n",
    "### Example: Netflix\n",
    "\n",
    "Netflix uses Spark to:\n",
    "- Process 450+ billion events per day\n",
    "- Generate personalized recommendations for 200+ million users\n",
    "- Analyze video streaming quality in real-time\n",
    "- A/B test new features across global user base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Why Learn PySpark?\n",
    "\n",
    "### Industry Demand\n",
    "\n",
    "- **38.7%** of data engineer job postings require Spark knowledge\n",
    "- **Top 3** most in-demand big data skills\n",
    "- **Higher salaries** for Spark-proficient data engineers\n",
    "\n",
    "### Career Paths\n",
    "\n",
    "PySpark skills are essential for:\n",
    "- **Data Engineer**: Building production data pipelines\n",
    "- **Machine Learning Engineer**: Training models on large datasets\n",
    "- **Data Scientist**: Analyzing big data in production\n",
    "- **Analytics Engineer**: Creating scalable analytics solutions\n",
    "\n",
    "### Modern Data Stack\n",
    "\n",
    "Spark integrates with:\n",
    "- **Cloud platforms**: AWS EMR, Azure Synapse, GCP Dataproc\n",
    "- **Databricks**: Managed Spark platform (industry standard)\n",
    "- **Data lakes**: S3, Azure Data Lake, Delta Lake\n",
    "- **Data warehouses**: Snowflake, BigQuery\n",
    "- **Orchestration**: Apache Airflow, Prefect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Identifying Big Data Use Cases\n",
    "\n",
    "For each scenario below, determine if you would use Pandas or Spark, and explain why:\n",
    "\n",
    "**Scenario A**: Analyzing 5GB of sales data for a quarterly business report\n",
    "\n",
    "**Scenario B**: Processing 500GB of server logs to detect security threats\n",
    "\n",
    "**Scenario C**: Building a real-time dashboard tracking 1 million IoT sensors\n",
    "\n",
    "**Scenario D**: Exploratory data analysis on a 500MB customer survey dataset\n",
    "\n",
    "**Scenario E**: Training a machine learning model on 5TB of historical customer data\n",
    "\n",
    "#### Your Answers:\n",
    "\n",
    "```\n",
    "A: [Your answer]\n",
    "   Reason: \n",
    "\n",
    "B: [Your answer]\n",
    "   Reason: \n",
    "\n",
    "C: [Your answer]\n",
    "   Reason: \n",
    "\n",
    "D: [Your answer]\n",
    "   Reason: \n",
    "\n",
    "E: [Your answer]\n",
    "   Reason: \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Understanding the Spark Ecosystem\n",
    "\n",
    "Match each Spark component to its primary use case:\n",
    "\n",
    "**Components**:\n",
    "1. Spark Core (RDDs)\n",
    "2. Spark SQL\n",
    "3. MLlib\n",
    "4. Spark Streaming\n",
    "5. GraphX\n",
    "\n",
    "**Use Cases**:\n",
    "- A. Building a recommendation system based on user-item relationships\n",
    "- B. Running SQL queries on large structured datasets\n",
    "- C. Training a logistic regression model on 100GB of data\n",
    "- D. Processing clickstream data in real-time\n",
    "- E. Low-level data transformations with maximum control\n",
    "\n",
    "#### Your Matches:\n",
    "\n",
    "```\n",
    "1 → [Letter]\n",
    "2 → [Letter]\n",
    "3 → [Letter]\n",
    "4 → [Letter]\n",
    "5 → [Letter]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Distributed Computing Concepts\n",
    "\n",
    "Answer the following questions about distributed computing:\n",
    "\n",
    "1. **Why is data partitioning important in Spark?**\n",
    "   \n",
    "   Your answer:\n",
    "\n",
    "2. **What is the advantage of lazy evaluation in Spark?**\n",
    "   \n",
    "   Your answer:\n",
    "\n",
    "3. **If you have a 100GB dataset and a cluster with 10 workers (each with 16GB RAM), how might Spark distribute the data?**\n",
    "   \n",
    "   Your answer:\n",
    "\n",
    "4. **Why is Spark considered fault-tolerant?**\n",
    "   \n",
    "   Your answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Career Application\n",
    "\n",
    "Research one company that uses Apache Spark in production:\n",
    "\n",
    "**Company**: [Name]\n",
    "\n",
    "**How they use Spark**: \n",
    "\n",
    "**Scale of data they process**: \n",
    "\n",
    "**Business impact**: \n",
    "\n",
    "**Source**: [Link or reference]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions\n",
    "\n",
    "### Exercise 1 Solutions:\n",
    "\n",
    "```\n",
    "A: Pandas\n",
    "   Reason: 5GB can fit in memory on most modern machines (16GB+ RAM). \n",
    "   Pandas will be faster for this size and allows easier exploration.\n",
    "\n",
    "B: Spark\n",
    "   Reason: 500GB exceeds single-machine memory. Spark's distributed \n",
    "   processing is necessary for this volume.\n",
    "\n",
    "C: Spark Streaming\n",
    "   Reason: Real-time processing of high-velocity data from 1M sensors \n",
    "   requires distributed streaming capabilities.\n",
    "\n",
    "D: Pandas\n",
    "   Reason: 500MB is small enough for in-memory processing. Pandas \n",
    "   provides faster iteration for exploratory analysis.\n",
    "\n",
    "E: Spark MLlib\n",
    "   Reason: 5TB dataset requires distributed training. Spark MLlib \n",
    "   enables model training across cluster.\n",
    "```\n",
    "\n",
    "### Exercise 2 Solutions:\n",
    "\n",
    "```\n",
    "1 → E (Low-level transformations)\n",
    "2 → B (SQL queries)\n",
    "3 → C (Machine learning)\n",
    "4 → D (Real-time streaming)\n",
    "5 → A (Graph relationships)\n",
    "```\n",
    "\n",
    "### Exercise 3 Solutions:\n",
    "\n",
    "1. **Partitioning** allows parallel processing across multiple machines, enabling \n",
    "   Spark to process large datasets that don't fit on a single machine.\n",
    "\n",
    "2. **Lazy evaluation** allows Spark to optimize the entire computation pipeline \n",
    "   before execution (Catalyst optimizer), reducing unnecessary computations and I/O.\n",
    "\n",
    "3. Spark would partition the 100GB dataset into ~10GB chunks (one per worker), \n",
    "   allowing each worker to process its partition in parallel. Data may also \n",
    "   spill to disk if needed.\n",
    "\n",
    "4. Spark maintains **lineage** (the sequence of operations to recreate data). \n",
    "   If a partition is lost due to worker failure, Spark can recompute it from \n",
    "   source data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts Covered\n",
    "\n",
    "✅ **Big Data Definition**: Volume, Velocity, Variety exceeding single-machine capacity\n",
    "\n",
    "✅ **Spark Ecosystem**: Core, SQL, MLlib, Streaming, GraphX components\n",
    "\n",
    "✅ **Distributed Architecture**: Master-worker pattern with partitioned data\n",
    "\n",
    "✅ **Spark vs Pandas**: Use Pandas for <10GB, Spark for larger datasets\n",
    "\n",
    "✅ **Real-World Applications**: E-commerce, finance, healthcare, social media, IoT\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. Spark is designed for **data that doesn't fit in memory**\n",
    "2. Distributed computing **scales horizontally** by adding more machines\n",
    "3. **Start with Pandas, scale to Spark** when needed\n",
    "4. Spark skills are **highly valued** in the job market (38.7% of data engineer roles)\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In **Module 01: PySpark Setup and SparkSession**, you will:\n",
    "- Install and configure PySpark on your local machine\n",
    "- Create your first SparkSession\n",
    "- Explore the Spark UI for monitoring\n",
    "- Understand Spark configuration options\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [Apache Spark Official Documentation](https://spark.apache.org/docs/latest/)\n",
    "- [Databricks Learning Academy](https://www.databricks.com/learn) - Free courses\n",
    "- [Learning Spark 2nd Edition](https://pages.databricks.com/rs/094-YMS-629/images/LearningSpark2.0.pdf) - Free ebook\n",
    "- [Spark: The Definitive Guide](https://www.oreilly.com/library/view/spark-the-definitive/9781491912201/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
