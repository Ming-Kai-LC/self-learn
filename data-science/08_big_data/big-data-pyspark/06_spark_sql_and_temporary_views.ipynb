{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 06: Spark SQL and Temporary Views\n",
    "\n",
    "**Difficulty**: ⭐⭐\n",
    "\n",
    "**Estimated Time**: 70-85 minutes\n",
    "\n",
    "**Prerequisites**: \n",
    "- Module 00: Introduction to Big Data and Spark Ecosystem\n",
    "- Module 01: PySpark Setup and SparkSession\n",
    "- Module 03: DataFrames and Datasets\n",
    "- Module 05: DataFrame Operations\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Create and use temporary views for SQL queries\n",
    "2. Write SQL queries on DataFrames using spark.sql()\n",
    "3. Understand the difference between temporary and global temporary views\n",
    "4. Use the Spark SQL catalog to manage tables and views\n",
    "5. Mix DataFrame API and SQL seamlessly in the same application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType\n",
    "from pyspark.sql.functions import col, sum, avg, count, round\n",
    "from datetime import date\n",
    "import random\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Module 06: Spark SQL and Temporary Views\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"✓ SparkSession created: {spark.sparkContext.appName}\")\n",
    "print(f\"  Spark version: {spark.version}\")\n",
    "print(f\"  Spark UI: {spark.sparkContext.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Sample DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample employees data\n",
    "employees_data = [\n",
    "    (1, \"Alice Johnson\", \"Engineering\", 75000, date(2020, 1, 15)),\n",
    "    (2, \"Bob Smith\", \"Sales\", 65000, date(2019, 6, 10)),\n",
    "    (3, \"Charlie Brown\", \"Engineering\", 80000, date(2021, 3, 22)),\n",
    "    (4, \"Diana Prince\", \"Marketing\", 70000, date(2020, 8, 5)),\n",
    "    (5, \"Eve Davis\", \"Engineering\", 85000, date(2018, 11, 30)),\n",
    "    (6, \"Frank Miller\", \"Sales\", 68000, date(2022, 2, 14)),\n",
    "    (7, \"Grace Lee\", \"Marketing\", 72000, date(2021, 9, 1)),\n",
    "    (8, \"Henry Wilson\", \"Engineering\", 78000, date(2019, 4, 20)),\n",
    "    (9, \"Ivy Chen\", \"Sales\", 71000, date(2020, 12, 10)),\n",
    "    (10, \"Jack Thompson\", \"Engineering\", 82000, date(2021, 7, 5))\n",
    "]\n",
    "\n",
    "df_employees = spark.createDataFrame(\n",
    "    employees_data,\n",
    "    [\"emp_id\", \"name\", \"department\", \"salary\", \"hire_date\"]\n",
    ")\n",
    "\n",
    "print(\"Employees DataFrame:\")\n",
    "df_employees.show()\n",
    "\n",
    "# Sample projects data\n",
    "projects_data = [\n",
    "    (101, \"Project Alpha\", 1, date(2024, 1, 1), date(2024, 6, 30)),\n",
    "    (102, \"Project Beta\", 3, date(2024, 2, 1), date(2024, 8, 31)),\n",
    "    (103, \"Project Gamma\", 5, date(2024, 3, 1), date(2024, 9, 30)),\n",
    "    (104, \"Project Delta\", 2, date(2024, 1, 15), date(2024, 5, 15)),\n",
    "    (105, \"Project Epsilon\", 8, date(2024, 4, 1), date(2024, 10, 31)),\n",
    "]\n",
    "\n",
    "df_projects = spark.createDataFrame(\n",
    "    projects_data,\n",
    "    [\"project_id\", \"project_name\", \"lead_emp_id\", \"start_date\", \"end_date\"]\n",
    ")\n",
    "\n",
    "print(\"\\nProjects DataFrame:\")\n",
    "df_projects.show()\n",
    "\n",
    "# Sample sales data\n",
    "sales_data = []\n",
    "products = ['Laptop', 'Phone', 'Tablet', 'Monitor', 'Keyboard']\n",
    "regions = ['North', 'South', 'East', 'West']\n",
    "\n",
    "for i in range(50):\n",
    "    sales_data.append((\n",
    "        i + 1,\n",
    "        date(2024, random.randint(1, 3), random.randint(1, 28)),\n",
    "        random.choice(products),\n",
    "        random.choice(regions),\n",
    "        random.randint(1, 10),\n",
    "        round(random.uniform(100, 2000), 2)\n",
    "    ))\n",
    "\n",
    "df_sales = spark.createDataFrame(\n",
    "    sales_data,\n",
    "    [\"sale_id\", \"sale_date\", \"product\", \"region\", \"quantity\", \"revenue\"]\n",
    ")\n",
    "\n",
    "print(\"\\nSales DataFrame (first 10 rows):\")\n",
    "df_sales.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Spark SQL\n",
    "\n",
    "### What is Spark SQL?\n",
    "\n",
    "**Spark SQL** is Spark's module for structured data processing. It provides:\n",
    "- SQL interface to query data\n",
    "- Integration with DataFrames\n",
    "- Optimized execution through Catalyst optimizer\n",
    "- Support for various data sources\n",
    "\n",
    "### Why Use SQL with Spark?\n",
    "\n",
    "✅ **Familiarity**: Most data analysts know SQL\n",
    "\n",
    "✅ **Readability**: Complex operations clearer in SQL\n",
    "\n",
    "✅ **Integration**: Mix SQL and DataFrame API\n",
    "\n",
    "✅ **Performance**: Same Catalyst optimizer as DataFrame API\n",
    "\n",
    "### DataFrame API vs SQL\n",
    "\n",
    "```python\n",
    "# DataFrame API\n",
    "df.filter(col(\"salary\") > 70000) \\\n",
    "  .groupBy(\"department\") \\\n",
    "  .agg(avg(\"salary\"))\n",
    "\n",
    "# SQL (equivalent)\n",
    "spark.sql(\"\"\"\n",
    "    SELECT department, AVG(salary)\n",
    "    FROM employees\n",
    "    WHERE salary > 70000\n",
    "    GROUP BY department\n",
    "\"\"\")\n",
    "```\n",
    "\n",
    "**Both produce the same optimized execution plan!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating Temporary Views\n",
    "\n",
    "To query a DataFrame with SQL, we first register it as a **temporary view**.\n",
    "\n",
    "### What is a Temporary View?\n",
    "\n",
    "- A named reference to a DataFrame\n",
    "- Exists only for the session\n",
    "- Doesn't store data (just a reference)\n",
    "- Can be queried with SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Temporary View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporary view\n",
    "df_employees.createOrReplaceTempView(\"employees\")\n",
    "df_projects.createOrReplaceTempView(\"projects\")\n",
    "df_sales.createOrReplaceTempView(\"sales\")\n",
    "\n",
    "print(\"✓ Temporary views created:\")\n",
    "print(\"  - employees\")\n",
    "print(\"  - projects\")\n",
    "print(\"  - sales\")\n",
    "\n",
    "# Note: createOrReplaceTempView() replaces if already exists\n",
    "# Use createTempView() to throw error if exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Running SQL Queries\n",
    "\n",
    "### Basic SELECT Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple SELECT query\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT * \n",
    "    FROM employees\n",
    "    LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "print(\"Simple SELECT:\")\n",
    "result.show()\n",
    "\n",
    "# Select specific columns\n",
    "result2 = spark.sql(\"\"\"\n",
    "    SELECT name, department, salary\n",
    "    FROM employees\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nSelect specific columns:\")\n",
    "result2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WHERE Clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter with WHERE\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT name, department, salary\n",
    "    FROM employees\n",
    "    WHERE salary > 75000\n",
    "\"\"\")\n",
    "\n",
    "print(\"Employees with salary > 75000:\")\n",
    "result.show()\n",
    "\n",
    "# Multiple conditions\n",
    "result2 = spark.sql(\"\"\"\n",
    "    SELECT name, department, salary\n",
    "    FROM employees\n",
    "    WHERE department = 'Engineering' \n",
    "      AND salary > 75000\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nEngineers with salary > 75000:\")\n",
    "result2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ORDER BY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by salary\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT name, department, salary\n",
    "    FROM employees\n",
    "    ORDER BY salary DESC\n",
    "    LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "print(\"Top 5 highest paid employees:\")\n",
    "result.show()\n",
    "\n",
    "# Sort by multiple columns\n",
    "result2 = spark.sql(\"\"\"\n",
    "    SELECT name, department, salary\n",
    "    FROM employees\n",
    "    ORDER BY department ASC, salary DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nSorted by department, then salary:\")\n",
    "result2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Aggregation in SQL\n",
    "\n",
    "### GROUP BY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count employees by department\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        department,\n",
    "        COUNT(*) as num_employees\n",
    "    FROM employees\n",
    "    GROUP BY department\n",
    "    ORDER BY num_employees DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"Employees by department:\")\n",
    "result.show()\n",
    "\n",
    "# Multiple aggregations\n",
    "result2 = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        department,\n",
    "        COUNT(*) as num_employees,\n",
    "        ROUND(AVG(salary), 2) as avg_salary,\n",
    "        MIN(salary) as min_salary,\n",
    "        MAX(salary) as max_salary,\n",
    "        SUM(salary) as total_payroll\n",
    "    FROM employees\n",
    "    GROUP BY department\n",
    "    ORDER BY avg_salary DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nDepartment statistics:\")\n",
    "result2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HAVING Clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter aggregated results with HAVING\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        department,\n",
    "        COUNT(*) as num_employees,\n",
    "        ROUND(AVG(salary), 2) as avg_salary\n",
    "    FROM employees\n",
    "    GROUP BY department\n",
    "    HAVING AVG(salary) > 70000\n",
    "    ORDER BY avg_salary DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"Departments with avg salary > 70000:\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. JOINs in SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INNER JOIN\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        p.project_name,\n",
    "        e.name as project_lead,\n",
    "        e.department,\n",
    "        p.start_date,\n",
    "        p.end_date\n",
    "    FROM projects p\n",
    "    INNER JOIN employees e\n",
    "        ON p.lead_emp_id = e.emp_id\n",
    "    ORDER BY p.start_date\n",
    "\"\"\")\n",
    "\n",
    "print(\"Projects with their leads:\")\n",
    "result.show(truncate=False)\n",
    "\n",
    "# LEFT JOIN\n",
    "result2 = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        e.name,\n",
    "        e.department,\n",
    "        p.project_name\n",
    "    FROM employees e\n",
    "    LEFT JOIN projects p\n",
    "        ON e.emp_id = p.lead_emp_id\n",
    "    ORDER BY e.name\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nEmployees and their projects (including non-leads):\")\n",
    "result2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Subqueries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subquery in WHERE clause\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT name, department, salary\n",
    "    FROM employees\n",
    "    WHERE salary > (\n",
    "        SELECT AVG(salary)\n",
    "        FROM employees\n",
    "    )\n",
    "    ORDER BY salary DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"Employees earning above average:\")\n",
    "result.show()\n",
    "\n",
    "# Subquery with IN\n",
    "result2 = spark.sql(\"\"\"\n",
    "    SELECT name, department, salary\n",
    "    FROM employees\n",
    "    WHERE emp_id IN (\n",
    "        SELECT lead_emp_id\n",
    "        FROM projects\n",
    "    )\n",
    "    ORDER BY name\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nEmployees who are project leads:\")\n",
    "result2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Common Table Expressions (CTEs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using WITH clause (CTE)\n",
    "result = spark.sql(\"\"\"\n",
    "    WITH dept_stats AS (\n",
    "        SELECT \n",
    "            department,\n",
    "            COUNT(*) as num_employees,\n",
    "            AVG(salary) as avg_salary\n",
    "        FROM employees\n",
    "        GROUP BY department\n",
    "    )\n",
    "    SELECT \n",
    "        department,\n",
    "        num_employees,\n",
    "        ROUND(avg_salary, 2) as avg_salary\n",
    "    FROM dept_stats\n",
    "    WHERE num_employees >= 3\n",
    "    ORDER BY avg_salary DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"Departments with 3+ employees:\")\n",
    "result.show()\n",
    "\n",
    "# Multiple CTEs\n",
    "result2 = spark.sql(\"\"\"\n",
    "    WITH \n",
    "    high_earners AS (\n",
    "        SELECT * \n",
    "        FROM employees \n",
    "        WHERE salary > 75000\n",
    "    ),\n",
    "    dept_counts AS (\n",
    "        SELECT department, COUNT(*) as count\n",
    "        FROM high_earners\n",
    "        GROUP BY department\n",
    "    )\n",
    "    SELECT *\n",
    "    FROM dept_counts\n",
    "    ORDER BY count DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nHigh earners by department:\")\n",
    "result2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Date and String Functions in SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date functions\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        name,\n",
    "        hire_date,\n",
    "        YEAR(hire_date) as hire_year,\n",
    "        MONTH(hire_date) as hire_month,\n",
    "        DATEDIFF(CURRENT_DATE(), hire_date) as days_employed\n",
    "    FROM employees\n",
    "    ORDER BY hire_date DESC\n",
    "    LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "print(\"Date functions:\")\n",
    "result.show()\n",
    "\n",
    "# String functions\n",
    "result2 = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        name,\n",
    "        UPPER(name) as upper_name,\n",
    "        LOWER(name) as lower_name,\n",
    "        SUBSTRING(name, 1, 5) as first_5_chars,\n",
    "        CONCAT(name, ' (', department, ')') as name_with_dept\n",
    "    FROM employees\n",
    "    LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nString functions:\")\n",
    "result2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Global Temporary Views\n",
    "\n",
    "### Temporary View vs Global Temporary View\n",
    "\n",
    "| Feature | Temporary View | Global Temporary View |\n",
    "|---------|----------------|----------------------|\n",
    "| **Scope** | Current session | All sessions |\n",
    "| **Prefix** | None | `global_temp.` |\n",
    "| **Lifecycle** | Session ends | Application ends |\n",
    "| **Use case** | Single session | Multi-session sharing |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create global temporary view\n",
    "df_employees.createOrReplaceGlobalTempView(\"employees_global\")\n",
    "\n",
    "print(\"✓ Global temporary view created\")\n",
    "\n",
    "# Query global temporary view\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT department, COUNT(*) as count\n",
    "    FROM global_temp.employees_global\n",
    "    GROUP BY department\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nQuerying global temporary view:\")\n",
    "result.show()\n",
    "\n",
    "# Note: Must use global_temp prefix!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Using the Spark Catalog\n",
    "\n",
    "The **catalog** manages all tables, views, and databases in Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all tables/views\n",
    "print(\"All tables and views:\")\n",
    "spark.catalog.listTables()\n",
    "\n",
    "# Show as DataFrame\n",
    "tables_df = spark.sql(\"SHOW TABLES\")\n",
    "tables_df.show(truncate=False)\n",
    "\n",
    "# List databases\n",
    "print(\"\\nDatabases:\")\n",
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if table exists\n",
    "print(f\"Does 'employees' exist? {spark.catalog.tableExists('employees')}\")\n",
    "print(f\"Does 'nonexistent' exist? {spark.catalog.tableExists('nonexistent')}\")\n",
    "\n",
    "# Get table columns\n",
    "print(\"\\nColumns in 'employees' table:\")\n",
    "for col in spark.catalog.listColumns(\"employees\"):\n",
    "    print(f\"  {col.name}: {col.dataType}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Mixing DataFrame API and SQL\n",
    "\n",
    "You can seamlessly mix SQL and DataFrame operations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with SQL\n",
    "sql_result = spark.sql(\"\"\"\n",
    "    SELECT department, AVG(salary) as avg_salary\n",
    "    FROM employees\n",
    "    GROUP BY department\n",
    "\"\"\")\n",
    "\n",
    "print(\"SQL result:\")\n",
    "sql_result.show()\n",
    "\n",
    "# Continue with DataFrame API\n",
    "final_result = sql_result \\\n",
    "    .filter(col(\"avg_salary\") > 70000) \\\n",
    "    .withColumn(\"avg_salary_rounded\", round(col(\"avg_salary\"), 2)) \\\n",
    "    .drop(\"avg_salary\") \\\n",
    "    .orderBy(col(\"avg_salary_rounded\").desc())\n",
    "\n",
    "print(\"\\nAfter DataFrame operations:\")\n",
    "final_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with DataFrame API\n",
    "df_filtered = df_employees.filter(col(\"salary\") > 70000)\n",
    "\n",
    "# Register as view\n",
    "df_filtered.createOrReplaceTempView(\"high_earners\")\n",
    "\n",
    "# Continue with SQL\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        department,\n",
    "        COUNT(*) as num_high_earners,\n",
    "        ROUND(AVG(salary), 2) as avg_high_earner_salary\n",
    "    FROM high_earners\n",
    "    GROUP BY department\n",
    "    ORDER BY avg_high_earner_salary DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"DataFrame → SQL:\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Sales Analysis with SQL\n",
    "\n",
    "Using the `sales` table, write SQL queries to:\n",
    "1. Find total revenue by product\n",
    "2. Find average revenue per sale by region\n",
    "3. Find products that generated > $10,000 total revenue\n",
    "4. Show results sorted by total revenue (descending)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n",
    "\n",
    "# Your SQL query here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Complex JOIN Query\n",
    "\n",
    "Write a SQL query that:\n",
    "1. Joins employees and projects\n",
    "2. Shows project name, project lead name, and lead's department\n",
    "3. Includes only projects led by Engineering department\n",
    "4. Sorts by project start date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your code here\n",
    "\n",
    "# Your SQL query here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: CTE for Multi-Step Analysis\n",
    "\n",
    "Use a CTE to:\n",
    "1. Calculate each employee's years of service (use DATEDIFF and hire_date)\n",
    "2. Categorize employees: \"Senior\" (>4 years), \"Mid\" (2-4 years), \"Junior\" (<2 years)\n",
    "3. Count employees in each category by department\n",
    "4. Show only departments with at least 2 employees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Your code here\n",
    "\n",
    "# Your SQL query here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Mixing SQL and DataFrame API\n",
    "\n",
    "1. Use SQL to find departments with average salary > $70,000\n",
    "2. Use DataFrame API to add a column \"salary_tier\" based on avg_salary:\n",
    "   - \"High\" if >= 80000\n",
    "   - \"Medium\" if >= 70000\n",
    "   - \"Low\" otherwise\n",
    "3. Sort by avg_salary descending\n",
    "4. Display the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Your code here\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions\n",
    "\n",
    "### Exercise 1 Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 1: Sales Analysis with SQL\n",
    "\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        product,\n",
    "        COUNT(*) as num_sales,\n",
    "        ROUND(SUM(revenue), 2) as total_revenue,\n",
    "        ROUND(AVG(revenue), 2) as avg_revenue\n",
    "    FROM sales\n",
    "    GROUP BY product\n",
    "    HAVING SUM(revenue) > 10000\n",
    "    ORDER BY total_revenue DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"Products with >$10,000 revenue:\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 2: Complex JOIN Query\n",
    "\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        p.project_name,\n",
    "        e.name as project_lead,\n",
    "        e.department,\n",
    "        p.start_date\n",
    "    FROM projects p\n",
    "    INNER JOIN employees e\n",
    "        ON p.lead_emp_id = e.emp_id\n",
    "    WHERE e.department = 'Engineering'\n",
    "    ORDER BY p.start_date\n",
    "\"\"\")\n",
    "\n",
    "print(\"Engineering-led projects:\")\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 3: CTE for Multi-Step Analysis\n",
    "\n",
    "result = spark.sql(\"\"\"\n",
    "    WITH employee_tenure AS (\n",
    "        SELECT \n",
    "            name,\n",
    "            department,\n",
    "            hire_date,\n",
    "            DATEDIFF(CURRENT_DATE(), hire_date) / 365 as years_service,\n",
    "            CASE \n",
    "                WHEN DATEDIFF(CURRENT_DATE(), hire_date) / 365 > 4 THEN 'Senior'\n",
    "                WHEN DATEDIFF(CURRENT_DATE(), hire_date) / 365 >= 2 THEN 'Mid'\n",
    "                ELSE 'Junior'\n",
    "            END as tenure_category\n",
    "        FROM employees\n",
    "    )\n",
    "    SELECT \n",
    "        department,\n",
    "        tenure_category,\n",
    "        COUNT(*) as num_employees\n",
    "    FROM employee_tenure\n",
    "    GROUP BY department, tenure_category\n",
    "    HAVING COUNT(*) >= 1\n",
    "    ORDER BY department, tenure_category\n",
    "\"\"\")\n",
    "\n",
    "print(\"Employee tenure analysis:\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4 Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 4: Mixing SQL and DataFrame API\n",
    "\n",
    "from pyspark.sql.functions import when, col, round\n",
    "\n",
    "# Step 1: SQL query\n",
    "sql_result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        department,\n",
    "        AVG(salary) as avg_salary\n",
    "    FROM employees\n",
    "    GROUP BY department\n",
    "    HAVING AVG(salary) > 70000\n",
    "\"\"\")\n",
    "\n",
    "# Step 2-3: DataFrame API\n",
    "final_result = sql_result \\\n",
    "    .withColumn(\n",
    "        \"salary_tier\",\n",
    "        when(col(\"avg_salary\") >= 80000, \"High\")\n",
    "            .when(col(\"avg_salary\") >= 70000, \"Medium\")\n",
    "            .otherwise(\"Low\")\n",
    "    ) \\\n",
    "    .withColumn(\"avg_salary\", round(col(\"avg_salary\"), 2)) \\\n",
    "    .orderBy(col(\"avg_salary\").desc())\n",
    "\n",
    "# Step 4: Display\n",
    "print(\"Department salary analysis:\")\n",
    "final_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts Covered\n",
    "\n",
    "✅ **Temporary Views**: Register DataFrames for SQL queries\n",
    "\n",
    "✅ **SQL Queries**: Full SQL support with spark.sql()\n",
    "\n",
    "✅ **Global Views**: Share views across sessions\n",
    "\n",
    "✅ **Catalog**: Manage tables, views, and databases\n",
    "\n",
    "✅ **API Mixing**: Seamlessly combine SQL and DataFrame operations\n",
    "\n",
    "### Creating Views\n",
    "\n",
    "**Temporary View** (session-scoped):\n",
    "```python\n",
    "df.createOrReplaceTempView(\"table_name\")\n",
    "spark.sql(\"SELECT * FROM table_name\")\n",
    "```\n",
    "\n",
    "**Global Temporary View** (application-scoped):\n",
    "```python\n",
    "df.createOrReplaceGlobalTempView(\"table_name\")\n",
    "spark.sql(\"SELECT * FROM global_temp.table_name\")\n",
    "```\n",
    "\n",
    "### SQL Features Supported\n",
    "\n",
    "✅ **SELECT, WHERE, ORDER BY, LIMIT**\n",
    "\n",
    "✅ **GROUP BY, HAVING**\n",
    "\n",
    "✅ **JOINs** (INNER, LEFT, RIGHT, FULL, CROSS)\n",
    "\n",
    "✅ **Subqueries**\n",
    "\n",
    "✅ **CTEs** (WITH clause)\n",
    "\n",
    "✅ **Aggregate functions** (COUNT, SUM, AVG, MIN, MAX)\n",
    "\n",
    "✅ **String functions** (UPPER, LOWER, CONCAT, SUBSTRING)\n",
    "\n",
    "✅ **Date functions** (YEAR, MONTH, DAY, DATEDIFF, DATE_ADD)\n",
    "\n",
    "### Catalog Operations\n",
    "\n",
    "```python\n",
    "spark.catalog.listTables()           # List all tables/views\n",
    "spark.catalog.listDatabases()        # List databases\n",
    "spark.catalog.tableExists(\"name\")    # Check if exists\n",
    "spark.catalog.listColumns(\"name\")    # Get columns\n",
    "```\n",
    "\n",
    "### When to Use SQL vs DataFrame API?\n",
    "\n",
    "**Use SQL when**:\n",
    "- Team is familiar with SQL\n",
    "- Complex queries easier to express in SQL\n",
    "- Migrating from traditional databases\n",
    "- Ad-hoc analysis and exploration\n",
    "\n",
    "**Use DataFrame API when**:\n",
    "- Building programmatic data pipelines\n",
    "- Need type safety (in Scala/Java)\n",
    "- Dynamic query construction\n",
    "- Using advanced DataFrame features (window functions, UDFs)\n",
    "\n",
    "**Best practice**: Mix both! Use whichever is clearer for each operation.\n",
    "\n",
    "### Performance Considerations\n",
    "\n",
    "- **Both SQL and DataFrame API use the same Catalyst optimizer**\n",
    "- **No performance difference** between SQL and DataFrame API\n",
    "- Choose based on readability and team preference\n",
    "- Spark optimizes the logical plan before execution\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In **Module 07: Window Functions and Advanced Transformations**, you will:\n",
    "- Use window functions for advanced analytics\n",
    "- Create and use User Defined Functions (UDFs)\n",
    "- Perform ranking, running totals, and moving averages\n",
    "- Handle complex data transformations\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [Spark SQL Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)\n",
    "- [SQL Reference](https://spark.apache.org/docs/latest/sql-ref.html)\n",
    "- [Catalog API](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Catalog.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "spark.stop()\n",
    "print(\"SparkSession stopped. ✓\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
