{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 10: Model Training and Evaluation\n",
    "\n",
    "**Difficulty**: ⭐⭐⭐  \n",
    "**Estimated Time**: 90 minutes  \n",
    "**Prerequisites**: \n",
    "- [Module 08: MLlib Basics](08_pyspark_machine_learning_mllib_basics.ipynb)\n",
    "- [Module 09: Feature Engineering at Scale](09_feature_engineering_at_scale.ipynb)\n",
    "- Understanding of model evaluation metrics\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Train and compare multiple classification algorithms (Logistic Regression, Decision Trees, Random Forests, GBT)\n",
    "2. Train and evaluate regression models (Linear Regression, Decision Tree Regressor, Gradient Boosted Trees)\n",
    "3. Implement cross-validation using CrossValidator for robust model selection\n",
    "4. Perform hyperparameter tuning with ParamGridBuilder to optimize model performance\n",
    "5. Use comprehensive evaluation metrics to select the best model for production deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Introduction\n",
    "\n",
    "**Model Selection Process:**\n",
    "\n",
    "1. **Algorithm Selection**: Choose candidate algorithms based on problem type\n",
    "2. **Training**: Fit models on training data\n",
    "3. **Validation**: Evaluate on validation set or cross-validation\n",
    "4. **Hyperparameter Tuning**: Optimize model parameters\n",
    "5. **Final Evaluation**: Test best model on held-out test set\n",
    "6. **Deployment**: Deploy the selected model to production\n",
    "\n",
    "**Classification Algorithms in MLlib:**\n",
    "- **Logistic Regression**: Linear classifier, fast, interpretable\n",
    "- **Decision Tree**: Non-linear, handles interactions, prone to overfitting\n",
    "- **Random Forest**: Ensemble of trees, reduces overfitting, robust\n",
    "- **Gradient Boosted Trees**: Sequential ensemble, often highest accuracy\n",
    "\n",
    "**Regression Algorithms in MLlib:**\n",
    "- **Linear Regression**: Simple, fast, assumes linear relationships\n",
    "- **Decision Tree Regressor**: Captures non-linearity, interpretable\n",
    "- **Random Forest Regressor**: Robust ensemble method\n",
    "- **Gradient Boosted Tree Regressor**: High accuracy, slower training\n",
    "\n",
    "**Cross-Validation:**\n",
    "- Splits data into k folds\n",
    "- Trains on k-1 folds, validates on 1 fold\n",
    "- Repeats k times and averages results\n",
    "- Provides more reliable performance estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, rand, when, expr\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
    "\n",
    "# Feature engineering\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, StandardScaler\n",
    "\n",
    "# Classification algorithms\n",
    "from pyspark.ml.classification import (\n",
    "    LogisticRegression,\n",
    "    DecisionTreeClassifier,\n",
    "    RandomForestClassifier,\n",
    "    GBTClassifier\n",
    ")\n",
    "\n",
    "# Regression algorithms\n",
    "from pyspark.ml.regression import (\n",
    "    LinearRegression,\n",
    "    DecisionTreeRegressor,\n",
    "    RandomForestRegressor,\n",
    "    GBTRegressor\n",
    ")\n",
    "\n",
    "# Model selection and tuning\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Evaluation\n",
    "from pyspark.ml.evaluation import (\n",
    "    BinaryClassificationEvaluator,\n",
    "    MulticlassClassificationEvaluator,\n",
    "    RegressionEvaluator\n",
    ")\n",
    "\n",
    "# Utilities\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark session with more memory for model training\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Model Training and Evaluation\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(\"Spark session created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classification: Comparing Multiple Algorithms\n",
    "\n",
    "We'll train and compare four classification algorithms on the same dataset.\n",
    "\n",
    "**Scenario**: Predict customer churn based on usage patterns and demographics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive customer churn dataset\n",
    "n_customers = 5000\n",
    "churn_data = []\n",
    "\n",
    "for _ in range(n_customers):\n",
    "    # Features\n",
    "    tenure_months = np.random.randint(1, 72)\n",
    "    monthly_charges = np.random.uniform(20, 120)\n",
    "    total_charges = monthly_charges * tenure_months + np.random.normal(0, 100)\n",
    "    num_products = np.random.randint(1, 5)\n",
    "    support_calls = np.random.randint(0, 10)\n",
    "    data_usage_gb = np.random.exponential(50)\n",
    "    \n",
    "    # Churn probability based on multiple factors\n",
    "    # Higher churn if: short tenure, high charges, many support calls, few products\n",
    "    churn_score = (\n",
    "        -tenure_months * 0.05 +\n",
    "        monthly_charges * 0.02 +\n",
    "        support_calls * 0.3 -\n",
    "        num_products * 0.5 +\n",
    "        np.random.normal(0, 1)\n",
    "    )\n",
    "    \n",
    "    churn_prob = 1 / (1 + np.exp(-churn_score))\n",
    "    churned = 1 if np.random.random() < churn_prob else 0\n",
    "    \n",
    "    churn_data.append((\n",
    "        tenure_months,\n",
    "        float(monthly_charges),\n",
    "        float(total_charges),\n",
    "        num_products,\n",
    "        support_calls,\n",
    "        float(data_usage_gb),\n",
    "        churned\n",
    "    ))\n",
    "\n",
    "df_churn = spark.createDataFrame(\n",
    "    churn_data,\n",
    "    [\"tenure_months\", \"monthly_charges\", \"total_charges\", \"num_products\", \"support_calls\", \"data_usage_gb\", \"churned\"]\n",
    ")\n",
    "\n",
    "print(f\"Total customers: {df_churn.count()}\")\n",
    "print(\"\\nChurn distribution:\")\n",
    "df_churn.groupBy(\"churned\").count().show()\n",
    "print(\"\\nSample data:\")\n",
    "df_churn.show(10)\n",
    "df_churn.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and split data\n",
    "# We'll use this same split for all models to ensure fair comparison\n",
    "feature_cols = [\"tenure_months\", \"monthly_charges\", \"total_charges\", \"num_products\", \"support_calls\", \"data_usage_gb\"]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"raw_features\")\n",
    "scaler = StandardScaler(inputCol=\"raw_features\", outputCol=\"features\", withStd=True, withMean=False)\n",
    "\n",
    "# Prepare features\n",
    "df_assembled = assembler.transform(df_churn)\n",
    "df_scaled = scaler.fit(df_assembled).transform(df_assembled)\n",
    "\n",
    "# Split: 60% train, 20% validation, 20% test\n",
    "train_df, val_df, test_df = df_scaled.randomSplit([0.6, 0.2, 0.2], seed=42)\n",
    "\n",
    "print(f\"Training samples: {train_df.count()}\")\n",
    "print(f\"Validation samples: {val_df.count()}\")\n",
    "print(f\"Test samples: {test_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: Logistic Regression\n",
    "print(\"Training Logistic Regression...\")\n",
    "start_time = time.time()\n",
    "\n",
    "lr = LogisticRegression(\n",
    "    labelCol=\"churned\",\n",
    "    featuresCol=\"features\",\n",
    "    maxIter=20,\n",
    "    regParam=0.01\n",
    ")\n",
    "\n",
    "lr_model = lr.fit(train_df)\n",
    "lr_train_time = time.time() - start_time\n",
    "\n",
    "# Evaluate on validation set\n",
    "lr_predictions = lr_model.transform(val_df)\n",
    "\n",
    "# Multiple metrics\n",
    "acc_evaluator = MulticlassClassificationEvaluator(labelCol=\"churned\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "auc_evaluator = BinaryClassificationEvaluator(labelCol=\"churned\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "f1_evaluator = MulticlassClassificationEvaluator(labelCol=\"churned\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "\n",
    "lr_accuracy = acc_evaluator.evaluate(lr_predictions)\n",
    "lr_auc = auc_evaluator.evaluate(lr_predictions)\n",
    "lr_f1 = f1_evaluator.evaluate(lr_predictions)\n",
    "\n",
    "print(f\"Logistic Regression - Time: {lr_train_time:.2f}s, Accuracy: {lr_accuracy:.4f}, AUC: {lr_auc:.4f}, F1: {lr_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: Decision Tree Classifier\n",
    "print(\"Training Decision Tree...\")\n",
    "start_time = time.time()\n",
    "\n",
    "dt = DecisionTreeClassifier(\n",
    "    labelCol=\"churned\",\n",
    "    featuresCol=\"features\",\n",
    "    maxDepth=10,\n",
    "    minInstancesPerNode=20\n",
    ")\n",
    "\n",
    "dt_model = dt.fit(train_df)\n",
    "dt_train_time = time.time() - start_time\n",
    "\n",
    "dt_predictions = dt_model.transform(val_df)\n",
    "\n",
    "dt_accuracy = acc_evaluator.evaluate(dt_predictions)\n",
    "dt_auc = auc_evaluator.evaluate(dt_predictions)\n",
    "dt_f1 = f1_evaluator.evaluate(dt_predictions)\n",
    "\n",
    "print(f\"Decision Tree - Time: {dt_train_time:.2f}s, Accuracy: {dt_accuracy:.4f}, AUC: {dt_auc:.4f}, F1: {dt_f1:.4f}\")\n",
    "print(f\"Tree depth: {dt_model.depth}, Number of nodes: {dt_model.numNodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3: Random Forest Classifier\n",
    "print(\"Training Random Forest...\")\n",
    "start_time = time.time()\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    labelCol=\"churned\",\n",
    "    featuresCol=\"features\",\n",
    "    numTrees=50,\n",
    "    maxDepth=10,\n",
    "    minInstancesPerNode=20,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "rf_model = rf.fit(train_df)\n",
    "rf_train_time = time.time() - start_time\n",
    "\n",
    "rf_predictions = rf_model.transform(val_df)\n",
    "\n",
    "rf_accuracy = acc_evaluator.evaluate(rf_predictions)\n",
    "rf_auc = auc_evaluator.evaluate(rf_predictions)\n",
    "rf_f1 = f1_evaluator.evaluate(rf_predictions)\n",
    "\n",
    "print(f\"Random Forest - Time: {rf_train_time:.2f}s, Accuracy: {rf_accuracy:.4f}, AUC: {rf_auc:.4f}, F1: {rf_f1:.4f}\")\n",
    "\n",
    "# Feature importances\n",
    "print(\"\\nFeature Importances:\")\n",
    "for i, importance in enumerate(rf_model.featureImportances):\n",
    "    print(f\"{feature_cols[i]}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 4: Gradient Boosted Trees Classifier\n",
    "print(\"Training Gradient Boosted Trees...\")\n",
    "start_time = time.time()\n",
    "\n",
    "gbt = GBTClassifier(\n",
    "    labelCol=\"churned\",\n",
    "    featuresCol=\"features\",\n",
    "    maxIter=20,\n",
    "    maxDepth=5,\n",
    "    stepSize=0.1,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "gbt_model = gbt.fit(train_df)\n",
    "gbt_train_time = time.time() - start_time\n",
    "\n",
    "gbt_predictions = gbt_model.transform(val_df)\n",
    "\n",
    "gbt_accuracy = acc_evaluator.evaluate(gbt_predictions)\n",
    "gbt_auc = auc_evaluator.evaluate(gbt_predictions)\n",
    "gbt_f1 = f1_evaluator.evaluate(gbt_predictions)\n",
    "\n",
    "print(f\"GBT Classifier - Time: {gbt_train_time:.2f}s, Accuracy: {gbt_accuracy:.4f}, AUC: {gbt_auc:.4f}, F1: {gbt_f1:.4f}\")\n",
    "\n",
    "# Feature importances\n",
    "print(\"\\nFeature Importances:\")\n",
    "for i, importance in enumerate(gbt_model.featureImportances):\n",
    "    print(f\"{feature_cols[i]}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all classification models\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CLASSIFICATION MODEL COMPARISON (Validation Set)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Model':<25} {'Time (s)':<12} {'Accuracy':<12} {'AUC':<12} {'F1 Score':<12}\")\n",
    "print(\"-\"*80)\n",
    "print(f\"{'Logistic Regression':<25} {lr_train_time:<12.2f} {lr_accuracy:<12.4f} {lr_auc:<12.4f} {lr_f1:<12.4f}\")\n",
    "print(f\"{'Decision Tree':<25} {dt_train_time:<12.2f} {dt_accuracy:<12.4f} {dt_auc:<12.4f} {dt_f1:<12.4f}\")\n",
    "print(f\"{'Random Forest':<25} {rf_train_time:<12.2f} {rf_accuracy:<12.4f} {rf_auc:<12.4f} {rf_f1:<12.4f}\")\n",
    "print(f\"{'Gradient Boosted Trees':<25} {gbt_train_time:<12.2f} {gbt_accuracy:<12.4f} {gbt_auc:<12.4f} {gbt_f1:<12.4f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Regression: Comparing Multiple Algorithms\n",
    "\n",
    "Now let's compare regression algorithms on a house price prediction task.\n",
    "\n",
    "**Scenario**: Predict house prices based on various features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate house price dataset\n",
    "n_houses = 5000\n",
    "house_data = []\n",
    "\n",
    "for _ in range(n_houses):\n",
    "    # Features\n",
    "    square_feet = np.random.uniform(800, 4000)\n",
    "    bedrooms = np.random.randint(1, 6)\n",
    "    bathrooms = np.random.randint(1, 5)\n",
    "    age_years = np.random.randint(0, 50)\n",
    "    lot_size = np.random.uniform(1000, 10000)\n",
    "    garage_spaces = np.random.randint(0, 4)\n",
    "    \n",
    "    # Price with non-linear relationships and interactions\n",
    "    base_price = (\n",
    "        square_feet * 200 +\n",
    "        bedrooms * 50000 +\n",
    "        bathrooms * 30000 -\n",
    "        age_years * 2000 +\n",
    "        lot_size * 10 +\n",
    "        garage_spaces * 15000 +\n",
    "        100000\n",
    "    )\n",
    "    \n",
    "    # Add some non-linearity\n",
    "    if square_feet > 3000:\n",
    "        base_price *= 1.2\n",
    "    if age_years > 30:\n",
    "        base_price *= 0.85\n",
    "    \n",
    "    # Add noise\n",
    "    price = base_price + np.random.normal(0, 50000)\n",
    "    price = max(50000, price)  # Minimum price\n",
    "    \n",
    "    house_data.append((\n",
    "        float(square_feet),\n",
    "        bedrooms,\n",
    "        bathrooms,\n",
    "        age_years,\n",
    "        float(lot_size),\n",
    "        garage_spaces,\n",
    "        float(price)\n",
    "    ))\n",
    "\n",
    "df_houses = spark.createDataFrame(\n",
    "    house_data,\n",
    "    [\"square_feet\", \"bedrooms\", \"bathrooms\", \"age_years\", \"lot_size\", \"garage_spaces\", \"price\"]\n",
    ")\n",
    "\n",
    "print(f\"Total houses: {df_houses.count()}\")\n",
    "print(\"\\nSample data:\")\n",
    "df_houses.show(10)\n",
    "df_houses.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare regression features\n",
    "reg_feature_cols = [\"square_feet\", \"bedrooms\", \"bathrooms\", \"age_years\", \"lot_size\", \"garage_spaces\"]\n",
    "\n",
    "reg_assembler = VectorAssembler(inputCols=reg_feature_cols, outputCol=\"features\")\n",
    "df_reg_assembled = reg_assembler.transform(df_houses)\n",
    "\n",
    "# Split data\n",
    "reg_train, reg_val, reg_test = df_reg_assembled.randomSplit([0.6, 0.2, 0.2], seed=42)\n",
    "\n",
    "print(f\"Training samples: {reg_train.count()}\")\n",
    "print(f\"Validation samples: {reg_val.count()}\")\n",
    "print(f\"Test samples: {reg_test.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression evaluator\n",
    "reg_evaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\")\n",
    "\n",
    "# Model 1: Linear Regression\n",
    "print(\"Training Linear Regression...\")\n",
    "start_time = time.time()\n",
    "\n",
    "lin_reg = LinearRegression(\n",
    "    labelCol=\"price\",\n",
    "    featuresCol=\"features\",\n",
    "    maxIter=20,\n",
    "    regParam=0.01,\n",
    "    elasticNetParam=0.0\n",
    ")\n",
    "\n",
    "lin_reg_model = lin_reg.fit(reg_train)\n",
    "lin_reg_time = time.time() - start_time\n",
    "\n",
    "lin_reg_pred = lin_reg_model.transform(reg_val)\n",
    "\n",
    "lin_reg_rmse = reg_evaluator.evaluate(lin_reg_pred, {reg_evaluator.metricName: \"rmse\"})\n",
    "lin_reg_r2 = reg_evaluator.evaluate(lin_reg_pred, {reg_evaluator.metricName: \"r2\"})\n",
    "lin_reg_mae = reg_evaluator.evaluate(lin_reg_pred, {reg_evaluator.metricName: \"mae\"})\n",
    "\n",
    "print(f\"Linear Regression - Time: {lin_reg_time:.2f}s, RMSE: ${lin_reg_rmse:,.0f}, R²: {lin_reg_r2:.4f}, MAE: ${lin_reg_mae:,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: Decision Tree Regressor\n",
    "print(\"Training Decision Tree Regressor...\")\n",
    "start_time = time.time()\n",
    "\n",
    "dt_reg = DecisionTreeRegressor(\n",
    "    labelCol=\"price\",\n",
    "    featuresCol=\"features\",\n",
    "    maxDepth=10,\n",
    "    minInstancesPerNode=20\n",
    ")\n",
    "\n",
    "dt_reg_model = dt_reg.fit(reg_train)\n",
    "dt_reg_time = time.time() - start_time\n",
    "\n",
    "dt_reg_pred = dt_reg_model.transform(reg_val)\n",
    "\n",
    "dt_reg_rmse = reg_evaluator.evaluate(dt_reg_pred, {reg_evaluator.metricName: \"rmse\"})\n",
    "dt_reg_r2 = reg_evaluator.evaluate(dt_reg_pred, {reg_evaluator.metricName: \"r2\"})\n",
    "dt_reg_mae = reg_evaluator.evaluate(dt_reg_pred, {reg_evaluator.metricName: \"mae\"})\n",
    "\n",
    "print(f\"Decision Tree - Time: {dt_reg_time:.2f}s, RMSE: ${dt_reg_rmse:,.0f}, R²: {dt_reg_r2:.4f}, MAE: ${dt_reg_mae:,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3: Random Forest Regressor\n",
    "print(\"Training Random Forest Regressor...\")\n",
    "start_time = time.time()\n",
    "\n",
    "rf_reg = RandomForestRegressor(\n",
    "    labelCol=\"price\",\n",
    "    featuresCol=\"features\",\n",
    "    numTrees=50,\n",
    "    maxDepth=10,\n",
    "    minInstancesPerNode=20,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "rf_reg_model = rf_reg.fit(reg_train)\n",
    "rf_reg_time = time.time() - start_time\n",
    "\n",
    "rf_reg_pred = rf_reg_model.transform(reg_val)\n",
    "\n",
    "rf_reg_rmse = reg_evaluator.evaluate(rf_reg_pred, {reg_evaluator.metricName: \"rmse\"})\n",
    "rf_reg_r2 = reg_evaluator.evaluate(rf_reg_pred, {reg_evaluator.metricName: \"r2\"})\n",
    "rf_reg_mae = reg_evaluator.evaluate(rf_reg_pred, {reg_evaluator.metricName: \"mae\"})\n",
    "\n",
    "print(f\"Random Forest - Time: {rf_reg_time:.2f}s, RMSE: ${rf_reg_rmse:,.0f}, R²: {rf_reg_r2:.4f}, MAE: ${rf_reg_mae:,.0f}\")\n",
    "\n",
    "print(\"\\nFeature Importances:\")\n",
    "for i, importance in enumerate(rf_reg_model.featureImportances):\n",
    "    print(f\"{reg_feature_cols[i]}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 4: Gradient Boosted Tree Regressor\n",
    "print(\"Training Gradient Boosted Tree Regressor...\")\n",
    "start_time = time.time()\n",
    "\n",
    "gbt_reg = GBTRegressor(\n",
    "    labelCol=\"price\",\n",
    "    featuresCol=\"features\",\n",
    "    maxIter=20,\n",
    "    maxDepth=5,\n",
    "    stepSize=0.1,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "gbt_reg_model = gbt_reg.fit(reg_train)\n",
    "gbt_reg_time = time.time() - start_time\n",
    "\n",
    "gbt_reg_pred = gbt_reg_model.transform(reg_val)\n",
    "\n",
    "gbt_reg_rmse = reg_evaluator.evaluate(gbt_reg_pred, {reg_evaluator.metricName: \"rmse\"})\n",
    "gbt_reg_r2 = reg_evaluator.evaluate(gbt_reg_pred, {reg_evaluator.metricName: \"r2\"})\n",
    "gbt_reg_mae = reg_evaluator.evaluate(gbt_reg_pred, {reg_evaluator.metricName: \"mae\"})\n",
    "\n",
    "print(f\"GBT Regressor - Time: {gbt_reg_time:.2f}s, RMSE: ${gbt_reg_rmse:,.0f}, R²: {gbt_reg_r2:.4f}, MAE: ${gbt_reg_mae:,.0f}\")\n",
    "\n",
    "print(\"\\nFeature Importances:\")\n",
    "for i, importance in enumerate(gbt_reg_model.featureImportances):\n",
    "    print(f\"{reg_feature_cols[i]}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all regression models\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"REGRESSION MODEL COMPARISON (Validation Set)\")\n",
    "print(\"=\"*90)\n",
    "print(f\"{'Model':<25} {'Time (s)':<12} {'RMSE':<15} {'R² Score':<12} {'MAE':<15}\")\n",
    "print(\"-\"*90)\n",
    "print(f\"{'Linear Regression':<25} {lin_reg_time:<12.2f} ${lin_reg_rmse:<14,.0f} {lin_reg_r2:<12.4f} ${lin_reg_mae:<14,.0f}\")\n",
    "print(f\"{'Decision Tree':<25} {dt_reg_time:<12.2f} ${dt_reg_rmse:<14,.0f} {dt_reg_r2:<12.4f} ${dt_reg_mae:<14,.0f}\")\n",
    "print(f\"{'Random Forest':<25} {rf_reg_time:<12.2f} ${rf_reg_rmse:<14,.0f} {rf_reg_r2:<12.4f} ${rf_reg_mae:<14,.0f}\")\n",
    "print(f\"{'Gradient Boosted Trees':<25} {gbt_reg_time:<12.2f} ${gbt_reg_rmse:<14,.0f} {gbt_reg_r2:<12.4f} ${gbt_reg_mae:<14,.0f}\")\n",
    "print(\"=\"*90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cross-Validation\n",
    "\n",
    "Cross-validation provides more reliable performance estimates by averaging results across multiple train/validation splits.\n",
    "\n",
    "**Benefits:**\n",
    "- Reduces variance in performance estimates\n",
    "- Uses all data for both training and validation\n",
    "- Detects overfitting more reliably\n",
    "\n",
    "**Trade-off:**\n",
    "- Training time increases by k-fold (e.g., 3x slower for 3-fold CV)\n",
    "\n",
    "**When to use:**\n",
    "- Small to medium datasets where each sample is valuable\n",
    "- When you need high confidence in model selection\n",
    "- For final model selection before production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for cross-validation (use train + val for CV)\n",
    "cv_data = train_df.union(val_df)\n",
    "\n",
    "print(f\"Cross-validation data size: {cv_data.count()}\")\n",
    "\n",
    "# Create a simple model pipeline\n",
    "cv_lr = LogisticRegression(\n",
    "    labelCol=\"churned\",\n",
    "    featuresCol=\"features\",\n",
    "    maxIter=10\n",
    ")\n",
    "\n",
    "# Create evaluator\n",
    "cv_evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"churned\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "# Create CrossValidator\n",
    "# We'll test different values of regParam\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(cv_lr.regParam, [0.001, 0.01, 0.1]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(\n",
    "    estimator=cv_lr,\n",
    "    estimatorParamMaps=param_grid,\n",
    "    evaluator=cv_evaluator,\n",
    "    numFolds=3,  # 3-fold cross-validation\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"\\nStarting 3-fold cross-validation...\")\n",
    "print(f\"Testing {len(param_grid)} parameter combinations\")\n",
    "print(f\"Total model training runs: {3 * len(param_grid)} (3 folds × {len(param_grid)} params)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run cross-validation\n",
    "start_time = time.time()\n",
    "cv_model = crossval.fit(cv_data)\n",
    "cv_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nCross-validation completed in {cv_time:.2f}s\")\n",
    "\n",
    "# Get best model and its parameters\n",
    "best_lr_model = cv_model.bestModel\n",
    "print(f\"\\nBest regParam: {best_lr_model.getRegParam()}\")\n",
    "\n",
    "# Average metrics for each parameter combination\n",
    "print(\"\\nAverage AUC for each parameter combination:\")\n",
    "for params, avg_metric in zip(param_grid, cv_model.avgMetrics):\n",
    "    reg_param = params[cv_lr.regParam]\n",
    "    print(f\"regParam={reg_param}: {avg_metric:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate best model on test set\n",
    "cv_test_predictions = cv_model.transform(test_df)\n",
    "cv_test_auc = cv_evaluator.evaluate(cv_test_predictions)\n",
    "\n",
    "print(f\"\\nBest model performance on test set:\")\n",
    "print(f\"AUC: {cv_test_auc:.4f}\")\n",
    "\n",
    "# Also calculate accuracy\n",
    "cv_test_accuracy = acc_evaluator.evaluate(cv_test_predictions)\n",
    "print(f\"Accuracy: {cv_test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hyperparameter Tuning with Grid Search\n",
    "\n",
    "**Hyperparameter Tuning** finds the best configuration for a model by systematically trying different combinations.\n",
    "\n",
    "**Common hyperparameters:**\n",
    "\n",
    "**Random Forest:**\n",
    "- `numTrees`: Number of trees (more = better, but slower)\n",
    "- `maxDepth`: Maximum tree depth (higher = more complex)\n",
    "- `minInstancesPerNode`: Minimum samples per leaf (higher = more regularization)\n",
    "\n",
    "**Gradient Boosted Trees:**\n",
    "- `maxIter`: Number of boosting iterations\n",
    "- `maxDepth`: Tree depth per iteration\n",
    "- `stepSize`: Learning rate (smaller = more conservative)\n",
    "\n",
    "**TrainValidationSplit vs CrossValidator:**\n",
    "- TrainValidationSplit: Faster (single split), less reliable\n",
    "- CrossValidator: Slower (k splits), more reliable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for Random Forest\n",
    "print(\"Hyperparameter Tuning for Random Forest Classifier\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create Random Forest estimator\n",
    "rf_tuning = RandomForestClassifier(\n",
    "    labelCol=\"churned\",\n",
    "    featuresCol=\"features\",\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Build parameter grid\n",
    "# Testing combinations of numTrees, maxDepth, and minInstancesPerNode\n",
    "param_grid_rf = ParamGridBuilder() \\\n",
    "    .addGrid(rf_tuning.numTrees, [20, 50]) \\\n",
    "    .addGrid(rf_tuning.maxDepth, [5, 10]) \\\n",
    "    .addGrid(rf_tuning.minInstancesPerNode, [10, 20]) \\\n",
    "    .build()\n",
    "\n",
    "print(f\"Total parameter combinations: {len(param_grid_rf)}\")\n",
    "print(f\"Parameters being tuned:\")\n",
    "print(f\"  - numTrees: [20, 50]\")\n",
    "print(f\"  - maxDepth: [5, 10]\")\n",
    "print(f\"  - minInstancesPerNode: [10, 20]\")\n",
    "\n",
    "# Use TrainValidationSplit for faster tuning\n",
    "# Splits data into 80% train, 20% validation\n",
    "tvs = TrainValidationSplit(\n",
    "    estimator=rf_tuning,\n",
    "    estimatorParamMaps=param_grid_rf,\n",
    "    evaluator=auc_evaluator,\n",
    "    trainRatio=0.8,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"\\nRunning hyperparameter tuning...\")\n",
    "start_time = time.time()\n",
    "tvs_model = tvs.fit(cv_data)\n",
    "tuning_time = time.time() - start_time\n",
    "\n",
    "print(f\"Tuning completed in {tuning_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best Random Forest model\n",
    "best_rf_model = tvs_model.bestModel\n",
    "\n",
    "print(\"\\nBest Random Forest Parameters:\")\n",
    "print(f\"  numTrees: {best_rf_model.getNumTrees}\")\n",
    "print(f\"  maxDepth: {best_rf_model.getMaxDepth()}\")\n",
    "print(f\"  minInstancesPerNode: {best_rf_model.getMinInstancesPerNode()}\")\n",
    "\n",
    "# Show all results\n",
    "print(\"\\nAll parameter combinations and their validation AUC:\")\n",
    "print(\"-\"*70)\n",
    "for params, metric in zip(param_grid_rf, tvs_model.validationMetrics):\n",
    "    num_trees = params[rf_tuning.numTrees]\n",
    "    max_depth = params[rf_tuning.maxDepth]\n",
    "    min_instances = params[rf_tuning.minInstancesPerNode]\n",
    "    print(f\"Trees={num_trees:2d}, Depth={max_depth:2d}, MinInst={min_instances:2d} → AUC: {metric:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate tuned model on test set\n",
    "tuned_predictions = tvs_model.transform(test_df)\n",
    "\n",
    "tuned_auc = auc_evaluator.evaluate(tuned_predictions)\n",
    "tuned_accuracy = acc_evaluator.evaluate(tuned_predictions)\n",
    "tuned_f1 = f1_evaluator.evaluate(tuned_predictions)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TUNED MODEL PERFORMANCE ON TEST SET\")\n",
    "print(\"=\"*60)\n",
    "print(f\"AUC:      {tuned_auc:.4f}\")\n",
    "print(f\"Accuracy: {tuned_accuracy:.4f}\")\n",
    "print(f\"F1 Score: {tuned_f1:.4f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Compare to default Random Forest from earlier\n",
    "print(\"\\nComparison to Default Random Forest:\")\n",
    "print(f\"Default RF (validation): AUC={rf_auc:.4f}, Acc={rf_accuracy:.4f}, F1={rf_f1:.4f}\")\n",
    "print(f\"Tuned RF (test):         AUC={tuned_auc:.4f}, Acc={tuned_accuracy:.4f}, F1={tuned_f1:.4f}\")\n",
    "print(f\"Improvement:             AUC={tuned_auc-rf_auc:+.4f}, Acc={tuned_accuracy-rf_accuracy:+.4f}, F1={tuned_f1-rf_f1:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Complete Model Selection Pipeline\n",
    "\n",
    "Let's build a complete workflow that:\n",
    "1. Preprocesses data\n",
    "2. Trains multiple models\n",
    "3. Uses cross-validation\n",
    "4. Tunes hyperparameters\n",
    "5. Selects the best model\n",
    "6. Evaluates on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete pipeline with preprocessing and model\n",
    "# We'll use the original unprocessed churn data\n",
    "\n",
    "# Split original data\n",
    "pipeline_train, pipeline_test = df_churn.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Build comprehensive pipeline\n",
    "# Stage 1: Assemble features\n",
    "pipeline_assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"raw_features\"\n",
    ")\n",
    "\n",
    "# Stage 2: Scale features\n",
    "pipeline_scaler = StandardScaler(\n",
    "    inputCol=\"raw_features\",\n",
    "    outputCol=\"features\",\n",
    "    withStd=True,\n",
    "    withMean=False\n",
    ")\n",
    "\n",
    "# Stage 3: Model (we'll swap this during tuning)\n",
    "pipeline_gbt = GBTClassifier(\n",
    "    labelCol=\"churned\",\n",
    "    featuresCol=\"features\",\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Create pipeline\n",
    "complete_pipeline = Pipeline(stages=[\n",
    "    pipeline_assembler,\n",
    "    pipeline_scaler,\n",
    "    pipeline_gbt\n",
    "])\n",
    "\n",
    "print(\"Complete ML Pipeline Created:\")\n",
    "print(\"  1. VectorAssembler: Combine features\")\n",
    "print(\"  2. StandardScaler: Normalize features\")\n",
    "print(\"  3. GBTClassifier: Train model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter grid for GBT\n",
    "gbt_param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(pipeline_gbt.maxIter, [10, 20]) \\\n",
    "    .addGrid(pipeline_gbt.maxDepth, [3, 5]) \\\n",
    "    .addGrid(pipeline_gbt.stepSize, [0.05, 0.1]) \\\n",
    "    .build()\n",
    "\n",
    "print(f\"Testing {len(gbt_param_grid)} GBT parameter combinations:\")\n",
    "print(f\"  - maxIter: [10, 20]\")\n",
    "print(f\"  - maxDepth: [3, 5]\")\n",
    "print(f\"  - stepSize: [0.05, 0.1]\")\n",
    "\n",
    "# Use CrossValidator for more reliable results\n",
    "pipeline_cv = CrossValidator(\n",
    "    estimator=complete_pipeline,\n",
    "    estimatorParamMaps=gbt_param_grid,\n",
    "    evaluator=auc_evaluator,\n",
    "    numFolds=3,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(f\"\\nUsing 3-fold cross-validation\")\n",
    "print(f\"Total training runs: {3 * len(gbt_param_grid)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with cross-validation and hyperparameter tuning\n",
    "print(\"\\nStarting complete model selection process...\")\n",
    "start_time = time.time()\n",
    "final_model = pipeline_cv.fit(pipeline_train)\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(f\"Model selection completed in {total_time:.2f}s ({total_time/60:.1f} minutes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract best model and parameters\n",
    "best_pipeline_model = final_model.bestModel\n",
    "best_gbt = best_pipeline_model.stages[-1]  # Last stage is the GBT model\n",
    "\n",
    "print(\"\\nBest Model Parameters:\")\n",
    "print(f\"  maxIter: {best_gbt.getMaxIter()}\")\n",
    "print(f\"  maxDepth: {best_gbt.getMaxDepth()}\")\n",
    "print(f\"  stepSize: {best_gbt.getStepSize()}\")\n",
    "\n",
    "# Show all CV results\n",
    "print(\"\\nCross-Validation Results (Average AUC across 3 folds):\")\n",
    "print(\"-\"*80)\n",
    "for params, avg_metric in zip(gbt_param_grid, final_model.avgMetrics):\n",
    "    max_iter = params[pipeline_gbt.maxIter]\n",
    "    max_depth = params[pipeline_gbt.maxDepth]\n",
    "    step_size = params[pipeline_gbt.stepSize]\n",
    "    print(f\"Iter={max_iter:2d}, Depth={max_depth}, Step={step_size:.2f} → Avg AUC: {avg_metric:.4f}\")\n",
    "\n",
    "best_idx = final_model.avgMetrics.index(max(final_model.avgMetrics))\n",
    "print(\"-\"*80)\n",
    "print(f\"Best configuration: Index {best_idx} with AUC={max(final_model.avgMetrics):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation on test set\n",
    "final_predictions = final_model.transform(pipeline_test)\n",
    "\n",
    "final_auc = auc_evaluator.evaluate(final_predictions)\n",
    "final_accuracy = acc_evaluator.evaluate(final_predictions)\n",
    "final_f1 = f1_evaluator.evaluate(final_predictions)\n",
    "final_precision = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"churned\", predictionCol=\"prediction\", metricName=\"weightedPrecision\"\n",
    ").evaluate(final_predictions)\n",
    "final_recall = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"churned\", predictionCol=\"prediction\", metricName=\"weightedRecall\"\n",
    ").evaluate(final_predictions)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL MODEL PERFORMANCE ON TEST SET\")\n",
    "print(\"=\"*80)\n",
    "print(f\"AUC:       {final_auc:.4f}\")\n",
    "print(f\"Accuracy:  {final_accuracy:.4f}\")\n",
    "print(f\"Precision: {final_precision:.4f}\")\n",
    "print(f\"Recall:    {final_recall:.4f}\")\n",
    "print(f\"F1 Score:  {final_f1:.4f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Feature importances from best model\n",
    "print(\"\\nFeature Importances (from best model):\")\n",
    "for i, importance in enumerate(best_gbt.featureImportances):\n",
    "    print(f\"  {feature_cols[i]:<20} {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Multi-Model Comparison\n",
    "\n",
    "Create a comprehensive comparison of all classification algorithms.\n",
    "\n",
    "**Tasks:**\n",
    "1. Generate a binary classification dataset with 3000 samples\n",
    "2. Train: Logistic Regression, Decision Tree, Random Forest, and GBT\n",
    "3. For each model, record: training time, accuracy, AUC, F1, precision, recall\n",
    "4. Create a summary table and identify the best model\n",
    "5. Discuss which model you would choose for production and why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# TODO: Generate classification data\n",
    "# TODO: Train all four models\n",
    "# TODO: Evaluate and compare\n",
    "# TODO: Create summary table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Cross-Validation Deep Dive\n",
    "\n",
    "Explore the impact of cross-validation fold count.\n",
    "\n",
    "**Tasks:**\n",
    "1. Use the churn dataset\n",
    "2. Perform cross-validation with k=2, 3, 5, and 10 folds\n",
    "3. For each k, record: total training time, average AUC, standard deviation\n",
    "4. Compare the trade-off between reliability (higher k) and speed (lower k)\n",
    "5. Recommend an optimal k value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# TODO: Run CV with different fold counts\n",
    "# TODO: Measure time and performance\n",
    "# TODO: Analyze trade-offs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Regression Hyperparameter Tuning\n",
    "\n",
    "Tune a Random Forest Regressor for the house price prediction task.\n",
    "\n",
    "**Tasks:**\n",
    "1. Use the house price dataset\n",
    "2. Create a parameter grid for Random Forest Regressor with:\n",
    "   - numTrees: [30, 50, 100]\n",
    "   - maxDepth: [5, 10, 15]\n",
    "   - minInstancesPerNode: [5, 10, 20]\n",
    "3. Use TrainValidationSplit to find the best parameters\n",
    "4. Evaluate the tuned model on the test set\n",
    "5. Compare to a default Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# TODO: Create parameter grid\n",
    "# TODO: Run hyperparameter tuning\n",
    "# TODO: Evaluate and compare to default model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Exercise Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1: Multi-Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset\n",
    "n = 3000\n",
    "compare_data = []\n",
    "for _ in range(n):\n",
    "    x1 = np.random.uniform(0, 10)\n",
    "    x2 = np.random.uniform(0, 10)\n",
    "    x3 = np.random.uniform(0, 10)\n",
    "    x4 = np.random.uniform(0, 10)\n",
    "    \n",
    "    score = x1 * 0.5 + x2 * 0.3 + x3 * x4 * 0.02 - 5\n",
    "    prob = 1 / (1 + np.exp(-score))\n",
    "    label = 1 if np.random.random() < prob else 0\n",
    "    \n",
    "    compare_data.append((float(x1), float(x2), float(x3), float(x4), label))\n",
    "\n",
    "df_compare = spark.createDataFrame(compare_data, [\"x1\", \"x2\", \"x3\", \"x4\", \"label\"])\n",
    "\n",
    "# Prepare features\n",
    "comp_assembler = VectorAssembler(inputCols=[\"x1\", \"x2\", \"x3\", \"x4\"], outputCol=\"features\")\n",
    "df_comp_features = comp_assembler.transform(df_compare)\n",
    "comp_train, comp_test = df_comp_features.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "print(f\"Dataset: {df_compare.count()} samples\")\n",
    "df_compare.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Evaluators\n",
    "comp_acc_eval = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "comp_auc_eval = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "comp_f1_eval = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "comp_prec_eval = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "comp_rec_eval = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "\n",
    "# Model 1: Logistic Regression\n",
    "start = time.time()\n",
    "comp_lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=20)\n",
    "comp_lr_model = comp_lr.fit(comp_train)\n",
    "comp_lr_pred = comp_lr_model.transform(comp_test)\n",
    "results['Logistic Regression'] = {\n",
    "    'time': time.time() - start,\n",
    "    'accuracy': comp_acc_eval.evaluate(comp_lr_pred),\n",
    "    'auc': comp_auc_eval.evaluate(comp_lr_pred),\n",
    "    'f1': comp_f1_eval.evaluate(comp_lr_pred),\n",
    "    'precision': comp_prec_eval.evaluate(comp_lr_pred),\n",
    "    'recall': comp_rec_eval.evaluate(comp_lr_pred)\n",
    "}\n",
    "\n",
    "# Model 2: Decision Tree\n",
    "start = time.time()\n",
    "comp_dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\", maxDepth=10)\n",
    "comp_dt_model = comp_dt.fit(comp_train)\n",
    "comp_dt_pred = comp_dt_model.transform(comp_test)\n",
    "results['Decision Tree'] = {\n",
    "    'time': time.time() - start,\n",
    "    'accuracy': comp_acc_eval.evaluate(comp_dt_pred),\n",
    "    'auc': comp_auc_eval.evaluate(comp_dt_pred),\n",
    "    'f1': comp_f1_eval.evaluate(comp_dt_pred),\n",
    "    'precision': comp_prec_eval.evaluate(comp_dt_pred),\n",
    "    'recall': comp_rec_eval.evaluate(comp_dt_pred)\n",
    "}\n",
    "\n",
    "# Model 3: Random Forest\n",
    "start = time.time()\n",
    "comp_rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=50, maxDepth=10, seed=42)\n",
    "comp_rf_model = comp_rf.fit(comp_train)\n",
    "comp_rf_pred = comp_rf_model.transform(comp_test)\n",
    "results['Random Forest'] = {\n",
    "    'time': time.time() - start,\n",
    "    'accuracy': comp_acc_eval.evaluate(comp_rf_pred),\n",
    "    'auc': comp_auc_eval.evaluate(comp_rf_pred),\n",
    "    'f1': comp_f1_eval.evaluate(comp_rf_pred),\n",
    "    'precision': comp_prec_eval.evaluate(comp_rf_pred),\n",
    "    'recall': comp_rec_eval.evaluate(comp_rf_pred)\n",
    "}\n",
    "\n",
    "# Model 4: GBT\n",
    "start = time.time()\n",
    "comp_gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\", maxIter=20, maxDepth=5, seed=42)\n",
    "comp_gbt_model = comp_gbt.fit(comp_train)\n",
    "comp_gbt_pred = comp_gbt_model.transform(comp_test)\n",
    "results['GBT'] = {\n",
    "    'time': time.time() - start,\n",
    "    'accuracy': comp_acc_eval.evaluate(comp_gbt_pred),\n",
    "    'auc': comp_auc_eval.evaluate(comp_gbt_pred),\n",
    "    'f1': comp_f1_eval.evaluate(comp_gbt_pred),\n",
    "    'precision': comp_prec_eval.evaluate(comp_gbt_pred),\n",
    "    'recall': comp_rec_eval.evaluate(comp_gbt_pred)\n",
    "}\n",
    "\n",
    "print(\"All models trained!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"COMPREHENSIVE MODEL COMPARISON\")\n",
    "print(\"=\"*120)\n",
    "print(f\"{'Model':<20} {'Time(s)':<10} {'Accuracy':<12} {'AUC':<12} {'F1':<12} {'Precision':<12} {'Recall':<12}\")\n",
    "print(\"-\"*120)\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"{model_name:<20} {metrics['time']:<10.2f} {metrics['accuracy']:<12.4f} {metrics['auc']:<12.4f} \"\n",
    "          f\"{metrics['f1']:<12.4f} {metrics['precision']:<12.4f} {metrics['recall']:<12.4f}\")\n",
    "print(\"=\"*120)\n",
    "\n",
    "# Find best model by AUC\n",
    "best_model = max(results.items(), key=lambda x: x[1]['auc'])\n",
    "print(f\"\\nBest Model (by AUC): {best_model[0]} with AUC={best_model[1]['auc']:.4f}\")\n",
    "print(\"\\nRecommendation for Production:\")\n",
    "print(f\"  - {best_model[0]} offers the best predictive performance\")\n",
    "print(f\"  - Consider Random Forest for balance of accuracy and interpretability\")\n",
    "print(f\"  - Use Logistic Regression if speed and explainability are critical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 2: Cross-Validation Deep Dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different fold counts\n",
    "fold_counts = [2, 3, 5]\n",
    "cv_results = {}\n",
    "\n",
    "base_lr = LogisticRegression(labelCol=\"churned\", featuresCol=\"features\", maxIter=10)\n",
    "base_evaluator = BinaryClassificationEvaluator(labelCol=\"churned\", rawPredictionCol=\"rawPrediction\")\n",
    "\n",
    "# Use smaller parameter grid for speed\n",
    "simple_grid = ParamGridBuilder().addGrid(base_lr.regParam, [0.01, 0.1]).build()\n",
    "\n",
    "print(\"Testing different cross-validation fold counts...\\n\")\n",
    "\n",
    "for k in fold_counts:\n",
    "    print(f\"Running {k}-fold cross-validation...\")\n",
    "    \n",
    "    cv = CrossValidator(\n",
    "        estimator=base_lr,\n",
    "        estimatorParamMaps=simple_grid,\n",
    "        evaluator=base_evaluator,\n",
    "        numFolds=k,\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    start = time.time()\n",
    "    cv_model = cv.fit(cv_data)\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    avg_metrics = cv_model.avgMetrics\n",
    "    best_metric = max(avg_metrics)\n",
    "    std_dev = np.std(avg_metrics)\n",
    "    \n",
    "    cv_results[k] = {\n",
    "        'time': elapsed,\n",
    "        'best_auc': best_metric,\n",
    "        'std_dev': std_dev\n",
    "    }\n",
    "    \n",
    "    print(f\"  Time: {elapsed:.2f}s, Best AUC: {best_metric:.4f}, Std Dev: {std_dev:.4f}\\n\")\n",
    "\n",
    "print(\"Cross-validation comparison complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CROSS-VALIDATION FOLD COUNT COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Folds':<10} {'Time (s)':<15} {'Best AUC':<15} {'Std Dev':<15}\")\n",
    "print(\"-\"*70)\n",
    "for k, metrics in cv_results.items():\n",
    "    print(f\"{k:<10} {metrics['time']:<15.2f} {metrics['best_auc']:<15.4f} {metrics['std_dev']:<15.6f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nRecommendation:\")\n",
    "print(\"  - For rapid prototyping: Use 2-3 folds (faster)\")\n",
    "print(\"  - For production model selection: Use 5 folds (good balance)\")\n",
    "print(\"  - For small datasets: Use 10 folds (maximum data utilization)\")\n",
    "print(\"  - Trade-off: Higher k = more reliable but slower\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 3: Regression Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare house data\n",
    "tune_train, tune_test = df_reg_assembled.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Default Random Forest\n",
    "print(\"Training default Random Forest Regressor...\")\n",
    "default_rfr = RandomForestRegressor(labelCol=\"price\", featuresCol=\"features\", seed=42)\n",
    "default_rfr_model = default_rfr.fit(tune_train)\n",
    "default_pred = default_rfr_model.transform(tune_test)\n",
    "\n",
    "default_rmse = reg_evaluator.evaluate(default_pred, {reg_evaluator.metricName: \"rmse\"})\n",
    "default_r2 = reg_evaluator.evaluate(default_pred, {reg_evaluator.metricName: \"r2\"})\n",
    "\n",
    "print(f\"Default RF: RMSE=${default_rmse:,.0f}, R²={default_r2:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "print(\"Starting hyperparameter tuning...\")\n",
    "\n",
    "tune_rfr = RandomForestRegressor(labelCol=\"price\", featuresCol=\"features\", seed=42)\n",
    "\n",
    "tune_grid = ParamGridBuilder() \\\n",
    "    .addGrid(tune_rfr.numTrees, [30, 50, 100]) \\\n",
    "    .addGrid(tune_rfr.maxDepth, [5, 10, 15]) \\\n",
    "    .addGrid(tune_rfr.minInstancesPerNode, [5, 10, 20]) \\\n",
    "    .build()\n",
    "\n",
    "print(f\"Total combinations: {len(tune_grid)}\\n\")\n",
    "\n",
    "tune_tvs = TrainValidationSplit(\n",
    "    estimator=tune_rfr,\n",
    "    estimatorParamMaps=tune_grid,\n",
    "    evaluator=RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\", metricName=\"rmse\"),\n",
    "    trainRatio=0.8,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "tune_model = tune_tvs.fit(tune_train)\n",
    "tune_time = time.time() - start\n",
    "\n",
    "print(f\"Tuning completed in {tune_time:.2f}s\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model\n",
    "best_rfr = tune_model.bestModel\n",
    "print(\"Best Parameters:\")\n",
    "print(f\"  numTrees: {best_rfr.getNumTrees}\")\n",
    "print(f\"  maxDepth: {best_rfr.getMaxDepth()}\")\n",
    "print(f\"  minInstancesPerNode: {best_rfr.getMinInstancesPerNode()}\\n\")\n",
    "\n",
    "# Evaluate tuned model\n",
    "tuned_pred = tune_model.transform(tune_test)\n",
    "tuned_rmse = reg_evaluator.evaluate(tuned_pred, {reg_evaluator.metricName: \"rmse\"})\n",
    "tuned_r2 = reg_evaluator.evaluate(tuned_pred, {reg_evaluator.metricName: \"r2\"})\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPARISON: Default vs Tuned Random Forest Regressor\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Model':<20} {'RMSE':<20} {'R² Score':<20}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'Default RF':<20} ${default_rmse:<19,.0f} {default_r2:<20.4f}\")\n",
    "print(f\"{'Tuned RF':<20} ${tuned_rmse:<19,.0f} {tuned_r2:<20.4f}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'Improvement':<20} ${default_rmse - tuned_rmse:<19,.0f} {tuned_r2 - default_r2:<20.4f}\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nRMSE improvement: {((default_rmse - tuned_rmse) / default_rmse * 100):.1f}%\")\n",
    "print(f\"Hyperparameter tuning improved model performance!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "Congratulations! You've mastered model training and evaluation in PySpark.\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "1. **Algorithm Selection**:\n",
    "   - Logistic Regression: Fast, interpretable, linear decision boundaries\n",
    "   - Decision Trees: Non-linear, interpretable, can overfit\n",
    "   - Random Forests: Robust, reduces overfitting, feature importances\n",
    "   - Gradient Boosted Trees: Often highest accuracy, slower, sequential training\n",
    "\n",
    "2. **Model Evaluation**:\n",
    "   - Classification: Accuracy, AUC, F1, Precision, Recall\n",
    "   - Regression: RMSE, MAE, R²\n",
    "   - Choose metrics based on business requirements\n",
    "   - Always evaluate on held-out test data\n",
    "\n",
    "3. **Cross-Validation**:\n",
    "   - Provides more reliable performance estimates\n",
    "   - Reduces variance in model selection\n",
    "   - Trade-off between reliability (higher k) and speed (lower k)\n",
    "   - Essential for small to medium datasets\n",
    "\n",
    "4. **Hyperparameter Tuning**:\n",
    "   - ParamGridBuilder creates parameter combinations\n",
    "   - CrossValidator: More reliable (k-fold), slower\n",
    "   - TrainValidationSplit: Faster (single split), less reliable\n",
    "   - Can significantly improve model performance\n",
    "\n",
    "5. **Complete ML Pipelines**:\n",
    "   - Combine preprocessing, feature engineering, and modeling\n",
    "   - Ensure reproducibility and prevent data leakage\n",
    "   - Make deployment straightforward\n",
    "   - Can be saved and loaded for production use\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "- Start with simple models (Logistic/Linear Regression) as baselines\n",
    "- Always use a separate test set for final evaluation\n",
    "- Use cross-validation for model selection, not just single validation split\n",
    "- Monitor both training and validation performance to detect overfitting\n",
    "- Consider training time vs accuracy trade-offs for production\n",
    "- Document hyperparameter choices and model selection rationale\n",
    "- Use feature importances to understand and explain model decisions\n",
    "\n",
    "### Production Considerations:\n",
    "\n",
    "- **Model Deployment**: Save best model using `model.save(path)`\n",
    "- **Monitoring**: Track model performance over time\n",
    "- **Retraining**: Schedule periodic retraining with new data\n",
    "- **A/B Testing**: Compare new models against production baseline\n",
    "- **Explainability**: Prefer interpretable models when transparency matters\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In [Module 11: Spark Streaming Basics](11_spark_streaming_basics.ipynb), you'll learn:\n",
    "- Real-time data processing with Structured Streaming\n",
    "- Reading from streaming sources\n",
    "- Window operations and aggregations\n",
    "- Output modes and sinks\n",
    "\n",
    "### Additional Resources:\n",
    "\n",
    "- [MLlib Classification Guide](https://spark.apache.org/docs/latest/ml-classification-regression.html)\n",
    "- [Model Selection and Tuning](https://spark.apache.org/docs/latest/ml-tuning.html)\n",
    "- [Evaluation Metrics](https://spark.apache.org/docs/latest/mllib-evaluation-metrics.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "spark.stop()\n",
    "print(\"Spark session stopped. Excellent work on model training and evaluation!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
