{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 05: DataFrame Operations\n",
    "\n",
    "**Difficulty**: â­â­\n",
    "\n",
    "**Estimated Time**: 80-95 minutes\n",
    "\n",
    "**Prerequisites**: \n",
    "- Module 00: Introduction to Big Data and Spark Ecosystem\n",
    "- Module 01: PySpark Setup and SparkSession\n",
    "- Module 03: DataFrames and Datasets\n",
    "- Module 04: Data Loading and Saving\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Filter and select data using various methods (filter, where, select, selectExpr)\n",
    "2. Perform grouping and aggregation operations (groupBy, agg, pivot)\n",
    "3. Join DataFrames using different join types (inner, left, right, full, cross)\n",
    "4. Sort and order data efficiently (orderBy, sort)\n",
    "5. Use built-in functions for data transformation and manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType\n",
    "from pyspark.sql.functions import (\n",
    "    col, lit, when, expr, concat, concat_ws,\n",
    "    sum, avg, count, min, max, countDistinct,\n",
    "    round, upper, lower, trim, substring,\n",
    "    year, month, dayofmonth, current_date,\n",
    "    datediff, date_add, date_sub\n",
    ")\n",
    "from datetime import date\n",
    "import random\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Module 05: DataFrame Operations\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"âœ“ SparkSession created: {spark.sparkContext.appName}\")\n",
    "print(f\"  Spark version: {spark.version}\")\n",
    "print(f\"  Spark UI: {spark.sparkContext.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Sample DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample employees data\n",
    "employees_data = [\n",
    "    (1, \"Alice Johnson\", \"Engineering\", 75000, date(2020, 1, 15)),\n",
    "    (2, \"Bob Smith\", \"Sales\", 65000, date(2019, 6, 10)),\n",
    "    (3, \"Charlie Brown\", \"Engineering\", 80000, date(2021, 3, 22)),\n",
    "    (4, \"Diana Prince\", \"Marketing\", 70000, date(2020, 8, 5)),\n",
    "    (5, \"Eve Davis\", \"Engineering\", 85000, date(2018, 11, 30)),\n",
    "    (6, \"Frank Miller\", \"Sales\", 68000, date(2022, 2, 14)),\n",
    "    (7, \"Grace Lee\", \"Marketing\", 72000, date(2021, 9, 1)),\n",
    "    (8, \"Henry Wilson\", \"Engineering\", 78000, date(2019, 4, 20)),\n",
    "    (9, \"Ivy Chen\", \"Sales\", 71000, date(2020, 12, 10)),\n",
    "    (10, \"Jack Thompson\", \"Engineering\", 82000, date(2021, 7, 5))\n",
    "]\n",
    "\n",
    "df_employees = spark.createDataFrame(\n",
    "    employees_data,\n",
    "    [\"id\", \"name\", \"department\", \"salary\", \"hire_date\"]\n",
    ")\n",
    "\n",
    "print(\"Employees DataFrame:\")\n",
    "df_employees.show()\n",
    "\n",
    "# Sample departments data (for joins)\n",
    "departments_data = [\n",
    "    (\"Engineering\", \"Building A\", \"Alice Johnson\"),\n",
    "    (\"Sales\", \"Building B\", \"Bob Smith\"),\n",
    "    (\"Marketing\", \"Building C\", \"Diana Prince\"),\n",
    "    (\"HR\", \"Building A\", \"Unknown\")  # No employees in HR\n",
    "]\n",
    "\n",
    "df_departments = spark.createDataFrame(\n",
    "    departments_data,\n",
    "    [\"dept_name\", \"location\", \"manager\"]\n",
    ")\n",
    "\n",
    "print(\"\\nDepartments DataFrame:\")\n",
    "df_departments.show()\n",
    "\n",
    "# Sample sales data\n",
    "sales_data = []\n",
    "products = ['Laptop', 'Phone', 'Tablet', 'Monitor', 'Keyboard']\n",
    "regions = ['North', 'South', 'East', 'West']\n",
    "\n",
    "for i in range(100):\n",
    "    sales_data.append((\n",
    "        i + 1,\n",
    "        random.choice(products),\n",
    "        random.choice(regions),\n",
    "        random.randint(1, 10),\n",
    "        round(random.uniform(100, 2000), 2)\n",
    "    ))\n",
    "\n",
    "df_sales = spark.createDataFrame(\n",
    "    sales_data,\n",
    "    [\"sale_id\", \"product\", \"region\", \"quantity\", \"revenue\"]\n",
    ")\n",
    "\n",
    "print(\"\\nSales DataFrame (first 10 rows):\")\n",
    "df_sales.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Filtering and Selecting Data\n",
    "\n",
    "### Filtering with filter() and where()\n",
    "\n",
    "**Note**: `filter()` and `where()` are aliases - they do the same thing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: String expression\n",
    "print(\"Filter using string expression:\")\n",
    "df_employees.filter(\"salary > 75000\").show()\n",
    "\n",
    "# Method 2: Column object\n",
    "print(\"\\nFilter using Column object:\")\n",
    "df_employees.filter(col(\"salary\") > 75000).show()\n",
    "\n",
    "# Method 3: where() - same as filter()\n",
    "print(\"\\nFilter using where():\")\n",
    "df_employees.where(col(\"department\") == \"Engineering\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple conditions with AND\n",
    "print(\"Multiple conditions (AND):\")\n",
    "df_employees.filter(\n",
    "    (col(\"department\") == \"Engineering\") & (col(\"salary\") > 75000)\n",
    ").show()\n",
    "\n",
    "# Multiple conditions with OR\n",
    "print(\"\\nMultiple conditions (OR):\")\n",
    "df_employees.filter(\n",
    "    (col(\"department\") == \"Engineering\") | (col(\"department\") == \"Sales\")\n",
    ").show()\n",
    "\n",
    "# NOT condition\n",
    "print(\"\\nNOT condition:\")\n",
    "df_employees.filter(~(col(\"department\") == \"Engineering\")).show()\n",
    "\n",
    "# âš ï¸ Important: Use & and | (not 'and' and 'or') and use parentheses!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter with IN\n",
    "print(\"Filter with IN:\")\n",
    "df_employees.filter(col(\"department\").isin(\"Engineering\", \"Sales\")).show()\n",
    "\n",
    "# Filter with LIKE (pattern matching)\n",
    "print(\"\\nFilter with LIKE:\")\n",
    "df_employees.filter(col(\"name\").like(\"%Johnson%\")).show()\n",
    "\n",
    "# Filter with BETWEEN\n",
    "print(\"\\nFilter with BETWEEN:\")\n",
    "df_employees.filter(col(\"salary\").between(70000, 80000)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select specific columns\n",
    "print(\"Select specific columns:\")\n",
    "df_employees.select(\"name\", \"department\", \"salary\").show(5)\n",
    "\n",
    "# Select with transformations\n",
    "print(\"\\nSelect with transformations:\")\n",
    "df_employees.select(\n",
    "    col(\"name\"),\n",
    "    (col(\"salary\") * 12).alias(\"annual_salary\")\n",
    ").show(5)\n",
    "\n",
    "# Select with multiple transformations\n",
    "print(\"\\nSelect with multiple transformations:\")\n",
    "df_employees.select(\n",
    "    col(\"name\"),\n",
    "    col(\"salary\"),\n",
    "    (col(\"salary\") * 1.10).alias(\"salary_with_raise\"),\n",
    "    ((col(\"salary\") * 1.10) - col(\"salary\")).alias(\"raise_amount\")\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selectExpr() - Use SQL expressions\n",
    "print(\"selectExpr() - SQL expressions:\")\n",
    "df_employees.selectExpr(\n",
    "    \"name\",\n",
    "    \"salary\",\n",
    "    \"salary * 12 as annual_salary\",\n",
    "    \"salary * 0.10 as bonus\"\n",
    ").show(5)\n",
    "\n",
    "# expr() - Use expressions inline\n",
    "print(\"\\nexpr() - Inline expressions:\")\n",
    "df_employees.select(\n",
    "    col(\"name\"),\n",
    "    expr(\"salary * 12\").alias(\"annual_salary\")\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Columns with when()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple when/otherwise (if/else)\n",
    "print(\"Simple when/otherwise:\")\n",
    "df_employees.select(\n",
    "    col(\"name\"),\n",
    "    col(\"salary\"),\n",
    "    when(col(\"salary\") > 75000, \"High\")\n",
    "        .otherwise(\"Standard\")\n",
    "        .alias(\"salary_tier\")\n",
    ").show()\n",
    "\n",
    "# Multiple conditions (if/elif/else)\n",
    "print(\"\\nMultiple conditions:\")\n",
    "df_employees.select(\n",
    "    col(\"name\"),\n",
    "    col(\"salary\"),\n",
    "    when(col(\"salary\") >= 80000, \"Senior\")\n",
    "        .when(col(\"salary\") >= 70000, \"Mid-Level\")\n",
    "        .otherwise(\"Junior\")\n",
    "        .alias(\"level\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sorting and Ordering\n",
    "\n",
    "### orderBy() and sort()\n",
    "\n",
    "**Note**: `orderBy()` and `sort()` are aliases - they do the same thing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort ascending (default)\n",
    "print(\"Sort by salary (ascending):\")\n",
    "df_employees.orderBy(\"salary\").show()\n",
    "\n",
    "# Sort descending\n",
    "print(\"\\nSort by salary (descending):\")\n",
    "df_employees.orderBy(col(\"salary\").desc()).show()\n",
    "\n",
    "# Alternative syntax for descending\n",
    "print(\"\\nAlternative descending syntax:\")\n",
    "df_employees.orderBy(\"salary\", ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by multiple columns\n",
    "print(\"Sort by department (asc), then salary (desc):\")\n",
    "df_employees.orderBy(\n",
    "    col(\"department\").asc(),\n",
    "    col(\"salary\").desc()\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Grouping and Aggregation\n",
    "\n",
    "### Basic groupBy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count by department\n",
    "print(\"Count employees by department:\")\n",
    "df_employees.groupBy(\"department\").count().show()\n",
    "\n",
    "# Average salary by department\n",
    "print(\"\\nAverage salary by department:\")\n",
    "df_employees.groupBy(\"department\") \\\n",
    "    .avg(\"salary\") \\\n",
    "    .withColumnRenamed(\"avg(salary)\", \"avg_salary\") \\\n",
    "    .show()\n",
    "\n",
    "# Multiple aggregations\n",
    "print(\"\\nMultiple aggregations:\")\n",
    "df_employees.groupBy(\"department\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"num_employees\"),\n",
    "        avg(\"salary\").alias(\"avg_salary\"),\n",
    "        min(\"salary\").alias(\"min_salary\"),\n",
    "        max(\"salary\").alias(\"max_salary\")\n",
    "    ) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round numeric results\n",
    "print(\"Aggregations with rounding:\")\n",
    "df_employees.groupBy(\"department\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"count\"),\n",
    "        round(avg(\"salary\"), 2).alias(\"avg_salary\")\n",
    "    ) \\\n",
    "    .orderBy(\"avg_salary\", ascending=False) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sales analysis\n",
    "print(\"Sales by product:\")\n",
    "df_sales.groupBy(\"product\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"num_sales\"),\n",
    "        sum(\"quantity\").alias(\"total_quantity\"),\n",
    "        round(sum(\"revenue\"), 2).alias(\"total_revenue\"),\n",
    "        round(avg(\"revenue\"), 2).alias(\"avg_revenue\")\n",
    "    ) \\\n",
    "    .orderBy(\"total_revenue\", ascending=False) \\\n",
    "    .show()\n",
    "\n",
    "# Sales by region and product\n",
    "print(\"\\nSales by region and product:\")\n",
    "df_sales.groupBy(\"region\", \"product\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"num_sales\"),\n",
    "        round(sum(\"revenue\"), 2).alias(\"total_revenue\")\n",
    "    ) \\\n",
    "    .orderBy(\"region\", \"total_revenue\", ascending=[True, False]) \\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivot Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pivot table: regions as rows, products as columns\n",
    "print(\"Pivot: Total revenue by region and product:\")\n",
    "df_sales.groupBy(\"region\") \\\n",
    "    .pivot(\"product\") \\\n",
    "    .agg(round(sum(\"revenue\"), 2)) \\\n",
    "    .show()\n",
    "\n",
    "# Pivot with multiple aggregations\n",
    "print(\"\\nPivot: Count of sales by region and product:\")\n",
    "df_sales.groupBy(\"region\") \\\n",
    "    .pivot(\"product\") \\\n",
    "    .count() \\\n",
    "    .fillna(0) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Joining DataFrames\n",
    "\n",
    "### Join Types\n",
    "\n",
    "| Join Type | Description | Result |\n",
    "|-----------|-------------|--------|\n",
    "| `inner` (default) | Only matching rows from both | Intersection |\n",
    "| `left` / `left_outer` | All from left + matching from right | Left + matches |\n",
    "| `right` / `right_outer` | All from right + matching from left | Right + matches |\n",
    "| `full` / `full_outer` | All rows from both | Union |\n",
    "| `cross` | Cartesian product | All combinations |\n",
    "| `left_semi` | Left rows with match in right | Like inner, but only left columns |\n",
    "| `left_anti` | Left rows WITHOUT match in right | Opposite of left_semi |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inner Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner join (default)\n",
    "print(\"Inner join:\")\n",
    "df_joined = df_employees.join(\n",
    "    df_departments,\n",
    "    df_employees[\"department\"] == df_departments[\"dept_name\"],\n",
    "    \"inner\"\n",
    ")\n",
    "df_joined.select(\"name\", \"department\", \"location\", \"manager\").show()\n",
    "\n",
    "# Note: HR department not shown (no employees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Left Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left join - all employees + matching departments\n",
    "print(\"Left join:\")\n",
    "df_left = df_employees.join(\n",
    "    df_departments,\n",
    "    df_employees[\"department\"] == df_departments[\"dept_name\"],\n",
    "    \"left\"\n",
    ")\n",
    "df_left.select(\"name\", \"department\", \"location\").show()\n",
    "\n",
    "# All employees included, even if no matching department"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Right Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Right join - all departments + matching employees\n",
    "print(\"Right join:\")\n",
    "df_right = df_employees.join(\n",
    "    df_departments,\n",
    "    df_employees[\"department\"] == df_departments[\"dept_name\"],\n",
    "    \"right\"\n",
    ")\n",
    "df_right.select(\"name\", \"dept_name\", \"location\").show()\n",
    "\n",
    "# HR department included (with null for employee columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Outer Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full outer join - all rows from both\n",
    "print(\"Full outer join:\")\n",
    "df_full = df_employees.join(\n",
    "    df_departments,\n",
    "    df_employees[\"department\"] == df_departments[\"dept_name\"],\n",
    "    \"full\"\n",
    ")\n",
    "df_full.select(\"name\", \"department\", \"dept_name\", \"location\").show()\n",
    "\n",
    "# Both employees without departments AND departments without employees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semi and Anti Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left semi join - employees with matching department\n",
    "print(\"Left semi join (employees in departments table):\")\n",
    "df_semi = df_employees.join(\n",
    "    df_departments,\n",
    "    df_employees[\"department\"] == df_departments[\"dept_name\"],\n",
    "    \"left_semi\"\n",
    ")\n",
    "df_semi.show()\n",
    "\n",
    "# Left anti join - employees WITHOUT matching department\n",
    "print(\"\\nLeft anti join (employees NOT in departments table):\")\n",
    "df_anti = df_employees.join(\n",
    "    df_departments,\n",
    "    df_employees[\"department\"] == df_departments[\"dept_name\"],\n",
    "    \"left_anti\"\n",
    ")\n",
    "df_anti.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Semi/anti joins are useful for filtering based on existence in another table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join on Multiple Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When column names match, you can use simpler syntax\n",
    "df_departments2 = df_departments.withColumnRenamed(\"dept_name\", \"department\")\n",
    "\n",
    "print(\"Join on matching column name:\")\n",
    "df_simple = df_employees.join(df_departments2, \"department\", \"inner\")\n",
    "df_simple.select(\"name\", \"department\", \"location\").show()\n",
    "\n",
    "# Multiple columns\n",
    "print(\"\\nJoin on multiple columns:\")\n",
    "# df1.join(df2, [\"col1\", \"col2\"], \"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. String Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String manipulation\n",
    "print(\"String functions:\")\n",
    "df_employees.select(\n",
    "    col(\"name\"),\n",
    "    upper(col(\"name\")).alias(\"upper_name\"),\n",
    "    lower(col(\"name\")).alias(\"lower_name\"),\n",
    "    substring(col(\"name\"), 1, 5).alias(\"first_5_chars\"),\n",
    "    concat(col(\"name\"), lit(\" - \"), col(\"department\")).alias(\"name_dept\")\n",
    ").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Date Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date manipulation\n",
    "print(\"Date functions:\")\n",
    "df_employees.select(\n",
    "    col(\"name\"),\n",
    "    col(\"hire_date\"),\n",
    "    year(col(\"hire_date\")).alias(\"hire_year\"),\n",
    "    month(col(\"hire_date\")).alias(\"hire_month\"),\n",
    "    datediff(current_date(), col(\"hire_date\")).alias(\"days_employed\"),\n",
    "    date_add(col(\"hire_date\"), 365).alias(\"one_year_anniversary\")\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Filtering and Selection\n",
    "\n",
    "Using the `df_employees` DataFrame:\n",
    "1. Filter employees hired after 2020-01-01\n",
    "2. Select only their name, department, and hire_date\n",
    "3. Add a column showing years since hire (use year and current_date)\n",
    "4. Sort by hire date (most recent first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Conditional Logic\n",
    "\n",
    "Create a salary analysis report showing:\n",
    "- Employee name and salary\n",
    "- A \"bonus_percentage\" column: 15% for salary >= 80000, 10% for >= 70000, 5% otherwise\n",
    "- A \"bonus_amount\" column (salary * bonus_percentage)\n",
    "- A \"total_compensation\" column (salary + bonus_amount)\n",
    "\n",
    "Sort by total_compensation descending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your code here\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Aggregation\n",
    "\n",
    "Using the `df_sales` DataFrame, create a report showing:\n",
    "1. Total revenue by region\n",
    "2. Average revenue per sale by region\n",
    "3. Number of sales by region\n",
    "4. Sort by total revenue (descending)\n",
    "5. Round all monetary values to 2 decimal places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Your code here\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Joins and Aggregation\n",
    "\n",
    "1. Join `df_employees` with `df_departments` (use appropriate join type)\n",
    "2. Group by department location\n",
    "3. Calculate:\n",
    "   - Number of employees per location\n",
    "   - Average salary per location\n",
    "   - Total payroll per location (sum of salaries)\n",
    "4. Sort by total payroll (descending)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Your code here\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions\n",
    "\n",
    "### Exercise 1 Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 1: Filtering and Selection\n",
    "\n",
    "from pyspark.sql.functions import year, current_date\n",
    "\n",
    "df_recent = df_employees \\\n",
    "    .filter(col(\"hire_date\") > \"2020-01-01\") \\\n",
    "    .select(\n",
    "        col(\"name\"),\n",
    "        col(\"department\"),\n",
    "        col(\"hire_date\"),\n",
    "        (year(current_date()) - year(col(\"hire_date\"))).alias(\"years_employed\")\n",
    "    ) \\\n",
    "    .orderBy(col(\"hire_date\").desc())\n",
    "\n",
    "print(\"Employees hired after 2020-01-01:\")\n",
    "df_recent.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 2: Conditional Logic\n",
    "\n",
    "df_bonus = df_employees.select(\n",
    "    col(\"name\"),\n",
    "    col(\"salary\"),\n",
    "    when(col(\"salary\") >= 80000, 0.15)\n",
    "        .when(col(\"salary\") >= 70000, 0.10)\n",
    "        .otherwise(0.05)\n",
    "        .alias(\"bonus_percentage\"),\n",
    ") \\\n",
    ".withColumn(\"bonus_amount\", round(col(\"salary\") * col(\"bonus_percentage\"), 2)) \\\n",
    ".withColumn(\"total_compensation\", col(\"salary\") + col(\"bonus_amount\")) \\\n",
    ".orderBy(col(\"total_compensation\").desc())\n",
    "\n",
    "print(\"Salary analysis with bonuses:\")\n",
    "df_bonus.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 3: Aggregation\n",
    "\n",
    "df_region_analysis = df_sales.groupBy(\"region\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"num_sales\"),\n",
    "        round(sum(\"revenue\"), 2).alias(\"total_revenue\"),\n",
    "        round(avg(\"revenue\"), 2).alias(\"avg_revenue_per_sale\")\n",
    "    ) \\\n",
    "    .orderBy(col(\"total_revenue\").desc())\n",
    "\n",
    "print(\"Sales analysis by region:\")\n",
    "df_region_analysis.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4 Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 4: Joins and Aggregation\n",
    "\n",
    "df_location_analysis = df_employees.join(\n",
    "    df_departments,\n",
    "    df_employees[\"department\"] == df_departments[\"dept_name\"],\n",
    "    \"left\"  # Use left to include all employees\n",
    ") \\\n",
    ".groupBy(\"location\") \\\n",
    ".agg(\n",
    "    count(\"*\").alias(\"num_employees\"),\n",
    "    round(avg(\"salary\"), 2).alias(\"avg_salary\"),\n",
    "    sum(\"salary\").alias(\"total_payroll\")\n",
    ") \\\n",
    ".orderBy(col(\"total_payroll\").desc())\n",
    "\n",
    "print(\"Employee and payroll analysis by location:\")\n",
    "df_location_analysis.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts Covered\n",
    "\n",
    "âœ… **Filtering**: Use `filter()` or `where()` with conditions\n",
    "\n",
    "âœ… **Selecting**: Use `select()`, `selectExpr()`, and transformations\n",
    "\n",
    "âœ… **Conditional Logic**: Use `when()` for if/else logic\n",
    "\n",
    "âœ… **Sorting**: Use `orderBy()` or `sort()` with `asc()` and `desc()`\n",
    "\n",
    "âœ… **Grouping**: Use `groupBy()` with aggregation functions\n",
    "\n",
    "âœ… **Joining**: Multiple join types for combining DataFrames\n",
    "\n",
    "### Important Operations Summary\n",
    "\n",
    "**Filtering**:\n",
    "- `df.filter(condition)` / `df.where(condition)`\n",
    "- Conditions: `==`, `!=`, `>`, `<`, `>=`, `<=`\n",
    "- Logical: `&` (and), `|` (or), `~` (not)\n",
    "- Special: `.isin()`, `.like()`, `.between()`\n",
    "\n",
    "**Selecting**:\n",
    "- `df.select(cols)`\n",
    "- `df.selectExpr(sql_expressions)`\n",
    "- `col.alias(new_name)` to rename\n",
    "\n",
    "**Aggregating**:\n",
    "- `df.groupBy(cols).agg(functions)`\n",
    "- Functions: `count()`, `sum()`, `avg()`, `min()`, `max()`\n",
    "- `round()` for numeric precision\n",
    "\n",
    "**Joining**:\n",
    "- `df1.join(df2, condition, join_type)`\n",
    "- Types: `inner`, `left`, `right`, `full`, `cross`, `left_semi`, `left_anti`\n",
    "\n",
    "### Common Functions\n",
    "\n",
    "**String**: `upper()`, `lower()`, `trim()`, `substring()`, `concat()`\n",
    "\n",
    "**Date**: `year()`, `month()`, `day()`, `datediff()`, `date_add()`, `date_sub()`\n",
    "\n",
    "**Math**: `round()`, `abs()`, `ceil()`, `floor()`\n",
    "\n",
    "**Conditional**: `when()`, `otherwise()`\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Use parentheses** with multiple conditions: `(cond1) & (cond2)`\n",
    "2. **Use `&` and `|`**, not `and` and `or`\n",
    "3. **Alias columns** for readability: `.alias(\"new_name\")`\n",
    "4. **Chain operations** for clarity\n",
    "5. **Round numbers** in final output for presentation\n",
    "6. **Choose appropriate join type** based on your needs\n",
    "7. **Use `agg()`** for multiple aggregations instead of chaining\n",
    "\n",
    "### Performance Tips\n",
    "\n",
    "- **Filter early**: Reduce data size before joins/aggregations\n",
    "- **Select only needed columns**: Don't carry unnecessary data\n",
    "- **Use broadcast joins** for small tables (covered in advanced modules)\n",
    "- **Avoid UDFs** when built-in functions exist (covered in Module 07)\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In **Module 06: Spark SQL and Temporary Views**, you will:\n",
    "- Write SQL queries on DataFrames\n",
    "- Create temporary and global views\n",
    "- Use the Spark SQL catalog\n",
    "- Mix DataFrame API and SQL seamlessly\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [PySpark Functions API](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html)\n",
    "- [DataFrame Operations Guide](https://spark.apache.org/docs/latest/sql-ref.html)\n",
    "- [Join Strategies](https://spark.apache.org/docs/latest/sql-performance-tuning.html#join-strategy-hints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "spark.stop()\n",
    "print(\"SparkSession stopped. âœ“\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
