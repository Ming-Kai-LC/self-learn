{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 01: PySpark Setup and SparkSession\n",
    "\n",
    "**Difficulty**: ‚≠ê\n",
    "\n",
    "**Estimated Time**: 45-60 minutes\n",
    "\n",
    "**Prerequisites**: \n",
    "- Module 00: Introduction to Big Data and Spark Ecosystem\n",
    "- Python 3.7+ installed\n",
    "- Basic command line knowledge\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Install and configure PySpark in a local environment\n",
    "2. Create and configure a SparkSession for local development\n",
    "3. Understand the relationship between SparkSession and SparkContext\n",
    "4. Navigate and interpret the Spark UI for monitoring applications\n",
    "5. Configure Spark settings for optimal local performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installing PySpark\n",
    "\n",
    "### Installation Methods\n",
    "\n",
    "There are several ways to install PySpark:\n",
    "\n",
    "#### Method 1: pip (Recommended for this course)\n",
    "```bash\n",
    "pip install pyspark\n",
    "```\n",
    "\n",
    "#### Method 2: conda\n",
    "```bash\n",
    "conda install -c conda-forge pyspark\n",
    "```\n",
    "\n",
    "#### Method 3: Install full Apache Spark distribution\n",
    "- Download from https://spark.apache.org/downloads.html\n",
    "- Set SPARK_HOME environment variable\n",
    "- More complex but gives you access to all Spark tools\n",
    "\n",
    "### Requirements\n",
    "\n",
    "PySpark requires:\n",
    "- **Python**: 3.7 or higher\n",
    "- **Java**: JDK 8 or 11 (Spark runs on JVM)\n",
    "- **Memory**: At least 4GB RAM (8GB+ recommended)\n",
    "\n",
    "### Verify Installation\n",
    "\n",
    "Run the cell below to verify PySpark is installed correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify PySpark installation\n",
    "import pyspark\n",
    "\n",
    "print(f\"PySpark version: {pyspark.__version__}\")\n",
    "print(f\"Installation path: {pyspark.__file__}\")\n",
    "\n",
    "# Check if we can import key modules\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "print(\"\\n‚úì PySpark is installed correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating Your First SparkSession\n",
    "\n",
    "### What is SparkSession?\n",
    "\n",
    "**SparkSession** is the entry point for Spark functionality:\n",
    "- Unified entry point (since Spark 2.0)\n",
    "- Replaces older SQLContext and HiveContext\n",
    "- Provides access to DataFrames, SQL, and Spark configuration\n",
    "\n",
    "### SparkSession vs SparkContext\n",
    "\n",
    "```\n",
    "SparkSession (High-level API)\n",
    "    ‚Üì contains\n",
    "SparkContext (Low-level API)\n",
    "    ‚Üì manages\n",
    "Cluster Connection and RDDs\n",
    "```\n",
    "\n",
    "- **SparkSession**: Modern, high-level API (use this in new code)\n",
    "- **SparkContext**: Low-level API (needed for RDD operations)\n",
    "\n",
    "### Creating a SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession\n",
    "# Builder pattern allows us to configure before creating\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Module 01: PySpark Setup\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"SparkSession created: {spark}\")\n",
    "print(f\"App Name: {spark.sparkContext.appName}\")\n",
    "print(f\"Master: {spark.sparkContext.master}\")\n",
    "print(f\"Spark Version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Configuration\n",
    "\n",
    "Let's break down the SparkSession creation:\n",
    "\n",
    "```python\n",
    ".appName(\"Module 01: PySpark Setup\")\n",
    "```\n",
    "- Sets a human-readable name for your application\n",
    "- Visible in Spark UI for monitoring\n",
    "- Helps identify your job in logs\n",
    "\n",
    "```python\n",
    ".master(\"local[*]\")\n",
    "```\n",
    "- **\"local\"**: Run Spark locally with 1 thread\n",
    "- **\"local[4]\"**: Run locally with 4 threads\n",
    "- **\"local[*]\"**: Use all available CPU cores (recommended for local dev)\n",
    "- **\"spark://host:port\"**: Connect to Spark cluster (production)\n",
    "\n",
    "```python\n",
    ".config(\"spark.driver.memory\", \"2g\")\n",
    "```\n",
    "- Allocates 2GB of memory to the driver program\n",
    "- Increase if working with large datasets locally\n",
    "\n",
    "```python\n",
    ".getOrCreate()\n",
    "```\n",
    "- Gets existing SparkSession or creates new one\n",
    "- Prevents creating multiple sessions (which causes errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Accessing SparkContext\n",
    "\n",
    "SparkContext is available through SparkSession for low-level operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access SparkContext from SparkSession\n",
    "sc = spark.sparkContext\n",
    "\n",
    "print(\"SparkContext Information:\")\n",
    "print(f\"  Application ID: {sc.applicationId}\")\n",
    "print(f\"  Application Name: {sc.appName}\")\n",
    "print(f\"  Master URL: {sc.master}\")\n",
    "print(f\"  Default Parallelism: {sc.defaultParallelism}\")\n",
    "print(f\"  Python Version: {sc.pythonVer}\")\n",
    "\n",
    "# Default parallelism = number of CPU cores being used\n",
    "print(f\"\\nSpark is using {sc.defaultParallelism} CPU cores for parallel processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploring the Spark UI\n",
    "\n",
    "### Accessing the UI\n",
    "\n",
    "The Spark UI is a web interface for monitoring your Spark applications:\n",
    "\n",
    "1. **Default URL**: http://localhost:4040\n",
    "2. If port 4040 is busy, Spark will try 4041, 4042, etc.\n",
    "3. The UI is only available while your SparkSession is active\n",
    "\n",
    "Run the cell below to get the UI URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Spark UI URL\n",
    "ui_url = spark.sparkContext.uiWebUrl\n",
    "print(f\"Spark UI is available at: {ui_url}\")\n",
    "print(\"\\nOpen this URL in your browser to monitor your Spark application\")\n",
    "print(\"\\nIMPORTANT: The UI will only work while this notebook is running!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark UI Tabs\n",
    "\n",
    "The Spark UI has several important tabs:\n",
    "\n",
    "1. **Jobs**: Shows all Spark jobs (actions) executed\n",
    "   - View completed and running jobs\n",
    "   - See execution time and stages\n",
    "\n",
    "2. **Stages**: Breakdown of each job into stages\n",
    "   - Understand shuffle operations\n",
    "   - Identify bottlenecks\n",
    "\n",
    "3. **Storage**: Shows cached/persisted RDDs and DataFrames\n",
    "   - Memory usage\n",
    "   - Partition information\n",
    "\n",
    "4. **Environment**: Configuration and system properties\n",
    "   - Spark settings\n",
    "   - JVM properties\n",
    "   - Classpath\n",
    "\n",
    "5. **Executors**: Information about executors (workers)\n",
    "   - Memory usage\n",
    "   - CPU cores\n",
    "   - Task metrics\n",
    "\n",
    "6. **SQL**: SQL query execution plans\n",
    "   - Physical and logical plans\n",
    "   - Query duration\n",
    "\n",
    "### Let's Generate Some Activity for the UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple DataFrame to generate activity in Spark UI\n",
    "data = [(\"Alice\", 34), (\"Bob\", 45), (\"Charlie\", 28), (\"Diana\", 31)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Trigger an action (this will create a job in the UI)\n",
    "print(\"Sample DataFrame:\")\n",
    "df.show()\n",
    "\n",
    "# Count operation (another action)\n",
    "count = df.count()\n",
    "print(f\"\\nTotal rows: {count}\")\n",
    "\n",
    "print(\"\\nüëâ Check the Spark UI now! You should see 2 jobs (show and count)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Spark Configuration\n",
    "\n",
    "### Viewing Current Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View all Spark configurations\n",
    "conf = spark.sparkContext.getConf()\n",
    "\n",
    "print(\"Current Spark Configuration:\")\n",
    "print(\"=\" * 50)\n",
    "for item in conf.getAll():\n",
    "    print(f\"{item[0]:40s} = {item[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Configuration Options\n",
    "\n",
    "#### Memory Configuration\n",
    "\n",
    "```python\n",
    "# Driver memory (machine running your notebook)\n",
    ".config(\"spark.driver.memory\", \"4g\")\n",
    "\n",
    "# Executor memory (workers in cluster)\n",
    ".config(\"spark.executor.memory\", \"4g\")\n",
    "```\n",
    "\n",
    "#### CPU Configuration\n",
    "\n",
    "```python\n",
    "# Cores per executor\n",
    ".config(\"spark.executor.cores\", \"4\")\n",
    "\n",
    "# Number of executors\n",
    ".config(\"spark.executor.instances\", \"2\")\n",
    "```\n",
    "\n",
    "#### Shuffle Configuration\n",
    "\n",
    "```python\n",
    "# Shuffle partitions (default: 200)\n",
    ".config(\"spark.sql.shuffle.partitions\", \"50\")\n",
    "```\n",
    "\n",
    "#### UI Configuration\n",
    "\n",
    "```python\n",
    "# Change UI port\n",
    ".config(\"spark.ui.port\", \"4050\")\n",
    "\n",
    "# Disable UI (for production)\n",
    ".config(\"spark.ui.enabled\", \"false\")\n",
    "```\n",
    "\n",
    "### Creating a Configured SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop existing session first (can't have multiple sessions)\n",
    "spark.stop()\n",
    "\n",
    "# Create new session with custom configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Configured Spark Session\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"‚úì Reconfigured SparkSession created\")\n",
    "print(f\"  App Name: {spark.sparkContext.appName}\")\n",
    "print(f\"  Shuffle Partitions: {spark.conf.get('spark.sql.shuffle.partitions')}\")\n",
    "print(f\"  Adaptive Query Execution: {spark.conf.get('spark.sql.adaptive.enabled')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Best Practices for Local Development\n",
    "\n",
    "### Recommended Configuration for Local Laptop\n",
    "\n",
    "```python\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"My Local App\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "```\n",
    "\n",
    "**Explanation**:\n",
    "- `local[*]`: Use all CPU cores\n",
    "- `driver.memory`: 2GB for small datasets (increase if needed)\n",
    "- `shuffle.partitions`: 4 instead of default 200 (faster for small data)\n",
    "- `adaptive.enabled`: Let Spark optimize query execution\n",
    "\n",
    "### Common Issues and Solutions\n",
    "\n",
    "#### Issue 1: \"Java not found\"\n",
    "```bash\n",
    "# Install Java 8 or 11\n",
    "# Ubuntu/Debian:\n",
    "sudo apt-get install openjdk-11-jdk\n",
    "\n",
    "# macOS (with Homebrew):\n",
    "brew install openjdk@11\n",
    "```\n",
    "\n",
    "#### Issue 2: \"Port 4040 already in use\"\n",
    "- Spark will automatically try 4041, 4042, etc.\n",
    "- Or specify a port: `.config(\"spark.ui.port\", \"4050\")`\n",
    "\n",
    "#### Issue 3: \"Out of memory\"\n",
    "- Increase driver memory: `.config(\"spark.driver.memory\", \"4g\")`\n",
    "- Reduce shuffle partitions: `.config(\"spark.sql.shuffle.partitions\", \"2\")`\n",
    "\n",
    "### Helper Function for Easy Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session(app_name=\"PySpark App\", memory=\"2g\", shuffle_partitions=4):\n",
    "    \"\"\"\n",
    "    Create a SparkSession with sensible defaults for local development.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    app_name : str\n",
    "        Name of your Spark application\n",
    "    memory : str\n",
    "        Driver memory (e.g., \"2g\", \"4g\")\n",
    "    shuffle_partitions : int\n",
    "        Number of partitions for shuffle operations\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    SparkSession\n",
    "    \"\"\"\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(app_name) \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.driver.memory\", memory) \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", str(shuffle_partitions)) \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    print(f\"‚úì SparkSession '{app_name}' created\")\n",
    "    print(f\"  Spark UI: {spark.sparkContext.uiWebUrl}\")\n",
    "    print(f\"  Using {spark.sparkContext.defaultParallelism} CPU cores\")\n",
    "    \n",
    "    return spark\n",
    "\n",
    "# Test the helper function\n",
    "spark.stop()  # Stop current session\n",
    "spark = create_spark_session(\"Helper Function Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Create a Custom SparkSession\n",
    "\n",
    "Create a SparkSession with the following requirements:\n",
    "- App name: \"My First Spark App\"\n",
    "- Driver memory: 3GB\n",
    "- Shuffle partitions: 8\n",
    "- Enable adaptive query execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n",
    "spark.stop()  # Stop current session first\n",
    "\n",
    "# Create your custom SparkSession\n",
    "my_spark = None  # Replace with your code\n",
    "\n",
    "# Verify your configuration\n",
    "# Uncomment these lines after creating your SparkSession:\n",
    "# print(f\"App Name: {my_spark.sparkContext.appName}\")\n",
    "# print(f\"Driver Memory: {my_spark.conf.get('spark.driver.memory')}\")\n",
    "# print(f\"Shuffle Partitions: {my_spark.conf.get('spark.sql.shuffle.partitions')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Explore SparkContext Properties\n",
    "\n",
    "Using the SparkContext, find and print:\n",
    "1. The application ID\n",
    "2. The master URL\n",
    "3. The default parallelism (number of cores)\n",
    "4. The Python version being used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your code here\n",
    "# Access SparkContext from your SparkSession\n",
    "# sc = my_spark.sparkContext\n",
    "\n",
    "# Print the required properties\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Monitoring in Spark UI\n",
    "\n",
    "1. Create a DataFrame with at least 100 rows of sample data\n",
    "2. Perform 3 different actions (e.g., count, show, collect)\n",
    "3. Open the Spark UI and identify the 3 jobs\n",
    "4. Answer the questions below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Your code here\n",
    "\n",
    "# 1. Create DataFrame with 100+ rows\n",
    "# Hint: Use range() or list comprehension\n",
    "\n",
    "# 2. Perform 3 actions\n",
    "\n",
    "# 3. Check Spark UI at the URL printed below\n",
    "print(f\"Spark UI: {my_spark.sparkContext.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions** (Answer after checking Spark UI):\n",
    "\n",
    "1. How many jobs were created? ___\n",
    "2. Which action took the longest time? ___\n",
    "3. How many stages did each job have? ___\n",
    "4. What was the total number of tasks executed? ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Configuration Optimization\n",
    "\n",
    "You're working on a laptop with:\n",
    "- 8 CPU cores\n",
    "- 16GB RAM\n",
    "- A dataset that's 5GB in size\n",
    "\n",
    "Create an optimized SparkSession configuration for this scenario. Explain your choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Your optimized configuration\n",
    "my_spark.stop()\n",
    "\n",
    "optimized_spark = None  # Create your optimized SparkSession here\n",
    "\n",
    "# Explain your configuration choices:\n",
    "\"\"\"\n",
    "Driver memory: ___ GB because:\n",
    "\n",
    "Shuffle partitions: ___ because:\n",
    "\n",
    "Other configurations:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions\n",
    "\n",
    "### Exercise 1 Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 1\n",
    "spark.stop()\n",
    "\n",
    "my_spark = SparkSession.builder \\\n",
    "    .appName(\"My First Spark App\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"3g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"App Name: {my_spark.sparkContext.appName}\")\n",
    "print(f\"Driver Memory: {my_spark.conf.get('spark.driver.memory')}\")\n",
    "print(f\"Shuffle Partitions: {my_spark.conf.get('spark.sql.shuffle.partitions')}\")\n",
    "print(f\"Adaptive Query: {my_spark.conf.get('spark.sql.adaptive.enabled')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 2\n",
    "sc = my_spark.sparkContext\n",
    "\n",
    "print(f\"1. Application ID: {sc.applicationId}\")\n",
    "print(f\"2. Master URL: {sc.master}\")\n",
    "print(f\"3. Default Parallelism: {sc.defaultParallelism}\")\n",
    "print(f\"4. Python Version: {sc.pythonVer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 3\n",
    "\n",
    "# Create DataFrame with 100 rows\n",
    "data = [(i, f\"Name_{i}\", i * 10) for i in range(100)]\n",
    "df = my_spark.createDataFrame(data, [\"id\", \"name\", \"value\"])\n",
    "\n",
    "# Action 1: count\n",
    "print(f\"Count: {df.count()}\")\n",
    "\n",
    "# Action 2: show\n",
    "df.show(5)\n",
    "\n",
    "# Action 3: collect (be careful with large datasets!)\n",
    "first_10 = df.limit(10).collect()\n",
    "print(f\"Collected {len(first_10)} rows\")\n",
    "\n",
    "print(f\"\\nCheck Spark UI: {my_spark.sparkContext.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4 Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 4\n",
    "my_spark.stop()\n",
    "\n",
    "optimized_spark = SparkSession.builder \\\n",
    "    .appName(\"Optimized Local Spark\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Optimized Configuration:\")\n",
    "print(\"  Driver Memory: 8g\")\n",
    "print(\"  Reason: Allocate ~50% of RAM (16GB / 2) for Spark, leave room for OS\")\n",
    "print(\"\")\n",
    "print(\"  Shuffle Partitions: 8\")\n",
    "print(\"  Reason: Match number of CPU cores for optimal parallelism\")\n",
    "print(\"\")\n",
    "print(\"  Adaptive Query Execution: true\")\n",
    "print(\"  Reason: Let Spark optimize query plans and coalesce partitions automatically\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts Covered\n",
    "\n",
    "‚úÖ **Installing PySpark**: Using pip for easy installation\n",
    "\n",
    "‚úÖ **SparkSession**: Entry point for Spark functionality (modern API)\n",
    "\n",
    "‚úÖ **SparkContext**: Low-level API accessible through SparkSession\n",
    "\n",
    "‚úÖ **Spark UI**: Web interface for monitoring jobs, stages, and executors\n",
    "\n",
    "‚úÖ **Configuration**: Memory, CPU, and shuffle partition settings\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Always use SparkSession** in new code (not SparkContext directly)\n",
    "2. **local[*]** is perfect for development (uses all CPU cores)\n",
    "3. **Spark UI** is essential for debugging and optimization\n",
    "4. **getOrCreate()** prevents multiple SparkSession creation errors\n",
    "5. **Configure wisely**: Balance memory allocation with system resources\n",
    "\n",
    "### Essential Code Pattern\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Your Spark code here\n",
    "\n",
    "spark.stop()  # Clean shutdown\n",
    "```\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In **Module 02: RDD Basics**, you will:\n",
    "- Understand RDDs (Resilient Distributed Datasets)\n",
    "- Learn about transformations vs actions\n",
    "- Explore lazy evaluation and DAG visualization\n",
    "- Work with key-value pair RDDs\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [SparkSession Documentation](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.SparkSession.html)\n",
    "- [Spark Configuration Guide](https://spark.apache.org/docs/latest/configuration.html)\n",
    "- [Spark UI Guide](https://spark.apache.org/docs/latest/web-ui.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up - stop SparkSession when done\n",
    "spark.stop()\n",
    "print(\"SparkSession stopped. ‚úì\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
