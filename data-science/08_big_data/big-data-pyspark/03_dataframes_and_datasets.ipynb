{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 03: DataFrames and Datasets\n",
    "\n",
    "**Difficulty**: ⭐⭐\n",
    "\n",
    "**Estimated Time**: 75-90 minutes\n",
    "\n",
    "**Prerequisites**: \n",
    "- Module 00: Introduction to Big Data and Spark Ecosystem\n",
    "- Module 01: PySpark Setup and SparkSession\n",
    "- Module 02: RDD Basics\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Understand the difference between RDDs, DataFrames, and Datasets\n",
    "2. Create DataFrames from various sources (lists, RDDs, dictionaries)\n",
    "3. Define and work with DataFrame schemas (implicit and explicit)\n",
    "4. Perform basic DataFrame operations (select, show, describe, printSchema)\n",
    "5. Convert between pandas DataFrames and Spark DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, \n",
    "    DoubleType, DateType, TimestampType\n",
    ")\n",
    "from pyspark.sql.functions import col, lit\n",
    "from datetime import datetime, date\n",
    "import pandas as pd\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Module 03: DataFrames and Datasets\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"✓ SparkSession created: {spark.sparkContext.appName}\")\n",
    "print(f\"  Spark version: {spark.version}\")\n",
    "print(f\"  Spark UI: {spark.sparkContext.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. DataFrames vs RDDs vs Datasets\n",
    "\n",
    "### Evolution of Spark APIs\n",
    "\n",
    "```\n",
    "Spark 1.0 (2014)    Spark 1.3 (2015)    Spark 1.6 (2016)    Today\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "    RDDs        →   DataFrames      →    Datasets       →  Unified\n",
    " (Low-level)      (Structured)      (Type-safe)       (Recommended)\n",
    "```\n",
    "\n",
    "### Comparison\n",
    "\n",
    "| Feature | RDD | DataFrame | Dataset |\n",
    "|---------|-----|-----------|----------|\n",
    "| **API Level** | Low-level | High-level | High-level |\n",
    "| **Schema** | None | Yes | Yes |\n",
    "| **Type Safety** | Runtime | Runtime | Compile-time (Scala/Java) |\n",
    "| **Optimization** | None | Catalyst + Tungsten | Catalyst + Tungsten |\n",
    "| **Ease of Use** | Complex | Simple | Simple |\n",
    "| **Performance** | Slower | Fast | Fast |\n",
    "| **Language** | All | All | Scala/Java only |\n",
    "\n",
    "### Why DataFrames?\n",
    "\n",
    "**DataFrames are the recommended API** because they provide:\n",
    "\n",
    "1. **Better Performance**: Catalyst optimizer creates efficient execution plans\n",
    "2. **Easier Syntax**: Similar to SQL and pandas\n",
    "3. **Schema Awareness**: Catches errors early\n",
    "4. **Automatic Optimization**: Spark chooses best execution strategy\n",
    "5. **Interoperability**: Works with SQL, Hive, Parquet, JSON, etc.\n",
    "\n",
    "### Visual Representation\n",
    "\n",
    "```\n",
    "RDD (Unstructured):\n",
    "┌────────────────────────────────┐\n",
    "│ [1, \"Alice\", 25]               │\n",
    "│ [2, \"Bob\", 30]                 │\n",
    "│ [3, \"Charlie\", 35]             │\n",
    "└────────────────────────────────┘\n",
    "  ↓ No schema, just objects\n",
    "\n",
    "DataFrame (Structured):\n",
    "┌────┬─────────┬─────┐\n",
    "│ id │  name   │ age │\n",
    "├────┼─────────┼─────┤\n",
    "│ 1  │ Alice   │ 25  │\n",
    "│ 2  │ Bob     │ 30  │\n",
    "│ 3  │ Charlie │ 35  │\n",
    "└────┴─────────┴─────┘\n",
    "  ↓ Schema: id:int, name:string, age:int\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating DataFrames\n",
    "\n",
    "### Method 1: From Python Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple list of values (single column)\n",
    "numbers = [1, 2, 3, 4, 5]\n",
    "df_numbers = spark.createDataFrame(numbers, \"int\")\n",
    "\n",
    "print(\"DataFrame from simple list:\")\n",
    "df_numbers.show()\n",
    "\n",
    "# List of tuples (multiple columns)\n",
    "data = [\n",
    "    (1, \"Alice\", 25),\n",
    "    (2, \"Bob\", 30),\n",
    "    (3, \"Charlie\", 35),\n",
    "    (4, \"Diana\", 28),\n",
    "    (5, \"Eve\", 32)\n",
    "]\n",
    "\n",
    "# Specify column names\n",
    "columns = [\"id\", \"name\", \"age\"]\n",
    "df_users = spark.createDataFrame(data, columns)\n",
    "\n",
    "print(\"\\nDataFrame from list of tuples:\")\n",
    "df_users.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: From List of Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dictionaries (keys become column names)\n",
    "users_dict = [\n",
    "    {\"id\": 1, \"name\": \"Alice\", \"department\": \"Engineering\", \"salary\": 75000},\n",
    "    {\"id\": 2, \"name\": \"Bob\", \"department\": \"Sales\", \"salary\": 65000},\n",
    "    {\"id\": 3, \"name\": \"Charlie\", \"department\": \"Engineering\", \"salary\": 80000},\n",
    "    {\"id\": 4, \"name\": \"Diana\", \"department\": \"Marketing\", \"salary\": 70000},\n",
    "    {\"id\": 5, \"name\": \"Eve\", \"department\": \"Engineering\", \"salary\": 85000}\n",
    "]\n",
    "\n",
    "df_employees = spark.createDataFrame(users_dict)\n",
    "\n",
    "print(\"DataFrame from list of dictionaries:\")\n",
    "df_employees.show()\n",
    "print(\"\\nInferred schema:\")\n",
    "df_employees.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3: From RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RDD first\n",
    "rdd = spark.sparkContext.parallelize([\n",
    "    (1, \"Product A\", 29.99),\n",
    "    (2, \"Product B\", 49.99),\n",
    "    (3, \"Product C\", 19.99),\n",
    "    (4, \"Product D\", 39.99)\n",
    "])\n",
    "\n",
    "# Convert RDD to DataFrame\n",
    "df_products = rdd.toDF([\"product_id\", \"product_name\", \"price\"])\n",
    "\n",
    "print(\"DataFrame from RDD:\")\n",
    "df_products.show()\n",
    "df_products.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 4: From Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pandas DataFrame\n",
    "pandas_df = pd.DataFrame({\n",
    "    'order_id': [101, 102, 103, 104, 105],\n",
    "    'customer': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n",
    "    'amount': [250.50, 180.75, 320.00, 95.25, 410.80],\n",
    "    'status': ['Shipped', 'Pending', 'Delivered', 'Shipped', 'Delivered']\n",
    "})\n",
    "\n",
    "print(\"Pandas DataFrame:\")\n",
    "print(pandas_df)\n",
    "\n",
    "# Convert to Spark DataFrame\n",
    "df_orders = spark.createDataFrame(pandas_df)\n",
    "\n",
    "print(\"\\nSpark DataFrame from pandas:\")\n",
    "df_orders.show()\n",
    "df_orders.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Understanding Schemas\n",
    "\n",
    "### What is a Schema?\n",
    "\n",
    "A **schema** defines:\n",
    "- Column names\n",
    "- Data types\n",
    "- Nullable constraints\n",
    "\n",
    "### Schema Inference vs Explicit Schema\n",
    "\n",
    "**Inference** (automatic):\n",
    "- ✅ Convenient for prototyping\n",
    "- ❌ Slower (scans data to determine types)\n",
    "- ❌ Can infer wrong types\n",
    "\n",
    "**Explicit** (defined by you):\n",
    "- ✅ Faster (no scanning needed)\n",
    "- ✅ More control\n",
    "- ✅ Type safety\n",
    "- ❌ More verbose code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Explicit Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema explicitly\n",
    "employee_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), nullable=False),\n",
    "    StructField(\"name\", StringType(), nullable=False),\n",
    "    StructField(\"department\", StringType(), nullable=True),\n",
    "    StructField(\"salary\", DoubleType(), nullable=True),\n",
    "    StructField(\"hire_date\", DateType(), nullable=True)\n",
    "])\n",
    "\n",
    "# Create data with dates\n",
    "employee_data = [\n",
    "    (1, \"Alice\", \"Engineering\", 75000.0, date(2020, 1, 15)),\n",
    "    (2, \"Bob\", \"Sales\", 65000.0, date(2019, 6, 10)),\n",
    "    (3, \"Charlie\", \"Engineering\", 80000.0, date(2021, 3, 22)),\n",
    "    (4, \"Diana\", \"Marketing\", 70000.0, date(2020, 8, 5)),\n",
    "    (5, \"Eve\", \"Engineering\", 85000.0, date(2018, 11, 30))\n",
    "]\n",
    "\n",
    "# Create DataFrame with explicit schema\n",
    "df_employees_typed = spark.createDataFrame(employee_data, schema=employee_schema)\n",
    "\n",
    "print(\"DataFrame with explicit schema:\")\n",
    "df_employees_typed.show()\n",
    "print(\"\\nSchema details:\")\n",
    "df_employees_typed.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Data Types\n",
    "\n",
    "| PySpark Type | Python Type | Description |\n",
    "|--------------|-------------|-------------|\n",
    "| `StringType()` | str | Text data |\n",
    "| `IntegerType()` | int | 32-bit integers |\n",
    "| `LongType()` | int | 64-bit integers |\n",
    "| `DoubleType()` | float | Double-precision floating point |\n",
    "| `FloatType()` | float | Single-precision floating point |\n",
    "| `BooleanType()` | bool | True/False |\n",
    "| `DateType()` | date | Date (year, month, day) |\n",
    "| `TimestampType()` | datetime | Date and time |\n",
    "| `ArrayType()` | list | Array/list of elements |\n",
    "| `MapType()` | dict | Key-value pairs |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing Schema Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: printSchema() - Tree format\n",
    "print(\"Method 1: printSchema()\")\n",
    "df_employees_typed.printSchema()\n",
    "\n",
    "# Method 2: schema - Returns StructType object\n",
    "print(\"\\nMethod 2: schema property\")\n",
    "print(df_employees_typed.schema)\n",
    "\n",
    "# Method 3: dtypes - List of (name, type) tuples\n",
    "print(\"\\nMethod 3: dtypes property\")\n",
    "print(df_employees_typed.dtypes)\n",
    "\n",
    "# Method 4: columns - Just column names\n",
    "print(\"\\nMethod 4: columns property\")\n",
    "print(df_employees_typed.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Basic DataFrame Operations\n",
    "\n",
    "### Viewing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show() - Display data (similar to pandas head)\n",
    "print(\"show() - Display all rows (default 20):\")\n",
    "df_employees_typed.show()\n",
    "\n",
    "# show(n) - Display n rows\n",
    "print(\"\\nshow(3) - Display first 3 rows:\")\n",
    "df_employees_typed.show(3)\n",
    "\n",
    "# show(truncate=False) - Don't truncate long strings\n",
    "print(\"\\nshow(truncate=False) - Full column width:\")\n",
    "df_employees_typed.show(truncate=False)\n",
    "\n",
    "# show(vertical=True) - Vertical format\n",
    "print(\"\\nshow(1, vertical=True) - Vertical display:\")\n",
    "df_employees_typed.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# head() and take() - Return Row objects (not displayed)\n",
    "print(\"head(3) - Returns first 3 rows as list:\")\n",
    "rows = df_employees_typed.head(3)\n",
    "for row in rows:\n",
    "    print(f\"  {row}\")\n",
    "\n",
    "# first() - Return first row\n",
    "print(\"\\nfirst() - Returns first row:\")\n",
    "first_row = df_employees_typed.first()\n",
    "print(f\"  {first_row}\")\n",
    "print(f\"  Access by name: {first_row['name']}\")\n",
    "print(f\"  Access by index: {first_row[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count() - Number of rows\n",
    "print(f\"Total rows: {df_employees_typed.count()}\")\n",
    "\n",
    "# columns - List of column names\n",
    "print(f\"\\nColumns: {df_employees_typed.columns}\")\n",
    "\n",
    "# dtypes - List of (column, type)\n",
    "print(\"\\nData types:\")\n",
    "for col_name, col_type in df_employees_typed.dtypes:\n",
    "    print(f\"  {col_name}: {col_type}\")\n",
    "\n",
    "# describe() - Statistical summary\n",
    "print(\"\\ndescribe() - Statistical summary:\")\n",
    "df_employees_typed.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: select() with string column names\n",
    "print(\"Method 1: select() with strings\")\n",
    "df_employees_typed.select(\"name\", \"department\").show()\n",
    "\n",
    "# Method 2: select() with column objects\n",
    "print(\"\\nMethod 2: select() with col()\")\n",
    "df_employees_typed.select(col(\"name\"), col(\"salary\")).show()\n",
    "\n",
    "# Method 3: select() with DataFrame.column syntax\n",
    "print(\"\\nMethod 3: select() with df.column\")\n",
    "df_employees_typed.select(\n",
    "    df_employees_typed.name, \n",
    "    df_employees_typed.salary\n",
    "").show()\n",
    "\n",
    "# Select all columns\n",
    "print(\"\\nselect(*) - All columns:\")\n",
    "df_employees_typed.select(\"*\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding and Renaming Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# withColumn() - Add or replace a column\n",
    "print(\"withColumn() - Add bonus column (10% of salary):\")\n",
    "df_with_bonus = df_employees_typed.withColumn(\n",
    "    \"bonus\", \n",
    "    col(\"salary\") * 0.10\n",
    ")\n",
    "df_with_bonus.show()\n",
    "\n",
    "# withColumnRenamed() - Rename a column\n",
    "print(\"\\nwithColumnRenamed() - Rename 'name' to 'employee_name':\")\n",
    "df_renamed = df_employees_typed.withColumnRenamed(\"name\", \"employee_name\")\n",
    "df_renamed.show()\n",
    "\n",
    "# Add literal column\n",
    "print(\"\\nAdd literal column (company name):\")\n",
    "df_with_literal = df_employees_typed.withColumn(\n",
    "    \"company\", \n",
    "    lit(\"Tech Corp\")\n",
    ")\n",
    "df_with_literal.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop() - Remove column(s)\n",
    "print(\"drop() - Remove 'hire_date' column:\")\n",
    "df_dropped = df_employees_typed.drop(\"hire_date\")\n",
    "df_dropped.show()\n",
    "\n",
    "# Drop multiple columns\n",
    "print(\"\\nDrop multiple columns:\")\n",
    "df_dropped_multi = df_employees_typed.drop(\"hire_date\", \"department\")\n",
    "df_dropped_multi.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Converting Between pandas and Spark\n",
    "\n",
    "### Why Convert?\n",
    "\n",
    "**Use Spark when**:\n",
    "- Data is large (>1GB)\n",
    "- Need distributed processing\n",
    "- Working with big data sources (HDFS, S3)\n",
    "\n",
    "**Use pandas when**:\n",
    "- Data fits in memory\n",
    "- Need complex operations not in Spark\n",
    "- Visualization (matplotlib, seaborn)\n",
    "- Final aggregated results are small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark → pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Spark DataFrame to pandas\n",
    "pandas_df = df_employees_typed.toPandas()\n",
    "\n",
    "print(\"Converted to pandas:\")\n",
    "print(type(pandas_df))\n",
    "print(pandas_df)\n",
    "\n",
    "# ⚠️ Warning: Only use toPandas() on small DataFrames!\n",
    "# It brings all data to driver memory\n",
    "print(\"\\n⚠️ Warning: toPandas() loads all data into driver memory\")\n",
    "print(\"   Only use on small DataFrames or aggregated results!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pandas → Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pandas DataFrame\n",
    "import numpy as np\n",
    "\n",
    "pandas_sales = pd.DataFrame({\n",
    "    'date': pd.date_range('2024-01-01', periods=10),\n",
    "    'product': np.random.choice(['A', 'B', 'C'], 10),\n",
    "    'quantity': np.random.randint(1, 100, 10),\n",
    "    'revenue': np.random.uniform(100, 1000, 10).round(2)\n",
    "})\n",
    "\n",
    "print(\"Pandas DataFrame:\")\n",
    "print(pandas_sales.head())\n",
    "\n",
    "# Convert to Spark DataFrame\n",
    "spark_sales = spark.createDataFrame(pandas_sales)\n",
    "\n",
    "print(\"\\nConverted to Spark:\")\n",
    "spark_sales.show(5)\n",
    "spark_sales.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type Mapping\n",
    "\n",
    "| pandas dtype | Spark Type |\n",
    "|--------------|------------|\n",
    "| object (str) | StringType |\n",
    "| int64 | LongType |\n",
    "| float64 | DoubleType |\n",
    "| bool | BooleanType |\n",
    "| datetime64 | TimestampType |\n",
    "| category | StringType |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Create DataFrame with Explicit Schema\n",
    "\n",
    "Create a DataFrame representing books with the following structure:\n",
    "- `book_id` (integer, not nullable)\n",
    "- `title` (string, not nullable)\n",
    "- `author` (string, nullable)\n",
    "- `price` (double, nullable)\n",
    "- `publication_date` (date, nullable)\n",
    "\n",
    "Create at least 5 book records and display the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n",
    "\n",
    "# Step 1: Define schema\n",
    "# Your code here\n",
    "\n",
    "# Step 2: Create data\n",
    "# Your code here\n",
    "\n",
    "# Step 3: Create DataFrame\n",
    "# Your code here\n",
    "\n",
    "# Step 4: Display and print schema\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: DataFrame Manipulation\n",
    "\n",
    "Using the `df_employees_typed` DataFrame:\n",
    "1. Select only `name` and `salary` columns\n",
    "2. Add a new column `annual_salary` (salary * 12)\n",
    "3. Add a column `salary_grade` with literal value \"Mid-Level\"\n",
    "4. Rename `name` to `full_name`\n",
    "5. Drop the original `salary` column\n",
    "6. Display the final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your code here\n",
    "\n",
    "# Start with df_employees_typed\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: pandas to Spark Conversion\n",
    "\n",
    "Create a pandas DataFrame with the following student data:\n",
    "```python\n",
    "{\n",
    "    'student_id': [1, 2, 3, 4, 5],\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n",
    "    'math_score': [85, 92, 78, 95, 88],\n",
    "    'english_score': [90, 85, 92, 88, 94]\n",
    "}\n",
    "```\n",
    "\n",
    "Then:\n",
    "1. Convert to Spark DataFrame\n",
    "2. Add a column `average_score` (mean of math and english)\n",
    "3. Display the result with schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Your code here\n",
    "\n",
    "# Step 1: Create pandas DataFrame\n",
    "# Your code here\n",
    "\n",
    "# Step 2: Convert to Spark\n",
    "# Your code here\n",
    "\n",
    "# Step 3: Add average_score column\n",
    "# Your code here\n",
    "\n",
    "# Step 4: Display\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Schema Exploration\n",
    "\n",
    "Using the `df_employees_typed` DataFrame:\n",
    "1. Print the schema in tree format\n",
    "2. Get a list of all column names\n",
    "3. Get all column names that contain numeric types (int, double, long)\n",
    "4. Count total number of columns\n",
    "5. Display statistical summary for numeric columns only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Your code here\n",
    "\n",
    "# Task 1: Print schema\n",
    "# Your code here\n",
    "\n",
    "# Task 2: Column names\n",
    "# Your code here\n",
    "\n",
    "# Task 3: Numeric columns only\n",
    "# Your code here\n",
    "\n",
    "# Task 4: Count columns\n",
    "# Your code here\n",
    "\n",
    "# Task 5: Statistical summary\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions\n",
    "\n",
    "### Exercise 1 Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 1: Create DataFrame with Explicit Schema\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, DateType\n",
    "from datetime import date\n",
    "\n",
    "# Define schema\n",
    "books_schema = StructType([\n",
    "    StructField(\"book_id\", IntegerType(), nullable=False),\n",
    "    StructField(\"title\", StringType(), nullable=False),\n",
    "    StructField(\"author\", StringType(), nullable=True),\n",
    "    StructField(\"price\", DoubleType(), nullable=True),\n",
    "    StructField(\"publication_date\", DateType(), nullable=True)\n",
    "])\n",
    "\n",
    "# Create data\n",
    "books_data = [\n",
    "    (1, \"The Great Gatsby\", \"F. Scott Fitzgerald\", 12.99, date(1925, 4, 10)),\n",
    "    (2, \"1984\", \"George Orwell\", 14.99, date(1949, 6, 8)),\n",
    "    (3, \"To Kill a Mockingbird\", \"Harper Lee\", 13.99, date(1960, 7, 11)),\n",
    "    (4, \"Pride and Prejudice\", \"Jane Austen\", 11.99, date(1813, 1, 28)),\n",
    "    (5, \"The Catcher in the Rye\", \"J.D. Salinger\", 15.99, date(1951, 7, 16))\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df_books = spark.createDataFrame(books_data, schema=books_schema)\n",
    "\n",
    "# Display\n",
    "print(\"Books DataFrame:\")\n",
    "df_books.show(truncate=False)\n",
    "print(\"\\nSchema:\")\n",
    "df_books.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 2: DataFrame Manipulation\n",
    "\n",
    "df_transformed = df_employees_typed \\\n",
    "    .select(\"name\", \"salary\") \\\n",
    "    .withColumn(\"annual_salary\", col(\"salary\") * 12) \\\n",
    "    .withColumn(\"salary_grade\", lit(\"Mid-Level\")) \\\n",
    "    .withColumnRenamed(\"name\", \"full_name\") \\\n",
    "    .drop(\"salary\")\n",
    "\n",
    "print(\"Transformed DataFrame:\")\n",
    "df_transformed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 3: pandas to Spark Conversion\n",
    "\n",
    "# Create pandas DataFrame\n",
    "students_pandas = pd.DataFrame({\n",
    "    'student_id': [1, 2, 3, 4, 5],\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n",
    "    'math_score': [85, 92, 78, 95, 88],\n",
    "    'english_score': [90, 85, 92, 88, 94]\n",
    "})\n",
    "\n",
    "print(\"Pandas DataFrame:\")\n",
    "print(students_pandas)\n",
    "\n",
    "# Convert to Spark\n",
    "df_students = spark.createDataFrame(students_pandas)\n",
    "\n",
    "# Add average_score column\n",
    "df_students = df_students.withColumn(\n",
    "    \"average_score\",\n",
    "    (col(\"math_score\") + col(\"english_score\")) / 2\n",
    ")\n",
    "\n",
    "print(\"\\nSpark DataFrame with average:\")\n",
    "df_students.show()\n",
    "df_students.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4 Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 4: Schema Exploration\n",
    "\n",
    "# Task 1: Print schema\n",
    "print(\"Task 1: Schema in tree format\")\n",
    "df_employees_typed.printSchema()\n",
    "\n",
    "# Task 2: Column names\n",
    "print(\"\\nTask 2: All column names\")\n",
    "print(df_employees_typed.columns)\n",
    "\n",
    "# Task 3: Numeric columns only\n",
    "print(\"\\nTask 3: Numeric columns\")\n",
    "numeric_types = ['int', 'bigint', 'double', 'float', 'long']\n",
    "numeric_cols = [col_name for col_name, col_type in df_employees_typed.dtypes \n",
    "                if col_type in numeric_types]\n",
    "print(numeric_cols)\n",
    "\n",
    "# Task 4: Count columns\n",
    "print(\"\\nTask 4: Total number of columns\")\n",
    "print(f\"Total columns: {len(df_employees_typed.columns)}\")\n",
    "\n",
    "# Task 5: Statistical summary\n",
    "print(\"\\nTask 5: Statistical summary for numeric columns\")\n",
    "df_employees_typed.select(numeric_cols).describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts Covered\n",
    "\n",
    "✅ **DataFrames vs RDDs**: DataFrames provide schema, optimization, and ease of use\n",
    "\n",
    "✅ **Creating DataFrames**: From lists, dictionaries, RDDs, pandas, and files\n",
    "\n",
    "✅ **Schemas**: Explicit schemas (StructType) vs inferred schemas\n",
    "\n",
    "✅ **Basic Operations**: select, show, withColumn, withColumnRenamed, drop\n",
    "\n",
    "✅ **pandas Integration**: Converting between pandas and Spark DataFrames\n",
    "\n",
    "### Important Methods Summary\n",
    "\n",
    "**Creation**:\n",
    "- `spark.createDataFrame(data, schema)` - Create from Python data\n",
    "- `rdd.toDF(columns)` - Convert RDD to DataFrame\n",
    "- `pandas_df → spark.createDataFrame()` - From pandas\n",
    "- `spark_df.toPandas()` - To pandas\n",
    "\n",
    "**Viewing**:\n",
    "- `df.show(n)` - Display n rows\n",
    "- `df.printSchema()` - Display schema\n",
    "- `df.describe()` - Statistical summary\n",
    "- `df.head(n)` / `df.take(n)` - Return rows as list\n",
    "\n",
    "**Schema**:\n",
    "- `df.schema` - Get schema object\n",
    "- `df.dtypes` - List of (column, type)\n",
    "- `df.columns` - List of column names\n",
    "\n",
    "**Manipulation**:\n",
    "- `df.select()` - Select columns\n",
    "- `df.withColumn()` - Add/replace column\n",
    "- `df.withColumnRenamed()` - Rename column\n",
    "- `df.drop()` - Remove column\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Use explicit schemas** for production code (faster, type-safe)\n",
    "2. **Avoid `toPandas()`** on large DataFrames (memory issues)\n",
    "3. **Use `show()`** instead of `collect()` for viewing data\n",
    "4. **Chain operations** for cleaner code: `df.select().withColumn().filter()`\n",
    "5. **Check schema early** with `printSchema()` to catch type issues\n",
    "\n",
    "### DataFrames are Immutable!\n",
    "\n",
    "Remember: Every operation creates a **new** DataFrame\n",
    "```python\n",
    "df2 = df1.withColumn(\"new_col\", lit(1))  # df1 unchanged!\n",
    "```\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In **Module 04: Data Loading and Saving**, you will:\n",
    "- Read data from CSV, JSON, Parquet files\n",
    "- Write DataFrames to various formats\n",
    "- Work with partitioned data\n",
    "- Handle schema evolution and data quality issues\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [PySpark DataFrame API](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html)\n",
    "- [Spark SQL Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)\n",
    "- [DataFrame Schema](https://spark.apache.org/docs/latest/sql-ref-datatypes.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "spark.stop()\n",
    "print(\"SparkSession stopped. ✓\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
