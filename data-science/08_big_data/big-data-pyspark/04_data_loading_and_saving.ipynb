{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 04: Data Loading and Saving\n",
    "\n",
    "**Difficulty**: ‚≠ê‚≠ê\n",
    "\n",
    "**Estimated Time**: 75-90 minutes\n",
    "\n",
    "**Prerequisites**: \n",
    "- Module 00: Introduction to Big Data and Spark Ecosystem\n",
    "- Module 01: PySpark Setup and SparkSession\n",
    "- Module 03: DataFrames and Datasets\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Read data from various file formats (CSV, JSON, Parquet, text files)\n",
    "2. Write DataFrames to different file formats with appropriate options\n",
    "3. Work with partitioned data for better performance\n",
    "4. Handle schema inference, schema evolution, and data quality issues\n",
    "5. Use DataFrameReader and DataFrameWriter options effectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, \n",
    "    DoubleType, DateType, TimestampType\n",
    ")\n",
    "from pyspark.sql.functions import col, lit, current_timestamp\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime, date\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Module 04: Data Loading and Saving\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"‚úì SparkSession created: {spark.sparkContext.appName}\")\n",
    "print(f\"  Spark version: {spark.version}\")\n",
    "print(f\"  Spark UI: {spark.sparkContext.uiWebUrl}\")\n",
    "\n",
    "# Create data directory for examples\n",
    "os.makedirs('sample_data', exist_ok=True)\n",
    "print(\"\\n‚úì Sample data directory created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. File Format Overview\n",
    "\n",
    "### Common Formats in Big Data\n",
    "\n",
    "| Format | Type | Schema | Compression | Use Case |\n",
    "|--------|------|--------|-------------|----------|\n",
    "| **CSV** | Text | No | Medium | Simple data exchange, human-readable |\n",
    "| **JSON** | Text | No | Medium | Semi-structured data, APIs |\n",
    "| **Parquet** | Binary | Yes | Excellent | Analytics, production systems |\n",
    "| **ORC** | Binary | Yes | Excellent | Hive tables, analytics |\n",
    "| **Avro** | Binary | Yes | Good | Data serialization, streaming |\n",
    "| **Text** | Text | No | Poor | Log files, unstructured text |\n",
    "\n",
    "### Format Recommendations\n",
    "\n",
    "**Use CSV when**:\n",
    "- Data exchange with non-technical users\n",
    "- Excel compatibility needed\n",
    "- Simple, flat data structures\n",
    "\n",
    "**Use JSON when**:\n",
    "- Nested/hierarchical data\n",
    "- API responses\n",
    "- Semi-structured data\n",
    "\n",
    "**Use Parquet when** (RECOMMENDED for Spark):\n",
    "- Large datasets in production\n",
    "- Need columnar storage benefits\n",
    "- Analytics workloads\n",
    "- Best compression and performance\n",
    "\n",
    "### Columnar vs Row-Based Storage\n",
    "\n",
    "```\n",
    "Row-Based (CSV, JSON):\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ Row 1: id=1, name=Alice, age=25 ‚îÇ\n",
    "‚îÇ Row 2: id=2, name=Bob, age=30   ‚îÇ\n",
    "‚îÇ Row 3: id=3, name=Charlie, age=35‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "  ‚úì Good for: Reading full rows\n",
    "  ‚úó Bad for: Reading specific columns\n",
    "\n",
    "Columnar (Parquet, ORC):\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ id:     ‚îÇ name:        ‚îÇ age:     ‚îÇ\n",
    "‚îÇ 1, 2, 3 ‚îÇ Alice, Bob,  ‚îÇ 25,30,35 ‚îÇ\n",
    "‚îÇ         ‚îÇ Charlie      ‚îÇ          ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "  ‚úì Good for: Analytics (SELECT specific columns)\n",
    "  ‚úì Better compression (similar values together)\n",
    "  ‚úì Predicate pushdown optimization\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reading CSV Files\n",
    "\n",
    "CSV is the most common format for data exchange, but has challenges:\n",
    "- No schema information\n",
    "- Must infer or specify data types\n",
    "- Various delimiters, quote characters, null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample CSV file\n",
    "csv_data = \"\"\"id,name,department,salary,hire_date\n",
    "1,Alice,Engineering,75000,2020-01-15\n",
    "2,Bob,Sales,65000,2019-06-10\n",
    "3,Charlie,Engineering,80000,2021-03-22\n",
    "4,Diana,Marketing,70000,2020-08-05\n",
    "5,Eve,Engineering,85000,2018-11-30\n",
    "6,Frank,,72000,2022-02-14\n",
    "7,Grace,Sales,,2021-09-01\"\"\"\n",
    "\n",
    "with open('sample_data/employees.csv', 'w') as f:\n",
    "    f.write(csv_data)\n",
    "\n",
    "print(\"‚úì Sample CSV file created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic CSV Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic read with schema inference\n",
    "df_csv_basic = spark.read.csv(\n",
    "    'sample_data/employees.csv',\n",
    "    header=True,        # First row is header\n",
    "    inferSchema=True    # Automatically infer data types\n",
    ")\n",
    "\n",
    "print(\"Basic CSV read:\")\n",
    "df_csv_basic.show()\n",
    "print(\"\\nInferred schema:\")\n",
    "df_csv_basic.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV Reading with Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read with explicit options\n",
    "df_csv_options = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"nullValue\", \"\") \\\n",
    "    .option(\"dateFormat\", \"yyyy-MM-dd\") \\\n",
    "    .option(\"mode\", \"DROPMALFORMED\") \\\n",
    "    .csv('sample_data/employees.csv')\n",
    "\n",
    "print(\"CSV with options:\")\n",
    "df_csv_options.show()\n",
    "\n",
    "# Alternative syntax using format()\n",
    "df_csv_format = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load('sample_data/employees.csv')\n",
    "\n",
    "print(\"\\n‚úì Both syntaxes produce same result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV Reading with Explicit Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define explicit schema (better performance, no inference needed)\n",
    "employee_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), nullable=False),\n",
    "    StructField(\"name\", StringType(), nullable=False),\n",
    "    StructField(\"department\", StringType(), nullable=True),\n",
    "    StructField(\"salary\", DoubleType(), nullable=True),\n",
    "    StructField(\"hire_date\", DateType(), nullable=True)\n",
    "])\n",
    "\n",
    "df_csv_schema = spark.read \\\n",
    "    .schema(employee_schema) \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"dateFormat\", \"yyyy-MM-dd\") \\\n",
    "    .csv('sample_data/employees.csv')\n",
    "\n",
    "print(\"CSV with explicit schema:\")\n",
    "df_csv_schema.show()\n",
    "df_csv_schema.printSchema()\n",
    "\n",
    "print(\"\\nüí° Explicit schema is faster (no scanning) and type-safe!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important CSV Options\n",
    "\n",
    "| Option | Default | Description |\n",
    "|--------|---------|-------------|\n",
    "| `header` | false | First row is header |\n",
    "| `inferSchema` | false | Infer column types (requires extra pass) |\n",
    "| `sep` | \",\" | Field delimiter |\n",
    "| `quote` | \"\\\"\" | Quote character |\n",
    "| `escape` | \"\\\\\" | Escape character |\n",
    "| `nullValue` | \"\" | String representing null |\n",
    "| `dateFormat` | yyyy-MM-dd | Date format pattern |\n",
    "| `timestampFormat` | yyyy-MM-dd'T'HH:mm:ss | Timestamp format |\n",
    "| `mode` | PERMISSIVE | Error handling mode |\n",
    "\n",
    "### Error Handling Modes\n",
    "\n",
    "- **PERMISSIVE** (default): Set malformed records to null\n",
    "- **DROPMALFORMED**: Drop rows with malformed data\n",
    "- **FAILFAST**: Throw exception on malformed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Reading JSON Files\n",
    "\n",
    "JSON is great for nested/hierarchical data from APIs and web services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample JSON file (one JSON object per line - JSON Lines format)\n",
    "json_data = [\n",
    "    {\"id\": 1, \"name\": \"Alice\", \"age\": 25, \"address\": {\"city\": \"New York\", \"country\": \"USA\"}},\n",
    "    {\"id\": 2, \"name\": \"Bob\", \"age\": 30, \"address\": {\"city\": \"London\", \"country\": \"UK\"}},\n",
    "    {\"id\": 3, \"name\": \"Charlie\", \"age\": 35, \"address\": {\"city\": \"Tokyo\", \"country\": \"Japan\"}},\n",
    "    {\"id\": 4, \"name\": \"Diana\", \"age\": 28},  # Missing address\n",
    "    {\"id\": 5, \"name\": \"Eve\", \"age\": 32, \"address\": {\"city\": \"Paris\", \"country\": \"France\"}}\n",
    "]\n",
    "\n",
    "with open('sample_data/users.json', 'w') as f:\n",
    "    for record in json_data:\n",
    "        f.write(json.dumps(record) + '\\n')\n",
    "\n",
    "print(\"‚úì Sample JSON file created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read JSON file\n",
    "df_json = spark.read.json('sample_data/users.json')\n",
    "\n",
    "print(\"JSON DataFrame:\")\n",
    "df_json.show(truncate=False)\n",
    "print(\"\\nJSON schema (note nested structure):\")\n",
    "df_json.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access nested fields\n",
    "print(\"Accessing nested fields:\")\n",
    "df_json.select(\n",
    "    \"name\",\n",
    "    \"age\",\n",
    "    col(\"address.city\").alias(\"city\"),\n",
    "    col(\"address.country\").alias(\"country\")\n",
    ").show()\n",
    "\n",
    "# Alternative: Use getField()\n",
    "df_json.select(\n",
    "    \"name\",\n",
    "    col(\"address\").getField(\"city\").alias(\"city\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Reading Parquet Files\n",
    "\n",
    "**Parquet is the recommended format** for Spark because:\n",
    "- Columnar storage (read only needed columns)\n",
    "- Built-in compression\n",
    "- Schema included in file\n",
    "- Excellent performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, write a DataFrame to Parquet (we'll learn more about writing soon)\n",
    "df_csv_schema.write.mode(\"overwrite\").parquet('sample_data/employees.parquet')\n",
    "\n",
    "# Read Parquet file\n",
    "df_parquet = spark.read.parquet('sample_data/employees.parquet')\n",
    "\n",
    "print(\"Parquet DataFrame:\")\n",
    "df_parquet.show()\n",
    "print(\"\\nParquet schema (automatically preserved):\")\n",
    "df_parquet.printSchema()\n",
    "\n",
    "print(\"\\n‚úì No need to specify schema - it's stored in the file!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parquet Advantages Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columnar storage benefit: Read only specific columns\n",
    "import time\n",
    "\n",
    "# Create larger dataset for demonstration\n",
    "large_data = [(i, f\"Name{i}\", f\"Dept{i%5}\", float(50000 + i*100)) \n",
    "              for i in range(100000)]\n",
    "df_large = spark.createDataFrame(large_data, [\"id\", \"name\", \"department\", \"salary\"])\n",
    "\n",
    "# Write to CSV and Parquet\n",
    "df_large.write.mode(\"overwrite\").csv('sample_data/large_data.csv')\n",
    "df_large.write.mode(\"overwrite\").parquet('sample_data/large_data.parquet')\n",
    "\n",
    "# Compare file sizes\n",
    "import os\n",
    "\n",
    "def get_dir_size(path):\n",
    "    total = 0\n",
    "    for entry in os.scandir(path):\n",
    "        if entry.is_file():\n",
    "            total += entry.stat().st_size\n",
    "        elif entry.is_dir():\n",
    "            total += get_dir_size(entry.path)\n",
    "    return total\n",
    "\n",
    "csv_size = get_dir_size('sample_data/large_data.csv')\n",
    "parquet_size = get_dir_size('sample_data/large_data.parquet')\n",
    "\n",
    "print(f\"CSV size: {csv_size / 1024 / 1024:.2f} MB\")\n",
    "print(f\"Parquet size: {parquet_size / 1024 / 1024:.2f} MB\")\n",
    "print(f\"Compression ratio: {csv_size / parquet_size:.2f}x\")\n",
    "\n",
    "print(\"\\n‚úì Parquet is much smaller due to compression!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Writing DataFrames\n",
    "\n",
    "### Write Modes\n",
    "\n",
    "| Mode | Behavior |\n",
    "|------|----------|\n",
    "| `overwrite` | Delete existing data and write |\n",
    "| `append` | Add to existing data |\n",
    "| `ignore` | Write only if doesn't exist |\n",
    "| `error` (default) | Throw error if exists |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to CSV\n",
    "df_csv_schema.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('sample_data/output_csv')\n",
    "\n",
    "print(\"‚úì Data written to CSV\")\n",
    "print(\"\\nFiles created:\")\n",
    "for file in os.listdir('sample_data/output_csv'):\n",
    "    print(f\"  {file}\")\n",
    "\n",
    "# Note: Spark writes to a directory, not a single file\n",
    "# This enables parallel writing across partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to JSON\n",
    "df_json.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .json('sample_data/output_json')\n",
    "\n",
    "print(\"‚úì Data written to JSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to Parquet with compression\n",
    "df_csv_schema.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"compression\", \"snappy\") \\\n",
    "    .parquet('sample_data/output_parquet')\n",
    "\n",
    "print(\"‚úì Data written to Parquet\")\n",
    "\n",
    "# Parquet compression options: snappy (default), gzip, lzo, none\n",
    "# snappy: Fast compression/decompression (recommended)\n",
    "# gzip: Better compression ratio, slower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing to a Single File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use coalesce(1) to write to single file\n",
    "# ‚ö†Ô∏è Only for small datasets!\n",
    "df_csv_schema.coalesce(1).write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('sample_data/single_file')\n",
    "\n",
    "print(\"‚úì Data written to single file (using coalesce)\")\n",
    "print(\"\\nFiles created:\")\n",
    "for file in os.listdir('sample_data/single_file'):\n",
    "    print(f\"  {file}\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è Warning: coalesce(1) reduces parallelism!\")\n",
    "print(\"   Only use for small final outputs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Partitioned Data\n",
    "\n",
    "### What is Partitioning?\n",
    "\n",
    "Partitioning organizes data into subdirectories based on column values.\n",
    "\n",
    "**Benefits**:\n",
    "- **Partition pruning**: Skip irrelevant data\n",
    "- **Better performance**: Read only needed partitions\n",
    "- **Organized storage**: Data grouped logically\n",
    "\n",
    "**Example**: Partition by date\n",
    "```\n",
    "sales/\n",
    "  ‚îú‚îÄ‚îÄ year=2023/\n",
    "  ‚îÇ   ‚îú‚îÄ‚îÄ month=01/\n",
    "  ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ data.parquet\n",
    "  ‚îÇ   ‚îî‚îÄ‚îÄ month=02/\n",
    "  ‚îÇ       ‚îî‚îÄ‚îÄ data.parquet\n",
    "  ‚îî‚îÄ‚îÄ year=2024/\n",
    "      ‚îî‚îÄ‚îÄ month=01/\n",
    "          ‚îî‚îÄ‚îÄ data.parquet\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample sales data with dates\n",
    "import random\n",
    "from datetime import timedelta\n",
    "\n",
    "sales_data = []\n",
    "base_date = date(2024, 1, 1)\n",
    "products = ['Product A', 'Product B', 'Product C']\n",
    "regions = ['North', 'South', 'East', 'West']\n",
    "\n",
    "for i in range(1000):\n",
    "    sale_date = base_date + timedelta(days=random.randint(0, 90))\n",
    "    sales_data.append((\n",
    "        i,\n",
    "        sale_date,\n",
    "        sale_date.year,\n",
    "        sale_date.month,\n",
    "        random.choice(products),\n",
    "        random.choice(regions),\n",
    "        random.randint(1, 100),\n",
    "        round(random.uniform(100, 1000), 2)\n",
    "    ))\n",
    "\n",
    "df_sales = spark.createDataFrame(\n",
    "    sales_data,\n",
    "    [\"sale_id\", \"sale_date\", \"year\", \"month\", \"product\", \"region\", \"quantity\", \"revenue\"]\n",
    ")\n",
    "\n",
    "print(\"Sales DataFrame:\")\n",
    "df_sales.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Partitioned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write partitioned by year and month\n",
    "df_sales.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"year\", \"month\") \\\n",
    "    .parquet('sample_data/sales_partitioned')\n",
    "\n",
    "print(\"‚úì Data written with partitions\")\n",
    "print(\"\\nPartition structure:\")\n",
    "for root, dirs, files in os.walk('sample_data/sales_partitioned'):\n",
    "    level = root.replace('sample_data/sales_partitioned', '').count(os.sep)\n",
    "    indent = ' ' * 2 * level\n",
    "    print(f\"{indent}{os.path.basename(root)}/\")\n",
    "    if level < 2:  # Only show first 2 levels\n",
    "        sub_indent = ' ' * 2 * (level + 1)\n",
    "        for file in files[:1]:  # Show only first file\n",
    "            print(f\"{sub_indent}{file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Partitioned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read partitioned data\n",
    "df_sales_read = spark.read.parquet('sample_data/sales_partitioned')\n",
    "\n",
    "print(\"Read partitioned data:\")\n",
    "df_sales_read.show(10)\n",
    "print(\"\\n‚úì Partition columns automatically included!\")\n",
    "\n",
    "# Demonstrate partition pruning\n",
    "print(\"\\nFiltering by partition column (year=2024, month=1):\")\n",
    "df_filtered = df_sales_read.filter((col(\"year\") == 2024) & (col(\"month\") == 1))\n",
    "df_filtered.show(10)\n",
    "\n",
    "print(\"\\nüí° Spark only reads year=2024/month=1 partition!\")\n",
    "print(\"   This is MUCH faster for large datasets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitioning Best Practices\n",
    "\n",
    "**DO**:\n",
    "- Partition by columns frequently used in filters (date, region, category)\n",
    "- Aim for partitions of 100MB-1GB each\n",
    "- Use 1-3 partition columns maximum\n",
    "\n",
    "**DON'T**:\n",
    "- Partition by high-cardinality columns (user_id, transaction_id)\n",
    "- Create too many small partitions (< 1MB)\n",
    "- Partition by columns rarely used in queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Handling Data Quality Issues\n",
    "\n",
    "Real-world data is messy. Spark provides options to handle common issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CSV with data quality issues\n",
    "messy_csv = \"\"\"id,name,age,salary\n",
    "1,Alice,25,50000\n",
    "2,Bob,thirty,60000\n",
    "3,Charlie,35,not_a_number\n",
    "4,Diana,,55000\n",
    "5,Eve,28,65000\n",
    ",Frank,32,70000\n",
    "7,Grace,29,\"\"\"\n",
    "\n",
    "with open('sample_data/messy_data.csv', 'w') as f:\n",
    "    f.write(messy_csv)\n",
    "\n",
    "print(\"‚úì Messy CSV created with quality issues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mode: PERMISSIVE (Default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PERMISSIVE: Set malformed values to null\n",
    "df_permissive = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"mode\", \"PERMISSIVE\") \\\n",
    "    .csv('sample_data/messy_data.csv')\n",
    "\n",
    "print(\"PERMISSIVE mode (sets malformed to null):\")\n",
    "df_permissive.show()\n",
    "df_permissive.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mode: DROPMALFORMED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DROPMALFORMED: Drop rows with malformed data\n",
    "df_dropmalformed = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"mode\", \"DROPMALFORMED\") \\\n",
    "    .csv('sample_data/messy_data.csv')\n",
    "\n",
    "print(\"DROPMALFORMED mode (drops bad rows):\")\n",
    "df_dropmalformed.show()\n",
    "print(f\"\\nRows kept: {df_dropmalformed.count()} out of {df_permissive.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capturing Malformed Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture malformed records in a separate column\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "schema_with_corrupt = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True),\n",
    "    StructField(\"_corrupt_record\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_with_corrupt = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"mode\", \"PERMISSIVE\") \\\n",
    "    .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\") \\\n",
    "    .schema(schema_with_corrupt) \\\n",
    "    .csv('sample_data/messy_data.csv')\n",
    "\n",
    "print(\"With corrupt record column:\")\n",
    "df_with_corrupt.show(truncate=False)\n",
    "\n",
    "print(\"\\nOnly corrupt records:\")\n",
    "df_with_corrupt.filter(col(\"_corrupt_record\").isNotNull()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Multi-Format Data Pipeline\n",
    "\n",
    "Create a data pipeline that:\n",
    "1. Reads the CSV file `sample_data/employees.csv`\n",
    "2. Adds a column `loaded_at` with current timestamp\n",
    "3. Writes the result to Parquet format\n",
    "4. Reads back the Parquet and displays it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n",
    "\n",
    "# Step 1: Read CSV\n",
    "# Your code here\n",
    "\n",
    "# Step 2: Add timestamp column\n",
    "# Your code here\n",
    "\n",
    "# Step 3: Write to Parquet\n",
    "# Your code here\n",
    "\n",
    "# Step 4: Read back and display\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Working with Partitions\n",
    "\n",
    "Using the `df_sales` DataFrame:\n",
    "1. Write the data partitioned by `region` and `product`\n",
    "2. Read back only the data for region=\"North\"\n",
    "3. Count records for each product in the North region\n",
    "4. Calculate total revenue by product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your code here\n",
    "\n",
    "# Step 1: Write partitioned data\n",
    "# Your code here\n",
    "\n",
    "# Step 2: Read only North region\n",
    "# Your code here\n",
    "\n",
    "# Step 3: Count by product\n",
    "# Your code here\n",
    "\n",
    "# Step 4: Total revenue by product\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Schema Enforcement\n",
    "\n",
    "Create an explicit schema for customer data with:\n",
    "- `customer_id` (integer, not null)\n",
    "- `email` (string, not null)\n",
    "- `registration_date` (date, nullable)\n",
    "- `total_purchases` (double, nullable)\n",
    "\n",
    "Then:\n",
    "1. Create sample data (5 customers)\n",
    "2. Create DataFrame with this schema\n",
    "3. Write to Parquet\n",
    "4. Read back and verify schema is preserved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Your code here\n",
    "\n",
    "# Step 1: Define schema\n",
    "# Your code here\n",
    "\n",
    "# Step 2: Create sample data\n",
    "# Your code here\n",
    "\n",
    "# Step 3: Create DataFrame\n",
    "# Your code here\n",
    "\n",
    "# Step 4: Write to Parquet\n",
    "# Your code here\n",
    "\n",
    "# Step 5: Read and verify\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Format Comparison\n",
    "\n",
    "Create a large DataFrame (10,000 rows) with columns:\n",
    "- `id`, `product`, `category`, `price`, `timestamp`\n",
    "\n",
    "Then:\n",
    "1. Write to CSV, JSON, and Parquet\n",
    "2. Compare file sizes\n",
    "3. Measure read times for each format\n",
    "4. Which format is smallest? Which is fastest to read?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Your code here\n",
    "\n",
    "# Step 1: Create large DataFrame\n",
    "# Your code here\n",
    "\n",
    "# Step 2: Write to all formats\n",
    "# Your code here\n",
    "\n",
    "# Step 3: Compare sizes\n",
    "# Your code here\n",
    "\n",
    "# Step 4: Measure read times\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions\n",
    "\n",
    "### Exercise 1 Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 1: Multi-Format Data Pipeline\n",
    "\n",
    "# Step 1: Read CSV\n",
    "df_pipeline = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv('sample_data/employees.csv')\n",
    "\n",
    "# Step 2: Add timestamp\n",
    "df_pipeline = df_pipeline.withColumn(\"loaded_at\", current_timestamp())\n",
    "\n",
    "print(\"Data with timestamp:\")\n",
    "df_pipeline.show(truncate=False)\n",
    "\n",
    "# Step 3: Write to Parquet\n",
    "df_pipeline.write.mode(\"overwrite\").parquet('sample_data/pipeline_output')\n",
    "\n",
    "# Step 4: Read back\n",
    "df_result = spark.read.parquet('sample_data/pipeline_output')\n",
    "print(\"\\nRead from Parquet:\")\n",
    "df_result.show(truncate=False)\n",
    "print(\"\\n‚úì Pipeline completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 2: Working with Partitions\n",
    "\n",
    "# Step 1: Write partitioned data\n",
    "df_sales.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"region\", \"product\") \\\n",
    "    .parquet('sample_data/sales_by_region_product')\n",
    "\n",
    "# Step 2: Read only North region\n",
    "df_north = spark.read \\\n",
    "    .parquet('sample_data/sales_by_region_product') \\\n",
    "    .filter(col(\"region\") == \"North\")\n",
    "\n",
    "print(\"North region sales:\")\n",
    "df_north.show(10)\n",
    "\n",
    "# Step 3: Count by product\n",
    "print(\"\\nCount by product in North:\")\n",
    "df_north.groupBy(\"product\").count().show()\n",
    "\n",
    "# Step 4: Total revenue by product\n",
    "print(\"\\nTotal revenue by product in North:\")\n",
    "df_north.groupBy(\"product\") \\\n",
    "    .sum(\"revenue\") \\\n",
    "    .withColumnRenamed(\"sum(revenue)\", \"total_revenue\") \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 3: Schema Enforcement\n",
    "\n",
    "# Step 1: Define schema\n",
    "customer_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), nullable=False),\n",
    "    StructField(\"email\", StringType(), nullable=False),\n",
    "    StructField(\"registration_date\", DateType(), nullable=True),\n",
    "    StructField(\"total_purchases\", DoubleType(), nullable=True)\n",
    "])\n",
    "\n",
    "# Step 2: Create sample data\n",
    "customer_data = [\n",
    "    (1, \"alice@example.com\", date(2024, 1, 15), 1250.50),\n",
    "    (2, \"bob@example.com\", date(2024, 2, 20), 890.75),\n",
    "    (3, \"charlie@example.com\", date(2024, 3, 10), 2340.00),\n",
    "    (4, \"diana@example.com\", date(2024, 1, 5), 567.25),\n",
    "    (5, \"eve@example.com\", date(2024, 2, 28), 1890.80)\n",
    "]\n",
    "\n",
    "# Step 3: Create DataFrame\n",
    "df_customers = spark.createDataFrame(customer_data, schema=customer_schema)\n",
    "\n",
    "print(\"Customer DataFrame:\")\n",
    "df_customers.show()\n",
    "df_customers.printSchema()\n",
    "\n",
    "# Step 4: Write to Parquet\n",
    "df_customers.write.mode(\"overwrite\").parquet('sample_data/customers')\n",
    "\n",
    "# Step 5: Read and verify\n",
    "df_customers_read = spark.read.parquet('sample_data/customers')\n",
    "print(\"\\nRead from Parquet:\")\n",
    "df_customers_read.printSchema()\n",
    "print(\"\\n‚úì Schema preserved in Parquet!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4 Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 4: Format Comparison\n",
    "\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Step 1: Create large DataFrame\n",
    "large_data = []\n",
    "products = ['Laptop', 'Phone', 'Tablet', 'Monitor', 'Keyboard']\n",
    "categories = ['Electronics', 'Accessories']\n",
    "base_time = datetime(2024, 1, 1)\n",
    "\n",
    "for i in range(10000):\n",
    "    large_data.append((\n",
    "        i,\n",
    "        random.choice(products),\n",
    "        random.choice(categories),\n",
    "        round(random.uniform(50, 2000), 2),\n",
    "        base_time + timedelta(hours=random.randint(0, 1000))\n",
    "    ))\n",
    "\n",
    "df_large_test = spark.createDataFrame(\n",
    "    large_data,\n",
    "    [\"id\", \"product\", \"category\", \"price\", \"timestamp\"]\n",
    ")\n",
    "\n",
    "print(f\"Created DataFrame with {df_large_test.count()} rows\")\n",
    "\n",
    "# Step 2: Write to all formats\n",
    "df_large_test.write.mode(\"overwrite\").csv('sample_data/compare_csv')\n",
    "df_large_test.write.mode(\"overwrite\").json('sample_data/compare_json')\n",
    "df_large_test.write.mode(\"overwrite\").parquet('sample_data/compare_parquet')\n",
    "\n",
    "# Step 3: Compare sizes\n",
    "csv_size = get_dir_size('sample_data/compare_csv')\n",
    "json_size = get_dir_size('sample_data/compare_json')\n",
    "parquet_size = get_dir_size('sample_data/compare_parquet')\n",
    "\n",
    "print(\"\\n=== File Size Comparison ===\")\n",
    "print(f\"CSV:     {csv_size / 1024:.2f} KB\")\n",
    "print(f\"JSON:    {json_size / 1024:.2f} KB\")\n",
    "print(f\"Parquet: {parquet_size / 1024:.2f} KB\")\n",
    "print(f\"\\nParquet is {csv_size/parquet_size:.2f}x smaller than CSV\")\n",
    "\n",
    "# Step 4: Measure read times\n",
    "print(\"\\n=== Read Time Comparison ===\")\n",
    "\n",
    "start = time.time()\n",
    "spark.read.csv('sample_data/compare_csv').count()\n",
    "csv_time = time.time() - start\n",
    "print(f\"CSV:     {csv_time:.4f} seconds\")\n",
    "\n",
    "start = time.time()\n",
    "spark.read.json('sample_data/compare_json').count()\n",
    "json_time = time.time() - start\n",
    "print(f\"JSON:    {json_time:.4f} seconds\")\n",
    "\n",
    "start = time.time()\n",
    "spark.read.parquet('sample_data/compare_parquet').count()\n",
    "parquet_time = time.time() - start\n",
    "print(f\"Parquet: {parquet_time:.4f} seconds\")\n",
    "\n",
    "print(\"\\n=== Conclusion ===\")\n",
    "print(f\"Smallest: Parquet ({parquet_size / 1024:.2f} KB)\")\n",
    "print(f\"Fastest:  Parquet ({parquet_time:.4f} seconds)\")\n",
    "print(\"\\n‚úì Parquet wins on both size AND speed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts Covered\n",
    "\n",
    "‚úÖ **File Formats**: CSV, JSON, Parquet - each with specific use cases\n",
    "\n",
    "‚úÖ **Reading Data**: DataFrameReader with various options and modes\n",
    "\n",
    "‚úÖ **Writing Data**: DataFrameWriter with modes (overwrite, append, ignore, error)\n",
    "\n",
    "‚úÖ **Partitioning**: Organizing data for better query performance\n",
    "\n",
    "‚úÖ **Data Quality**: Handling malformed data with PERMISSIVE, DROPMALFORMED, FAILFAST\n",
    "\n",
    "### Format Recommendations\n",
    "\n",
    "**For Production Spark Applications**: Use **Parquet**\n",
    "- Best compression\n",
    "- Fastest performance\n",
    "- Schema included\n",
    "- Columnar benefits\n",
    "\n",
    "**For Data Exchange**: Use **CSV** or **JSON**\n",
    "- Human-readable\n",
    "- Tool compatibility\n",
    "- Simple structure\n",
    "\n",
    "### Important Methods\n",
    "\n",
    "**Reading**:\n",
    "- `spark.read.csv()` / `json()` / `parquet()`\n",
    "- `spark.read.format().load()`\n",
    "- `.option()` for configuration\n",
    "- `.schema()` for explicit schema\n",
    "\n",
    "**Writing**:\n",
    "- `df.write.csv()` / `json()` / `parquet()`\n",
    "- `df.write.format().save()`\n",
    "- `.mode()` for write behavior\n",
    "- `.partitionBy()` for partitioning\n",
    "- `.option()` for configuration\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Use Parquet** for production Spark workloads\n",
    "2. **Define explicit schemas** instead of inferring (faster, type-safe)\n",
    "3. **Partition wisely** by columns used in filters (date, region, category)\n",
    "4. **Handle errors** appropriately with mode settings\n",
    "5. **Monitor file sizes** - avoid too many small files\n",
    "6. **Use compression** (snappy for Parquet)\n",
    "\n",
    "### Partitioning Guidelines\n",
    "\n",
    "‚úÖ **DO**:\n",
    "- Partition by date (year, month, day)\n",
    "- Use low-cardinality columns (region, category, status)\n",
    "- Aim for partition sizes of 100MB-1GB\n",
    "\n",
    "‚ùå **DON'T**:\n",
    "- Partition by high-cardinality (user_id, transaction_id)\n",
    "- Create thousands of tiny partitions\n",
    "- Partition by columns not used in queries\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In **Module 05: DataFrame Operations**, you will:\n",
    "- Perform powerful transformations (select, filter, groupBy)\n",
    "- Join multiple DataFrames\n",
    "- Aggregate data with built-in functions\n",
    "- Use window functions for advanced analytics\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [Spark Data Sources](https://spark.apache.org/docs/latest/sql-data-sources.html)\n",
    "- [Parquet Format](https://parquet.apache.org/)\n",
    "- [Best Practices for File Formats](https://spark.apache.org/docs/latest/sql-performance-tuning.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "spark.stop()\n",
    "print(\"SparkSession stopped. ‚úì\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
