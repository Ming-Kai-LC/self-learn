{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 12: Performance Optimization\n",
    "\n",
    "**Difficulty**: ⭐⭐⭐  \n",
    "**Estimated Time**: 75 minutes  \n",
    "**Prerequisites**: \n",
    "- [Module 03: DataFrames and Datasets](03_dataframes_and_datasets.ipynb)\n",
    "- [Module 05: DataFrame Operations](05_dataframe_operations.ipynb)\n",
    "- Understanding of Spark execution model\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Apply partitioning strategies (repartition vs coalesce) to optimize data distribution\n",
    "2. Use caching and persistence effectively with appropriate storage levels\n",
    "3. Implement broadcast variables and broadcast joins for small-to-large table joins\n",
    "4. Identify and avoid expensive shuffle operations in transformations\n",
    "5. Monitor and tune Spark performance using configuration settings and execution plans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Introduction\n",
    "\n",
    "**Performance Optimization in Spark**\n",
    "\n",
    "Spark's performance depends on several factors:\n",
    "\n",
    "1. **Data Partitioning**: How data is distributed across executors\n",
    "2. **Shuffles**: Data movement between nodes (expensive!)\n",
    "3. **Memory Management**: Caching, spilling, storage levels\n",
    "4. **Parallelism**: Number of tasks running concurrently\n",
    "5. **Query Optimization**: Catalyst optimizer decisions\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "**Partitions:**\n",
    "- Logical chunks of data\n",
    "- Each partition processed by one task\n",
    "- Optimal: 2-4 partitions per CPU core\n",
    "\n",
    "**Shuffle:**\n",
    "- Data redistribution across cluster\n",
    "- Triggered by: groupBy, join, repartition, distinct, etc.\n",
    "- Expensive: disk I/O, network I/O, serialization\n",
    "\n",
    "**Caching:**\n",
    "- Store DataFrames in memory/disk\n",
    "- Avoid recomputation\n",
    "- Trade-off: memory vs computation\n",
    "\n",
    "**Broadcast:**\n",
    "- Send small data to all executors\n",
    "- Avoid shuffle for small-to-large joins\n",
    "- Limited by driver memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, count, sum as spark_sum, avg, broadcast,\n",
    "    rand, round as spark_round, monotonically_increasing_id,\n",
    "    when, expr, lit\n",
    ")\n",
    "from pyspark import StorageLevel\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark session with custom configurations\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Performance Optimization\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .config(\"spark.default.parallelism\", \"8\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Default parallelism: {spark.sparkContext.defaultParallelism}\")\n",
    "print(f\"SQL shuffle partitions: {spark.conf.get('spark.sql.shuffle.partitions')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Partitioning Strategies\n",
    "\n",
    "**Why Partitioning Matters:**\n",
    "- Controls parallelism\n",
    "- Affects shuffle performance\n",
    "- Impacts memory usage\n",
    "- Determines task granularity\n",
    "\n",
    "**repartition() vs coalesce():**\n",
    "\n",
    "**repartition(n)**:\n",
    "- Can increase or decrease partitions\n",
    "- Triggers full shuffle\n",
    "- Distributes data evenly\n",
    "- Use when: increasing partitions or need even distribution\n",
    "\n",
    "**coalesce(n)**:\n",
    "- Can only decrease partitions\n",
    "- Avoids full shuffle (more efficient)\n",
    "- May create uneven partitions\n",
    "- Use when: reducing partitions after filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample DataFrame\n",
    "df = spark.range(0, 10000000).toDF(\"id\") \\\n",
    "    .withColumn(\"value\", (col(\"id\") * 3.14).cast(\"double\")) \\\n",
    "    .withColumn(\"category\", (col(\"id\") % 100).cast(\"string\"))\n",
    "\n",
    "print(f\"Original partitions: {df.rdd.getNumPartitions()}\")\n",
    "print(f\"Total rows: {df.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test repartition (increases partitions, triggers shuffle)\n",
    "print(\"\\n=== Testing repartition() ===\")\n",
    "\n",
    "start = time.time()\n",
    "df_repartitioned = df.repartition(20)\n",
    "df_repartitioned.write.mode(\"overwrite\").format(\"noop\").save()\n",
    "repartition_time = time.time() - start\n",
    "\n",
    "print(f\"Partitions after repartition(20): {df_repartitioned.rdd.getNumPartitions()}\")\n",
    "print(f\"Time: {repartition_time:.2f}s\")\n",
    "\n",
    "# Check partition sizes\n",
    "partition_sizes = df_repartitioned.rdd.glom().map(len).collect()\n",
    "print(f\"Partition sizes (first 10): {partition_sizes[:10]}\")\n",
    "print(f\"Min: {min(partition_sizes)}, Max: {max(partition_sizes)}, Avg: {np.mean(partition_sizes):.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_counts>": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test coalesce (decreases partitions, avoids full shuffle)\n",
    "print(\"\\n=== Testing coalesce() ===\")\n",
    "\n",
    "start = time.time()\n",
    "df_coalesced = df_repartitioned.coalesce(5)\n",
    "df_coalesced.write.mode(\"overwrite\").format(\"noop\").save()\n",
    "coalesce_time = time.time() - start\n",
    "\n",
    "print(f\"Partitions after coalesce(5): {df_coalesced.rdd.getNumPartitions()}\")\n",
    "print(f\"Time: {coalesce_time:.2f}s\")\n",
    "print(f\"Speedup vs repartition: {repartition_time/coalesce_time:.2f}x faster\")\n",
    "\n",
    "# Coalesce may create uneven partitions\n",
    "coalesce_sizes = df_coalesced.rdd.glom().map(len).collect()\n",
    "print(f\"\\nPartition sizes: {coalesce_sizes}\")\n",
    "print(f\"Notice: Partitions may be uneven (that's okay for coalesce)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repartition by column for better data locality\n",
    "print(\"\\n=== Repartitioning by Column ===\")\n",
    "\n",
    "# Repartition by category for grouped operations\n",
    "df_by_category = df.repartition(10, \"category\")\n",
    "\n",
    "print(f\"Partitions: {df_by_category.rdd.getNumPartitions()}\")\n",
    "print(\"\\nBenefit: All rows with same category are in same partition\")\n",
    "print(\"This avoids shuffle in subsequent groupBy(category) operations\")\n",
    "\n",
    "# Demonstrate benefit\n",
    "start = time.time()\n",
    "result1 = df.groupBy(\"category\").count().count()\n",
    "time1 = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "result2 = df_by_category.groupBy(\"category\").count().count()\n",
    "time2 = time.time() - start\n",
    "\n",
    "print(f\"\\nGroupBy without pre-partitioning: {time1:.2f}s\")\n",
    "print(f\"GroupBy with pre-partitioning: {time2:.2f}s\")\n",
    "print(f\"Speedup: {time1/time2:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Caching and Persistence\n",
    "\n",
    "**When to Cache:**\n",
    "- DataFrame used multiple times\n",
    "- Expensive computation that doesn't change\n",
    "- Iterative algorithms (ML training)\n",
    "- Interactive exploration\n",
    "\n",
    "**When NOT to Cache:**\n",
    "- DataFrame used only once\n",
    "- Limited memory available\n",
    "- Data too large to fit in memory\n",
    "\n",
    "**Storage Levels:**\n",
    "- `MEMORY_ONLY`: Fast but uses memory, data lost if evicted\n",
    "- `MEMORY_AND_DISK`: Spills to disk if memory full\n",
    "- `DISK_ONLY`: Uses disk storage, slower but reliable\n",
    "- `MEMORY_ONLY_SER`: Serialized (saves memory, slower access)\n",
    "- Add `_2` for replication (fault tolerance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame with expensive computation\n",
    "expensive_df = spark.range(0, 5000000).toDF(\"id\") \\\n",
    "    .withColumn(\"value1\", expr(\"sin(id / 1000.0)\")) \\\n",
    "    .withColumn(\"value2\", expr(\"cos(id / 1000.0)\")) \\\n",
    "    .withColumn(\"value3\", expr(\"sqrt(abs(id))\")) \\\n",
    "    .withColumn(\"category\", (col(\"id\") % 50).cast(\"string\"))\n",
    "\n",
    "print(\"Created expensive DataFrame with trigonometric computations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario: Use DataFrame 3 times WITHOUT caching\n",
    "print(\"\\n=== WITHOUT Caching ===\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# First use: count\n",
    "count1 = expensive_df.count()\n",
    "\n",
    "# Second use: aggregation\n",
    "agg1 = expensive_df.groupBy(\"category\").agg(avg(\"value1\")).count()\n",
    "\n",
    "# Third use: filter and count\n",
    "filtered1 = expensive_df.filter(col(\"value2\") > 0).count()\n",
    "\n",
    "no_cache_time = time.time() - start\n",
    "\n",
    "print(f\"Time without caching: {no_cache_time:.2f}s\")\n",
    "print(\"Note: DataFrame was recomputed 3 times (sin, cos, sqrt calculated 3x)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same scenario WITH caching\n",
    "print(\"\\n=== WITH Caching ===\")\n",
    "\n",
    "# Cache the DataFrame\n",
    "cached_df = expensive_df.cache()\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# First use: count (triggers caching)\n",
    "count2 = cached_df.count()\n",
    "\n",
    "# Second use: aggregation (uses cache)\n",
    "agg2 = cached_df.groupBy(\"category\").agg(avg(\"value1\")).count()\n",
    "\n",
    "# Third use: filter and count (uses cache)\n",
    "filtered2 = cached_df.filter(col(\"value2\") > 0).count()\n",
    "\n",
    "cache_time = time.time() - start\n",
    "\n",
    "print(f\"Time with caching: {cache_time:.2f}s\")\n",
    "print(f\"Speedup: {no_cache_time/cache_time:.2f}x faster\")\n",
    "print(\"Note: Expensive computation done only once, then reused from cache\")\n",
    "\n",
    "# Don't forget to unpersist when done\n",
    "cached_df.unpersist()\n",
    "print(\"\\nCache cleared with unpersist()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different storage levels\n",
    "print(\"\\n=== Storage Levels Comparison ===\")\n",
    "\n",
    "test_df = spark.range(0, 3000000).toDF(\"id\") \\\n",
    "    .withColumn(\"data\", expr(\"cast(rand() * 1000 as int)\"))\n",
    "\n",
    "# MEMORY_ONLY (default cache())\n",
    "df_mem_only = test_df.persist(StorageLevel.MEMORY_ONLY)\n",
    "start = time.time()\n",
    "df_mem_only.count()\n",
    "mem_only_time = time.time() - start\n",
    "print(f\"MEMORY_ONLY: {mem_only_time:.2f}s\")\n",
    "df_mem_only.unpersist()\n",
    "\n",
    "# MEMORY_AND_DISK\n",
    "df_mem_disk = test_df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "start = time.time()\n",
    "df_mem_disk.count()\n",
    "mem_disk_time = time.time() - start\n",
    "print(f\"MEMORY_AND_DISK: {mem_disk_time:.2f}s (more reliable, slight overhead)\")\n",
    "df_mem_disk.unpersist()\n",
    "\n",
    "# MEMORY_ONLY_SER (serialized, saves memory)\n",
    "df_mem_ser = test_df.persist(StorageLevel.MEMORY_ONLY_SER)\n",
    "start = time.time()\n",
    "df_mem_ser.count()\n",
    "mem_ser_time = time.time() - start\n",
    "print(f\"MEMORY_ONLY_SER: {mem_ser_time:.2f}s (slower but uses less memory)\")\n",
    "df_mem_ser.unpersist()\n",
    "\n",
    "print(\"\\nRecommendation: Use MEMORY_AND_DISK for production (fault tolerant)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Broadcast Joins\n",
    "\n",
    "**Join Strategies:**\n",
    "\n",
    "1. **Shuffle Join (Sort-Merge Join)**:\n",
    "   - Both datasets shuffled and sorted\n",
    "   - Expensive: network I/O, disk I/O\n",
    "   - Used for large-to-large joins\n",
    "\n",
    "2. **Broadcast Join (Map-Side Join)**:\n",
    "   - Small dataset sent to all executors\n",
    "   - No shuffle needed!\n",
    "   - Used when one side < 10MB (configurable)\n",
    "   - Much faster for small-to-large joins\n",
    "\n",
    "**When to Broadcast:**\n",
    "- Small dimension tables (< 1GB)\n",
    "- Lookup tables\n",
    "- Reference data\n",
    "\n",
    "**How to Broadcast:**\n",
    "- Automatic: Spark broadcasts if < `spark.sql.autoBroadcastJoinThreshold`\n",
    "- Manual: `broadcast(small_df)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create large fact table\n",
    "large_df = spark.range(0, 2000000).toDF(\"id\") \\\n",
    "    .withColumn(\"category_id\", (col(\"id\") % 100).cast(\"int\")) \\\n",
    "    .withColumn(\"value\", (rand() * 1000).cast(\"int\"))\n",
    "\n",
    "# Create small dimension table\n",
    "small_df = spark.range(0, 100).toDF(\"category_id\") \\\n",
    "    .withColumn(\"category_name\", expr(\"concat('Category_', category_id)\")) \\\n",
    "    .withColumn(\"department\", \n",
    "                when(col(\"category_id\") < 33, \"Electronics\")\n",
    "                .when(col(\"category_id\") < 66, \"Clothing\")\n",
    "                .otherwise(\"Home\"))\n",
    "\n",
    "print(f\"Large table: {large_df.count():,} rows\")\n",
    "print(f\"Small table: {small_df.count():,} rows\")\n",
    "print(f\"Size ratio: {large_df.count() / small_df.count():.0f}:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular join (may use shuffle)\n",
    "print(\"\\n=== Regular Join (Potential Shuffle) ===\")\n",
    "\n",
    "# Disable auto broadcast to force shuffle join\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "\n",
    "start = time.time()\n",
    "regular_join = large_df.join(small_df, \"category_id\") \\\n",
    "    .groupBy(\"department\") \\\n",
    "    .agg(spark_sum(\"value\").alias(\"total_value\"))\n",
    "regular_result = regular_join.count()\n",
    "regular_time = time.time() - start\n",
    "\n",
    "print(f\"Regular join time: {regular_time:.2f}s\")\n",
    "print(\"Note: This used shuffle join (both sides shuffled)\")\n",
    "\n",
    "# Check execution plan\n",
    "print(\"\\nExecution plan (look for 'SortMergeJoin'):\")\n",
    "regular_join.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcast join (no shuffle!)\n",
    "print(\"\\n=== Broadcast Join (No Shuffle) ===\")\n",
    "\n",
    "start = time.time()\n",
    "broadcast_join = large_df.join(broadcast(small_df), \"category_id\") \\\n",
    "    .groupBy(\"department\") \\\n",
    "    .agg(spark_sum(\"value\").alias(\"total_value\"))\n",
    "broadcast_result = broadcast_join.count()\n",
    "broadcast_time = time.time() - start\n",
    "\n",
    "print(f\"Broadcast join time: {broadcast_time:.2f}s\")\n",
    "print(f\"Speedup: {regular_time/broadcast_time:.2f}x faster\")\n",
    "print(\"Note: Small table broadcasted to all executors, no shuffle!\")\n",
    "\n",
    "# Check execution plan\n",
    "print(\"\\nExecution plan (look for 'BroadcastHashJoin'):\")\n",
    "broadcast_join.explain()\n",
    "\n",
    "# Re-enable auto broadcast\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"10485760\")  # 10MB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Avoiding Shuffle Operations\n",
    "\n",
    "**Shuffle is Expensive Because:**\n",
    "- Data written to disk\n",
    "- Data sent over network\n",
    "- Data serialized/deserialized\n",
    "- Creates intermediate files\n",
    "\n",
    "**Operations that Trigger Shuffle:**\n",
    "- `groupBy`, `agg`\n",
    "- `join` (except broadcast join)\n",
    "- `distinct`, `dropDuplicates`\n",
    "- `repartition`\n",
    "- `sortBy`, `orderBy`\n",
    "- Window functions\n",
    "\n",
    "**How to Minimize Shuffles:**\n",
    "1. Use broadcast joins for small tables\n",
    "2. Pre-partition data by join/groupBy keys\n",
    "3. Use `coalesce` instead of `repartition` when reducing partitions\n",
    "4. Filter data early to reduce shuffle volume\n",
    "5. Combine multiple aggregations into one\n",
    "6. Use DataFrames instead of RDDs (better optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Inefficient - Multiple shuffles\n",
    "print(\"=== INEFFICIENT: Multiple Shuffles ===\")\n",
    "\n",
    "test_df = spark.range(0, 1000000).toDF(\"id\") \\\n",
    "    .withColumn(\"category\", (col(\"id\") % 10).cast(\"string\")) \\\n",
    "    .withColumn(\"value\", (rand() * 100).cast(\"int\"))\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Separate aggregations (each triggers shuffle)\n",
    "avg_value = test_df.groupBy(\"category\").agg(avg(\"value\")).count()\n",
    "max_value = test_df.groupBy(\"category\").agg(spark_max(\"value\")).count()\n",
    "count_value = test_df.groupBy(\"category\").count().count()\n",
    "\n",
    "inefficient_time = time.time() - start\n",
    "print(f\"Time with multiple shuffles: {inefficient_time:.2f}s\")\n",
    "print(\"Note: Data was shuffled 3 times!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Efficient - Single shuffle\n",
    "print(\"\\n=== EFFICIENT: Single Shuffle ===\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Combined aggregation (one shuffle)\n",
    "combined = test_df.groupBy(\"category\").agg(\n",
    "    avg(\"value\").alias(\"avg_value\"),\n",
    "    spark_max(\"value\").alias(\"max_value\"),\n",
    "    count(\"*\").alias(\"count_value\")\n",
    ").count()\n",
    "\n",
    "efficient_time = time.time() - start\n",
    "print(f\"Time with single shuffle: {efficient_time:.2f}s\")\n",
    "print(f\"Speedup: {inefficient_time/efficient_time:.2f}x faster\")\n",
    "print(\"Note: Data shuffled only once!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter early to reduce shuffle volume\n",
    "print(\"\\n=== Filter Early ===\")\n",
    "\n",
    "large_dataset = spark.range(0, 5000000).toDF(\"id\") \\\n",
    "    .withColumn(\"category\", (col(\"id\") % 100).cast(\"string\")) \\\n",
    "    .withColumn(\"value\", (rand() * 1000).cast(\"int\"))\n",
    "\n",
    "# Inefficient: Filter after aggregation\n",
    "start = time.time()\n",
    "result1 = large_dataset.groupBy(\"category\").agg(avg(\"value\").alias(\"avg_value\")) \\\n",
    "    .filter(col(\"avg_value\") > 500) \\\n",
    "    .count()\n",
    "time1 = time.time() - start\n",
    "print(f\"Filter after aggregation: {time1:.2f}s (shuffles all data)\")\n",
    "\n",
    "# Efficient: Filter before aggregation\n",
    "start = time.time()\n",
    "result2 = large_dataset.filter(col(\"value\") > 500) \\\n",
    "    .groupBy(\"category\").agg(avg(\"value\").alias(\"avg_value\")) \\\n",
    "    .count()\n",
    "time2 = time.time() - start\n",
    "print(f\"Filter before aggregation: {time2:.2f}s (shuffles less data)\")\n",
    "print(f\"Improvement: {time1/time2:.2f}x faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Configuration Tuning\n",
    "\n",
    "**Key Configuration Parameters:**\n",
    "\n",
    "**Memory:**\n",
    "- `spark.executor.memory`: Memory per executor\n",
    "- `spark.driver.memory`: Memory for driver\n",
    "- `spark.memory.fraction`: Fraction for execution/storage (default 0.6)\n",
    "\n",
    "**Parallelism:**\n",
    "- `spark.default.parallelism`: Partitions for RDD operations\n",
    "- `spark.sql.shuffle.partitions`: Partitions for DataFrame shuffles (default 200)\n",
    "\n",
    "**Shuffle:**\n",
    "- `spark.sql.autoBroadcastJoinThreshold`: Max size for broadcast (default 10MB)\n",
    "- `spark.sql.adaptive.enabled`: Enable adaptive query execution\n",
    "\n",
    "**Rules of Thumb:**\n",
    "- Set `spark.sql.shuffle.partitions` to 2-4x number of cores\n",
    "- Each partition should be 100-200MB\n",
    "- Executor memory: 4-8GB per executor\n",
    "- Number of executors: Leave 1-2 cores for OS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current configurations\n",
    "print(\"=== Current Spark Configurations ===\")\n",
    "important_configs = [\n",
    "    \"spark.executor.memory\",\n",
    "    \"spark.driver.memory\",\n",
    "    \"spark.sql.shuffle.partitions\",\n",
    "    \"spark.default.parallelism\",\n",
    "    \"spark.sql.autoBroadcastJoinThreshold\",\n",
    "    \"spark.sql.adaptive.enabled\"\n",
    "]\n",
    "\n",
    "for config in important_configs:\n",
    "    try:\n",
    "        value = spark.conf.get(config)\n",
    "        print(f\"{config}: {value}\")\n",
    "    except:\n",
    "        print(f\"{config}: (not set)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different shuffle partition settings\n",
    "print(\"\\n=== Impact of Shuffle Partitions ===\")\n",
    "\n",
    "test_df = spark.range(0, 2000000).toDF(\"id\") \\\n",
    "    .withColumn(\"group\", (col(\"id\") % 1000).cast(\"string\"))\n",
    "\n",
    "partition_counts = [4, 8, 16, 32]\n",
    "results = []\n",
    "\n",
    "for n_partitions in partition_counts:\n",
    "    spark.conf.set(\"spark.sql.shuffle.partitions\", str(n_partitions))\n",
    "    \n",
    "    start = time.time()\n",
    "    test_df.groupBy(\"group\").count().count()\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    results.append((n_partitions, elapsed))\n",
    "    print(f\"Partitions={n_partitions:2d}: {elapsed:.2f}s\")\n",
    "\n",
    "# Find optimal\n",
    "optimal = min(results, key=lambda x: x[1])\n",
    "print(f\"\\nOptimal: {optimal[0]} partitions with time {optimal[1]:.2f}s\")\n",
    "print(\"Note: Optimal depends on data size and cluster resources\")\n",
    "\n",
    "# Reset to default\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Query Execution Plans\n",
    "\n",
    "**Understanding Execution Plans:**\n",
    "\n",
    "Spark's Catalyst optimizer creates execution plans:\n",
    "- **Logical Plan**: What to compute\n",
    "- **Optimized Plan**: How to compute (after optimizations)\n",
    "- **Physical Plan**: Actual execution strategy\n",
    "\n",
    "**Using explain():**\n",
    "- `explain()`: Shows physical plan\n",
    "- `explain(True)`: Shows all plans\n",
    "- `explain(\"formatted\")`: Pretty formatted\n",
    "\n",
    "**Look For:**\n",
    "- Shuffle operations (Exchange)\n",
    "- Join strategies (BroadcastHashJoin vs SortMergeJoin)\n",
    "- Filter pushdown\n",
    "- Column pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create query to analyze\n",
    "df1 = spark.range(0, 1000000).toDF(\"id\") \\\n",
    "    .withColumn(\"value\", (rand() * 100).cast(\"int\")) \\\n",
    "    .withColumn(\"category\", (col(\"id\") % 10).cast(\"string\"))\n",
    "\n",
    "df2 = spark.range(0, 10).toDF(\"category_id\") \\\n",
    "    .withColumn(\"category\", col(\"category_id\").cast(\"string\")) \\\n",
    "    .withColumn(\"name\", expr(\"concat('Cat_', category)\"))\n",
    "\n",
    "# Complex query\n",
    "result = df1.filter(col(\"value\") > 50) \\\n",
    "    .join(broadcast(df2), \"category\") \\\n",
    "    .groupBy(\"name\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"count\"),\n",
    "        avg(\"value\").alias(\"avg_value\")\n",
    "    ) \\\n",
    "    .orderBy(col(\"count\").desc())\n",
    "\n",
    "print(\"=== Query Execution Plan ===\")\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed explanation\n",
    "print(\"\\n=== Extended Explanation ===\")\n",
    "result.explain(True)\n",
    "\n",
    "print(\"\\n=== What to Look For ===\")\n",
    "print(\"1. BroadcastHashJoin: Good! Small table broadcasted\")\n",
    "print(\"2. Exchange: Shuffle operation (minimize these)\")\n",
    "print(\"3. Filter pushed before Join: Good! Less data to join\")\n",
    "print(\"4. Project (column pruning): Good! Only needed columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Partition Optimization\n",
    "\n",
    "Optimize a query using appropriate partitioning.\n",
    "\n",
    "**Tasks:**\n",
    "1. Create a DataFrame with 10M rows, columns: id, user_id (1000 users), value\n",
    "2. Calculate average value per user WITHOUT pre-partitioning\n",
    "3. Repartition by user_id and calculate average again\n",
    "4. Compare execution times\n",
    "5. Determine the optimal number of partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# TODO: Create DataFrame\n",
    "# TODO: Test without partitioning\n",
    "# TODO: Test with partitioning\n",
    "# TODO: Compare results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Caching Strategy\n",
    "\n",
    "Determine when caching improves performance.\n",
    "\n",
    "**Tasks:**\n",
    "1. Create an expensive DataFrame (multiple transformations)\n",
    "2. Use it 5 times in different queries WITHOUT caching\n",
    "3. Repeat WITH caching\n",
    "4. Calculate the break-even point (how many uses justify caching)\n",
    "5. Test different storage levels and compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# TODO: Create expensive computation\n",
    "# TODO: Test without caching\n",
    "# TODO: Test with caching\n",
    "# TODO: Analyze break-even point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Join Optimization\n",
    "\n",
    "Optimize a multi-table join scenario.\n",
    "\n",
    "**Tasks:**\n",
    "1. Create 3 tables: orders (large), customers (medium), products (small)\n",
    "2. Join all three tables together\n",
    "3. Optimize using broadcast joins where appropriate\n",
    "4. Compare shuffle join vs broadcast join performance\n",
    "5. Examine execution plans to verify optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# TODO: Create tables\n",
    "# TODO: Join without optimization\n",
    "# TODO: Join with broadcast\n",
    "# TODO: Compare and analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Exercise Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1: Partition Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "user_df = spark.range(0, 10000000).toDF(\"id\") \\\n",
    "    .withColumn(\"user_id\", (col(\"id\") % 1000).cast(\"string\")) \\\n",
    "    .withColumn(\"value\", (rand() * 1000).cast(\"int\"))\n",
    "\n",
    "print(f\"Created DataFrame with {user_df.count():,} rows\")\n",
    "print(f\"Default partitions: {user_df.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Without pre-partitioning\n",
    "print(\"\\n=== Without Pre-Partitioning ===\")\n",
    "start = time.time()\n",
    "result1 = user_df.groupBy(\"user_id\").agg(avg(\"value\").alias(\"avg_value\")).count()\n",
    "time_no_part = time.time() - start\n",
    "print(f\"Time: {time_no_part:.2f}s\")\n",
    "\n",
    "# With pre-partitioning\n",
    "print(\"\\n=== With Pre-Partitioning ===\")\n",
    "partitioned_df = user_df.repartition(20, \"user_id\")\n",
    "\n",
    "start = time.time()\n",
    "result2 = partitioned_df.groupBy(\"user_id\").agg(avg(\"value\").alias(\"avg_value\")).count()\n",
    "time_with_part = time.time() - start\n",
    "print(f\"Time: {time_with_part:.2f}s\")\n",
    "print(f\"Speedup: {time_no_part/time_with_part:.2f}x\")\n",
    "\n",
    "# Test different partition counts\n",
    "print(\"\\n=== Testing Different Partition Counts ===\")\n",
    "for n_parts in [10, 20, 30, 40]:\n",
    "    test_df = user_df.repartition(n_parts, \"user_id\")\n",
    "    start = time.time()\n",
    "    test_df.groupBy(\"user_id\").agg(avg(\"value\")).count()\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"Partitions={n_parts}: {elapsed:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 2: Caching Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create expensive DataFrame\n",
    "expensive = spark.range(0, 3000000).toDF(\"id\") \\\n",
    "    .withColumn(\"x\", expr(\"sin(id / 1000.0) * 100\")) \\\n",
    "    .withColumn(\"y\", expr(\"cos(id / 1000.0) * 100\")) \\\n",
    "    .withColumn(\"z\", expr(\"sqrt(abs(id)) / 10\"))\n",
    "\n",
    "# Test different usage counts\n",
    "print(\"=== Caching Break-Even Analysis ===\")\n",
    "\n",
    "for num_uses in [1, 2, 3, 4, 5]:\n",
    "    # Without caching\n",
    "    start = time.time()\n",
    "    for _ in range(num_uses):\n",
    "        expensive.filter(col(\"x\") > 0).count()\n",
    "    time_no_cache = time.time() - start\n",
    "    \n",
    "    # With caching\n",
    "    cached = expensive.cache()\n",
    "    start = time.time()\n",
    "    for _ in range(num_uses):\n",
    "        cached.filter(col(\"x\") > 0).count()\n",
    "    time_with_cache = time.time() - start\n",
    "    cached.unpersist()\n",
    "    \n",
    "    speedup = time_no_cache / time_with_cache\n",
    "    print(f\"Uses={num_uses}: No cache={time_no_cache:.2f}s, Cache={time_with_cache:.2f}s, Speedup={speedup:.2f}x\")\n",
    "    \n",
    "print(\"\\nConclusion: Caching beneficial when DataFrame used 2+ times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 3: Join Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tables\n",
    "orders = spark.range(0, 1000000).toDF(\"order_id\") \\\n",
    "    .withColumn(\"customer_id\", (col(\"order_id\") % 10000).cast(\"int\")) \\\n",
    "    .withColumn(\"product_id\", (col(\"order_id\") % 100).cast(\"int\")) \\\n",
    "    .withColumn(\"amount\", (rand() * 1000).cast(\"int\"))\n",
    "\n",
    "customers = spark.range(0, 10000).toDF(\"customer_id\") \\\n",
    "    .withColumn(\"customer_name\", expr(\"concat('Customer_', customer_id)\"))\n",
    "\n",
    "products = spark.range(0, 100).toDF(\"product_id\") \\\n",
    "    .withColumn(\"product_name\", expr(\"concat('Product_', product_id)\")) \\\n",
    "    .withColumn(\"category\", \n",
    "                when(col(\"product_id\") < 33, \"Electronics\")\n",
    "                .when(col(\"product_id\") < 66, \"Clothing\")\n",
    "                .otherwise(\"Home\"))\n",
    "\n",
    "print(f\"Orders: {orders.count():,} rows\")\n",
    "print(f\"Customers: {customers.count():,} rows\")\n",
    "print(f\"Products: {products.count():,} rows\")\n",
    "\n",
    "# Disable auto broadcast\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "\n",
    "# Without optimization (shuffle joins)\n",
    "print(\"\\n=== Without Broadcast (Shuffle Joins) ===\")\n",
    "start = time.time()\n",
    "result_shuffle = orders \\\n",
    "    .join(customers, \"customer_id\") \\\n",
    "    .join(products, \"product_id\") \\\n",
    "    .groupBy(\"category\") \\\n",
    "    .agg(spark_sum(\"amount\").alias(\"total\")) \\\n",
    "    .count()\n",
    "time_shuffle = time.time() - start\n",
    "print(f\"Time: {time_shuffle:.2f}s\")\n",
    "\n",
    "# With broadcast optimization\n",
    "print(\"\\n=== With Broadcast ===\")\n",
    "start = time.time()\n",
    "result_broadcast = orders \\\n",
    "    .join(broadcast(customers), \"customer_id\") \\\n",
    "    .join(broadcast(products), \"product_id\") \\\n",
    "    .groupBy(\"category\") \\\n",
    "    .agg(spark_sum(\"amount\").alias(\"total\")) \\\n",
    "    .count()\n",
    "time_broadcast = time.time() - start\n",
    "print(f\"Time: {time_broadcast:.2f}s\")\n",
    "print(f\"Speedup: {time_shuffle/time_broadcast:.2f}x faster\")\n",
    "\n",
    "# Reset\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"10485760\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "Congratulations! You've learned advanced performance optimization techniques for Spark.\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Partitioning:**\n",
    "   - Use `repartition()` to increase partitions or redistribute evenly\n",
    "   - Use `coalesce()` to decrease partitions efficiently\n",
    "   - Pre-partition by groupBy/join keys to avoid shuffles\n",
    "   - Optimal: 2-4 partitions per CPU core, 100-200MB per partition\n",
    "\n",
    "2. **Caching:**\n",
    "   - Cache DataFrames used multiple times (2+)\n",
    "   - Use `MEMORY_AND_DISK` for production (fault tolerant)\n",
    "   - Always `unpersist()` when done to free memory\n",
    "   - Cache after expensive computations, before multiple uses\n",
    "\n",
    "3. **Broadcast Joins:**\n",
    "   - Dramatically faster for small-to-large joins\n",
    "   - Use for dimension tables < 1GB\n",
    "   - Explicit `broadcast()` or tune `autoBroadcastJoinThreshold`\n",
    "   - Eliminates shuffle for the small table\n",
    "\n",
    "4. **Minimizing Shuffles:**\n",
    "   - Combine aggregations into single operation\n",
    "   - Filter data early to reduce shuffle volume\n",
    "   - Use broadcast joins when possible\n",
    "   - Pre-partition data by common keys\n",
    "\n",
    "5. **Configuration:**\n",
    "   - Tune `spark.sql.shuffle.partitions` based on data size\n",
    "   - Adjust `autoBroadcastJoinThreshold` for your use case\n",
    "   - Monitor and tune memory settings\n",
    "   - Enable adaptive query execution for automatic optimization\n",
    "\n",
    "6. **Execution Plans:**\n",
    "   - Use `explain()` to understand query execution\n",
    "   - Look for shuffle operations (Exchange)\n",
    "   - Verify broadcast joins (BroadcastHashJoin)\n",
    "   - Check for filter pushdown and column pruning\n",
    "\n",
    "### Performance Checklist:\n",
    "\n",
    "Before deploying to production:\n",
    "- [ ] Appropriate number of partitions configured\n",
    "- [ ] Expensive computations cached when reused\n",
    "- [ ] Small tables broadcasted in joins\n",
    "- [ ] Filters applied as early as possible\n",
    "- [ ] Aggregations combined when possible\n",
    "- [ ] Execution plans reviewed for inefficiencies\n",
    "- [ ] Shuffle operations minimized\n",
    "- [ ] Memory settings tuned for workload\n",
    "\n",
    "### Common Pitfalls:\n",
    "\n",
    "- Over-partitioning: Too many small partitions (overhead)\n",
    "- Under-partitioning: Too few large partitions (poor parallelism)\n",
    "- Caching everything: Wastes memory\n",
    "- Not unpersisting: Memory leaks\n",
    "- Shuffle joins for small tables: Use broadcast\n",
    "- Multiple separate aggregations: Combine them\n",
    "- Late filtering: Filter early to reduce data volume\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In [Module 13: Spark on Clusters](13_spark_on_clusters.ipynb), you'll learn:\n",
    "- Cluster architecture and deployment modes\n",
    "- Resource allocation strategies\n",
    "- Submitting applications with spark-submit\n",
    "- Monitoring and troubleshooting cluster jobs\n",
    "- Production deployment best practices\n",
    "\n",
    "### Additional Resources:\n",
    "\n",
    "- [Tuning Spark](https://spark.apache.org/docs/latest/tuning.html)\n",
    "- [Performance Tuning Guide](https://spark.apache.org/docs/latest/sql-performance-tuning.html)\n",
    "- [Memory Management](https://spark.apache.org/docs/latest/tuning.html#memory-management-overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "spark.stop()\n",
    "print(\"Spark session stopped. Excellent work on performance optimization!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
