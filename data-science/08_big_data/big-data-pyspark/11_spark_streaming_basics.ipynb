{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 11: Spark Streaming Basics\n",
    "\n",
    "**Difficulty**: ⭐⭐⭐  \n",
    "**Estimated Time**: 80 minutes  \n",
    "**Prerequisites**: \n",
    "- [Module 03: DataFrames and Datasets](03_dataframes_and_datasets.ipynb)\n",
    "- [Module 05: DataFrame Operations](05_dataframe_operations.ipynb)\n",
    "- Understanding of streaming concepts\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Understand Structured Streaming architecture and concepts (micro-batches, watermarks, triggers)\n",
    "2. Read streaming data from various sources (files, sockets, generated streams)\n",
    "3. Apply window operations (tumbling and sliding windows) for time-based aggregations\n",
    "4. Use different output modes (append, complete, update) appropriately\n",
    "5. Build real-time streaming applications with stateful operations and aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Introduction\n",
    "\n",
    "**What is Spark Structured Streaming?**\n",
    "\n",
    "Structured Streaming is a scalable and fault-tolerant stream processing engine built on the Spark SQL engine. It treats streaming data as a table that is continuously appended to.\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "1. **Streaming DataFrame**: Unbounded table that grows as new data arrives\n",
    "2. **Micro-batch Processing**: Processes data in small batches (default: every second)\n",
    "3. **Trigger**: Defines when Spark should process new data (e.g., every 5 seconds)\n",
    "4. **Watermark**: Handles late-arriving data in time-windowed operations\n",
    "5. **State Management**: Maintains intermediate state for aggregations\n",
    "\n",
    "**Output Modes:**\n",
    "\n",
    "- **Append**: Only new rows are written to the sink (for non-aggregated queries)\n",
    "- **Complete**: Entire result table is written every trigger (for aggregations)\n",
    "- **Update**: Only updated rows are written (for aggregations, efficient)\n",
    "\n",
    "**Sources:**\n",
    "- File sources (JSON, CSV, Parquet)\n",
    "- Socket sources (for testing)\n",
    "- Kafka (production message queues)\n",
    "- Rate source (for testing, generates data)\n",
    "\n",
    "**Sinks:**\n",
    "- Console (for debugging)\n",
    "- File sink (Parquet, JSON, CSV)\n",
    "- Memory sink (for testing)\n",
    "- Kafka sink (for downstream processing)\n",
    "- ForeachBatch (custom logic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, window, count, sum as spark_sum, avg, max as spark_max, min as spark_min,\n",
    "    current_timestamp, to_timestamp, date_format, hour, minute,\n",
    "    explode, split, length, lower, when, lit, expr\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, \n",
    "    DoubleType, TimestampType, LongType\n",
    ")\n",
    "\n",
    "import time\n",
    "import random\n",
    "import threading\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark session for streaming\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark Streaming Basics\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.sql.streaming.checkpointLocation\", \"/tmp/checkpoint\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(\"Spark session created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reading Streaming Data\n",
    "\n",
    "We'll start with the **Rate Source**, which generates streaming data automatically.\n",
    "\n",
    "**Rate Source** generates rows with:\n",
    "- `timestamp`: The time the row was generated\n",
    "- `value`: A counter (0, 1, 2, ...)\n",
    "\n",
    "**Parameters:**\n",
    "- `rowsPerSecond`: How many rows to generate per second\n",
    "- `rampUpTime`: Time to reach full rate (optional)\n",
    "- `numPartitions`: Number of partitions for generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a streaming DataFrame using rate source\n",
    "# Generates 5 rows per second\n",
    "rate_stream = spark.readStream \\\n",
    "    .format(\"rate\") \\\n",
    "    .option(\"rowsPerSecond\", 5) \\\n",
    "    .load()\n",
    "\n",
    "# Check if it's a streaming DataFrame\n",
    "print(f\"Is streaming: {rate_stream.isStreaming}\")\n",
    "print(\"\\nSchema:\")\n",
    "rate_stream.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to console sink to see the data\n",
    "# This starts a streaming query\n",
    "query = rate_stream.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .option(\"numRows\", 10) \\\n",
    "    .start()\n",
    "\n",
    "print(\"Streaming query started. Will show data for 10 seconds...\")\n",
    "time.sleep(10)  # Let it run for 10 seconds\n",
    "\n",
    "# Stop the query\n",
    "query.stop()\n",
    "print(\"\\nQuery stopped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic Stream Transformations\n",
    "\n",
    "Streaming DataFrames support most of the same operations as batch DataFrames:\n",
    "- `select`, `filter`, `where`\n",
    "- `withColumn`, `withColumnRenamed`\n",
    "- Aggregations (with special considerations)\n",
    "\n",
    "Let's create a more realistic streaming scenario: **sensor data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform rate stream into sensor data\n",
    "# Simulate temperature sensors from different rooms\n",
    "sensor_stream = rate_stream \\\n",
    "    .select(\n",
    "        col(\"timestamp\"),\n",
    "        col(\"value\"),\n",
    "        # Simulate room IDs (Room_0 to Room_4)\n",
    "        (col(\"value\") % 5).cast(\"string\").alias(\"room_id\"),\n",
    "        # Simulate temperature readings (18-28°C with some variation)\n",
    "        (20 + (col(\"value\") % 10) + (col(\"value\") % 3) / 10.0).alias(\"temperature\"),\n",
    "        # Simulate humidity (40-70%)\n",
    "        (50 + (col(\"value\") % 20)).alias(\"humidity\")\n",
    "    ) \\\n",
    "    .withColumn(\"room_name\", expr(\"concat('Room_', room_id)\")) \\\n",
    "    .select(\"timestamp\", \"room_name\", \"temperature\", \"humidity\")\n",
    "\n",
    "print(\"Sensor stream schema:\")\n",
    "sensor_stream.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter and transform the stream\n",
    "# Alert when temperature is too high (>25°C)\n",
    "high_temp_stream = sensor_stream \\\n",
    "    .filter(col(\"temperature\") > 25) \\\n",
    "    .select(\n",
    "        col(\"timestamp\"),\n",
    "        col(\"room_name\"),\n",
    "        col(\"temperature\"),\n",
    "        lit(\"HIGH TEMPERATURE ALERT\").alias(\"alert_type\")\n",
    "    )\n",
    "\n",
    "# Start query to see high temperature alerts\n",
    "alert_query = high_temp_stream.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"High temperature alert stream started. Running for 10 seconds...\")\n",
    "time.sleep(10)\n",
    "\n",
    "alert_query.stop()\n",
    "print(\"Alert query stopped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Aggregations in Streaming\n",
    "\n",
    "**Streaming aggregations** are more complex than batch because:\n",
    "- Data arrives continuously\n",
    "- We need to maintain state\n",
    "- Late-arriving data must be handled\n",
    "\n",
    "**Types of Aggregations:**\n",
    "\n",
    "1. **Global Aggregations**: Aggregate all data (needs `complete` mode)\n",
    "2. **Grouped Aggregations**: Group by key (can use `update` mode)\n",
    "3. **Windowed Aggregations**: Group by time windows (append/update/complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global aggregation: count all events\n",
    "# Requires outputMode(\"complete\") because we're maintaining global state\n",
    "global_count = sensor_stream \\\n",
    "    .groupBy() \\\n",
    "    .count()\n",
    "\n",
    "global_query = global_count.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"Global count query started. Running for 10 seconds...\")\n",
    "time.sleep(10)\n",
    "\n",
    "global_query.stop()\n",
    "print(\"Global count query stopped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouped aggregation: statistics per room\n",
    "# Can use outputMode(\"update\") to only show changed rows\n",
    "room_stats = sensor_stream \\\n",
    "    .groupBy(\"room_name\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"num_readings\"),\n",
    "        avg(\"temperature\").alias(\"avg_temperature\"),\n",
    "        spark_max(\"temperature\").alias(\"max_temperature\"),\n",
    "        spark_min(\"temperature\").alias(\"min_temperature\"),\n",
    "        avg(\"humidity\").alias(\"avg_humidity\")\n",
    "    )\n",
    "\n",
    "room_query = room_stats.writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"Room statistics query started. Running for 15 seconds...\")\n",
    "time.sleep(15)\n",
    "\n",
    "room_query.stop()\n",
    "print(\"Room statistics query stopped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Window Operations\n",
    "\n",
    "**Time Windows** allow aggregating data over sliding or tumbling time intervals.\n",
    "\n",
    "**Tumbling Windows:**\n",
    "- Non-overlapping, fixed-size intervals\n",
    "- Example: Every 1 minute (00:00-01:00, 01:00-02:00, ...)\n",
    "- Syntax: `window(col(\"timestamp\"), \"1 minute\")`\n",
    "\n",
    "**Sliding Windows:**\n",
    "- Overlapping intervals\n",
    "- Example: 10-minute window, sliding every 5 minutes\n",
    "- Syntax: `window(col(\"timestamp\"), \"10 minutes\", \"5 minutes\")`\n",
    "\n",
    "**Window Column:**\n",
    "- Contains `start` and `end` timestamps\n",
    "- Can be used in `groupBy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tumbling window: 10-second windows\n",
    "# Count events in each 10-second window\n",
    "tumbling_window = sensor_stream \\\n",
    "    .groupBy(\n",
    "        window(col(\"timestamp\"), \"10 seconds\")\n",
    "    ) \\\n",
    "    .count() \\\n",
    "    .select(\n",
    "        col(\"window.start\").alias(\"window_start\"),\n",
    "        col(\"window.end\").alias(\"window_end\"),\n",
    "        col(\"count\")\n",
    "    )\n",
    "\n",
    "tumbling_query = tumbling_window.writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"Tumbling window query (10-second windows) started...\")\n",
    "time.sleep(25)  # Run long enough to see multiple windows\n",
    "\n",
    "tumbling_query.stop()\n",
    "print(\"Tumbling window query stopped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sliding window: 20-second window, sliding every 10 seconds\n",
    "# This creates overlapping windows\n",
    "sliding_window = sensor_stream \\\n",
    "    .groupBy(\n",
    "        window(col(\"timestamp\"), \"20 seconds\", \"10 seconds\"),\n",
    "        col(\"room_name\")\n",
    "    ) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"num_readings\"),\n",
    "        avg(\"temperature\").alias(\"avg_temp\")\n",
    "    ) \\\n",
    "    .select(\n",
    "        col(\"room_name\"),\n",
    "        col(\"window.start\").alias(\"window_start\"),\n",
    "        col(\"window.end\").alias(\"window_end\"),\n",
    "        col(\"num_readings\"),\n",
    "        col(\"avg_temp\")\n",
    "    )\n",
    "\n",
    "sliding_query = sliding_window.writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"Sliding window query (20s window, 10s slide) started...\")\n",
    "time.sleep(30)\n",
    "\n",
    "sliding_query.stop()\n",
    "print(\"Sliding window query stopped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Watermarking for Late Data\n",
    "\n",
    "**Problem**: In real-world streaming, data can arrive late (network delays, clock skew, etc.)\n",
    "\n",
    "**Solution**: Watermarks define how long to wait for late data\n",
    "\n",
    "**How it works:**\n",
    "- Watermark = `max_event_time - threshold`\n",
    "- Events older than watermark are dropped\n",
    "- Allows cleaning up old state\n",
    "\n",
    "**Example:**\n",
    "- If watermark is \"10 minutes\"\n",
    "- And latest event time is 12:30\n",
    "- Then events before 12:20 will be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window aggregation with watermark\n",
    "# Wait up to 30 seconds for late data\n",
    "watermarked_stream = sensor_stream \\\n",
    "    .withWatermark(\"timestamp\", \"30 seconds\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"timestamp\"), \"10 seconds\"),\n",
    "        col(\"room_name\")\n",
    "    ) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"count\"),\n",
    "        avg(\"temperature\").alias(\"avg_temp\")\n",
    "    ) \\\n",
    "    .select(\n",
    "        col(\"room_name\"),\n",
    "        col(\"window.start\").alias(\"start\"),\n",
    "        col(\"window.end\").alias(\"end\"),\n",
    "        col(\"count\"),\n",
    "        col(\"avg_temp\")\n",
    "    )\n",
    "\n",
    "# With watermark, we can use append mode for windowed aggregations\n",
    "watermark_query = watermarked_stream.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"Watermarked window query started (30s watermark)...\")\n",
    "print(\"Windows will be finalized after watermark delay...\")\n",
    "time.sleep(45)  # Need to wait longer than watermark\n",
    "\n",
    "watermark_query.stop()\n",
    "print(\"Watermarked query stopped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Output Modes in Detail\n",
    "\n",
    "Let's demonstrate all three output modes with the same query.\n",
    "\n",
    "**Append Mode:**\n",
    "- Only outputs new rows\n",
    "- Works for: non-aggregated queries, watermarked aggregations\n",
    "- Doesn't work for: aggregations without watermark\n",
    "\n",
    "**Complete Mode:**\n",
    "- Outputs entire result table every time\n",
    "- Works for: aggregations\n",
    "- Expensive for large result sets\n",
    "\n",
    "**Update Mode:**\n",
    "- Only outputs rows that changed since last trigger\n",
    "- Works for: aggregations\n",
    "- Most efficient for aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same aggregation query to test different modes\n",
    "test_agg = sensor_stream \\\n",
    "    .groupBy(\"room_name\") \\\n",
    "    .agg(count(\"*\").alias(\"count\"))\n",
    "\n",
    "# Complete mode: Shows ALL rooms every time\n",
    "print(\"\\n=== COMPLETE MODE ===\")\n",
    "print(\"Shows all rooms every trigger (even unchanged ones)\\n\")\n",
    "\n",
    "complete_query = test_agg.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .trigger(processingTime=\"5 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "time.sleep(12)\n",
    "complete_query.stop()\n",
    "print(\"\\nComplete mode query stopped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update mode: Shows only CHANGED rooms\n",
    "print(\"\\n=== UPDATE MODE ===\")\n",
    "print(\"Shows only rooms with new data since last trigger\\n\")\n",
    "\n",
    "update_query = test_agg.writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"console\") \\\n",
    "    .trigger(processingTime=\"5 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "time.sleep(12)\n",
    "update_query.stop()\n",
    "print(\"\\nUpdate mode query stopped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Triggers\n",
    "\n",
    "**Triggers** control when Spark processes new data.\n",
    "\n",
    "**Types:**\n",
    "\n",
    "1. **Default (micro-batch)**: Processes as soon as previous batch finishes\n",
    "2. **Fixed interval**: `trigger(processingTime=\"10 seconds\")`\n",
    "3. **One-time**: `trigger(once=True)` - processes all available data and stops\n",
    "4. **Continuous**: Low-latency (experimental)\n",
    "\n",
    "**Choosing triggers:**\n",
    "- **High throughput**: Default or long intervals (e.g., 1 minute)\n",
    "- **Low latency**: Short intervals (e.g., 1 second) or continuous\n",
    "- **Batch-like**: One-time trigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed interval trigger: Process every 5 seconds\n",
    "triggered_stream = sensor_stream \\\n",
    "    .groupBy(\"room_name\") \\\n",
    "    .count()\n",
    "\n",
    "print(\"Starting query with 5-second trigger interval...\")\n",
    "print(\"Notice it processes exactly every 5 seconds\\n\")\n",
    "\n",
    "trigger_query = triggered_stream.writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"console\") \\\n",
    "    .trigger(processingTime=\"5 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "time.sleep(16)  # See 3 triggers\n",
    "trigger_query.stop()\n",
    "print(\"Triggered query stopped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Memory Sink for Testing\n",
    "\n",
    "**Memory sink** writes to an in-memory table that can be queried.\n",
    "\n",
    "**Use cases:**\n",
    "- Testing streaming logic\n",
    "- Debugging aggregations\n",
    "- Quick prototyping\n",
    "\n",
    "**Warning**: Only for testing! Doesn't persist and uses memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to memory sink with table name\n",
    "memory_stream = sensor_stream \\\n",
    "    .groupBy(\"room_name\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_readings\"),\n",
    "        avg(\"temperature\").alias(\"avg_temp\"),\n",
    "        avg(\"humidity\").alias(\"avg_humidity\")\n",
    "    )\n",
    "\n",
    "memory_query = memory_stream.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"room_stats_table\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"Memory sink query started. Letting it accumulate data...\")\n",
    "time.sleep(10)\n",
    "\n",
    "# Query the in-memory table\n",
    "print(\"\\nQuerying the in-memory table:\")\n",
    "spark.sql(\"SELECT * FROM room_stats_table ORDER BY room_name\").show(truncate=False)\n",
    "\n",
    "# Wait and query again to see updates\n",
    "time.sleep(5)\n",
    "print(\"\\nQuerying again after 5 more seconds:\")\n",
    "spark.sql(\"SELECT * FROM room_stats_table ORDER BY room_name\").show(truncate=False)\n",
    "\n",
    "memory_query.stop()\n",
    "print(\"\\nMemory query stopped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Click Stream Analysis\n",
    "\n",
    "Simulate and analyze website click stream data.\n",
    "\n",
    "**Tasks:**\n",
    "1. Create a streaming source using rate source\n",
    "2. Transform into click events with: user_id, page, action (view/click/purchase)\n",
    "3. Count actions per page in 30-second tumbling windows\n",
    "4. Filter for 'purchase' actions and display in real-time\n",
    "5. Run for 1 minute and observe results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# TODO: Create click stream\n",
    "# TODO: Add window aggregations\n",
    "# TODO: Filter and display purchases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Real-time Stock Price Monitoring\n",
    "\n",
    "Build a real-time stock price monitoring system.\n",
    "\n",
    "**Tasks:**\n",
    "1. Generate streaming stock prices (symbol, price, volume)\n",
    "2. Calculate 1-minute moving average for each stock\n",
    "3. Alert when price changes by >5% in a 1-minute window\n",
    "4. Display top 3 stocks by volume every 30 seconds\n",
    "5. Use update mode for efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# TODO: Generate stock price stream\n",
    "# TODO: Calculate moving averages\n",
    "# TODO: Implement price change alerts\n",
    "# TODO: Track volume leaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: IoT Device Monitoring with Watermarks\n",
    "\n",
    "Monitor IoT devices and handle late-arriving data.\n",
    "\n",
    "**Tasks:**\n",
    "1. Simulate IoT device data (device_id, metric_type, value, timestamp)\n",
    "2. Add watermark of 1 minute to handle late data\n",
    "3. Calculate statistics per device in 2-minute sliding windows (slide: 1 minute)\n",
    "4. Use append mode to only output finalized windows\n",
    "5. Verify that late data beyond watermark is dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# TODO: Generate IoT data stream\n",
    "# TODO: Add watermark\n",
    "# TODO: Implement sliding windows\n",
    "# TODO: Use append mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Exercise Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1: Click Stream Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate click stream data\n",
    "click_stream = spark.readStream \\\n",
    "    .format(\"rate\") \\\n",
    "    .option(\"rowsPerSecond\", 10) \\\n",
    "    .load() \\\n",
    "    .select(\n",
    "        col(\"timestamp\"),\n",
    "        (col(\"value\") % 100).alias(\"user_id\"),\n",
    "        expr(\"CASE WHEN value % 5 = 0 THEN 'home' \"\n",
    "             \"WHEN value % 5 = 1 THEN 'products' \"\n",
    "             \"WHEN value % 5 = 2 THEN 'cart' \"\n",
    "             \"WHEN value % 5 = 3 THEN 'checkout' \"\n",
    "             \"ELSE 'about' END\").alias(\"page\"),\n",
    "        expr(\"CASE WHEN value % 10 < 6 THEN 'view' \"\n",
    "             \"WHEN value % 10 < 9 THEN 'click' \"\n",
    "             \"ELSE 'purchase' END\").alias(\"action\")\n",
    "    )\n",
    "\n",
    "# Window aggregation: count actions per page in 30-second windows\n",
    "click_agg = click_stream \\\n",
    "    .groupBy(\n",
    "        window(col(\"timestamp\"), \"30 seconds\"),\n",
    "        col(\"page\"),\n",
    "        col(\"action\")\n",
    "    ) \\\n",
    "    .count() \\\n",
    "    .select(\n",
    "        col(\"window.start\").alias(\"window_start\"),\n",
    "        col(\"page\"),\n",
    "        col(\"action\"),\n",
    "        col(\"count\")\n",
    "    )\n",
    "\n",
    "# Start aggregation query\n",
    "agg_query = click_agg.writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .start()\n",
    "\n",
    "# Filter for purchases\n",
    "purchases = click_stream \\\n",
    "    .filter(col(\"action\") == \"purchase\") \\\n",
    "    .select(\"timestamp\", \"user_id\", \"page\")\n",
    "\n",
    "purchase_query = purchases.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"Click stream analysis started. Running for 1 minute...\\n\")\n",
    "time.sleep(60)\n",
    "\n",
    "agg_query.stop()\n",
    "purchase_query.stop()\n",
    "print(\"\\nClick stream queries stopped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 2: Real-time Stock Price Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate stock price stream\n",
    "stocks = [\"AAPL\", \"GOOGL\", \"MSFT\", \"AMZN\", \"TSLA\"]\n",
    "\n",
    "stock_stream = spark.readStream \\\n",
    "    .format(\"rate\") \\\n",
    "    .option(\"rowsPerSecond\", 15) \\\n",
    "    .load() \\\n",
    "    .select(\n",
    "        col(\"timestamp\"),\n",
    "        expr(f\"CASE WHEN value % 5 = 0 THEN '{stocks[0]}' \"\n",
    "             f\"WHEN value % 5 = 1 THEN '{stocks[1]}' \"\n",
    "             f\"WHEN value % 5 = 2 THEN '{stocks[2]}' \"\n",
    "             f\"WHEN value % 5 = 3 THEN '{stocks[3]}' \"\n",
    "             f\"ELSE '{stocks[4]}' END\").alias(\"symbol\"),\n",
    "        (100 + (col(\"value\") % 50) + (col(\"value\") % 10) / 10.0).alias(\"price\"),\n",
    "        (1000 + (col(\"value\") % 500) * 100).alias(\"volume\")\n",
    "    )\n",
    "\n",
    "# 1-minute moving average per stock\n",
    "moving_avg = stock_stream \\\n",
    "    .withWatermark(\"timestamp\", \"2 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"timestamp\"), \"1 minute\"),\n",
    "        col(\"symbol\")\n",
    "    ) \\\n",
    "    .agg(\n",
    "        avg(\"price\").alias(\"avg_price\"),\n",
    "        spark_min(\"price\").alias(\"min_price\"),\n",
    "        spark_max(\"price\").alias(\"max_price\"),\n",
    "        spark_sum(\"volume\").alias(\"total_volume\")\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        \"price_change_pct\",\n",
    "        ((col(\"max_price\") - col(\"min_price\")) / col(\"min_price\") * 100)\n",
    "    ) \\\n",
    "    .select(\n",
    "        col(\"symbol\"),\n",
    "        col(\"window.start\").alias(\"window_start\"),\n",
    "        col(\"avg_price\"),\n",
    "        col(\"min_price\"),\n",
    "        col(\"max_price\"),\n",
    "        col(\"price_change_pct\"),\n",
    "        col(\"total_volume\")\n",
    "    )\n",
    "\n",
    "# Alert on >5% price change\n",
    "price_alerts = moving_avg \\\n",
    "    .filter(col(\"price_change_pct\") > 5) \\\n",
    "    .select(\n",
    "        col(\"symbol\"),\n",
    "        col(\"window_start\"),\n",
    "        col(\"price_change_pct\"),\n",
    "        lit(\"SIGNIFICANT PRICE MOVEMENT\").alias(\"alert\")\n",
    "    )\n",
    "\n",
    "# Start queries\n",
    "avg_query = moving_avg.writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .trigger(processingTime=\"30 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "alert_query = price_alerts.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"Stock monitoring started. Running for 90 seconds...\\n\")\n",
    "time.sleep(90)\n",
    "\n",
    "avg_query.stop()\n",
    "alert_query.stop()\n",
    "print(\"\\nStock monitoring stopped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 3: IoT Device Monitoring with Watermarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate IoT device stream\n",
    "iot_stream = spark.readStream \\\n",
    "    .format(\"rate\") \\\n",
    "    .option(\"rowsPerSecond\", 8) \\\n",
    "    .load() \\\n",
    "    .select(\n",
    "        col(\"timestamp\"),\n",
    "        expr(\"concat('device_', cast(value % 10 as string))\").alias(\"device_id\"),\n",
    "        expr(\"CASE WHEN value % 3 = 0 THEN 'temperature' \"\n",
    "             \"WHEN value % 3 = 1 THEN 'humidity' \"\n",
    "             \"ELSE 'pressure' END\").alias(\"metric_type\"),\n",
    "        (20 + (col(\"value\") % 30) + (col(\"value\") % 5) / 10.0).alias(\"value\")\n",
    "    )\n",
    "\n",
    "# Apply watermark and sliding windows\n",
    "iot_stats = iot_stream \\\n",
    "    .withWatermark(\"timestamp\", \"1 minute\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"timestamp\"), \"2 minutes\", \"1 minute\"),\n",
    "        col(\"device_id\"),\n",
    "        col(\"metric_type\")\n",
    "    ) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"num_readings\"),\n",
    "        avg(\"value\").alias(\"avg_value\"),\n",
    "        spark_min(\"value\").alias(\"min_value\"),\n",
    "        spark_max(\"value\").alias(\"max_value\")\n",
    "    ) \\\n",
    "    .select(\n",
    "        col(\"device_id\"),\n",
    "        col(\"metric_type\"),\n",
    "        col(\"window.start\").alias(\"window_start\"),\n",
    "        col(\"window.end\").alias(\"window_end\"),\n",
    "        col(\"num_readings\"),\n",
    "        col(\"avg_value\"),\n",
    "        col(\"min_value\"),\n",
    "        col(\"max_value\")\n",
    "    )\n",
    "\n",
    "# Use append mode (only works with watermark)\n",
    "iot_query = iot_stats.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"IoT monitoring with watermark started...\")\n",
    "print(\"Windows will be finalized 1 minute after their end time.\\n\")\n",
    "time.sleep(180)  # Run for 3 minutes to see multiple windows\n",
    "\n",
    "iot_query.stop()\n",
    "print(\"\\nIoT monitoring stopped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary\n",
    "\n",
    "Congratulations! You've learned the fundamentals of Spark Structured Streaming.\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Structured Streaming Basics**:\n",
    "   - Streaming DataFrames are unbounded tables\n",
    "   - Same API as batch DataFrames\n",
    "   - Micro-batch processing model\n",
    "   - Fault-tolerant with checkpointing\n",
    "\n",
    "2. **Data Sources**:\n",
    "   - Rate source: For testing and demos\n",
    "   - File source: JSON, CSV, Parquet from directories\n",
    "   - Socket source: For simple network streams\n",
    "   - Kafka: Production message queues (not covered here)\n",
    "\n",
    "3. **Aggregations**:\n",
    "   - Global aggregations: Need complete mode\n",
    "   - Grouped aggregations: Can use update mode\n",
    "   - Windowed aggregations: Time-based grouping\n",
    "   - State is automatically managed by Spark\n",
    "\n",
    "4. **Window Operations**:\n",
    "   - Tumbling windows: Non-overlapping, fixed intervals\n",
    "   - Sliding windows: Overlapping intervals\n",
    "   - Used with `window()` function in `groupBy`\n",
    "   - Essential for time-series analytics\n",
    "\n",
    "5. **Watermarks**:\n",
    "   - Handle late-arriving data\n",
    "   - Allow state cleanup\n",
    "   - Enable append mode for aggregations\n",
    "   - Balance between completeness and resource usage\n",
    "\n",
    "6. **Output Modes**:\n",
    "   - Append: New rows only (non-agg or watermarked agg)\n",
    "   - Complete: Full result every time (memory intensive)\n",
    "   - Update: Only changed rows (most efficient for agg)\n",
    "\n",
    "7. **Triggers**:\n",
    "   - Control processing frequency\n",
    "   - Trade-off between latency and throughput\n",
    "   - Fixed intervals for predictable processing\n",
    "   - One-time for batch-like processing\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "- Use watermarks for time-windowed aggregations to prevent unbounded state\n",
    "- Choose appropriate trigger intervals based on latency requirements\n",
    "- Use update mode instead of complete mode when possible (more efficient)\n",
    "- Always configure checkpoint location for production\n",
    "- Monitor streaming query progress and metrics\n",
    "- Test with rate/socket sources before deploying with Kafka\n",
    "- Handle schema evolution carefully in production\n",
    "\n",
    "### Production Considerations:\n",
    "\n",
    "- **Checkpointing**: Enable for fault tolerance (`checkpointLocation`)\n",
    "- **Monitoring**: Track metrics like processing time, input rate, batch duration\n",
    "- **Backpressure**: Configure max offsets per trigger to prevent overload\n",
    "- **State Management**: Tune state store configurations for large state\n",
    "- **Resource Allocation**: Allocate enough memory for state and processing\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In [Module 12: Performance Optimization](12_performance_optimization.ipynb), you'll learn:\n",
    "- Partitioning strategies for optimal data distribution\n",
    "- Caching and persistence levels\n",
    "- Broadcast variables and joins\n",
    "- Avoiding shuffle operations\n",
    "- Performance tuning and monitoring\n",
    "\n",
    "### Additional Resources:\n",
    "\n",
    "- [Structured Streaming Programming Guide](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)\n",
    "- [Structured Streaming + Kafka Integration](https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html)\n",
    "- [Watermarking in Structured Streaming](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#handling-late-data-and-watermarking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "spark.stop()\n",
    "print(\"Spark session stopped. Excellent work on streaming!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
