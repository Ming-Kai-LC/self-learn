{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 02: RDD Basics (Resilient Distributed Datasets)\n",
    "\n",
    "**Difficulty**: â­â­\n",
    "\n",
    "**Estimated Time**: 60-75 minutes\n",
    "\n",
    "**Prerequisites**: \n",
    "- Module 00: Introduction to Big Data and Spark Ecosystem\n",
    "- Module 01: PySpark Setup and SparkSession\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Understand what RDDs are and their key properties (immutability, partitioning, fault tolerance)\n",
    "2. Create RDDs from Python collections and external files\n",
    "3. Distinguish between transformations and actions\n",
    "4. Explain lazy evaluation and how Spark optimizes computation\n",
    "5. Work with key-value pair RDDs for advanced operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Module 02: RDD Basics\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Access SparkContext (needed for RDD operations)\n",
    "sc = spark.sparkContext\n",
    "\n",
    "print(f\"âœ“ SparkSession created: {spark.sparkContext.appName}\")\n",
    "print(f\"  Spark UI: {spark.sparkContext.uiWebUrl}\")\n",
    "print(f\"  Using {sc.defaultParallelism} CPU cores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is an RDD?\n",
    "\n",
    "### Definition\n",
    "\n",
    "**RDD (Resilient Distributed Dataset)** is Spark's fundamental data abstraction:\n",
    "\n",
    "- **Resilient**: Fault-tolerant, can recover from node failures\n",
    "- **Distributed**: Data is partitioned across multiple machines\n",
    "- **Dataset**: Collection of objects (like a Python list)\n",
    "\n",
    "### Key Properties\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚            RDD Properties           â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ 1. Immutable   â†’ Cannot be changed  â”‚\n",
    "â”‚ 2. Distributed â†’ Split into parts   â”‚\n",
    "â”‚ 3. Partitioned â†’ Parallel processingâ”‚\n",
    "â”‚ 4. Lazy        â†’ Computed on demand â”‚\n",
    "â”‚ 5. Typed       â†’ Contains objects   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### RDD vs Python List\n",
    "\n",
    "| Feature | Python List | RDD |\n",
    "|---------|-------------|-----|\n",
    "| **Location** | Single machine (in memory) | Distributed across cluster |\n",
    "| **Size limit** | Machine's RAM | Virtually unlimited |\n",
    "| **Processing** | Sequential | Parallel |\n",
    "| **Mutability** | Mutable | Immutable |\n",
    "| **Fault tolerance** | None | Automatic recovery |\n",
    "\n",
    "### When to Use RDDs?\n",
    "\n",
    "**Modern Spark (prefer DataFrames for most tasks)**:\n",
    "- DataFrames are optimized and easier to use\n",
    "- Use RDDs only when you need:\n",
    "  - Low-level control over data partitioning\n",
    "  - Custom partitioners\n",
    "  - Unstructured data (text files, logs)\n",
    "  - Working with existing RDD-based codebases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating RDDs\n",
    "\n",
    "### Method 1: Parallelizing a Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RDD from Python list\n",
    "numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "numbers_rdd = sc.parallelize(numbers)\n",
    "\n",
    "print(f\"RDD created: {numbers_rdd}\")\n",
    "print(f\"Type: {type(numbers_rdd)}\")\n",
    "print(f\"Number of partitions: {numbers_rdd.getNumPartitions()}\")\n",
    "\n",
    "# View the data (this is an ACTION - triggers computation)\n",
    "print(f\"\\nFirst 5 elements: {numbers_rdd.take(5)}\")\n",
    "print(f\"All elements: {numbers_rdd.collect()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controlling Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RDD with specific number of partitions\n",
    "numbers_4_partitions = sc.parallelize(numbers, numSlices=4)\n",
    "numbers_2_partitions = sc.parallelize(numbers, numSlices=2)\n",
    "\n",
    "print(f\"Default partitions: {numbers_rdd.getNumPartitions()}\")\n",
    "print(f\"With 4 partitions: {numbers_4_partitions.getNumPartitions()}\")\n",
    "print(f\"With 2 partitions: {numbers_2_partitions.getNumPartitions()}\")\n",
    "\n",
    "# View how data is distributed across partitions\n",
    "print(\"\\nData distribution with 4 partitions:\")\n",
    "partitions = numbers_4_partitions.glom().collect()\n",
    "for i, partition in enumerate(partitions):\n",
    "    print(f\"  Partition {i}: {list(partition)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Reading from External Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample text file first\n",
    "sample_text = \"\"\"Apache Spark is a unified analytics engine\n",
    "For large-scale data processing\n",
    "Spark provides high-level APIs in Java, Scala, Python and R\n",
    "It also supports a rich set of higher-level tools\n",
    "Including Spark SQL for SQL and structured data processing\"\"\"\n",
    "\n",
    "# Save to file\n",
    "with open('sample_text.txt', 'w') as f:\n",
    "    f.write(sample_text)\n",
    "\n",
    "# Create RDD from text file\n",
    "text_rdd = sc.textFile('sample_text.txt')\n",
    "\n",
    "print(f\"Text RDD: {text_rdd}\")\n",
    "print(f\"Number of lines: {text_rdd.count()}\")\n",
    "print(f\"\\nFirst 3 lines:\")\n",
    "for line in text_rdd.take(3):\n",
    "    print(f\"  {line}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Transformations vs Actions\n",
    "\n",
    "This is one of the most important concepts in Spark!\n",
    "\n",
    "### Transformations (Lazy)\n",
    "\n",
    "**Transformations** create a new RDD from an existing one:\n",
    "- **Lazy**: Not computed immediately\n",
    "- **Return**: New RDD\n",
    "- **Examples**: `map()`, `filter()`, `flatMap()`, `reduceByKey()`\n",
    "\n",
    "### Actions (Eager)\n",
    "\n",
    "**Actions** trigger computation and return results:\n",
    "- **Eager**: Execute immediately\n",
    "- **Return**: Values to driver program\n",
    "- **Examples**: `collect()`, `count()`, `take()`, `reduce()`\n",
    "\n",
    "### Visual Representation\n",
    "\n",
    "```\n",
    "RDD1 â†’ map() â†’ RDD2 â†’ filter() â†’ RDD3 â†’ collect()\n",
    "       â†‘                â†‘                  â†‘\n",
    "   Transformation   Transformation      Action\n",
    "   (not executed)   (not executed)   (executes all!)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample RDD\n",
    "numbers = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "\n",
    "# 1. map() - Apply function to each element\n",
    "squared = numbers.map(lambda x: x ** 2)\n",
    "print(f\"map() - Square each number: {squared.collect()}\")\n",
    "\n",
    "# 2. filter() - Keep elements matching condition\n",
    "evens = numbers.filter(lambda x: x % 2 == 0)\n",
    "print(f\"filter() - Even numbers only: {evens.collect()}\")\n",
    "\n",
    "# 3. flatMap() - Map then flatten\n",
    "words = sc.parallelize([\"Hello World\", \"Apache Spark\"])\n",
    "flat_words = words.flatMap(lambda x: x.split(\" \"))\n",
    "print(f\"flatMap() - Split into words: {flat_words.collect()}\")\n",
    "\n",
    "# 4. distinct() - Remove duplicates\n",
    "duplicates = sc.parallelize([1, 2, 2, 3, 3, 3, 4])\n",
    "unique = duplicates.distinct()\n",
    "print(f\"distinct() - Remove duplicates: {unique.collect()}\")\n",
    "\n",
    "# 5. union() - Combine two RDDs\n",
    "rdd1 = sc.parallelize([1, 2, 3])\n",
    "rdd2 = sc.parallelize([4, 5, 6])\n",
    "combined = rdd1.union(rdd2)\n",
    "print(f\"union() - Combine RDDs: {combined.collect()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "\n",
    "# 1. collect() - Return all elements (be careful with large datasets!)\n",
    "all_nums = numbers.collect()\n",
    "print(f\"collect() - All elements: {all_nums}\")\n",
    "\n",
    "# 2. count() - Count number of elements\n",
    "total = numbers.count()\n",
    "print(f\"count() - Total elements: {total}\")\n",
    "\n",
    "# 3. first() - Get first element\n",
    "first_num = numbers.first()\n",
    "print(f\"first() - First element: {first_num}\")\n",
    "\n",
    "# 4. take(n) - Get first n elements\n",
    "first_three = numbers.take(3)\n",
    "print(f\"take(3) - First 3 elements: {first_three}\")\n",
    "\n",
    "# 5. reduce() - Aggregate using function\n",
    "sum_all = numbers.reduce(lambda a, b: a + b)\n",
    "print(f\"reduce() - Sum of all: {sum_all}\")\n",
    "\n",
    "# 6. foreach() - Apply function to each element (no return)\n",
    "# Useful for side effects like printing or saving\n",
    "print(\"foreach() - Print each element:\")\n",
    "numbers.take(3).foreach(print)  # Using take to limit output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Lazy Evaluation\n",
    "\n",
    "### Understanding Lazy Evaluation\n",
    "\n",
    "Spark doesn't execute transformations immediately. Instead:\n",
    "\n",
    "1. **Builds a DAG** (Directed Acyclic Graph) of operations\n",
    "2. **Optimizes** the computation plan\n",
    "3. **Executes** only when an action is called\n",
    "\n",
    "### Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RDD\n",
    "print(\"Step 1: Creating RDD...\")\n",
    "data = sc.parallelize(range(1, 1001))  # 1000 numbers\n",
    "print(\"  RDD created (not computed yet!)\\n\")\n",
    "\n",
    "# Apply transformations (these are LAZY)\n",
    "print(\"Step 2: Applying transformations...\")\n",
    "squared = data.map(lambda x: x ** 2)\n",
    "print(\"  Transformation 1: map (not executed!)\")\n",
    "\n",
    "filtered = squared.filter(lambda x: x > 100000)\n",
    "print(\"  Transformation 2: filter (not executed!)\")\n",
    "\n",
    "sorted_rdd = filtered.sortBy(lambda x: x)\n",
    "print(\"  Transformation 3: sortBy (not executed!)\\n\")\n",
    "\n",
    "# Trigger action (this executes ALL transformations)\n",
    "print(\"Step 3: Calling action (collect)...\")\n",
    "start_time = time.time()\n",
    "result = sorted_rdd.take(5)  # Action!\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"  Action completed in {end_time - start_time:.4f} seconds\")\n",
    "print(f\"  Result: {result}\")\n",
    "print(\"\\nâœ“ All transformations executed when action was called!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benefits of Lazy Evaluation\n",
    "\n",
    "1. **Optimization**: Spark can optimize the entire pipeline before execution\n",
    "2. **Efficiency**: Avoids computing intermediate results that aren't needed\n",
    "3. **Fault tolerance**: Can recreate lost partitions by replaying transformations\n",
    "\n",
    "### Viewing the DAG\n",
    "\n",
    "You can visualize the execution plan in the Spark UI:\n",
    "1. Open Spark UI (check URL printed at the beginning)\n",
    "2. Go to \"DAG Visualization\" tab\n",
    "3. See how Spark optimized your transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print lineage (shows transformation chain)\n",
    "print(\"RDD Lineage (transformation chain):\")\n",
    "print(sorted_rdd.toDebugString().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Key-Value Pair RDDs\n",
    "\n",
    "### Creating Pair RDDs\n",
    "\n",
    "Pair RDDs are RDDs of tuples: `(key, value)`\n",
    "\n",
    "They enable powerful operations like:\n",
    "- `reduceByKey()`: Aggregate by key\n",
    "- `groupByKey()`: Group by key\n",
    "- `sortByKey()`: Sort by key\n",
    "- `join()`: Join two RDDs by key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pair RDD from regular RDD\n",
    "fruits = sc.parallelize([\"apple\", \"banana\", \"apple\", \"orange\", \"banana\", \"apple\"])\n",
    "\n",
    "# Convert to (fruit, 1) pairs for counting\n",
    "fruit_pairs = fruits.map(lambda x: (x, 1))\n",
    "\n",
    "print(\"Pair RDD:\")\n",
    "print(fruit_pairs.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduceByKey() - Aggregate by Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurrences of each fruit\n",
    "fruit_counts = fruit_pairs.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "print(\"Fruit counts (using reduceByKey):\")\n",
    "print(fruit_counts.collect())\n",
    "\n",
    "# More examples with numbers\n",
    "sales = sc.parallelize([\n",
    "    (\"Product A\", 100),\n",
    "    (\"Product B\", 200),\n",
    "    (\"Product A\", 150),\n",
    "    (\"Product C\", 300),\n",
    "    (\"Product B\", 50)\n",
    "])\n",
    "\n",
    "total_sales = sales.reduceByKey(lambda a, b: a + b)\n",
    "print(\"\\nTotal sales by product:\")\n",
    "for product, total in total_sales.collect():\n",
    "    print(f\"  {product}: ${total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### groupByKey() vs reduceByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupByKey() - Groups all values for each key\n",
    "sales_grouped = sales.groupByKey()\n",
    "\n",
    "print(\"Grouped sales (using groupByKey):\")\n",
    "for product, values in sales_grouped.collect():\n",
    "    print(f\"  {product}: {list(values)}\")\n",
    "\n",
    "# âš ï¸ Important: reduceByKey() is usually more efficient!\n",
    "# - reduceByKey: Reduces locally before shuffling (less data transfer)\n",
    "# - groupByKey: Shuffles all data (can cause memory issues)\n",
    "\n",
    "print(\"\\nðŸ’¡ Tip: Prefer reduceByKey() over groupByKey() for aggregations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Pair RDD Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sortByKey() - Sort by key\n",
    "sorted_sales = total_sales.sortByKey()\n",
    "print(\"Sorted by product name:\")\n",
    "print(sorted_sales.collect())\n",
    "\n",
    "# mapValues() - Apply function to values only (keys unchanged)\n",
    "sales_with_tax = total_sales.mapValues(lambda x: x * 1.10)  # Add 10% tax\n",
    "print(\"\\nSales with 10% tax:\")\n",
    "for product, total in sales_with_tax.collect():\n",
    "    print(f\"  {product}: ${total:.2f}\")\n",
    "\n",
    "# keys() and values() - Extract keys or values\n",
    "products = total_sales.keys().collect()\n",
    "amounts = total_sales.values().collect()\n",
    "print(f\"\\nProducts: {products}\")\n",
    "print(f\"Amounts: {amounts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Word Count (Classic Spark Example)\n",
    "\n",
    "Implement the classic word count program:\n",
    "1. Read the text file created earlier\n",
    "2. Split each line into words\n",
    "3. Count occurrences of each word\n",
    "4. Sort by count (descending)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n",
    "\n",
    "# Step 1: Read text file\n",
    "text = sc.textFile('sample_text.txt')\n",
    "\n",
    "# Step 2: Split into words (use flatMap)\n",
    "# Your code here\n",
    "\n",
    "# Step 3: Create (word, 1) pairs\n",
    "# Your code here\n",
    "\n",
    "# Step 4: Count by word\n",
    "# Your code here\n",
    "\n",
    "# Step 5: Sort by count (descending)\n",
    "# Your code here\n",
    "\n",
    "# Print top 5 words\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Filter and Transform\n",
    "\n",
    "Given a list of temperatures in Celsius:\n",
    "1. Create an RDD\n",
    "2. Filter temperatures above 25Â°C\n",
    "3. Convert to Fahrenheit: `F = C * 9/5 + 32`\n",
    "4. Count how many hot days (>77Â°F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your code here\n",
    "\n",
    "temperatures_c = [22, 28, 31, 19, 26, 33, 18, 29, 25, 30]\n",
    "\n",
    "# Create RDD\n",
    "# Your code here\n",
    "\n",
    "# Filter > 25Â°C\n",
    "# Your code here\n",
    "\n",
    "# Convert to Fahrenheit\n",
    "# Your code here\n",
    "\n",
    "# Count > 77Â°F\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Sales Analysis\n",
    "\n",
    "Analyze sales data:\n",
    "```python\n",
    "sales_data = [\n",
    "    (\"Electronics\", 500),\n",
    "    (\"Clothing\", 200),\n",
    "    (\"Electronics\", 700),\n",
    "    (\"Food\", 150),\n",
    "    (\"Clothing\", 300),\n",
    "    (\"Electronics\", 400),\n",
    "    (\"Food\", 250)\n",
    "]\n",
    "```\n",
    "\n",
    "Tasks:\n",
    "1. Calculate total sales per category\n",
    "2. Find the category with highest sales\n",
    "3. Calculate average sales per category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Your code here\n",
    "\n",
    "sales_data = [\n",
    "    (\"Electronics\", 500),\n",
    "    (\"Clothing\", 200),\n",
    "    (\"Electronics\", 700),\n",
    "    (\"Food\", 150),\n",
    "    (\"Clothing\", 300),\n",
    "    (\"Electronics\", 400),\n",
    "    (\"Food\", 250)\n",
    "]\n",
    "\n",
    "# Create RDD\n",
    "# Your code here\n",
    "\n",
    "# Task 1: Total sales per category\n",
    "# Your code here\n",
    "\n",
    "# Task 2: Category with highest sales\n",
    "# Your code here\n",
    "\n",
    "# Task 3: Average sales per category\n",
    "# Hint: Use mapValues with groupByKey or aggregate functions\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Understanding Lazy Evaluation\n",
    "\n",
    "Predict the output:\n",
    "```python\n",
    "rdd1 = sc.parallelize([1, 2, 3, 4])\n",
    "rdd2 = rdd1.map(lambda x: x * 2)\n",
    "rdd3 = rdd2.filter(lambda x: x > 4)\n",
    "print(\"Done with transformations\")\n",
    "result = rdd3.collect()\n",
    "print(result)\n",
    "```\n",
    "\n",
    "Questions:\n",
    "1. At which point does Spark actually compute the values?\n",
    "2. What would `rdd2.collect()` return?\n",
    "3. What would `rdd3.collect()` return?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Test your predictions\n",
    "\n",
    "rdd1 = sc.parallelize([1, 2, 3, 4])\n",
    "rdd2 = rdd1.map(lambda x: x * 2)\n",
    "rdd3 = rdd2.filter(lambda x: x > 4)\n",
    "print(\"Done with transformations\")\n",
    "\n",
    "# Uncomment to test:\n",
    "# print(f\"rdd2: {rdd2.collect()}\")\n",
    "# print(f\"rdd3: {rdd3.collect()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions\n",
    "\n",
    "### Exercise 1 Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 1: Word Count\n",
    "\n",
    "text = sc.textFile('sample_text.txt')\n",
    "words = text.flatMap(lambda line: line.split())\n",
    "word_pairs = words.map(lambda word: (word, 1))\n",
    "word_counts = word_pairs.reduceByKey(lambda a, b: a + b)\n",
    "sorted_counts = word_counts.sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "print(\"Top 10 words:\")\n",
    "for word, count in sorted_counts.take(10):\n",
    "    print(f\"  {word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 2: Temperature Analysis\n",
    "\n",
    "temperatures_c = [22, 28, 31, 19, 26, 33, 18, 29, 25, 30]\n",
    "\n",
    "temps_rdd = sc.parallelize(temperatures_c)\n",
    "hot_days_c = temps_rdd.filter(lambda t: t > 25)\n",
    "hot_days_f = hot_days_c.map(lambda c: c * 9/5 + 32)\n",
    "very_hot_count = hot_days_f.filter(lambda f: f > 77).count()\n",
    "\n",
    "print(f\"Hot days in Celsius (>25Â°C): {hot_days_c.collect()}\")\n",
    "print(f\"Hot days in Fahrenheit: {hot_days_f.collect()}\")\n",
    "print(f\"Number of very hot days (>77Â°F): {very_hot_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 3: Sales Analysis\n",
    "\n",
    "sales_data = [\n",
    "    (\"Electronics\", 500), (\"Clothing\", 200), (\"Electronics\", 700),\n",
    "    (\"Food\", 150), (\"Clothing\", 300), (\"Electronics\", 400), (\"Food\", 250)\n",
    "]\n",
    "\n",
    "sales_rdd = sc.parallelize(sales_data)\n",
    "\n",
    "# Task 1: Total sales per category\n",
    "total_by_category = sales_rdd.reduceByKey(lambda a, b: a + b)\n",
    "print(\"Total sales by category:\")\n",
    "for category, total in total_by_category.collect():\n",
    "    print(f\"  {category}: ${total}\")\n",
    "\n",
    "# Task 2: Highest sales category\n",
    "highest = total_by_category.sortBy(lambda x: x[1], ascending=False).first()\n",
    "print(f\"\\nHighest sales: {highest[0]} (${highest[1]})\")\n",
    "\n",
    "# Task 3: Average sales per category\n",
    "count_by_category = sales_rdd.mapValues(lambda x: (x, 1)) \\\n",
    "    .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))\n",
    "avg_by_category = count_by_category.mapValues(lambda x: x[0] / x[1])\n",
    "\n",
    "print(\"\\nAverage sales by category:\")\n",
    "for category, avg in avg_by_category.collect():\n",
    "    print(f\"  {category}: ${avg:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4 Solution\n",
    "\n",
    "**Answers**:\n",
    "1. Spark computes values when `collect()` is called (an action)\n",
    "2. `rdd2.collect()` would return `[2, 4, 6, 8]`\n",
    "3. `rdd3.collect()` would return `[6, 8]` (values > 4)\n",
    "\n",
    "The transformations are not executed until an action is called!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts Covered\n",
    "\n",
    "âœ… **RDD Properties**: Resilient, Distributed, Immutable, Partitioned, Lazy\n",
    "\n",
    "âœ… **Creating RDDs**: From collections (`parallelize`) and files (`textFile`)\n",
    "\n",
    "âœ… **Transformations**: Lazy operations that create new RDDs (map, filter, flatMap)\n",
    "\n",
    "âœ… **Actions**: Eager operations that return results (collect, count, reduce)\n",
    "\n",
    "âœ… **Lazy Evaluation**: Transformations are not computed until an action is called\n",
    "\n",
    "âœ… **Pair RDDs**: Key-value operations (reduceByKey, groupByKey, sortByKey)\n",
    "\n",
    "### Important Transformations and Actions\n",
    "\n",
    "**Transformations** (return RDD):\n",
    "- `map()`, `flatMap()`, `filter()`, `distinct()`\n",
    "- `union()`, `intersection()`, `subtract()`\n",
    "- `reduceByKey()`, `groupByKey()`, `sortByKey()`\n",
    "\n",
    "**Actions** (return values):\n",
    "- `collect()`, `count()`, `first()`, `take(n)`\n",
    "- `reduce()`, `foreach()`, `saveAsTextFile()`\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Avoid `collect()` on large RDDs**: It brings all data to driver (memory issues)\n",
    "2. **Use `reduceByKey()` over `groupByKey()`**: More efficient, less shuffle\n",
    "3. **Control partitions**: More partitions = more parallelism (but overhead)\n",
    "4. **Understand lazy evaluation**: Helps with debugging and optimization\n",
    "5. **Prefer DataFrames**: In modern Spark, use DataFrames for structured data\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In **Module 03: DataFrames and Datasets**, you will:\n",
    "- Learn the DataFrame API (higher-level abstraction)\n",
    "- Understand schema and structured data\n",
    "- Convert between RDDs and DataFrames\n",
    "- Use Spark's Catalyst optimizer\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [RDD Programming Guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html)\n",
    "- [PySpark RDD API](https://spark.apache.org/docs/latest/api/python/reference/pyspark.html#rdd-apis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "spark.stop()\n",
    "print(\"SparkSession stopped. âœ“\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
