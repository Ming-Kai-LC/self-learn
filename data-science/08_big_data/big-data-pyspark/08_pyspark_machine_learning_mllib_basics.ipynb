{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 08: PySpark Machine Learning - MLlib Basics\n",
    "\n",
    "**Difficulty**: ⭐⭐  \n",
    "**Estimated Time**: 60 minutes  \n",
    "**Prerequisites**: \n",
    "- [Module 03: DataFrames and Datasets](03_dataframes_and_datasets.ipynb)\n",
    "- [Module 05: DataFrame Operations](05_dataframe_operations.ipynb)\n",
    "- Basic understanding of machine learning concepts\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Understand the MLlib library architecture and the ML Pipeline concept\n",
    "2. Build simple classification models using Logistic Regression in PySpark\n",
    "3. Build simple regression models using Linear Regression in PySpark\n",
    "4. Use feature transformers like StringIndexer, OneHotEncoder, and VectorAssembler\n",
    "5. Evaluate machine learning models using built-in metrics and methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Introduction\n",
    "\n",
    "**What is MLlib?**\n",
    "\n",
    "MLlib is Spark's machine learning library that provides:\n",
    "- Distributed ML algorithms that scale to large datasets\n",
    "- A unified API for building ML pipelines\n",
    "- Feature transformers and extractors\n",
    "- Model evaluation and tuning utilities\n",
    "\n",
    "**Two APIs:**\n",
    "- `spark.ml` - DataFrame-based API (recommended, what we'll use)\n",
    "- `spark.mllib` - RDD-based API (legacy, in maintenance mode)\n",
    "\n",
    "**ML Pipeline Architecture:**\n",
    "- **Transformer**: Converts one DataFrame to another (e.g., StringIndexer, trained model)\n",
    "- **Estimator**: Fits on a DataFrame to produce a Transformer (e.g., LogisticRegression)\n",
    "- **Pipeline**: Chains multiple Transformers and Estimators together\n",
    "- **Parameter**: Configuration for Transformers and Estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, rand, when, round as spark_round\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
    "\n",
    "# ML imports\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# For generating sample data\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark session with appropriate memory settings for ML\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MLlib Basics\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to reduce verbose output\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(\"Spark session created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Transformers\n",
    "\n",
    "Before building models, we need to prepare features. MLlib requires features to be in a **Vector** format.\n",
    "\n",
    "### Common Feature Transformers:\n",
    "\n",
    "**VectorAssembler**: Combines multiple columns into a single vector column\n",
    "- Input: Multiple numeric columns\n",
    "- Output: Single `features` column of type Vector\n",
    "\n",
    "**StringIndexer**: Converts string labels to numeric indices\n",
    "- Input: String column (e.g., \"cat\", \"dog\", \"bird\")\n",
    "- Output: Numeric column (e.g., 0, 1, 2)\n",
    "- Handles unseen labels during prediction\n",
    "\n",
    "**OneHotEncoder**: Converts categorical indices to binary vectors\n",
    "- Input: Numeric indices (from StringIndexer)\n",
    "- Output: Sparse vector with one-hot encoding\n",
    "- Prevents ordinality assumptions in categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data to demonstrate transformers\n",
    "sample_data = [\n",
    "    (\"blue\", \"small\", 10.5),\n",
    "    (\"red\", \"medium\", 15.2),\n",
    "    (\"blue\", \"large\", 20.0),\n",
    "    (\"green\", \"small\", 8.5),\n",
    "    (\"red\", \"small\", 12.0),\n",
    "    (\"green\", \"medium\", 18.5)\n",
    "]\n",
    "\n",
    "df_demo = spark.createDataFrame(sample_data, [\"color\", \"size\", \"value\"])\n",
    "df_demo.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StringIndexer: Convert categorical strings to numeric indices\n",
    "color_indexer = StringIndexer(inputCol=\"color\", outputCol=\"color_index\")\n",
    "size_indexer = StringIndexer(inputCol=\"size\", outputCol=\"size_index\")\n",
    "\n",
    "# Fit the indexers on the data (learns the unique values)\n",
    "color_model = color_indexer.fit(df_demo)\n",
    "size_model = size_indexer.fit(df_demo)\n",
    "\n",
    "# Transform the data\n",
    "df_indexed = color_model.transform(df_demo)\n",
    "df_indexed = size_model.transform(df_indexed)\n",
    "\n",
    "df_indexed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneHotEncoder: Convert indices to binary vectors\n",
    "# This prevents the model from assuming ordering (e.g., red > blue > green)\n",
    "color_encoder = OneHotEncoder(inputCol=\"color_index\", outputCol=\"color_vec\")\n",
    "size_encoder = OneHotEncoder(inputCol=\"size_index\", outputCol=\"size_vec\")\n",
    "\n",
    "df_encoded = color_encoder.fit(df_indexed).transform(df_indexed)\n",
    "df_encoded = size_encoder.fit(df_encoded).transform(df_encoded)\n",
    "\n",
    "df_encoded.select(\"color\", \"color_index\", \"color_vec\", \"size\", \"size_index\", \"size_vec\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VectorAssembler: Combine multiple feature columns into a single vector\n",
    "# This is required by MLlib algorithms which expect a single 'features' column\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"color_vec\", \"size_vec\", \"value\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "df_features = assembler.transform(df_encoded)\n",
    "df_features.select(\"color\", \"size\", \"value\", \"features\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classification with Logistic Regression\n",
    "\n",
    "**Logistic Regression** is a linear classifier used for binary or multi-class classification.\n",
    "\n",
    "**Use cases:**\n",
    "- Spam detection (spam vs. not spam)\n",
    "- Customer churn prediction (churn vs. retain)\n",
    "- Disease diagnosis (positive vs. negative)\n",
    "\n",
    "**How it works in PySpark:**\n",
    "1. Prepare features as a vector column\n",
    "2. Create a label column (numeric: 0, 1 for binary)\n",
    "3. Instantiate LogisticRegression estimator\n",
    "4. Fit the estimator on training data → produces a model (transformer)\n",
    "5. Use model to make predictions on test data\n",
    "6. Evaluate using appropriate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic classification data\n",
    "# Scenario: Predict customer purchase based on age, income, and browsing time\n",
    "n_samples = 1000\n",
    "\n",
    "classification_data = []\n",
    "for _ in range(n_samples):\n",
    "    age = np.random.randint(18, 70)\n",
    "    income = np.random.uniform(20000, 150000)\n",
    "    browsing_time = np.random.uniform(0, 120)  # minutes\n",
    "    \n",
    "    # Create a decision rule with some randomness\n",
    "    # More likely to purchase if: younger, higher income, more browsing time\n",
    "    score = (70 - age) * 0.01 + income * 0.00001 + browsing_time * 0.02\n",
    "    probability = 1 / (1 + np.exp(-score + 1.5))\n",
    "    purchased = 1 if np.random.random() < probability else 0\n",
    "    \n",
    "    classification_data.append((age, float(income), float(browsing_time), purchased))\n",
    "\n",
    "df_classification = spark.createDataFrame(\n",
    "    classification_data,\n",
    "    [\"age\", \"income\", \"browsing_time\", \"purchased\"]\n",
    ")\n",
    "\n",
    "print(f\"Total samples: {df_classification.count()}\")\n",
    "print(\"\\nClass distribution:\")\n",
    "df_classification.groupBy(\"purchased\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample data\n",
    "df_classification.show(10)\n",
    "\n",
    "# Basic statistics\n",
    "df_classification.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for classification\n",
    "# Combine all feature columns into a single vector\n",
    "feature_assembler = VectorAssembler(\n",
    "    inputCols=[\"age\", \"income\", \"browsing_time\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "df_class_features = feature_assembler.transform(df_classification)\n",
    "df_class_features.select(\"features\", \"purchased\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "# 70% for training, 30% for testing\n",
    "# Setting a seed ensures reproducibility\n",
    "train_df, test_df = df_class_features.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "print(f\"Training samples: {train_df.count()}\")\n",
    "print(f\"Test samples: {test_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Logistic Regression model\n",
    "# labelCol: the column containing the target variable\n",
    "# featuresCol: the column containing the feature vector\n",
    "# maxIter: maximum number of iterations for optimization\n",
    "lr = LogisticRegression(\n",
    "    labelCol=\"purchased\",\n",
    "    featuresCol=\"features\",\n",
    "    maxIter=10,\n",
    "    regParam=0.01  # Regularization parameter to prevent overfitting\n",
    ")\n",
    "\n",
    "# Fit the model (this is where learning happens)\n",
    "lr_model = lr.fit(train_df)\n",
    "\n",
    "print(\"Model trained successfully!\")\n",
    "print(f\"Coefficients: {lr_model.coefficients}\")\n",
    "print(f\"Intercept: {lr_model.intercept}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test data\n",
    "predictions = lr_model.transform(test_df)\n",
    "\n",
    "# Show predictions with probabilities\n",
    "# rawPrediction: raw confidence scores\n",
    "# probability: calibrated probabilities for each class\n",
    "# prediction: final predicted class\n",
    "predictions.select(\"features\", \"purchased\", \"rawPrediction\", \"probability\", \"prediction\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "# BinaryClassificationEvaluator uses Area Under ROC as default metric\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"purchased\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "auc = evaluator.evaluate(predictions)\n",
    "print(f\"Area Under ROC: {auc:.4f}\")\n",
    "\n",
    "# Also calculate accuracy using MulticlassClassificationEvaluator\n",
    "multi_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"purchased\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "accuracy = multi_evaluator.evaluate(predictions)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# F1 Score\n",
    "f1_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"purchased\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ")\n",
    "f1 = f1_evaluator.evaluate(predictions)\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Regression with Linear Regression\n",
    "\n",
    "**Linear Regression** predicts a continuous numeric value based on input features.\n",
    "\n",
    "**Use cases:**\n",
    "- House price prediction\n",
    "- Sales forecasting\n",
    "- Temperature prediction\n",
    "\n",
    "**Key differences from classification:**\n",
    "- Label is continuous (not discrete classes)\n",
    "- Evaluation uses metrics like RMSE, R², MAE\n",
    "- No probability or class prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic regression data\n",
    "# Scenario: Predict house price based on size, bedrooms, and age\n",
    "n_samples = 1000\n",
    "\n",
    "regression_data = []\n",
    "for _ in range(n_samples):\n",
    "    size_sqft = np.random.uniform(500, 3500)\n",
    "    bedrooms = np.random.randint(1, 6)\n",
    "    age_years = np.random.randint(0, 50)\n",
    "    \n",
    "    # True relationship with some noise\n",
    "    # Price increases with size and bedrooms, decreases with age\n",
    "    price = (\n",
    "        150 * size_sqft +\n",
    "        50000 * bedrooms -\n",
    "        1000 * age_years +\n",
    "        100000 +\n",
    "        np.random.normal(0, 50000)  # Random noise\n",
    "    )\n",
    "    \n",
    "    regression_data.append((float(size_sqft), bedrooms, age_years, float(price)))\n",
    "\n",
    "df_regression = spark.createDataFrame(\n",
    "    regression_data,\n",
    "    [\"size_sqft\", \"bedrooms\", \"age_years\", \"price\"]\n",
    ")\n",
    "\n",
    "print(f\"Total samples: {df_regression.count()}\")\n",
    "df_regression.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for regression data\n",
    "df_regression.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for regression\n",
    "reg_assembler = VectorAssembler(\n",
    "    inputCols=[\"size_sqft\", \"bedrooms\", \"age_years\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "df_reg_features = reg_assembler.transform(df_regression)\n",
    "df_reg_features.select(\"features\", \"price\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and test sets\n",
    "reg_train, reg_test = df_reg_features.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "print(f\"Training samples: {reg_train.count()}\")\n",
    "print(f\"Test samples: {reg_test.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Linear Regression model\n",
    "lin_reg = LinearRegression(\n",
    "    labelCol=\"price\",\n",
    "    featuresCol=\"features\",\n",
    "    maxIter=10,\n",
    "    regParam=0.01,  # L2 regularization\n",
    "    elasticNetParam=0.0  # 0 = L2, 1 = L1, between = mix\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "lin_reg_model = lin_reg.fit(reg_train)\n",
    "\n",
    "print(\"Model trained successfully!\")\n",
    "print(f\"Coefficients: {lin_reg_model.coefficients}\")\n",
    "print(f\"Intercept: {lin_reg_model.intercept}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "reg_predictions = lin_reg_model.transform(reg_test)\n",
    "\n",
    "# Show actual vs predicted prices\n",
    "reg_predictions.select(\"features\", \"price\", \"prediction\").show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate regression model\n",
    "reg_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"price\",\n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "\n",
    "# RMSE: Root Mean Squared Error (lower is better)\n",
    "rmse = reg_evaluator.evaluate(reg_predictions, {reg_evaluator.metricName: \"rmse\"})\n",
    "print(f\"RMSE: ${rmse:,.2f}\")\n",
    "\n",
    "# MAE: Mean Absolute Error (lower is better)\n",
    "mae = reg_evaluator.evaluate(reg_predictions, {reg_evaluator.metricName: \"mae\"})\n",
    "print(f\"MAE: ${mae:,.2f}\")\n",
    "\n",
    "# R²: Coefficient of determination (higher is better, 1.0 is perfect)\n",
    "r2 = reg_evaluator.evaluate(reg_predictions, {reg_evaluator.metricName: \"r2\"})\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "# Training summary (additional metrics from the model itself)\n",
    "print(\"\\nTraining Summary:\")\n",
    "print(f\"Training RMSE: ${lin_reg_model.summary.rootMeanSquaredError:,.2f}\")\n",
    "print(f\"Training R²: {lin_reg_model.summary.r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Building ML Pipelines\n",
    "\n",
    "**Why use Pipelines?**\n",
    "- Combine multiple steps into a single workflow\n",
    "- Ensure transformations are applied consistently\n",
    "- Make it easier to deploy models to production\n",
    "- Prevent data leakage (transformations fit only on training data)\n",
    "\n",
    "**Pipeline stages:**\n",
    "1. Feature transformers (StringIndexer, OneHotEncoder, VectorAssembler)\n",
    "2. Model estimator (LogisticRegression, LinearRegression, etc.)\n",
    "\n",
    "The pipeline is fitted on training data and produces a PipelineModel that can be used on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data with categorical features\n",
    "pipeline_data = [\n",
    "    (\"male\", \"bachelor\", 35000, 0),\n",
    "    (\"female\", \"master\", 55000, 1),\n",
    "    (\"male\", \"phd\", 75000, 1),\n",
    "    (\"female\", \"bachelor\", 40000, 0),\n",
    "    (\"male\", \"master\", 60000, 1),\n",
    "    (\"female\", \"phd\", 80000, 1),\n",
    "    (\"male\", \"bachelor\", 32000, 0),\n",
    "    (\"female\", \"master\", 58000, 1),\n",
    "] * 100  # Repeat to have more data\n",
    "\n",
    "df_pipeline = spark.createDataFrame(\n",
    "    pipeline_data,\n",
    "    [\"gender\", \"education\", \"salary\", \"promoted\"]\n",
    ")\n",
    "\n",
    "df_pipeline.show(10)\n",
    "print(f\"Total samples: {df_pipeline.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data FIRST before building pipeline\n",
    "# This prevents data leakage\n",
    "pipeline_train, pipeline_test = df_pipeline.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "print(f\"Training samples: {pipeline_train.count()}\")\n",
    "print(f\"Test samples: {pipeline_test.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a complete ML pipeline\n",
    "# Stage 1: Index categorical features\n",
    "gender_indexer = StringIndexer(inputCol=\"gender\", outputCol=\"gender_index\")\n",
    "education_indexer = StringIndexer(inputCol=\"education\", outputCol=\"education_index\")\n",
    "\n",
    "# Stage 2: One-hot encode the indexed features\n",
    "gender_encoder = OneHotEncoder(inputCol=\"gender_index\", outputCol=\"gender_vec\")\n",
    "education_encoder = OneHotEncoder(inputCol=\"education_index\", outputCol=\"education_vec\")\n",
    "\n",
    "# Stage 3: Assemble all features into a vector\n",
    "pipeline_assembler = VectorAssembler(\n",
    "    inputCols=[\"gender_vec\", \"education_vec\", \"salary\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Stage 4: Train the model\n",
    "pipeline_lr = LogisticRegression(\n",
    "    labelCol=\"promoted\",\n",
    "    featuresCol=\"features\",\n",
    "    maxIter=10\n",
    ")\n",
    "\n",
    "# Create the pipeline with all stages\n",
    "ml_pipeline = Pipeline(stages=[\n",
    "    gender_indexer,\n",
    "    education_indexer,\n",
    "    gender_encoder,\n",
    "    education_encoder,\n",
    "    pipeline_assembler,\n",
    "    pipeline_lr\n",
    "])\n",
    "\n",
    "print(\"Pipeline created with 6 stages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the entire pipeline on training data\n",
    "# This fits each stage sequentially:\n",
    "# 1. StringIndexers learn the categorical mappings\n",
    "# 2. OneHotEncoders create the encoding scheme\n",
    "# 3. VectorAssembler combines features\n",
    "# 4. LogisticRegression trains the model\n",
    "pipeline_model = ml_pipeline.fit(pipeline_train)\n",
    "\n",
    "print(\"Pipeline fitted successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using the pipeline model\n",
    "# The model automatically applies all transformations\n",
    "pipeline_predictions = pipeline_model.transform(pipeline_test)\n",
    "\n",
    "# Show original features and predictions\n",
    "pipeline_predictions.select(\n",
    "    \"gender\", \"education\", \"salary\", \"promoted\", \"prediction\", \"probability\"\n",
    ").show(15, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the pipeline model\n",
    "pipeline_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"promoted\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "pipeline_accuracy = pipeline_evaluator.evaluate(pipeline_predictions)\n",
    "print(f\"Pipeline Model Accuracy: {pipeline_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Exercises\n",
    "\n",
    "Now it's your turn to practice! Complete the following exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Customer Churn Classification\n",
    "\n",
    "Create a classification model to predict customer churn.\n",
    "\n",
    "**Tasks:**\n",
    "1. Generate synthetic customer data with features: monthly_charges, tenure_months, total_charges, churn (0 or 1)\n",
    "2. Create a logistic regression model to predict churn\n",
    "3. Evaluate the model using accuracy and AUC metrics\n",
    "4. Print the model coefficients and interpret which features are most important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# TODO: Generate customer data\n",
    "# TODO: Prepare features\n",
    "# TODO: Train logistic regression model\n",
    "# TODO: Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Student Score Prediction\n",
    "\n",
    "Build a regression model to predict student exam scores.\n",
    "\n",
    "**Tasks:**\n",
    "1. Generate student data with: study_hours, previous_score, attendance_rate, final_score\n",
    "2. Create a linear regression model to predict final_score\n",
    "3. Evaluate using RMSE, MAE, and R² metrics\n",
    "4. Make predictions for new students with specific characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# TODO: Generate student data\n",
    "# TODO: Prepare features and split data\n",
    "# TODO: Train linear regression model\n",
    "# TODO: Evaluate and make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Complete ML Pipeline\n",
    "\n",
    "Build a complete pipeline for a multi-class classification problem.\n",
    "\n",
    "**Tasks:**\n",
    "1. Create data with categorical features (product_category, customer_type) and numeric features (price, quantity)\n",
    "2. Predict purchase_rating (1, 2, 3, 4, or 5 stars)\n",
    "3. Build a pipeline that includes:\n",
    "   - StringIndexer for categorical features\n",
    "   - OneHotEncoder for the indexed features\n",
    "   - VectorAssembler to combine all features\n",
    "   - LogisticRegression for multi-class classification\n",
    "4. Evaluate the pipeline using accuracy and F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# TODO: Create sample data with categorical and numeric features\n",
    "# TODO: Build pipeline with all transformation stages\n",
    "# TODO: Fit pipeline and make predictions\n",
    "# TODO: Evaluate multi-class classification performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Exercise Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1: Customer Churn Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate customer churn data\n",
    "n_customers = 1000\n",
    "churn_data = []\n",
    "\n",
    "for _ in range(n_customers):\n",
    "    monthly_charges = np.random.uniform(20, 100)\n",
    "    tenure_months = np.random.randint(1, 72)\n",
    "    total_charges = monthly_charges * tenure_months + np.random.normal(0, 100)\n",
    "    \n",
    "    # Higher charges and shorter tenure increase churn probability\n",
    "    churn_prob = 1 / (1 + np.exp(-(monthly_charges * 0.02 - tenure_months * 0.05 + 1)))\n",
    "    churn = 1 if np.random.random() < churn_prob else 0\n",
    "    \n",
    "    churn_data.append((float(monthly_charges), tenure_months, float(total_charges), churn))\n",
    "\n",
    "df_churn = spark.createDataFrame(churn_data, [\"monthly_charges\", \"tenure_months\", \"total_charges\", \"churn\"])\n",
    "\n",
    "print(f\"Total customers: {df_churn.count()}\")\n",
    "print(\"\\nChurn distribution:\")\n",
    "df_churn.groupBy(\"churn\").count().show()\n",
    "df_churn.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features\n",
    "churn_assembler = VectorAssembler(\n",
    "    inputCols=[\"monthly_charges\", \"tenure_months\", \"total_charges\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "df_churn_features = churn_assembler.transform(df_churn)\n",
    "\n",
    "# Split data\n",
    "churn_train, churn_test = df_churn_features.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# Train model\n",
    "churn_lr = LogisticRegression(labelCol=\"churn\", featuresCol=\"features\", maxIter=10)\n",
    "churn_model = churn_lr.fit(churn_train)\n",
    "\n",
    "print(\"Churn model coefficients:\")\n",
    "print(f\"Monthly Charges: {churn_model.coefficients[0]:.6f}\")\n",
    "print(f\"Tenure Months: {churn_model.coefficients[1]:.6f}\")\n",
    "print(f\"Total Charges: {churn_model.coefficients[2]:.6f}\")\n",
    "print(f\"\\nInterpretation: Positive coefficients increase churn probability, negative decrease it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "churn_predictions = churn_model.transform(churn_test)\n",
    "\n",
    "churn_acc_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"churn\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    ")\n",
    "churn_auc_evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"churn\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "churn_accuracy = churn_acc_evaluator.evaluate(churn_predictions)\n",
    "churn_auc = churn_auc_evaluator.evaluate(churn_predictions)\n",
    "\n",
    "print(f\"Churn Model Accuracy: {churn_accuracy:.4f}\")\n",
    "print(f\"Churn Model AUC: {churn_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 2: Student Score Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate student score data\n",
    "n_students = 800\n",
    "student_data = []\n",
    "\n",
    "for _ in range(n_students):\n",
    "    study_hours = np.random.uniform(0, 10)\n",
    "    previous_score = np.random.uniform(40, 100)\n",
    "    attendance_rate = np.random.uniform(0.5, 1.0)\n",
    "    \n",
    "    # Final score depends on all three factors with some noise\n",
    "    final_score = (\n",
    "        study_hours * 3.5 +\n",
    "        previous_score * 0.5 +\n",
    "        attendance_rate * 20 +\n",
    "        np.random.normal(0, 5)\n",
    "    )\n",
    "    final_score = min(100, max(0, final_score))  # Clamp to 0-100\n",
    "    \n",
    "    student_data.append((float(study_hours), float(previous_score), float(attendance_rate), float(final_score)))\n",
    "\n",
    "df_students = spark.createDataFrame(\n",
    "    student_data,\n",
    "    [\"study_hours\", \"previous_score\", \"attendance_rate\", \"final_score\"]\n",
    ")\n",
    "\n",
    "print(f\"Total students: {df_students.count()}\")\n",
    "df_students.show(10)\n",
    "df_students.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and train model\n",
    "student_assembler = VectorAssembler(\n",
    "    inputCols=[\"study_hours\", \"previous_score\", \"attendance_rate\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "df_student_features = student_assembler.transform(df_students)\n",
    "\n",
    "# Split data\n",
    "student_train, student_test = df_student_features.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# Train linear regression\n",
    "student_lr = LinearRegression(labelCol=\"final_score\", featuresCol=\"features\", maxIter=10)\n",
    "student_model = student_lr.fit(student_train)\n",
    "\n",
    "print(\"Student score model trained!\")\n",
    "print(f\"Coefficients: {student_model.coefficients}\")\n",
    "print(f\"Intercept: {student_model.intercept}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "student_predictions = student_model.transform(student_test)\n",
    "student_evaluator = RegressionEvaluator(labelCol=\"final_score\", predictionCol=\"prediction\")\n",
    "\n",
    "student_rmse = student_evaluator.evaluate(student_predictions, {student_evaluator.metricName: \"rmse\"})\n",
    "student_mae = student_evaluator.evaluate(student_predictions, {student_evaluator.metricName: \"mae\"})\n",
    "student_r2 = student_evaluator.evaluate(student_predictions, {student_evaluator.metricName: \"r2\"})\n",
    "\n",
    "print(f\"RMSE: {student_rmse:.2f}\")\n",
    "print(f\"MAE: {student_mae:.2f}\")\n",
    "print(f\"R² Score: {student_r2:.4f}\")\n",
    "\n",
    "# Make predictions for new students\n",
    "new_students = spark.createDataFrame([\n",
    "    (8.0, 85.0, 0.95),  # High effort student\n",
    "    (2.0, 60.0, 0.70),  # Average student\n",
    "    (5.0, 75.0, 0.85)   # Good student\n",
    "], [\"study_hours\", \"previous_score\", \"attendance_rate\"])\n",
    "\n",
    "new_student_features = student_assembler.transform(new_students)\n",
    "new_predictions = student_model.transform(new_student_features)\n",
    "\n",
    "print(\"\\nPredictions for new students:\")\n",
    "new_predictions.select(\"study_hours\", \"previous_score\", \"attendance_rate\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 3: Complete ML Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate multi-class classification data\n",
    "categories = [\"Electronics\", \"Clothing\", \"Books\", \"Home\"]\n",
    "customer_types = [\"Regular\", \"Premium\", \"VIP\"]\n",
    "n_purchases = 1000\n",
    "\n",
    "purchase_data = []\n",
    "for _ in range(n_purchases):\n",
    "    category = random.choice(categories)\n",
    "    customer_type = random.choice(customer_types)\n",
    "    price = np.random.uniform(10, 500)\n",
    "    quantity = np.random.randint(1, 10)\n",
    "    \n",
    "    # Rating depends on customer type, price, and category\n",
    "    base_rating = 3\n",
    "    if customer_type == \"Premium\":\n",
    "        base_rating += 0.5\n",
    "    elif customer_type == \"VIP\":\n",
    "        base_rating += 1.0\n",
    "    \n",
    "    if category == \"Electronics\" and price > 200:\n",
    "        base_rating += 0.5\n",
    "    \n",
    "    # Add randomness and clamp to 1-5\n",
    "    rating = int(min(5, max(1, base_rating + np.random.normal(0, 0.8))))\n",
    "    \n",
    "    purchase_data.append((category, customer_type, float(price), quantity, rating))\n",
    "\n",
    "df_purchases = spark.createDataFrame(\n",
    "    purchase_data,\n",
    "    [\"product_category\", \"customer_type\", \"price\", \"quantity\", \"purchase_rating\"]\n",
    ")\n",
    "\n",
    "print(f\"Total purchases: {df_purchases.count()}\")\n",
    "print(\"\\nRating distribution:\")\n",
    "df_purchases.groupBy(\"purchase_rating\").count().orderBy(\"purchase_rating\").show()\n",
    "df_purchases.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data first\n",
    "purchase_train, purchase_test = df_purchases.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# Build complete pipeline\n",
    "category_indexer = StringIndexer(inputCol=\"product_category\", outputCol=\"category_index\")\n",
    "customer_indexer = StringIndexer(inputCol=\"customer_type\", outputCol=\"customer_index\")\n",
    "\n",
    "category_encoder = OneHotEncoder(inputCol=\"category_index\", outputCol=\"category_vec\")\n",
    "customer_encoder = OneHotEncoder(inputCol=\"customer_index\", outputCol=\"customer_vec\")\n",
    "\n",
    "purchase_assembler = VectorAssembler(\n",
    "    inputCols=[\"category_vec\", \"customer_vec\", \"price\", \"quantity\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# For multi-class, we need to ensure label is indexed (0, 1, 2, 3, 4)\n",
    "# Subtract 1 from rating to get 0-indexed labels\n",
    "from pyspark.sql.functions import col\n",
    "purchase_train = purchase_train.withColumn(\"label\", col(\"purchase_rating\") - 1)\n",
    "purchase_test = purchase_test.withColumn(\"label\", col(\"purchase_rating\") - 1)\n",
    "\n",
    "multiclass_lr = LogisticRegression(\n",
    "    labelCol=\"label\",\n",
    "    featuresCol=\"features\",\n",
    "    maxIter=10,\n",
    "    family=\"multinomial\"  # Explicitly set for multi-class\n",
    ")\n",
    "\n",
    "# Create pipeline\n",
    "purchase_pipeline = Pipeline(stages=[\n",
    "    category_indexer,\n",
    "    customer_indexer,\n",
    "    category_encoder,\n",
    "    customer_encoder,\n",
    "    purchase_assembler,\n",
    "    multiclass_lr\n",
    "])\n",
    "\n",
    "print(\"Purchase rating pipeline created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and evaluate pipeline\n",
    "purchase_pipeline_model = purchase_pipeline.fit(purchase_train)\n",
    "purchase_predictions = purchase_pipeline_model.transform(purchase_test)\n",
    "\n",
    "# Show predictions (add 1 back to get original ratings)\n",
    "purchase_predictions_display = purchase_predictions.withColumn(\n",
    "    \"predicted_rating\", col(\"prediction\") + 1\n",
    ")\n",
    "purchase_predictions_display.select(\n",
    "    \"product_category\", \"customer_type\", \"price\", \"quantity\", \"purchase_rating\", \"predicted_rating\"\n",
    ").show(15)\n",
    "\n",
    "# Evaluate\n",
    "multiclass_acc_eval = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    ")\n",
    "multiclass_f1_eval = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\"\n",
    ")\n",
    "\n",
    "purchase_accuracy = multiclass_acc_eval.evaluate(purchase_predictions)\n",
    "purchase_f1 = multiclass_f1_eval.evaluate(purchase_predictions)\n",
    "\n",
    "print(f\"\\nPurchase Rating Model Accuracy: {purchase_accuracy:.4f}\")\n",
    "print(f\"Purchase Rating Model F1 Score: {purchase_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "Congratulations! You've learned the fundamentals of machine learning with PySpark MLlib.\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "1. **MLlib Architecture**:\n",
    "   - DataFrame-based API (`spark.ml`) is the modern approach\n",
    "   - Transformers convert DataFrames (e.g., trained models, encoders)\n",
    "   - Estimators fit on data to produce Transformers (e.g., algorithms)\n",
    "   - Pipelines chain multiple stages for reproducible workflows\n",
    "\n",
    "2. **Feature Preparation**:\n",
    "   - StringIndexer: Convert categorical strings to numeric indices\n",
    "   - OneHotEncoder: Create binary vectors from categorical indices\n",
    "   - VectorAssembler: Combine features into a single vector column\n",
    "   - All MLlib algorithms require features in vector format\n",
    "\n",
    "3. **Classification**:\n",
    "   - Logistic Regression for binary and multi-class problems\n",
    "   - Evaluation metrics: Accuracy, AUC, F1 score\n",
    "   - Produces probabilities and class predictions\n",
    "\n",
    "4. **Regression**:\n",
    "   - Linear Regression for continuous predictions\n",
    "   - Evaluation metrics: RMSE, MAE, R²\n",
    "   - Learns linear relationships between features and target\n",
    "\n",
    "5. **ML Pipelines**:\n",
    "   - Combine preprocessing and modeling into one object\n",
    "   - Prevent data leakage by fitting only on training data\n",
    "   - Make deployment and reuse easier\n",
    "   - Ensure consistent transformations across train/test/production\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "- Always split data BEFORE building pipelines\n",
    "- Use appropriate evaluation metrics for your problem\n",
    "- Set random seeds for reproducibility\n",
    "- Include regularization to prevent overfitting\n",
    "- Validate on held-out test data\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In [Module 09: Feature Engineering at Scale](09_feature_engineering_at_scale.ipynb), you'll learn:\n",
    "- Advanced feature transformations and scaling techniques\n",
    "- Feature selection methods\n",
    "- Handling imbalanced datasets\n",
    "- Custom transformers and feature engineering pipelines\n",
    "\n",
    "### Additional Resources:\n",
    "\n",
    "- [PySpark MLlib Guide](https://spark.apache.org/docs/latest/ml-guide.html)\n",
    "- [ML Pipelines Documentation](https://spark.apache.org/docs/latest/ml-pipeline.html)\n",
    "- [MLlib API Reference](https://spark.apache.org/docs/latest/api/python/reference/pyspark.ml.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "spark.stop()\n",
    "print(\"Spark session stopped. Great work!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
