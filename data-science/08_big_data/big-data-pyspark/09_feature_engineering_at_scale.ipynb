{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 09: Feature Engineering at Scale\n",
    "\n",
    "**Difficulty**: ⭐⭐  \n",
    "**Estimated Time**: 70 minutes  \n",
    "**Prerequisites**: \n",
    "- [Module 08: MLlib Basics](08_pyspark_machine_learning_mllib_basics.ipynb)\n",
    "- Understanding of feature transformation concepts\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Apply various scaling techniques (StandardScaler, MinMaxScaler, Normalizer) to normalize features\n",
    "2. Use VectorAssembler effectively to combine heterogeneous features into ML-ready vectors\n",
    "3. Implement categorical encoding strategies (StringIndexer, OneHotEncoder, label encoding)\n",
    "4. Perform feature selection using statistical methods and model-based approaches\n",
    "5. Build robust feature engineering pipelines that scale to large datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Introduction\n",
    "\n",
    "**What is Feature Engineering?**\n",
    "\n",
    "Feature engineering is the process of transforming raw data into features that better represent the underlying patterns for machine learning models. Good features can:\n",
    "- Improve model accuracy significantly\n",
    "- Reduce training time\n",
    "- Make models more interpretable\n",
    "- Handle data quality issues\n",
    "\n",
    "**Why Scale Features?**\n",
    "\n",
    "Many ML algorithms are sensitive to feature scales:\n",
    "- **Distance-based algorithms** (K-NN, SVM): Features with larger ranges dominate\n",
    "- **Gradient descent** (Neural Networks, Linear Models): Convergence is faster with normalized features\n",
    "- **Regularization**: Works better when features are on similar scales\n",
    "\n",
    "**Common Scaling Techniques:**\n",
    "- **StandardScaler**: Z-score normalization (mean=0, std=1)\n",
    "- **MinMaxScaler**: Scale to a specific range (e.g., [0, 1])\n",
    "- **Normalizer**: Scale each sample to unit norm\n",
    "- **MaxAbsScaler**: Scale by maximum absolute value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, rand, when, expr, round as spark_round\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
    "\n",
    "# Feature engineering imports\n",
    "from pyspark.ml.feature import (\n",
    "    VectorAssembler, StandardScaler, MinMaxScaler, Normalizer, MaxAbsScaler,\n",
    "    StringIndexer, OneHotEncoder, IndexToString,\n",
    "    Bucketizer, QuantileDiscretizer, VectorIndexer,\n",
    "    ChiSqSelector, UnivariateFeatureSelector\n",
    ")\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# For generating data\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Feature Engineering at Scale\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(\"Spark session created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Scaling Techniques\n",
    "\n",
    "We'll explore different scaling methods and understand when to use each one.\n",
    "\n",
    "### StandardScaler (Z-score Normalization)\n",
    "\n",
    "**Formula**: `(x - mean) / std`\n",
    "\n",
    "**When to use:**\n",
    "- Features follow roughly normal distribution\n",
    "- You want zero mean and unit variance\n",
    "- For algorithms sensitive to feature variance (SVM, Neural Networks)\n",
    "\n",
    "**Parameters:**\n",
    "- `withMean`: Center data to mean=0 (default: False, because it makes vectors dense)\n",
    "- `withStd`: Scale to std=1 (default: True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data with different scales\n",
    "# Features on very different scales to demonstrate the effect of scaling\n",
    "sample_data = [\n",
    "    (1, 100000.0, 25, 1.75),   # Salary: 100k, Age: 25, Height: 1.75m\n",
    "    (2, 55000.0, 35, 1.68),\n",
    "    (3, 80000.0, 28, 1.82),\n",
    "    (4, 45000.0, 42, 1.65),\n",
    "    (5, 120000.0, 30, 1.78),\n",
    "    (6, 65000.0, 38, 1.70),\n",
    "    (7, 90000.0, 26, 1.88),\n",
    "    (8, 75000.0, 33, 1.72)\n",
    "]\n",
    "\n",
    "df_scale = spark.createDataFrame(\n",
    "    sample_data,\n",
    "    [\"id\", \"salary\", \"age\", \"height\"]\n",
    ")\n",
    "\n",
    "df_scale.show()\n",
    "df_scale.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, assemble features into a vector\n",
    "# This is required for all MLlib transformers\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"salary\", \"age\", \"height\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "df_assembled = assembler.transform(df_scale)\n",
    "df_assembled.select(\"id\", \"features\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply StandardScaler\n",
    "# withMean=False keeps vectors sparse (important for large-scale data)\n",
    "# withStd=True scales to unit variance\n",
    "standard_scaler = StandardScaler(\n",
    "    inputCol=\"features\",\n",
    "    outputCol=\"scaled_features\",\n",
    "    withMean=False,\n",
    "    withStd=True\n",
    ")\n",
    "\n",
    "scaler_model = standard_scaler.fit(df_assembled)\n",
    "df_standard_scaled = scaler_model.transform(df_assembled)\n",
    "\n",
    "print(\"Original vs Standard Scaled features:\")\n",
    "df_standard_scaled.select(\"id\", \"features\", \"scaled_features\").show(truncate=False)\n",
    "\n",
    "print(f\"\\nStandard deviations: {scaler_model.std}\")\n",
    "print(f\"Means: {scaler_model.mean}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinMaxScaler\n",
    "\n",
    "**Formula**: `(x - min) / (max - min) * (max_range - min_range) + min_range`\n",
    "\n",
    "**When to use:**\n",
    "- You need features in a specific range (e.g., [0, 1])\n",
    "- For image processing (pixel values)\n",
    "- When you want to preserve zero values in sparse data\n",
    "\n",
    "**Advantages:**\n",
    "- Preserves relationships between values\n",
    "- Less affected by outliers than StandardScaler\n",
    "- Bounded output range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply MinMaxScaler\n",
    "# Default scales to [0, 1]\n",
    "minmax_scaler = MinMaxScaler(\n",
    "    inputCol=\"features\",\n",
    "    outputCol=\"minmax_features\"\n",
    ")\n",
    "\n",
    "minmax_model = minmax_scaler.fit(df_assembled)\n",
    "df_minmax_scaled = minmax_model.transform(df_assembled)\n",
    "\n",
    "print(\"Original vs MinMax Scaled features:\")\n",
    "df_minmax_scaled.select(\"id\", \"features\", \"minmax_features\").show(truncate=False)\n",
    "\n",
    "print(f\"\\nOriginal mins: {minmax_model.originalMin}\")\n",
    "print(f\"Original maxs: {minmax_model.originalMax}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizer\n",
    "\n",
    "**Purpose**: Scales each **row** (sample) to have unit norm\n",
    "\n",
    "**When to use:**\n",
    "- Text classification (TF-IDF vectors)\n",
    "- When the magnitude of feature vectors matters\n",
    "- Comparing similarity between samples\n",
    "\n",
    "**Norms:**\n",
    "- L1: Sum of absolute values = 1\n",
    "- L2: Sum of squares = 1 (default, Euclidean norm)\n",
    "- L∞: Maximum absolute value = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Normalizer with L2 norm (default)\n",
    "normalizer = Normalizer(\n",
    "    inputCol=\"features\",\n",
    "    outputCol=\"normalized_features\",\n",
    "    p=2.0  # L2 norm\n",
    ")\n",
    "\n",
    "# Normalizer is a Transformer, not an Estimator\n",
    "# It doesn't need to be fitted\n",
    "df_normalized = normalizer.transform(df_assembled)\n",
    "\n",
    "print(\"Original vs Normalized features (L2):\")\n",
    "df_normalized.select(\"id\", \"features\", \"normalized_features\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare L1, L2, and L∞ normalization\n",
    "normalizer_l1 = Normalizer(inputCol=\"features\", outputCol=\"norm_l1\", p=1.0)\n",
    "normalizer_l2 = Normalizer(inputCol=\"features\", outputCol=\"norm_l2\", p=2.0)\n",
    "normalizer_linf = Normalizer(inputCol=\"features\", outputCol=\"norm_linf\", p=float('inf'))\n",
    "\n",
    "df_norm_comparison = df_assembled\n",
    "df_norm_comparison = normalizer_l1.transform(df_norm_comparison)\n",
    "df_norm_comparison = normalizer_l2.transform(df_norm_comparison)\n",
    "df_norm_comparison = normalizer_linf.transform(df_norm_comparison)\n",
    "\n",
    "print(\"Comparison of different norms:\")\n",
    "df_norm_comparison.select(\"id\", \"features\", \"norm_l1\", \"norm_l2\", \"norm_linf\").show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advanced Categorical Encoding\n",
    "\n",
    "Beyond basic StringIndexer and OneHotEncoder, we'll explore more sophisticated encoding strategies.\n",
    "\n",
    "### Encoding Strategies:\n",
    "\n",
    "1. **Label Encoding** (StringIndexer): Assigns integer indices\n",
    "   - Good for: Tree-based models, ordinal categories\n",
    "   - Bad for: Linear models (implies ordering)\n",
    "\n",
    "2. **One-Hot Encoding**: Creates binary columns\n",
    "   - Good for: Linear models, low-cardinality categories\n",
    "   - Bad for: High-cardinality features (too many columns)\n",
    "\n",
    "3. **Target Encoding**: Uses target statistics (advanced, not covered here)\n",
    "\n",
    "4. **Frequency Encoding**: Encodes by category frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data with multiple categorical features\n",
    "n_samples = 500\n",
    "categories = [\"Electronics\", \"Clothing\", \"Books\", \"Home\", \"Sports\"]\n",
    "sizes = [\"Small\", \"Medium\", \"Large\", \"XLarge\"]\n",
    "colors = [\"Red\", \"Blue\", \"Green\", \"Black\", \"White\"]\n",
    "\n",
    "categorical_data = []\n",
    "for i in range(n_samples):\n",
    "    category = random.choice(categories)\n",
    "    size = random.choice(sizes)\n",
    "    color = random.choice(colors)\n",
    "    price = np.random.uniform(10, 500)\n",
    "    quantity = np.random.randint(1, 20)\n",
    "    \n",
    "    # Create target based on features\n",
    "    score = 0\n",
    "    if category == \"Electronics\":\n",
    "        score += 2\n",
    "    if size in [\"Large\", \"XLarge\"]:\n",
    "        score += 1\n",
    "    if price > 200:\n",
    "        score += 1\n",
    "    \n",
    "    purchased = 1 if (score >= 2 and np.random.random() > 0.3) else 0\n",
    "    \n",
    "    categorical_data.append((category, size, color, float(price), quantity, purchased))\n",
    "\n",
    "df_categorical = spark.createDataFrame(\n",
    "    categorical_data,\n",
    "    [\"category\", \"size\", \"color\", \"price\", \"quantity\", \"purchased\"]\n",
    ")\n",
    "\n",
    "print(f\"Total samples: {df_categorical.count()}\")\n",
    "print(\"\\nCategory distribution:\")\n",
    "df_categorical.groupBy(\"category\").count().show()\n",
    "df_categorical.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 1: Label Encoding Only (for tree-based models)\n",
    "# Tree models can handle categorical features as indices\n",
    "category_indexer = StringIndexer(inputCol=\"category\", outputCol=\"category_idx\")\n",
    "size_indexer = StringIndexer(inputCol=\"size\", outputCol=\"size_idx\")\n",
    "color_indexer = StringIndexer(inputCol=\"color\", outputCol=\"color_idx\")\n",
    "\n",
    "df_indexed = category_indexer.fit(df_categorical).transform(df_categorical)\n",
    "df_indexed = size_indexer.fit(df_indexed).transform(df_indexed)\n",
    "df_indexed = color_indexer.fit(df_indexed).transform(df_indexed)\n",
    "\n",
    "print(\"Label Encoded Features:\")\n",
    "df_indexed.select(\"category\", \"category_idx\", \"size\", \"size_idx\", \"color\", \"color_idx\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 2: One-Hot Encoding (for linear models)\n",
    "# Convert indices to binary vectors\n",
    "category_encoder = OneHotEncoder(inputCol=\"category_idx\", outputCol=\"category_vec\")\n",
    "size_encoder = OneHotEncoder(inputCol=\"size_idx\", outputCol=\"size_vec\")\n",
    "color_encoder = OneHotEncoder(inputCol=\"color_idx\", outputCol=\"color_vec\")\n",
    "\n",
    "df_encoded = category_encoder.fit(df_indexed).transform(df_indexed)\n",
    "df_encoded = size_encoder.fit(df_encoded).transform(df_encoded)\n",
    "df_encoded = color_encoder.fit(df_encoded).transform(df_encoded)\n",
    "\n",
    "print(\"One-Hot Encoded Features:\")\n",
    "df_encoded.select(\"category\", \"category_vec\", \"size\", \"size_vec\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all features for a linear model\n",
    "# Use one-hot encoded vectors for categorical features\n",
    "linear_assembler = VectorAssembler(\n",
    "    inputCols=[\"category_vec\", \"size_vec\", \"color_vec\", \"price\", \"quantity\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "df_linear_features = linear_assembler.transform(df_encoded)\n",
    "print(\"Feature vector for linear model:\")\n",
    "df_linear_features.select(\"features\", \"purchased\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine features for tree-based model\n",
    "# Use indexed features (no one-hot encoding needed)\n",
    "tree_assembler = VectorAssembler(\n",
    "    inputCols=[\"category_idx\", \"size_idx\", \"color_idx\", \"price\", \"quantity\"],\n",
    "    outputCol=\"tree_features\"\n",
    ")\n",
    "\n",
    "df_tree_features = tree_assembler.transform(df_indexed)\n",
    "print(\"Feature vector for tree model:\")\n",
    "df_tree_features.select(\"tree_features\", \"purchased\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Binning and Discretization\n",
    "\n",
    "Converting continuous features into discrete bins can:\n",
    "- Reduce the effect of outliers\n",
    "- Capture non-linear relationships\n",
    "- Make features more interpretable\n",
    "\n",
    "### Bucketizer vs QuantileDiscretizer:\n",
    "\n",
    "**Bucketizer**: Uses fixed bin edges\n",
    "- You specify exact split points\n",
    "- Bins may have unequal counts\n",
    "\n",
    "**QuantileDiscretizer**: Creates bins with equal counts\n",
    "- You specify number of bins\n",
    "- Automatically finds split points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data for binning\n",
    "age_data = [(float(age),) for age in np.random.randint(18, 80, 200)]\n",
    "df_ages = spark.createDataFrame(age_data, [\"age\"])\n",
    "\n",
    "print(\"Age distribution:\")\n",
    "df_ages.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucketizer: Fixed bin edges\n",
    "# Create age groups: 18-30, 30-45, 45-60, 60+\n",
    "bucketizer = Bucketizer(\n",
    "    splits=[18, 30, 45, 60, 80],  # Note: splits has n+1 values for n bins\n",
    "    inputCol=\"age\",\n",
    "    outputCol=\"age_bucket\"\n",
    ")\n",
    "\n",
    "df_bucketed = bucketizer.transform(df_ages)\n",
    "print(\"Bucketed ages:\")\n",
    "df_bucketed.show(10)\n",
    "\n",
    "print(\"\\nBucket distribution:\")\n",
    "df_bucketed.groupBy(\"age_bucket\").count().orderBy(\"age_bucket\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QuantileDiscretizer: Equal-frequency bins\n",
    "# Automatically finds split points for 4 bins with roughly equal counts\n",
    "quantile_discretizer = QuantileDiscretizer(\n",
    "    numBuckets=4,\n",
    "    inputCol=\"age\",\n",
    "    outputCol=\"age_quantile\"\n",
    ")\n",
    "\n",
    "quantile_model = quantile_discretizer.fit(df_ages)\n",
    "df_quantiled = quantile_model.transform(df_ages)\n",
    "\n",
    "print(\"Quantile-based bins:\")\n",
    "df_quantiled.show(10)\n",
    "\n",
    "print(f\"\\nSplit points found: {quantile_model.getSplits()}\")\n",
    "print(\"\\nQuantile distribution (should be roughly equal):\")\n",
    "df_quantiled.groupBy(\"age_quantile\").count().orderBy(\"age_quantile\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Selection\n",
    "\n",
    "**Why Feature Selection?**\n",
    "- Reduce overfitting by removing irrelevant features\n",
    "- Improve model performance and training speed\n",
    "- Enhance interpretability\n",
    "- Reduce storage and computation costs\n",
    "\n",
    "**Methods in PySpark:**\n",
    "\n",
    "1. **ChiSqSelector**: Chi-squared test for categorical targets\n",
    "2. **UnivariateFeatureSelector**: Supports multiple statistical tests\n",
    "3. **VectorIndexer**: Identifies and handles categorical features in vectors\n",
    "4. **Model-based**: Use feature importances from tree models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data with relevant and irrelevant features\n",
    "n_samples = 1000\n",
    "feature_data = []\n",
    "\n",
    "for _ in range(n_samples):\n",
    "    # Relevant features (actually influence target)\n",
    "    f1 = np.random.uniform(0, 10)\n",
    "    f2 = np.random.uniform(0, 10)\n",
    "    # Irrelevant features (random noise)\n",
    "    f3 = np.random.uniform(0, 10)\n",
    "    f4 = np.random.uniform(0, 10)\n",
    "    f5 = np.random.uniform(0, 10)\n",
    "    \n",
    "    # Target depends only on f1 and f2\n",
    "    label = 1 if (f1 + f2 > 10) else 0\n",
    "    \n",
    "    feature_data.append((float(f1), float(f2), float(f3), float(f4), float(f5), label))\n",
    "\n",
    "df_features = spark.createDataFrame(\n",
    "    feature_data,\n",
    "    [\"f1\", \"f2\", \"f3\", \"f4\", \"f5\", \"label\"]\n",
    ")\n",
    "\n",
    "print(f\"Total samples: {df_features.count()}\")\n",
    "print(\"Label distribution:\")\n",
    "df_features.groupBy(\"label\").count().show()\n",
    "df_features.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble features\n",
    "feature_assembler = VectorAssembler(\n",
    "    inputCols=[\"f1\", \"f2\", \"f3\", \"f4\", \"f5\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "df_assembled_features = feature_assembler.transform(df_features)\n",
    "df_assembled_features.select(\"features\", \"label\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChiSqSelector: Select top k features based on chi-squared test\n",
    "# This will identify that f1 and f2 are most relevant\n",
    "chi_selector = ChiSqSelector(\n",
    "    numTopFeatures=3,  # Select top 3 features\n",
    "    featuresCol=\"features\",\n",
    "    outputCol=\"selected_features\",\n",
    "    labelCol=\"label\"\n",
    ")\n",
    "\n",
    "chi_model = chi_selector.fit(df_assembled_features)\n",
    "df_selected = chi_model.transform(df_assembled_features)\n",
    "\n",
    "print(\"Original features vs Selected features:\")\n",
    "df_selected.select(\"features\", \"selected_features\", \"label\").show(10, truncate=False)\n",
    "\n",
    "print(f\"\\nSelected feature indices: {chi_model.selectedFeatures}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UnivariateFeatureSelector: More flexible feature selection\n",
    "# Supports different selection modes and feature types\n",
    "univariate_selector = UnivariateFeatureSelector(\n",
    "    featuresCol=\"features\",\n",
    "    outputCol=\"univariate_features\",\n",
    "    labelCol=\"label\",\n",
    "    selectionMode=\"numTopFeatures\"  # Can also use \"percentile\", \"fpr\", \"fdr\", \"fwe\"\n",
    ")\n",
    "\n",
    "# Set number of features to select\n",
    "univariate_selector.setFeatureType(\"continuous\").setLabelType(\"categorical\")\n",
    "univariate_selector.setSelectionThreshold(2)  # Select top 2 features\n",
    "\n",
    "univariate_model = univariate_selector.fit(df_assembled_features)\n",
    "df_univariate = univariate_model.transform(df_assembled_features)\n",
    "\n",
    "print(\"Univariate feature selection:\")\n",
    "df_univariate.select(\"features\", \"univariate_features\", \"label\").show(10, truncate=False)\n",
    "\n",
    "print(f\"\\nSelected feature indices: {univariate_model.selectedFeatures}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performance with and without feature selection\n",
    "train_df, test_df = df_assembled_features.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# Model with all features\n",
    "lr_all = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n",
    "model_all = lr_all.fit(train_df)\n",
    "predictions_all = model_all.transform(test_df)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy_all = evaluator.evaluate(predictions_all)\n",
    "\n",
    "print(f\"Accuracy with all features (5): {accuracy_all:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with selected features\n",
    "train_selected = chi_model.transform(train_df)\n",
    "test_selected = chi_model.transform(test_df)\n",
    "\n",
    "lr_selected = LogisticRegression(labelCol=\"label\", featuresCol=\"selected_features\", maxIter=10)\n",
    "model_selected = lr_selected.fit(train_selected)\n",
    "predictions_selected = model_selected.transform(test_selected)\n",
    "\n",
    "accuracy_selected = evaluator.evaluate(predictions_selected)\n",
    "\n",
    "print(f\"Accuracy with selected features (3): {accuracy_selected:.4f}\")\n",
    "print(f\"\\nDifference: {accuracy_selected - accuracy_all:.4f}\")\n",
    "print(\"Feature selection removed noise without hurting performance!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Building Robust Feature Engineering Pipelines\n",
    "\n",
    "A complete feature engineering pipeline should:\n",
    "1. Handle missing values (if any)\n",
    "2. Encode categorical features appropriately\n",
    "3. Scale numerical features\n",
    "4. Perform feature selection\n",
    "5. Assemble final feature vector\n",
    "\n",
    "All steps should be in a Pipeline to ensure reproducibility and prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create realistic dataset for complete pipeline\n",
    "n_samples = 1000\n",
    "complete_data = []\n",
    "\n",
    "for _ in range(n_samples):\n",
    "    # Categorical features\n",
    "    department = random.choice([\"Sales\", \"Engineering\", \"Marketing\", \"HR\"])\n",
    "    education = random.choice([\"High School\", \"Bachelor\", \"Master\", \"PhD\"])\n",
    "    \n",
    "    # Numerical features\n",
    "    age = np.random.randint(22, 65)\n",
    "    years_experience = min(age - 22, np.random.randint(0, 40))\n",
    "    salary = np.random.uniform(30000, 150000)\n",
    "    satisfaction_score = np.random.uniform(1, 10)\n",
    "    \n",
    "    # Target: Will employee be promoted?\n",
    "    promo_score = (\n",
    "        years_experience * 0.1 +\n",
    "        satisfaction_score * 0.5 +\n",
    "        (1 if department == \"Engineering\" else 0) * 2 +\n",
    "        (1 if education in [\"Master\", \"PhD\"] else 0) * 1\n",
    "    )\n",
    "    promoted = 1 if promo_score > 6 and np.random.random() > 0.3 else 0\n",
    "    \n",
    "    complete_data.append((\n",
    "        department, education, age, years_experience,\n",
    "        float(salary), float(satisfaction_score), promoted\n",
    "    ))\n",
    "\n",
    "df_complete = spark.createDataFrame(\n",
    "    complete_data,\n",
    "    [\"department\", \"education\", \"age\", \"years_experience\", \"salary\", \"satisfaction_score\", \"promoted\"]\n",
    ")\n",
    "\n",
    "print(f\"Total samples: {df_complete.count()}\")\n",
    "print(\"\\nPromotion distribution:\")\n",
    "df_complete.groupBy(\"promoted\").count().show()\n",
    "df_complete.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data first (before any transformations)\n",
    "complete_train, complete_test = df_complete.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "print(f\"Training samples: {complete_train.count()}\")\n",
    "print(f\"Test samples: {complete_test.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build complete feature engineering pipeline\n",
    "\n",
    "# Stage 1: Index categorical features\n",
    "dept_indexer = StringIndexer(inputCol=\"department\", outputCol=\"dept_idx\")\n",
    "edu_indexer = StringIndexer(inputCol=\"education\", outputCol=\"edu_idx\")\n",
    "\n",
    "# Stage 2: One-hot encode\n",
    "dept_encoder = OneHotEncoder(inputCol=\"dept_idx\", outputCol=\"dept_vec\")\n",
    "edu_encoder = OneHotEncoder(inputCol=\"edu_idx\", outputCol=\"edu_vec\")\n",
    "\n",
    "# Stage 3: Assemble numerical features for scaling\n",
    "numeric_assembler = VectorAssembler(\n",
    "    inputCols=[\"age\", \"years_experience\", \"salary\", \"satisfaction_score\"],\n",
    "    outputCol=\"numeric_features\"\n",
    ")\n",
    "\n",
    "# Stage 4: Scale numerical features\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"numeric_features\",\n",
    "    outputCol=\"scaled_numeric\",\n",
    "    withStd=True,\n",
    "    withMean=False\n",
    ")\n",
    "\n",
    "# Stage 5: Assemble all features (categorical + scaled numerical)\n",
    "final_assembler = VectorAssembler(\n",
    "    inputCols=[\"dept_vec\", \"edu_vec\", \"scaled_numeric\"],\n",
    "    outputCol=\"raw_features\"\n",
    ")\n",
    "\n",
    "# Stage 6: Feature selection\n",
    "feature_selector = ChiSqSelector(\n",
    "    numTopFeatures=8,\n",
    "    featuresCol=\"raw_features\",\n",
    "    outputCol=\"features\",\n",
    "    labelCol=\"promoted\"\n",
    ")\n",
    "\n",
    "# Stage 7: Model\n",
    "lr_model = LogisticRegression(\n",
    "    labelCol=\"promoted\",\n",
    "    featuresCol=\"features\",\n",
    "    maxIter=10\n",
    ")\n",
    "\n",
    "# Create the complete pipeline\n",
    "complete_pipeline = Pipeline(stages=[\n",
    "    dept_indexer,\n",
    "    edu_indexer,\n",
    "    dept_encoder,\n",
    "    edu_encoder,\n",
    "    numeric_assembler,\n",
    "    scaler,\n",
    "    final_assembler,\n",
    "    feature_selector,\n",
    "    lr_model\n",
    "])\n",
    "\n",
    "print(\"Complete feature engineering pipeline created with 9 stages!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the pipeline\n",
    "# All transformations are learned from training data only\n",
    "complete_model = complete_pipeline.fit(complete_train)\n",
    "\n",
    "print(\"Pipeline fitted successfully!\")\n",
    "\n",
    "# Make predictions\n",
    "complete_predictions = complete_model.transform(complete_test)\n",
    "\n",
    "# Show results\n",
    "complete_predictions.select(\n",
    "    \"department\", \"education\", \"age\", \"years_experience\",\n",
    "    \"promoted\", \"prediction\", \"probability\"\n",
    ").show(15, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the complete pipeline\n",
    "complete_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"promoted\",\n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "\n",
    "accuracy = complete_evaluator.evaluate(complete_predictions, {complete_evaluator.metricName: \"accuracy\"})\n",
    "f1 = complete_evaluator.evaluate(complete_predictions, {complete_evaluator.metricName: \"f1\"})\n",
    "precision = complete_evaluator.evaluate(complete_predictions, {complete_evaluator.metricName: \"weightedPrecision\"})\n",
    "recall = complete_evaluator.evaluate(complete_predictions, {complete_evaluator.metricName: \"weightedRecall\"})\n",
    "\n",
    "print(\"Complete Pipeline Performance:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Compare Scaling Methods\n",
    "\n",
    "Create a dataset and compare the performance of a model with different scaling methods.\n",
    "\n",
    "**Tasks:**\n",
    "1. Generate synthetic data with features on different scales\n",
    "2. Train three models: no scaling, StandardScaler, MinMaxScaler\n",
    "3. Compare accuracy and training time\n",
    "4. Determine which scaling method works best for this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# TODO: Generate data with different scales\n",
    "# TODO: Create three pipelines with different scaling\n",
    "# TODO: Train and compare results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Categorical Encoding Strategy\n",
    "\n",
    "Experiment with different categorical encoding strategies for high-cardinality features.\n",
    "\n",
    "**Tasks:**\n",
    "1. Create data with a high-cardinality categorical feature (e.g., 50+ unique values)\n",
    "2. Try: (a) Label encoding, (b) One-hot encoding, (c) Frequency-based encoding (count occurrences)\n",
    "3. Compare model performance and feature vector size\n",
    "4. Discuss the trade-offs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# TODO: Generate data with high-cardinality categorical feature\n",
    "# TODO: Implement different encoding strategies\n",
    "# TODO: Compare results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Feature Selection Impact\n",
    "\n",
    "Build a pipeline that demonstrates the impact of feature selection.\n",
    "\n",
    "**Tasks:**\n",
    "1. Create a dataset with 20 features, where only 5 are actually relevant\n",
    "2. Build two pipelines: one with all features, one with ChiSqSelector selecting top 5\n",
    "3. Compare training time, model complexity, and accuracy\n",
    "4. Verify that the selector identified the correct relevant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# TODO: Generate data with relevant and irrelevant features\n",
    "# TODO: Build pipelines with and without selection\n",
    "# TODO: Compare and analyze results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Exercise Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1: Compare Scaling Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data with vastly different scales\n",
    "import time\n",
    "\n",
    "n = 2000\n",
    "scale_data = []\n",
    "for _ in range(n):\n",
    "    # Features on very different scales\n",
    "    tiny_feature = np.random.uniform(0, 1)\n",
    "    small_feature = np.random.uniform(0, 100)\n",
    "    large_feature = np.random.uniform(0, 100000)\n",
    "    \n",
    "    # Label depends on all three (but large_feature dominates without scaling)\n",
    "    label = 1 if (tiny_feature * 10000 + small_feature * 100 + large_feature * 0.1 > 6000) else 0\n",
    "    \n",
    "    scale_data.append((float(tiny_feature), float(small_feature), float(large_feature), label))\n",
    "\n",
    "df_scale_test = spark.createDataFrame(scale_data, [\"tiny\", \"small\", \"large\", \"label\"])\n",
    "train_scale, test_scale = df_scale_test.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "print(\"Sample data:\")\n",
    "df_scale_test.show(5)\n",
    "df_scale_test.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline 1: No scaling\n",
    "no_scale_assembler = VectorAssembler(inputCols=[\"tiny\", \"small\", \"large\"], outputCol=\"features\")\n",
    "no_scale_lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=20)\n",
    "no_scale_pipeline = Pipeline(stages=[no_scale_assembler, no_scale_lr])\n",
    "\n",
    "start = time.time()\n",
    "no_scale_model = no_scale_pipeline.fit(train_scale)\n",
    "no_scale_time = time.time() - start\n",
    "no_scale_pred = no_scale_model.transform(test_scale)\n",
    "no_scale_acc = evaluator.evaluate(no_scale_pred)\n",
    "\n",
    "print(f\"No Scaling - Accuracy: {no_scale_acc:.4f}, Time: {no_scale_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline 2: StandardScaler\n",
    "std_assembler = VectorAssembler(inputCols=[\"tiny\", \"small\", \"large\"], outputCol=\"raw_features\")\n",
    "std_scaler = StandardScaler(inputCol=\"raw_features\", outputCol=\"features\", withStd=True, withMean=False)\n",
    "std_lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=20)\n",
    "std_pipeline = Pipeline(stages=[std_assembler, std_scaler, std_lr])\n",
    "\n",
    "start = time.time()\n",
    "std_model = std_pipeline.fit(train_scale)\n",
    "std_time = time.time() - start\n",
    "std_pred = std_model.transform(test_scale)\n",
    "std_acc = evaluator.evaluate(std_pred)\n",
    "\n",
    "print(f\"StandardScaler - Accuracy: {std_acc:.4f}, Time: {std_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline 3: MinMaxScaler\n",
    "mm_assembler = VectorAssembler(inputCols=[\"tiny\", \"small\", \"large\"], outputCol=\"raw_features\")\n",
    "mm_scaler = MinMaxScaler(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "mm_lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=20)\n",
    "mm_pipeline = Pipeline(stages=[mm_assembler, mm_scaler, mm_lr])\n",
    "\n",
    "start = time.time()\n",
    "mm_model = mm_pipeline.fit(train_scale)\n",
    "mm_time = time.time() - start\n",
    "mm_pred = mm_model.transform(test_scale)\n",
    "mm_acc = evaluator.evaluate(mm_pred)\n",
    "\n",
    "print(f\"MinMaxScaler - Accuracy: {mm_acc:.4f}, Time: {mm_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary comparison\n",
    "print(\"\\n=== Scaling Method Comparison ===\")\n",
    "print(f\"No Scaling:      Accuracy={no_scale_acc:.4f}, Time={no_scale_time:.2f}s\")\n",
    "print(f\"StandardScaler:  Accuracy={std_acc:.4f}, Time={std_time:.2f}s\")\n",
    "print(f\"MinMaxScaler:    Accuracy={mm_acc:.4f}, Time={mm_time:.2f}s\")\n",
    "print(\"\\nConclusion: Scaling improves both accuracy and convergence speed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 2: Categorical Encoding Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate high-cardinality categorical data\n",
    "# Scenario: User ID (100 unique users)\n",
    "n = 1000\n",
    "user_ids = [f\"user_{i:03d}\" for i in range(100)]\n",
    "\n",
    "high_card_data = []\n",
    "for _ in range(n):\n",
    "    user_id = random.choice(user_ids)\n",
    "    activity_score = np.random.uniform(0, 100)\n",
    "    \n",
    "    # Some users are more likely to convert\n",
    "    user_num = int(user_id.split(\"_\")[1])\n",
    "    base_prob = 0.5 if user_num < 50 else 0.3\n",
    "    converted = 1 if (activity_score > 50 and np.random.random() < base_prob) else 0\n",
    "    \n",
    "    high_card_data.append((user_id, float(activity_score), converted))\n",
    "\n",
    "df_high_card = spark.createDataFrame(high_card_data, [\"user_id\", \"activity_score\", \"converted\"])\n",
    "\n",
    "print(f\"Total samples: {df_high_card.count()}\")\n",
    "print(f\"Unique users: {df_high_card.select('user_id').distinct().count()}\")\n",
    "df_high_card.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "hc_train, hc_test = df_high_card.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# Strategy 1: Label encoding (small vector)\n",
    "label_indexer = StringIndexer(inputCol=\"user_id\", outputCol=\"user_idx\")\n",
    "label_assembler = VectorAssembler(inputCols=[\"user_idx\", \"activity_score\"], outputCol=\"features\")\n",
    "label_lr = LogisticRegression(labelCol=\"converted\", featuresCol=\"features\", maxIter=10)\n",
    "label_pipeline = Pipeline(stages=[label_indexer, label_assembler, label_lr])\n",
    "\n",
    "label_model = label_pipeline.fit(hc_train)\n",
    "label_pred = label_model.transform(hc_test)\n",
    "label_acc = evaluator.evaluate(label_pred.withColumnRenamed(\"converted\", \"label\"))\n",
    "\n",
    "print(f\"Label Encoding - Accuracy: {label_acc:.4f}\")\n",
    "print(f\"Feature vector size: 2 (user_idx + activity_score)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 2: One-hot encoding (large sparse vector)\n",
    "oh_indexer = StringIndexer(inputCol=\"user_id\", outputCol=\"user_idx\")\n",
    "oh_encoder = OneHotEncoder(inputCol=\"user_idx\", outputCol=\"user_vec\")\n",
    "oh_assembler = VectorAssembler(inputCols=[\"user_vec\", \"activity_score\"], outputCol=\"features\")\n",
    "oh_lr = LogisticRegression(labelCol=\"converted\", featuresCol=\"features\", maxIter=10)\n",
    "oh_pipeline = Pipeline(stages=[oh_indexer, oh_encoder, oh_assembler, oh_lr])\n",
    "\n",
    "oh_model = oh_pipeline.fit(hc_train)\n",
    "oh_pred = oh_model.transform(hc_test)\n",
    "oh_acc = evaluator.evaluate(oh_pred.withColumnRenamed(\"converted\", \"label\"))\n",
    "\n",
    "print(f\"One-Hot Encoding - Accuracy: {oh_acc:.4f}\")\n",
    "print(f\"Feature vector size: ~100 (one per user + activity_score)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 3: Frequency encoding (count-based)\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "# Calculate user frequency in training data\n",
    "user_counts = hc_train.groupBy(\"user_id\").agg(count(\"*\").alias(\"user_frequency\"))\n",
    "hc_train_freq = hc_train.join(user_counts, on=\"user_id\", how=\"left\")\n",
    "hc_test_freq = hc_test.join(user_counts, on=\"user_id\", how=\"left\").fillna(0, subset=[\"user_frequency\"])\n",
    "\n",
    "freq_assembler = VectorAssembler(inputCols=[\"user_frequency\", \"activity_score\"], outputCol=\"features\")\n",
    "freq_lr = LogisticRegression(labelCol=\"converted\", featuresCol=\"features\", maxIter=10)\n",
    "freq_pipeline = Pipeline(stages=[freq_assembler, freq_lr])\n",
    "\n",
    "freq_model = freq_pipeline.fit(hc_train_freq)\n",
    "freq_pred = freq_model.transform(hc_test_freq)\n",
    "freq_acc = evaluator.evaluate(freq_pred.withColumnRenamed(\"converted\", \"label\"))\n",
    "\n",
    "print(f\"Frequency Encoding - Accuracy: {freq_acc:.4f}\")\n",
    "print(f\"Feature vector size: 2 (user_frequency + activity_score)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison summary\n",
    "print(\"\\n=== Encoding Strategy Comparison ===\")\n",
    "print(f\"Label Encoding:     Acc={label_acc:.4f}, Vector Size=2\")\n",
    "print(f\"One-Hot Encoding:   Acc={oh_acc:.4f}, Vector Size=~100\")\n",
    "print(f\"Frequency Encoding: Acc={freq_acc:.4f}, Vector Size=2\")\n",
    "print(\"\\nTrade-offs:\")\n",
    "print(\"- One-hot: Better for linear models but creates huge sparse vectors\")\n",
    "print(\"- Label: Compact but implies ordering\")\n",
    "print(\"- Frequency: Compact and captures popularity information\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 3: Feature Selection Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data with 20 features (only 5 relevant)\n",
    "n = 2000\n",
    "many_features_data = []\n",
    "\n",
    "for _ in range(n):\n",
    "    # 5 relevant features\n",
    "    relevant = [np.random.uniform(0, 10) for _ in range(5)]\n",
    "    # 15 irrelevant features (noise)\n",
    "    irrelevant = [np.random.uniform(0, 10) for _ in range(15)]\n",
    "    \n",
    "    # Label depends only on relevant features\n",
    "    label = 1 if sum(relevant) > 25 else 0\n",
    "    \n",
    "    all_features = relevant + irrelevant\n",
    "    many_features_data.append(tuple(all_features + [label]))\n",
    "\n",
    "feature_cols = [f\"f{i}\" for i in range(20)]\n",
    "df_many = spark.createDataFrame(many_features_data, feature_cols + [\"label\"])\n",
    "\n",
    "print(f\"Total samples: {df_many.count()}\")\n",
    "print(f\"Total features: 20 (5 relevant + 15 noise)\")\n",
    "print(\"\\nLabel distribution:\")\n",
    "df_many.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "many_train, many_test = df_many.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# Pipeline 1: All features\n",
    "all_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "all_lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n",
    "all_pipeline = Pipeline(stages=[all_assembler, all_lr])\n",
    "\n",
    "start = time.time()\n",
    "all_model = all_pipeline.fit(many_train)\n",
    "all_time = time.time() - start\n",
    "all_pred = all_model.transform(many_test)\n",
    "all_acc = evaluator.evaluate(all_pred)\n",
    "\n",
    "print(f\"All Features (20) - Accuracy: {all_acc:.4f}, Time: {all_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline 2: Selected features (top 5)\n",
    "sel_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"all_features\")\n",
    "selector = ChiSqSelector(numTopFeatures=5, featuresCol=\"all_features\", outputCol=\"features\", labelCol=\"label\")\n",
    "sel_lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n",
    "sel_pipeline = Pipeline(stages=[sel_assembler, selector, sel_lr])\n",
    "\n",
    "start = time.time()\n",
    "sel_model = sel_pipeline.fit(many_train)\n",
    "sel_time = time.time() - start\n",
    "sel_pred = sel_model.transform(many_test)\n",
    "sel_acc = evaluator.evaluate(sel_pred)\n",
    "\n",
    "# Get selected features\n",
    "selector_stage = sel_model.stages[1]  # The ChiSqSelector stage\n",
    "selected_indices = selector_stage.selectedFeatures\n",
    "\n",
    "print(f\"Selected Features (5) - Accuracy: {sel_acc:.4f}, Time: {sel_time:.2f}s\")\n",
    "print(f\"\\nSelected feature indices: {selected_indices}\")\n",
    "print(f\"Expected relevant indices: [0, 1, 2, 3, 4]\")\n",
    "print(f\"\\nCorrectly identified: {sum(1 for i in selected_indices if i < 5)}/5 relevant features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"\\n=== Feature Selection Impact ===\")\n",
    "print(f\"All Features (20):      Acc={all_acc:.4f}, Time={all_time:.2f}s\")\n",
    "print(f\"Selected Features (5):  Acc={sel_acc:.4f}, Time={sel_time:.2f}s\")\n",
    "print(f\"\\nBenefits:\")\n",
    "print(f\"- Training speedup: {all_time/sel_time:.2f}x faster\")\n",
    "print(f\"- Model simplicity: 75% fewer features\")\n",
    "print(f\"- Accuracy change: {sel_acc - all_acc:+.4f}\")\n",
    "print(f\"\\nFeature selection removed noise and improved efficiency!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "Congratulations! You've mastered feature engineering at scale with PySpark.\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Feature Scaling**:\n",
    "   - StandardScaler: Best for normally distributed data, zero-mean unit-variance\n",
    "   - MinMaxScaler: Best for bounded ranges, preserves zero values\n",
    "   - Normalizer: Best for sample-wise normalization (text, similarity)\n",
    "   - Always scale before training distance-based or gradient-based models\n",
    "\n",
    "2. **Categorical Encoding**:\n",
    "   - Label encoding: Compact, good for tree models, implies ordering\n",
    "   - One-hot encoding: No ordering assumption, good for linear models, can be sparse\n",
    "   - Choose based on cardinality and model type\n",
    "\n",
    "3. **Binning and Discretization**:\n",
    "   - Bucketizer: Fixed bin edges, interpretable bins\n",
    "   - QuantileDiscretizer: Equal-frequency bins, handles skewed distributions\n",
    "   - Useful for capturing non-linear patterns and reducing outlier effects\n",
    "\n",
    "4. **Feature Selection**:\n",
    "   - Reduces overfitting and improves generalization\n",
    "   - Speeds up training and prediction\n",
    "   - ChiSqSelector: Statistical approach for categorical targets\n",
    "   - UnivariateFeatureSelector: More flexible, supports multiple tests\n",
    "\n",
    "5. **Production Pipelines**:\n",
    "   - Always split data before building pipelines\n",
    "   - Chain all transformations to ensure consistency\n",
    "   - Fit transformations only on training data\n",
    "   - Pipelines make deployment and reuse straightforward\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "- Understand your data distribution before choosing scaling methods\n",
    "- Consider model type when encoding categorical features\n",
    "- Use feature selection to combat the curse of dimensionality\n",
    "- Always validate transformations on held-out test data\n",
    "- Document your feature engineering decisions for reproducibility\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In [Module 10: Model Training and Evaluation](10_model_training_and_evaluation.ipynb), you'll learn:\n",
    "- Training multiple classification and regression algorithms\n",
    "- Cross-validation for robust model selection\n",
    "- Hyperparameter tuning with ParamGridBuilder\n",
    "- Comprehensive model evaluation strategies\n",
    "- Comparing and selecting the best model\n",
    "\n",
    "### Additional Resources:\n",
    "\n",
    "- [Feature Transformers Guide](https://spark.apache.org/docs/latest/ml-features.html)\n",
    "- [Feature Selection in MLlib](https://spark.apache.org/docs/latest/ml-features.html#feature-selectors)\n",
    "- [ML Pipelines Documentation](https://spark.apache.org/docs/latest/ml-pipeline.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "spark.stop()\n",
    "print(\"Spark session stopped. Excellent work on feature engineering!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
