{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 11: Git for Data Science - Specialized Workflows\n",
    "\n",
    "**Difficulty**: ‚≠ê‚≠ê (Intermediate)\n",
    "\n",
    "**Estimated Time**: 120-150 minutes\n",
    "\n",
    "**Prerequisites**: \n",
    "- Module 01: Git Fundamentals\n",
    "- Module 02: GitHub Essentials\n",
    "- Module 10: Git Best Practices\n",
    "- Basic understanding of data science workflows\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Version control Jupyter notebooks effectively\n",
    "2. Handle large datasets using Git LFS\n",
    "3. Implement data versioning with DVC\n",
    "4. Manage machine learning models in Git\n",
    "5. Collaborate on notebooks without conflicts\n",
    "6. Track experiments and model versions\n",
    "7. Set up data science-specific Git workflows\n",
    "8. Use nbdime for notebook diff and merge\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Data Science Git Challenge\n",
    "\n",
    "### Why Data Science is Different\n",
    "\n",
    "Traditional software development:\n",
    "- ‚úÖ Small text files (source code)\n",
    "- ‚úÖ Deterministic outputs\n",
    "- ‚úÖ Clear separation of code and data\n",
    "\n",
    "Data science adds complexity:\n",
    "- ‚ö†Ô∏è Large binary files (datasets, models)\n",
    "- ‚ö†Ô∏è Non-deterministic experiments\n",
    "- ‚ö†Ô∏è Notebooks mix code, outputs, and visualizations\n",
    "- ‚ö†Ô∏è Data preprocessing pipelines\n",
    "- ‚ö†Ô∏è Model artifacts and checkpoints\n",
    "- ‚ö†Ô∏è Multiple experiment iterations\n",
    "\n",
    "### What Should You Version Control?\n",
    "\n",
    "```\n",
    "‚úÖ YES - Version in Git:\n",
    "‚îú‚îÄ‚îÄ Source code (.py files)\n",
    "‚îú‚îÄ‚îÄ Notebooks (.ipynb, without outputs)\n",
    "‚îú‚îÄ‚îÄ Configuration files\n",
    "‚îú‚îÄ‚îÄ Requirements and dependencies\n",
    "‚îú‚îÄ‚îÄ Documentation\n",
    "‚îú‚îÄ‚îÄ Small sample datasets (<10MB)\n",
    "‚îî‚îÄ‚îÄ Scripts and utilities\n",
    "\n",
    "‚ùå NO - Don't version in Git:\n",
    "‚îú‚îÄ‚îÄ Large datasets (>10MB)\n",
    "‚îú‚îÄ‚îÄ Trained models (>10MB)\n",
    "‚îú‚îÄ‚îÄ Notebook outputs\n",
    "‚îú‚îÄ‚îÄ Checkpoint files\n",
    "‚îú‚îÄ‚îÄ Cache directories\n",
    "‚îî‚îÄ‚îÄ Virtual environments\n",
    "\n",
    "üîß ALTERNATIVE - Use specialized tools:\n",
    "‚îú‚îÄ‚îÄ Large files ‚Üí Git LFS\n",
    "‚îú‚îÄ‚îÄ Datasets ‚Üí DVC, S3, cloud storage\n",
    "‚îú‚îÄ‚îÄ Models ‚Üí MLflow, DVC, model registry\n",
    "‚îî‚îÄ‚îÄ Experiments ‚Üí MLflow, Weights & Biases\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Jupyter Notebooks in Version Control\n",
    "\n",
    "### The Notebook Problem\n",
    "\n",
    "Jupyter notebooks are **JSON files** that contain:\n",
    "- Code cells\n",
    "- Markdown cells\n",
    "- Cell outputs (text, images, data)\n",
    "- Execution counts\n",
    "- Metadata\n",
    "\n",
    "**Example notebook structure**:\n",
    "```json\n",
    "{\n",
    "  \"cells\": [\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": 5,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"data\": {\n",
    "            \"text/plain\": [\"Result: 42\"]\n",
    "          },\n",
    "          \"output_type\": \"execute_result\"\n",
    "        }\n",
    "      ],\n",
    "      \"source\": [\"x = 42\\n\", \"x\"]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "### Problems with Versioning Notebooks\n",
    "\n",
    "1. **Outputs bloat**: Images can be MBs in base64\n",
    "2. **Execution counts change**: Causes unnecessary diffs\n",
    "3. **Metadata noise**: Irrelevant changes show up\n",
    "4. **Merge conflicts**: JSON structure is hard to merge\n",
    "5. **Not human-readable**: Diffs are hard to review\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Solution 1: nbstripout - Strip Notebook Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install nbstripout\n",
    "!pip install -q nbstripout\n",
    "\n",
    "print(\"‚úì nbstripout installed\")\n",
    "print(\"\\nWhat is nbstripout?\")\n",
    "print(\"A tool that removes outputs from Jupyter notebooks before committing.\")\n",
    "print(\"\\nWhy use it?\")\n",
    "print(\"- Keeps repository size small\")\n",
    "print(\"- Focuses diffs on actual code changes\")\n",
    "print(\"- Prevents accidental commit of sensitive outputs\")\n",
    "print(\"- Reduces merge conflicts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Create practice environment\n",
    "practice_dir = Path(\"ds_git_practice\")\n",
    "practice_dir.mkdir(exist_ok=True)\n",
    "os.chdir(practice_dir)\n",
    "\n",
    "# Initialize Git repository\n",
    "!git init\n",
    "!git config user.name \"DS Learner\"\n",
    "!git config user.email \"ds@example.com\"\n",
    "\n",
    "print(\"‚úì Created practice repository\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install nbstripout for this repository\n",
    "# This sets up a Git filter that automatically strips outputs\n",
    "!nbstripout --install\n",
    "\n",
    "print(\"‚úì nbstripout filter installed\")\n",
    "print(\"\\nFrom now on, all notebooks will be stripped before commit!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check .gitattributes (created by nbstripout)\n",
    "if Path(\".gitattributes\").exists():\n",
    "    with open(\".gitattributes\", \"r\") as f:\n",
    "        print(\"Contents of .gitattributes:\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f.read())\n",
    "        print(\"=\" * 50)\n",
    "        print(\"\\nThis tells Git to filter .ipynb files through nbstripout\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Usage\n",
    "\n",
    "```bash\n",
    "# Strip outputs from a single notebook\n",
    "nbstripout notebook.ipynb\n",
    "\n",
    "# Strip outputs from all notebooks in directory\n",
    "nbstripout notebooks/*.ipynb\n",
    "\n",
    "# Restore outputs (if you have the original)\n",
    "# Not possible - outputs are permanently removed!\n",
    "```\n",
    "\n",
    "**Best Practice**: Keep a separate directory for executed notebooks\n",
    "\n",
    "```\n",
    "notebooks/\n",
    "‚îú‚îÄ‚îÄ development/     # Working notebooks with outputs (gitignored)\n",
    "‚îî‚îÄ‚îÄ final/          # Clean notebooks without outputs (versioned)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Solution 2: nbdime - Notebook Diff and Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install nbdime\n",
    "!pip install -q nbdime\n",
    "\n",
    "print(\"‚úì nbdime installed\")\n",
    "print(\"\\nWhat is nbdime?\")\n",
    "print(\"Notebook-aware diff and merge tool that:\")\n",
    "print(\"- Shows meaningful diffs between notebooks\")\n",
    "print(\"- Provides visual diff in browser\")\n",
    "print(\"- Handles notebook merges intelligently\")\n",
    "print(\"- Integrates with Git and Jupyter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure nbdime for Git\n",
    "!nbdime config-git --enable\n",
    "\n",
    "print(\"‚úì nbdime configured for Git\")\n",
    "print(\"\\nGit will now use nbdime for notebook diffs and merges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using nbdime\n",
    "\n",
    "```bash\n",
    "# Compare two notebooks\n",
    "nbdiff notebook1.ipynb notebook2.ipynb\n",
    "\n",
    "# Visual diff in browser\n",
    "nbdiff-web notebook1.ipynb notebook2.ipynb\n",
    "\n",
    "# Diff with Git\n",
    "git diff notebook.ipynb\n",
    "# Now shows notebook-friendly diff!\n",
    "\n",
    "# 3-way merge during conflicts\n",
    "nbmerge base.ipynb local.ipynb remote.ipynb\n",
    "\n",
    "# Visual merge tool\n",
    "nbmerge-web base.ipynb local.ipynb remote.ipynb\n",
    "```\n",
    "\n",
    "### Jupyter Integration\n",
    "\n",
    "```bash\n",
    "# Enable nbdime in Jupyter\n",
    "nbdime extensions --enable\n",
    "\n",
    "# Launch Jupyter with nbdime\n",
    "jupyter notebook\n",
    "# Now has \"git\" button for diffs!\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Git LFS (Large File Storage)\n",
    "\n",
    "### What is Git LFS?\n",
    "\n",
    "Git LFS is an extension that handles large files efficiently:\n",
    "- Stores large files on a separate server\n",
    "- Keeps only **pointers** in your Git repository\n",
    "- Downloads large files only when needed\n",
    "- Works with GitHub, GitLab, Bitbucket\n",
    "\n",
    "### How It Works\n",
    "\n",
    "```\n",
    "Without LFS:                     With LFS:\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "Git Repo                         Git Repo\n",
    "‚îú‚îÄ‚îÄ code.py                      ‚îú‚îÄ‚îÄ code.py\n",
    "‚îú‚îÄ‚îÄ data.csv (100MB)  ‚Üí  SLOW   ‚îú‚îÄ‚îÄ data.csv (pointer) ‚Üí  FAST\n",
    "‚îî‚îÄ‚îÄ model.pkl (500MB) ‚Üí  SLOW   ‚îî‚îÄ‚îÄ model.pkl (pointer) ‚Üí FAST\n",
    "                                              ‚Üì\n",
    "                                        LFS Storage\n",
    "                                        ‚îú‚îÄ‚îÄ data.csv (100MB)\n",
    "                                        ‚îî‚îÄ‚îÄ model.pkl (500MB)\n",
    "```\n",
    "\n",
    "### Installation\n",
    "\n",
    "```bash\n",
    "# Install Git LFS\n",
    "# On Ubuntu/Debian:\n",
    "sudo apt-get install git-lfs\n",
    "\n",
    "# On macOS:\n",
    "brew install git-lfs\n",
    "\n",
    "# On Windows:\n",
    "# Download from https://git-lfs.github.com/\n",
    "\n",
    "# Initialize Git LFS\n",
    "git lfs install\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Using Git LFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if Git LFS is available\n",
    "!git lfs version 2>/dev/null || echo \"Git LFS not installed. Install from https://git-lfs.github.com/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track file types with LFS\n",
    "# This creates .gitattributes entries\n",
    "\n",
    "lfs_config = \"\"\"# Track data files\n",
    "*.csv filter=lfs diff=lfs merge=lfs -text\n",
    "*.parquet filter=lfs diff=lfs merge=lfs -text\n",
    "*.h5 filter=lfs diff=lfs merge=lfs -text\n",
    "\n",
    "# Track model files\n",
    "*.pkl filter=lfs diff=lfs merge=lfs -text\n",
    "*.h5 filter=lfs diff=lfs merge=lfs -text\n",
    "*.pb filter=lfs diff=lfs merge=lfs -text\n",
    "*.pth filter=lfs diff=lfs merge=lfs -text\n",
    "*.onnx filter=lfs diff=lfs merge=lfs -text\n",
    "\n",
    "# Track image datasets\n",
    "*.zip filter=lfs diff=lfs merge=lfs -text\n",
    "*.tar.gz filter=lfs diff=lfs merge=lfs -text\n",
    "\"\"\"\n",
    "\n",
    "# Append to .gitattributes\n",
    "with open(\".gitattributes\", \"a\") as f:\n",
    "    f.write(\"\\n\" + lfs_config)\n",
    "\n",
    "print(\"‚úì Configured Git LFS for common data science files\")\n",
    "print(\"\\nTracked file types:\")\n",
    "print(\"- Data: .csv, .parquet, .h5\")\n",
    "print(\"- Models: .pkl, .h5, .pb, .pth, .onnx\")\n",
    "print(\"- Archives: .zip, .tar.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Git LFS Commands\n",
    "\n",
    "```bash\n",
    "# Track specific file types\n",
    "git lfs track \"*.csv\"\n",
    "git lfs track \"*.pkl\"\n",
    "git lfs track \"models/*.h5\"\n",
    "\n",
    "# See what's being tracked\n",
    "git lfs track\n",
    "\n",
    "# See which files are stored in LFS\n",
    "git lfs ls-files\n",
    "\n",
    "# Pull LFS files\n",
    "git lfs pull\n",
    "\n",
    "# Fetch LFS files without checking out\n",
    "git lfs fetch\n",
    "\n",
    "# Migrate existing files to LFS\n",
    "git lfs migrate import --include=\"*.csv\"\n",
    "```\n",
    "\n",
    "### GitHub LFS Limits\n",
    "\n",
    "**Free accounts**:\n",
    "- 1 GB storage\n",
    "- 1 GB bandwidth per month\n",
    "\n",
    "**Paid accounts**:\n",
    "- Additional packs available\n",
    "- $5/month for 50GB storage + 50GB bandwidth\n",
    "\n",
    "For larger datasets, consider:\n",
    "- AWS S3\n",
    "- Google Cloud Storage\n",
    "- Azure Blob Storage\n",
    "- DVC (Data Version Control)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. DVC - Data Version Control\n",
    "\n",
    "### What is DVC?\n",
    "\n",
    "DVC is like **Git for data**:\n",
    "- Versions large datasets\n",
    "- Tracks machine learning models\n",
    "- Manages ML pipelines\n",
    "- Works with any storage (S3, GCS, Azure, local)\n",
    "- Integrates seamlessly with Git\n",
    "\n",
    "### How DVC Works\n",
    "\n",
    "```\n",
    "1. Add data file to DVC:\n",
    "   dvc add data/large_dataset.csv\n",
    "   \n",
    "   Creates:\n",
    "   ‚îú‚îÄ‚îÄ data/large_dataset.csv.dvc  (tracked in Git)\n",
    "   ‚îî‚îÄ‚îÄ data/large_dataset.csv      (stored in DVC cache)\n",
    "\n",
    "2. Git tracks only the .dvc file:\n",
    "   git add data/large_dataset.csv.dvc\n",
    "   git commit -m \"Add large dataset\"\n",
    "\n",
    "3. Push data to remote storage:\n",
    "   dvc push\n",
    "```\n",
    "\n",
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install DVC\n",
    "!pip install -q dvc\n",
    "\n",
    "print(\"‚úì DVC installed\")\n",
    "print(\"\\nDVC capabilities:\")\n",
    "print(\"- Version large files efficiently\")\n",
    "print(\"- Track ML experiments\")\n",
    "print(\"- Define reproducible pipelines\")\n",
    "print(\"- Share data across team\")\n",
    "print(\"- Works with any cloud storage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DVC in repository\n",
    "!dvc init\n",
    "\n",
    "print(\"\\n‚úì DVC initialized\")\n",
    "print(\"\\nCreated:\")\n",
    "print(\"- .dvc/ directory (DVC config and cache)\")\n",
    "print(\"- .dvcignore (like .gitignore for DVC)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Using DVC - Practical Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample \"large\" dataset\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simulate 100,000 rows of sensor data\n",
    "large_dataset = pd.DataFrame({\n",
    "    'timestamp': pd.date_range('2024-01-01', periods=100000, freq='1min'),\n",
    "    'sensor_1': np.random.normal(100, 15, 100000),\n",
    "    'sensor_2': np.random.normal(50, 10, 100000),\n",
    "    'sensor_3': np.random.normal(75, 20, 100000),\n",
    "    'temperature': np.random.normal(20, 5, 100000),\n",
    "    'humidity': np.random.uniform(30, 80, 100000),\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "os.makedirs('data', exist_ok=True)\n",
    "large_dataset.to_csv('data/sensor_data.csv', index=False)\n",
    "\n",
    "print(\"‚úì Created sample dataset\")\n",
    "print(f\"\\nDataset shape: {large_dataset.shape}\")\n",
    "print(f\"File size: {os.path.getsize('data/sensor_data.csv') / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add dataset to DVC\n",
    "!dvc add data/sensor_data.csv\n",
    "\n",
    "print(\"\\n‚úì Added to DVC\")\n",
    "print(\"\\nWhat happened:\")\n",
    "print(\"1. DVC moved file to cache (.dvc/cache/)\")\n",
    "print(\"2. Created data/sensor_data.csv.dvc (metadata file)\")\n",
    "print(\"3. Added data/sensor_data.csv to .gitignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the .dvc file\n",
    "with open('data/sensor_data.csv.dvc', 'r') as f:\n",
    "    dvc_file = f.read()\n",
    "\n",
    "print(\"Contents of sensor_data.csv.dvc:\")\n",
    "print(\"=\" * 50)\n",
    "print(dvc_file)\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nThis file contains:\")\n",
    "print(\"- MD5 hash of the data file\")\n",
    "print(\"- Size of the data file\")\n",
    "print(\"- Path to the data file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commit the .dvc file to Git\n",
    "!git add data/sensor_data.csv.dvc data/.gitignore\n",
    "!git commit -m \"Add sensor dataset with DVC\"\n",
    "\n",
    "print(\"‚úì Committed .dvc file to Git\")\n",
    "print(\"\\nNow:\")\n",
    "print(\"- Git tracks only the small .dvc file (~100 bytes)\")\n",
    "print(\"- DVC manages the large data file (~8 MB)\")\n",
    "print(\"- Team members can pull data with 'dvc pull'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DVC Remote Storage\n",
    "\n",
    "Configure where DVC stores your data:\n",
    "\n",
    "```bash\n",
    "# Local remote (for testing)\n",
    "dvc remote add -d myremote /tmp/dvc-storage\n",
    "\n",
    "# AWS S3\n",
    "dvc remote add -d myremote s3://mybucket/dvc-storage\n",
    "\n",
    "# Google Cloud Storage\n",
    "dvc remote add -d myremote gs://mybucket/dvc-storage\n",
    "\n",
    "# Azure Blob Storage\n",
    "dvc remote add -d myremote azure://mycontainer/dvc-storage\n",
    "\n",
    "# Push data to remote\n",
    "dvc push\n",
    "\n",
    "# Pull data from remote\n",
    "dvc pull\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Versioning Machine Learning Models\n",
    "\n",
    "### Strategy 1: DVC for Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "import pickle\n",
    "\n",
    "# Train a simple model\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Save model\n",
    "os.makedirs('models', exist_ok=True)\n",
    "with open('models/classifier_v1.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "print(\"‚úì Trained and saved model\")\n",
    "print(f\"Model size: {os.path.getsize('models/classifier_v1.pkl') / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version the model with DVC\n",
    "!dvc add models/classifier_v1.pkl\n",
    "!git add models/classifier_v1.pkl.dvc models/.gitignore\n",
    "!git commit -m \"Add classifier model v1\"\n",
    "\n",
    "print(\"‚úì Model versioned with DVC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy 2: Model Registry\n",
    "\n",
    "For production systems, use a model registry:\n",
    "\n",
    "**MLflow**:\n",
    "```python\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# Log model\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_params({\"n_estimators\": 100})\n",
    "    mlflow.log_metric(\"accuracy\", 0.95)\n",
    "    mlflow.sklearn.log_model(model, \"classifier\")\n",
    "\n",
    "# Register model\n",
    "mlflow.register_model(\n",
    "    \"runs:/abc123/classifier\",\n",
    "    \"SensorClassifier\"\n",
    ")\n",
    "```\n",
    "\n",
    "**Benefits**:\n",
    "- Centralized model storage\n",
    "- Metadata tracking (metrics, parameters)\n",
    "- Model lineage\n",
    "- Deployment integration\n",
    "- Role-based access control\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Data Science .gitignore Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive .gitignore for data science\n",
    "ds_gitignore = \"\"\"# Byte-compiled / optimized / DLL files\n",
    "__pycache__/\n",
    "*.py[cod]\n",
    "*$py.class\n",
    "\n",
    "# Virtual Environments\n",
    "venv/\n",
    "env/\n",
    ".venv/\n",
    "ENV/\n",
    "conda_env/\n",
    "\n",
    "# Jupyter Notebook\n",
    ".ipynb_checkpoints/\n",
    "*-checkpoint.ipynb\n",
    "\n",
    "# Data files (use DVC instead)\n",
    "*.csv\n",
    "*.tsv\n",
    "*.xlsx\n",
    "*.xls\n",
    "*.parquet\n",
    "*.feather\n",
    "*.h5\n",
    "*.hdf5\n",
    "*.db\n",
    "*.sqlite\n",
    "# Exception: small sample/test data\n",
    "!data/sample/**\n",
    "!data/test/**\n",
    "\n",
    "# Model files (use DVC or MLflow)\n",
    "*.pkl\n",
    "*.pickle\n",
    "*.joblib\n",
    "*.h5\n",
    "*.pb\n",
    "*.pt\n",
    "*.pth\n",
    "*.onnx\n",
    "*.tflite\n",
    "models/\n",
    "checkpoints/\n",
    "saved_models/\n",
    "\n",
    "# Large files\n",
    "*.zip\n",
    "*.tar\n",
    "*.tar.gz\n",
    "*.rar\n",
    "*.7z\n",
    "\n",
    "# Image datasets\n",
    "*.jpg\n",
    "*.jpeg\n",
    "*.png\n",
    "*.gif\n",
    "*.bmp\n",
    "*.tiff\n",
    "*.svg\n",
    "# Exception: documentation images\n",
    "!docs/images/**\n",
    "!reports/figures/**\n",
    "!README_images/**\n",
    "\n",
    "# Video/Audio\n",
    "*.mp4\n",
    "*.avi\n",
    "*.mov\n",
    "*.mp3\n",
    "*.wav\n",
    "*.flac\n",
    "\n",
    "# DVC\n",
    "/dvc.lock\n",
    "\n",
    "# MLflow\n",
    "mlruns/\n",
    "mlartifacts/\n",
    "\n",
    "# Weights & Biases\n",
    "wandb/\n",
    "\n",
    "# TensorBoard\n",
    "runs/\n",
    "logs/\n",
    "tensorboard/\n",
    "\n",
    "# Experiment tracking\n",
    "experiments/\n",
    ".experiments/\n",
    "\n",
    "# Secrets and credentials\n",
    ".env\n",
    ".env.local\n",
    ".env.*.local\n",
    "secrets.yaml\n",
    "credentials.json\n",
    "*.pem\n",
    "*.key\n",
    "config/secrets/\n",
    "\n",
    "# IDE\n",
    ".vscode/\n",
    ".idea/\n",
    "*.swp\n",
    "*.swo\n",
    "*~\n",
    ".DS_Store\n",
    "\n",
    "# Testing\n",
    ".pytest_cache/\n",
    ".coverage\n",
    "htmlcov/\n",
    ".tox/\n",
    "\n",
    "# Distribution / packaging\n",
    "build/\n",
    "dist/\n",
    "*.egg-info/\n",
    "\n",
    "# Documentation\n",
    "docs/_build/\n",
    "site/\n",
    "\"\"\"\n",
    "\n",
    "with open(\".gitignore\", \"w\") as f:\n",
    "    f.write(ds_gitignore)\n",
    "\n",
    "print(\"‚úì Created comprehensive data science .gitignore\")\n",
    "print(\"\\nKey exclusions:\")\n",
    "print(\"- Data files (CSV, Parquet, etc.)\")\n",
    "print(\"- Model files (PKL, H5, PT, etc.)\")\n",
    "print(\"- Large media files\")\n",
    "print(\"- Experiment tracking directories\")\n",
    "print(\"- Secrets and credentials\")\n",
    "print(\"\\nExceptions:\")\n",
    "print(\"- Small sample/test data\")\n",
    "print(\"- Documentation images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. ML Experiment Tracking Workflow\n",
    "\n",
    "### Recommended Structure\n",
    "\n",
    "```\n",
    "ml-project/\n",
    "‚îú‚îÄ‚îÄ .git/                   # Git repository\n",
    "‚îú‚îÄ‚îÄ .dvc/                   # DVC cache\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ data/                   # Data directory\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ raw/               # Original data (DVC tracked)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ processed/         # Processed data (DVC tracked)\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ sample/            # Small samples (Git tracked)\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ notebooks/             # Jupyter notebooks (outputs stripped)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 01_eda.ipynb\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 02_preprocessing.ipynb\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ 03_modeling.ipynb\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ src/                   # Source code (Git tracked)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ data/\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ load.py\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ preprocess.py\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ features/\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ build_features.py\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ models/\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train.py\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ predict.py\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ visualization/\n",
    "‚îÇ       ‚îî‚îÄ‚îÄ visualize.py\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ models/                # Trained models (DVC tracked)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ model_v1.pkl.dvc\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ model_v2.pkl.dvc\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ experiments/           # Experiment logs (gitignored)\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ mlruns/           # MLflow tracking\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ configs/               # Configuration files (Git tracked)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ model_config.yaml\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ training_config.yaml\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ tests/                 # Unit tests (Git tracked)\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ test_preprocessing.py\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ dvc.yaml              # DVC pipeline (Git tracked)\n",
    "‚îú‚îÄ‚îÄ requirements.txt       # Dependencies (Git tracked)\n",
    "‚îú‚îÄ‚îÄ .gitignore            # Git ignore patterns\n",
    "‚îú‚îÄ‚îÄ .dvcignore            # DVC ignore patterns\n",
    "‚îî‚îÄ‚îÄ README.md             # Documentation (Git tracked)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. DVC Pipelines - Reproducible ML\n",
    "\n",
    "### What are DVC Pipelines?\n",
    "\n",
    "Define your ML workflow as a pipeline:\n",
    "- Each stage is a command\n",
    "- DVC tracks dependencies and outputs\n",
    "- Automatically re-runs only what changed\n",
    "- Fully reproducible\n",
    "\n",
    "### Example Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple pipeline definition\n",
    "dvc_pipeline = \"\"\"stages:\n",
    "  prepare:\n",
    "    cmd: python src/prepare.py\n",
    "    deps:\n",
    "      - data/raw/sensor_data.csv\n",
    "      - src/prepare.py\n",
    "    outs:\n",
    "      - data/processed/clean_data.csv\n",
    "\n",
    "  train:\n",
    "    cmd: python src/train.py\n",
    "    deps:\n",
    "      - data/processed/clean_data.csv\n",
    "      - src/train.py\n",
    "    params:\n",
    "      - train.n_estimators\n",
    "      - train.max_depth\n",
    "    outs:\n",
    "      - models/classifier.pkl\n",
    "    metrics:\n",
    "      - metrics/train_metrics.json\n",
    "\n",
    "  evaluate:\n",
    "    cmd: python src/evaluate.py\n",
    "    deps:\n",
    "      - models/classifier.pkl\n",
    "      - data/processed/clean_data.csv\n",
    "      - src/evaluate.py\n",
    "    metrics:\n",
    "      - metrics/test_metrics.json\n",
    "\"\"\"\n",
    "\n",
    "with open(\"dvc.yaml\", \"w\") as f:\n",
    "    f.write(dvc_pipeline)\n",
    "\n",
    "print(\"‚úì Created DVC pipeline\")\n",
    "print(\"\\nPipeline stages:\")\n",
    "print(\"1. prepare: Clean raw data\")\n",
    "print(\"2. train: Train model\")\n",
    "print(\"3. evaluate: Evaluate model\")\n",
    "print(\"\\nRun with: dvc repro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Pipeline\n",
    "\n",
    "```bash\n",
    "# Run entire pipeline\n",
    "dvc repro\n",
    "\n",
    "# DVC will:\n",
    "# 1. Check which stages have changed dependencies\n",
    "# 2. Re-run only those stages\n",
    "# 3. Cache all outputs\n",
    "# 4. Track metrics and parameters\n",
    "\n",
    "# View pipeline\n",
    "dvc dag\n",
    "\n",
    "# Compare experiments\n",
    "dvc metrics show\n",
    "dvc metrics diff\n",
    "\n",
    "# Compare parameters\n",
    "dvc params diff\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Collaboration Workflow for DS Teams\n",
    "\n",
    "### Recommended Workflow\n",
    "\n",
    "```bash\n",
    "# 1. Clone repository\n",
    "git clone https://github.com/team/ml-project.git\n",
    "cd ml-project\n",
    "\n",
    "# 2. Install dependencies\n",
    "pip install -r requirements.txt\n",
    "\n",
    "# 3. Pull data with DVC\n",
    "dvc pull\n",
    "\n",
    "# 4. Create experiment branch\n",
    "git checkout -b experiment/new-features\n",
    "\n",
    "# 5. Make changes, train models\n",
    "jupyter notebook notebooks/experiment.ipynb\n",
    "\n",
    "# 6. Track new data/models with DVC\n",
    "dvc add data/processed/new_features.csv\n",
    "dvc add models/improved_model.pkl\n",
    "\n",
    "# 7. Commit code and .dvc files\n",
    "git add notebooks/experiment.ipynb\n",
    "git add data/processed/new_features.csv.dvc\n",
    "git add models/improved_model.pkl.dvc\n",
    "git commit -m \"feat: Add new feature engineering approach\"\n",
    "\n",
    "# 8. Push code to Git, data to DVC\n",
    "git push origin experiment/new-features\n",
    "dvc push\n",
    "\n",
    "# 9. Create pull request\n",
    "# Team reviews code and can pull your data with 'dvc pull'\n",
    "```\n",
    "\n",
    "### Team Benefits\n",
    "\n",
    "- **Code**: Versioned in Git\n",
    "- **Data**: Versioned in DVC, stored centrally\n",
    "- **Models**: Tracked with DVC or MLflow\n",
    "- **Experiments**: Reproducible with DVC pipelines\n",
    "- **Notebooks**: Clean diffs with nbdime\n",
    "- **Collaboration**: Everyone has access to same data/models\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Exercise 1: Set Up nbstripout\n",
    "\n",
    "**Task**: Configure a repository to automatically strip notebook outputs.\n",
    "\n",
    "**Steps**:\n",
    "1. Create a new Git repository\n",
    "2. Install nbstripout\n",
    "3. Configure Git filter\n",
    "4. Create a notebook with outputs\n",
    "5. Verify outputs are stripped on commit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your solution here\n",
    "\n",
    "# TODO: Implement the exercise\n",
    "\n",
    "print(\"TODO: Complete this exercise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Exercise 2: Version a Dataset with DVC\n",
    "\n",
    "**Task**: Create a dataset, version it with DVC, and simulate updating it.\n",
    "\n",
    "**Requirements**:\n",
    "1. Generate a CSV dataset\n",
    "2. Add it to DVC\n",
    "3. Commit to Git\n",
    "4. Modify the dataset\n",
    "5. Update with DVC\n",
    "6. Show version history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your solution here\n",
    "\n",
    "# TODO: Implement the exercise\n",
    "\n",
    "print(\"TODO: Complete this exercise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Exercise 3: Design a DS Team Workflow\n",
    "\n",
    "**Task**: Design a complete workflow for a data science team.\n",
    "\n",
    "**Team Context**:\n",
    "- 4 data scientists\n",
    "- Working on a customer churn prediction model\n",
    "- Large dataset (5GB)\n",
    "- Multiple experiments running in parallel\n",
    "- Need to track model performance\n",
    "- Monthly production deployments\n",
    "\n",
    "**Address**:\n",
    "1. How to handle the large dataset?\n",
    "2. How to version notebooks?\n",
    "3. How to track experiments?\n",
    "4. How to version models?\n",
    "5. What's the Git workflow?\n",
    "6. How to ensure reproducibility?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Workflow Design Here\n",
    "\n",
    "TODO: Describe your complete data science workflow\n",
    "\n",
    "Consider:\n",
    "- Tools (Git, DVC, MLflow, etc.)\n",
    "- Repository structure\n",
    "- Branching strategy\n",
    "- Data management\n",
    "- Model versioning\n",
    "- Experiment tracking\n",
    "- Deployment process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Summary\n",
    "\n",
    "### Key Concepts Learned\n",
    "\n",
    "1. **Jupyter Notebooks**:\n",
    "   - Use nbstripout to remove outputs\n",
    "   - Use nbdime for meaningful diffs\n",
    "   - Keep notebooks clean in Git\n",
    "\n",
    "2. **Large Files**:\n",
    "   - Git LFS for files 10MB-100MB\n",
    "   - DVC for very large datasets\n",
    "   - Cloud storage for massive data\n",
    "\n",
    "3. **Data Versioning**:\n",
    "   - DVC tracks data like Git tracks code\n",
    "   - Metadata in Git, data in DVC cache\n",
    "   - Works with any cloud storage\n",
    "\n",
    "4. **Model Management**:\n",
    "   - Version with DVC for simple projects\n",
    "   - Use MLflow for complex projects\n",
    "   - Track metrics and parameters\n",
    "\n",
    "5. **Pipelines**:\n",
    "   - DVC pipelines ensure reproducibility\n",
    "   - Cache intermediate results\n",
    "   - Re-run only what changed\n",
    "\n",
    "6. **Team Collaboration**:\n",
    "   - Git for code, DVC for data\n",
    "   - Shared remote storage\n",
    "   - Reproducible experiments\n",
    "\n",
    "### Essential Tools\n",
    "\n",
    "```bash\n",
    "# Notebooks\n",
    "pip install nbstripout nbdime\n",
    "nbstripout --install\n",
    "nbdime config-git --enable\n",
    "\n",
    "# Large files\n",
    "git lfs install\n",
    "git lfs track \"*.csv\"\n",
    "\n",
    "# Data versioning\n",
    "pip install dvc\n",
    "dvc init\n",
    "dvc add data/large_file.csv\n",
    "dvc push\n",
    "\n",
    "# Experiment tracking\n",
    "pip install mlflow\n",
    "mlflow ui\n",
    "```\n",
    "\n",
    "### Best Practices Checklist\n",
    "\n",
    "- [ ] Strip notebook outputs before committing\n",
    "- [ ] Use DVC for datasets > 10MB\n",
    "- [ ] Version models with DVC or MLflow\n",
    "- [ ] Define reproducible pipelines\n",
    "- [ ] Track experiment metrics\n",
    "- [ ] Document data dependencies\n",
    "- [ ] Use cloud storage for team data\n",
    "- [ ] Separate code and data clearly\n",
    "- [ ] Never commit secrets or credentials\n",
    "- [ ] Test reproducibility on clean checkout\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. What's Next?\n",
    "\n",
    "You've mastered Git for data science workflows! Continue with:\n",
    "\n",
    "**Module 12: GitHub Pages and Portfolio Hosting**\n",
    "- Host your portfolio on GitHub Pages\n",
    "- Create project documentation sites\n",
    "- Showcase your data science projects\n",
    "- Build your professional brand\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "**Tools**:\n",
    "- [nbstripout Documentation](https://github.com/kynan/nbstripout)\n",
    "- [nbdime Documentation](https://nbdime.readthedocs.io/)\n",
    "- [Git LFS](https://git-lfs.github.com/)\n",
    "- [DVC Documentation](https://dvc.org/doc)\n",
    "- [MLflow](https://mlflow.org/)\n",
    "\n",
    "**Tutorials**:\n",
    "- [DVC Tutorial](https://dvc.org/doc/start)\n",
    "- [MLflow Tutorial](https://mlflow.org/docs/latest/tutorials-and-examples/tutorial.html)\n",
    "- [Effective Jupyter Notebooks](https://jupyter-notebook.readthedocs.io/)\n",
    "\n",
    "**Articles**:\n",
    "- [Data Version Control in Practice](https://realpython.com/python-data-version-control/)\n",
    "- [ML Model Versioning](https://neptune.ai/blog/version-control-for-ml-models)\n",
    "- [Reproducible Data Science](https://towardsdatascience.com/)\n",
    "\n",
    "### Keep Learning\n",
    "\n",
    "Advanced topics to explore:\n",
    "1. **CI/CD for ML**: Automated model training and deployment\n",
    "2. **Feature Stores**: Centralized feature management\n",
    "3. **Model Monitoring**: Track model performance in production\n",
    "4. **A/B Testing**: Compare model versions\n",
    "5. **MLOps**: End-to-end ML operations\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You now have the skills to manage complex data science projects with professional version control. üéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
