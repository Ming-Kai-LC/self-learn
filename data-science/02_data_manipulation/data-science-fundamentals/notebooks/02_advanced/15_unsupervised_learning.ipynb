{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 15: Unsupervised Learning\n",
    "\n",
    "**Estimated Time**: 75 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this module, you will master unsupervised learning.\n",
    "\n",
    "Topics covered:\n",
    "- Clustering Fundamentals\n",
    "- K-Means Clustering\n",
    "- DBSCAN for Density-Based Clustering\n",
    "- Hierarchical Clustering\n",
    "- Dimensionality Reduction with PCA\n",
    "- t-SNE for Visualization\n",
    "- Anomaly Detection\n",
    "- Customer Segmentation Project\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Modules 00-11 completed\n",
    "- Intermediate Python and ML knowledge\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Clustering Fundamentals\n",
    "\n",
    "**Clustering** is an unsupervised learning technique that groups similar data points together without predefined labels.\n",
    "\n",
    "> **\"Birds of a feather flock together\"** - Clustering finds natural groupings in data\n",
    "\n",
    "### What is Unsupervised Learning?\n",
    "\n",
    "Unlike supervised learning (where we have labels), **unsupervised learning** finds patterns in unlabeled data:\n",
    "\n",
    "- **No target variable** - We don't know the \"correct answer\"\n",
    "- **Discover hidden patterns** - Find structure in data\n",
    "- **Exploratory analysis** - Understand data better\n",
    "\n",
    "### The Clustering Problem\n",
    "\n",
    "**Given**: Dataset with features but no labels\n",
    "**Goal**: Group similar items together\n",
    "\n",
    "**Example Use Cases:**\n",
    "- üõçÔ∏è **Customer Segmentation**: Group customers by behavior\n",
    "- üì∞ **Document Clustering**: Organize articles by topic\n",
    "- üß¨ **Gene Expression**: Find genes with similar patterns\n",
    "- üåç **Image Segmentation**: Group pixels by similarity\n",
    "- üéµ **Music Recommendation**: Find similar songs\n",
    "\n",
    "### How Clustering Works\n",
    "\n",
    "1. **Choose similarity metric** (usually Euclidean distance)\n",
    "2. **Select number of clusters** (k)\n",
    "3. **Algorithm assigns each point** to a cluster\n",
    "4. **Evaluate clustering quality**\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "**Similarity/Distance Metrics:**\n",
    "- **Euclidean Distance**: Straight-line distance (most common)\n",
    "- **Manhattan Distance**: Sum of absolute differences\n",
    "- **Cosine Similarity**: Angle between vectors\n",
    "\n",
    "**Within-Cluster Sum of Squares (WCSS):**\n",
    "- Measures compactness of clusters\n",
    "- Lower is better\n",
    "- Used to find optimal k (Elbow Method)\n",
    "\n",
    "**Silhouette Score:**\n",
    "- Measures how well-separated clusters are\n",
    "- Range: -1 to 1 (higher is better)\n",
    "- Score > 0.5 indicates good clustering\n",
    "\n",
    "### Types of Clustering Algorithms\n",
    "\n",
    "| Algorithm | Type | Strengths | Use When |\n",
    "|-----------|------|-----------|----------|\n",
    "| **K-Means** | Centroid-based | Fast, scalable | Spherical clusters, known k |\n",
    "| **DBSCAN** | Density-based | Finds arbitrary shapes | Unknown k, outliers |\n",
    "| **Hierarchical** | Hierarchical | Creates dendrogram | Small datasets, explore k |\n",
    "| **Gaussian Mixture** | Probabilistic | Soft clustering | Overlapping clusters |\n",
    "\n",
    "Let's visualize clustering with a simple example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering Fundamentals - Visualization\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CLUSTERING FUNDAMENTALS DEMONSTRATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Generate synthetic data with 3 clear clusters\n",
    "np.random.seed(42)\n",
    "X, y_true = make_blobs(n_samples=300, centers=3, cluster_std=0.6, random_state=42)\n",
    "\n",
    "print(f\"\\nGenerated {X.shape[0]} samples with {X.shape[1]} features\")\n",
    "print(\n",
    "    f\"Data range: X1=[{X[:, 0].min():.2f}, {X[:, 0].max():.2f}], X2=[{X[:, 1].min():.2f}, {X[:, 1].max():.2f}]\"\n",
    ")\n",
    "\n",
    "# Visualize the raw data\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# 1. Raw unlabeled data\n",
    "axes[0].scatter(X[:, 0], X[:, 1], c=\"gray\", alpha=0.6, edgecolors=\"k\", s=50)\n",
    "axes[0].set_title(\n",
    "    \"Raw Data (Unlabeled)\\nCan you spot the clusters?\", fontsize=12, fontweight=\"bold\"\n",
    ")\n",
    "axes[0].set_xlabel(\"Feature 1\")\n",
    "axes[0].set_ylabel(\"Feature 2\")\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. True clusters (hidden in real scenarios)\n",
    "axes[1].scatter(X[:, 0], X[:, 1], c=y_true, cmap=\"viridis\", alpha=0.6, edgecolors=\"k\", s=50)\n",
    "axes[1].set_title(\n",
    "    \"True Clusters (Unknown in Real Life)\\n3 distinct groups\", fontsize=12, fontweight=\"bold\"\n",
    ")\n",
    "axes[1].set_xlabel(\"Feature 1\")\n",
    "axes[1].set_ylabel(\"Feature 2\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Clustered data with K-Means\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "y_pred = kmeans.fit_predict(X)\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "axes[2].scatter(X[:, 0], X[:, 1], c=y_pred, cmap=\"viridis\", alpha=0.6, edgecolors=\"k\", s=50)\n",
    "axes[2].scatter(\n",
    "    centers[:, 0],\n",
    "    centers[:, 1],\n",
    "    c=\"red\",\n",
    "    marker=\"X\",\n",
    "    s=300,\n",
    "    edgecolors=\"black\",\n",
    "    linewidths=2,\n",
    "    label=\"Centroids\",\n",
    ")\n",
    "axes[2].set_title(\"K-Means Clustering Result\\nFound 3 clusters!\", fontsize=12, fontweight=\"bold\")\n",
    "axes[2].set_xlabel(\"Feature 1\")\n",
    "axes[2].set_ylabel(\"Feature 2\")\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate clustering quality metrics\n",
    "inertia = kmeans.inertia_  # WCSS\n",
    "silhouette = silhouette_score(X, y_pred)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CLUSTERING QUALITY METRICS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nWithin-Cluster Sum of Squares (WCSS): {inertia:.2f}\")\n",
    "print(f\"  ‚Üí Lower is better (measures compactness)\")\n",
    "print(f\"\\nSilhouette Score: {silhouette:.4f}\")\n",
    "print(f\"  ‚Üí Range: -1 to 1 (higher is better)\")\n",
    "print(\n",
    "    f\"  ‚Üí Interpretation: {'Excellent' if silhouette > 0.7 else 'Good' if silhouette > 0.5 else 'Fair' if silhouette > 0.3 else 'Poor'}\"\n",
    ")\n",
    "\n",
    "# Distance metrics demonstration\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DISTANCE METRICS COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "point_a = np.array([1, 2])\n",
    "point_b = np.array([4, 6])\n",
    "\n",
    "# Euclidean distance\n",
    "euclidean = np.sqrt(np.sum((point_a - point_b) ** 2))\n",
    "print(f\"\\nPoint A: {point_a}, Point B: {point_b}\")\n",
    "print(f\"\\n1. Euclidean Distance: {euclidean:.4f}\")\n",
    "print(f\"   Formula: ‚àö[(x‚ÇÅ-x‚ÇÇ)¬≤ + (y‚ÇÅ-y‚ÇÇ)¬≤]\")\n",
    "\n",
    "# Manhattan distance\n",
    "manhattan = np.sum(np.abs(point_a - point_b))\n",
    "print(f\"\\n2. Manhattan Distance: {manhattan:.4f}\")\n",
    "print(f\"   Formula: |x‚ÇÅ-x‚ÇÇ| + |y‚ÇÅ-y‚ÇÇ|\")\n",
    "\n",
    "# Cosine similarity\n",
    "cosine_sim = np.dot(point_a, point_b) / (np.linalg.norm(point_a) * np.linalg.norm(point_b))\n",
    "cosine_dist = 1 - cosine_sim\n",
    "print(f\"\\n3. Cosine Similarity: {cosine_sim:.4f}\")\n",
    "print(f\"   Cosine Distance: {cosine_dist:.4f}\")\n",
    "print(f\"   ‚Üí Measures angle between vectors (0=identical direction, 1=orthogonal)\")\n",
    "\n",
    "print(\"\\n‚úì Clustering fundamentals demonstrated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. K-Means Clustering\n",
    "\n",
    "**K-Means** is the most popular clustering algorithm - simple, fast, and effective for many use cases.\n",
    "\n",
    "### How K-Means Works\n",
    "\n",
    "**Algorithm Steps:**\n",
    "\n",
    "1. **Initialize**: Randomly select k centroids (cluster centers)\n",
    "2. **Assignment**: Assign each point to nearest centroid\n",
    "3. **Update**: Recalculate centroids as mean of assigned points\n",
    "4. **Repeat**: Steps 2-3 until convergence (centroids stop moving)\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "**Objective**: Minimize Within-Cluster Sum of Squares (WCSS)\n",
    "\n",
    "$$\\text{WCSS} = \\sum_{i=1}^{k} \\sum_{x \\in C_i} ||x - \\mu_i||^2$$\n",
    "\n",
    "Where:\n",
    "- $k$ = number of clusters\n",
    "- $C_i$ = cluster i\n",
    "- $\\mu_i$ = centroid of cluster i\n",
    "- $||x - \\mu_i||^2$ = squared Euclidean distance\n",
    "\n",
    "### The Elbow Method\n",
    "\n",
    "**Problem**: How do we choose k?\n",
    "\n",
    "**Solution**: Plot WCSS vs k and find the \"elbow\"\n",
    "\n",
    "- **Too few clusters**: High WCSS (poor fit)\n",
    "- **Too many clusters**: Overfitting, no meaningful groups\n",
    "- **Elbow point**: Sweet spot where adding more clusters doesn't help much\n",
    "\n",
    "### Pros & Cons\n",
    "\n",
    "**Advantages:**\n",
    "- ‚úì Simple and intuitive\n",
    "- ‚úì Fast - O(n √ó k √ó i) where i = iterations\n",
    "- ‚úì Scales well to large datasets\n",
    "- ‚úì Guaranteed to converge\n",
    "\n",
    "**Limitations:**\n",
    "- ‚úó Must specify k in advance\n",
    "- ‚úó Sensitive to initial centroids (use k-means++)\n",
    "- ‚úó Assumes spherical clusters of similar size\n",
    "- ‚úó Affected by outliers\n",
    "\n",
    "### K-Means++ Initialization\n",
    "\n",
    "**Problem**: Random initialization can lead to poor results\n",
    "\n",
    "**Solution**: K-Means++ chooses initial centroids smartly:\n",
    "1. Choose first centroid randomly\n",
    "2. For each next centroid, select point farthest from existing centroids\n",
    "3. This spreads out initial centroids\n",
    "\n",
    "**Result**: Faster convergence and better clusters\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Standardize features** - Different scales can distort distances\n",
    "2. **Use K-Means++** initialization (`init='k-means++'`)\n",
    "3. **Run multiple times** (`n_init=10`) and keep best result\n",
    "4. **Validate with silhouette score** to confirm k is reasonable\n",
    "\n",
    "Let's implement K-Means and find the optimal k!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means Clustering - Complete Implementation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"K-MEANS CLUSTERING WITH ELBOW METHOD\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load customer data for segmentation\n",
    "df = pd.read_csv(\"../../data_advanced/feature_engineering.csv\")\n",
    "print(f\"\\nDataset: {df.shape[0]} customers, {df.shape[1]} features\")\n",
    "\n",
    "# Select features for clustering\n",
    "features = [\"age\", \"income\", \"education_years\", \"experience_years\", \"num_dependents\"]\n",
    "X = df[features].copy()\n",
    "\n",
    "# Standardize features (crucial for K-Means!)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(f\"\\nFeatures for clustering: {features}\")\n",
    "print(f\"\\nOriginal data range:\")\n",
    "print(X.describe().loc[[\"min\", \"max\"]])\n",
    "print(f\"\\nStandardized data range (mean=0, std=1):\")\n",
    "print(pd.DataFrame(X_scaled, columns=features).describe().loc[[\"mean\", \"std\"]])\n",
    "\n",
    "# Find optimal k using Elbow Method\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ELBOW METHOD: Finding Optimal k\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "K_range = range(2, 11)\n",
    "wcss = []\n",
    "silhouette_scores = []\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, init=\"k-means++\", n_init=10, random_state=42)\n",
    "    kmeans.fit(X_scaled)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(X_scaled, kmeans.labels_))\n",
    "    print(f\"k={k}: WCSS={kmeans.inertia_:.2f}, Silhouette={silhouette_scores[-1]:.4f}\")\n",
    "\n",
    "# Visualize Elbow Method and Silhouette Scores\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# 1. Elbow Plot\n",
    "axes[0].plot(K_range, wcss, \"bo-\", linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel(\"Number of Clusters (k)\", fontsize=12)\n",
    "axes[0].set_ylabel(\"WCSS (Within-Cluster Sum of Squares)\", fontsize=12)\n",
    "axes[0].set_title('Elbow Method\\nFind the \"elbow\" point', fontsize=14, fontweight=\"bold\")\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].axvline(x=3, color=\"red\", linestyle=\"--\", alpha=0.5, label=\"Suggested k=3\")\n",
    "axes[0].legend()\n",
    "\n",
    "# 2. Silhouette Score\n",
    "axes[1].plot(K_range, silhouette_scores, \"go-\", linewidth=2, markersize=8)\n",
    "axes[1].set_xlabel(\"Number of Clusters (k)\", fontsize=12)\n",
    "axes[1].set_ylabel(\"Silhouette Score\", fontsize=12)\n",
    "axes[1].set_title(\"Silhouette Score by k\\nHigher is better\", fontsize=14, fontweight=\"bold\")\n",
    "axes[1].axhline(y=0.5, color=\"orange\", linestyle=\"--\", alpha=0.5, label=\"Good threshold (0.5)\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Use optimal k=3\n",
    "optimal_k = 3\n",
    "print(f\"\\n‚úì Optimal k = {optimal_k} (based on elbow and silhouette)\")\n",
    "\n",
    "# Fit final K-Means model\n",
    "kmeans_final = KMeans(n_clusters=optimal_k, init=\"k-means++\", n_init=10, random_state=42)\n",
    "clusters = kmeans_final.fit_predict(X_scaled)\n",
    "\n",
    "# Add cluster labels to dataframe\n",
    "df[\"cluster\"] = clusters\n",
    "\n",
    "# Analyze clusters\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CLUSTER ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i in range(optimal_k):\n",
    "    cluster_data = df[df[\"cluster\"] == i]\n",
    "    print(f\"\\nüìä Cluster {i} ({len(cluster_data)} customers, {len(cluster_data)/len(df)*100:.1f}%)\")\n",
    "    print(cluster_data[features].mean().to_string())\n",
    "\n",
    "# Visualize clusters (using PCA for 2D)\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# 1. Clusters in PCA space\n",
    "for i in range(optimal_k):\n",
    "    mask = clusters == i\n",
    "    axes[0].scatter(\n",
    "        X_pca[mask, 0], X_pca[mask, 1], label=f\"Cluster {i}\", alpha=0.6, s=50, edgecolors=\"k\"\n",
    "    )\n",
    "\n",
    "# Plot centroids\n",
    "centroids_pca = pca.transform(kmeans_final.cluster_centers_)\n",
    "axes[0].scatter(\n",
    "    centroids_pca[:, 0],\n",
    "    centroids_pca[:, 1],\n",
    "    c=\"red\",\n",
    "    marker=\"X\",\n",
    "    s=300,\n",
    "    edgecolors=\"black\",\n",
    "    linewidths=2,\n",
    "    label=\"Centroids\",\n",
    "    zorder=5,\n",
    ")\n",
    "axes[0].set_xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)\")\n",
    "axes[0].set_ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)\")\n",
    "axes[0].set_title(\"K-Means Clustering Results\\n(PCA Visualization)\", fontsize=14, fontweight=\"bold\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Feature comparison across clusters\n",
    "cluster_means = df.groupby(\"cluster\")[features].mean()\n",
    "cluster_means_scaled = (cluster_means - cluster_means.mean()) / cluster_means.std()\n",
    "\n",
    "im = axes[1].imshow(cluster_means_scaled.T, cmap=\"RdYlGn\", aspect=\"auto\", vmin=-2, vmax=2)\n",
    "axes[1].set_xticks(range(optimal_k))\n",
    "axes[1].set_xticklabels([f\"Cluster {i}\" for i in range(optimal_k)])\n",
    "axes[1].set_yticks(range(len(features)))\n",
    "axes[1].set_yticklabels(features)\n",
    "axes[1].set_title(\"Cluster Profiles\\n(Standardized Feature Means)\", fontsize=14, fontweight=\"bold\")\n",
    "plt.colorbar(im, ax=axes[1], label=\"Standardized Value\")\n",
    "\n",
    "for i in range(optimal_k):\n",
    "    for j in range(len(features)):\n",
    "        text = axes[1].text(\n",
    "            i,\n",
    "            j,\n",
    "            f\"{cluster_means_scaled.iloc[i, j]:.2f}\",\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            color=\"black\",\n",
    "            fontsize=10,\n",
    "        )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úì K-Means clustering complete with k={optimal_k}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DBSCAN for Density-Based Clustering\n",
    "\n",
    "**DBSCAN** (Density-Based Spatial Clustering of Applications with Noise) discovers clusters of arbitrary shapes and identifies outliers.\n",
    "\n",
    "### Why DBSCAN?\n",
    "\n",
    "**K-Means Limitations:**\n",
    "- Assumes spherical clusters\n",
    "- Requires specifying k\n",
    "- Sensitive to outliers\n",
    "\n",
    "**DBSCAN Advantages:**\n",
    "- ‚úì No need to specify number of clusters\n",
    "- ‚úì Finds arbitrarily shaped clusters\n",
    "- ‚úì Identifies outliers as noise\n",
    "- ‚úì Works well with spatial data\n",
    "\n",
    "### How DBSCAN Works\n",
    "\n",
    "**Core Concepts:**\n",
    "\n",
    "1. **Œµ (epsilon)**: Maximum distance between two points to be neighbors\n",
    "2. **MinPts**: Minimum points to form a dense region\n",
    "\n",
    "**Point Types:**\n",
    "- **Core Point**: Has ‚â• MinPts neighbors within Œµ\n",
    "- **Border Point**: Within Œµ of a core point, but has < MinPts neighbors\n",
    "- **Noise Point**: Neither core nor border (outlier)\n",
    "\n",
    "**Algorithm:**\n",
    "1. Pick an unvisited point\n",
    "2. If it's a core point, start a cluster and expand it\n",
    "3. Add all density-reachable points to the cluster\n",
    "4. Repeat until all points visited\n",
    "5. Points not in any cluster = noise\n",
    "\n",
    "### Choosing Parameters\n",
    "\n",
    "**Œµ (epsilon):**\n",
    "- Too small: Many small clusters and noise\n",
    "- Too large: All points in one cluster\n",
    "- **Method**: k-distance plot (look for \"knee\")\n",
    "\n",
    "**MinPts:**\n",
    "- Rule of thumb: `MinPts ‚â• D + 1` where D = dimensions\n",
    "- For 2D data: MinPts = 4 is common\n",
    "- Higher values = more noise detected\n",
    "\n",
    "### Pros & Cons\n",
    "\n",
    "**Advantages:**\n",
    "- ‚úì No need to specify k\n",
    "- ‚úì Handles non-spherical clusters\n",
    "- ‚úì Robust to outliers (labels them as noise)\n",
    "- ‚úì Only 2 parameters to tune\n",
    "\n",
    "**Limitations:**\n",
    "- ‚úó Struggles with varying densities\n",
    "- ‚úó Sensitive to Œµ and MinPts\n",
    "- ‚úó Not deterministic for border points\n",
    "- ‚úó Computationally expensive for large datasets\n",
    "\n",
    "### When to Use DBSCAN\n",
    "\n",
    "- **Spatial clustering**: Geographic data, maps\n",
    "- **Anomaly detection**: Outliers are valuable\n",
    "- **Non-spherical clusters**: Moon shapes, rings\n",
    "- **Unknown k**: Don't know number of clusters\n",
    "\n",
    "Let's compare DBSCAN with K-Means on different cluster shapes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN - Comprehensive Demonstration\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.datasets import make_moons, make_circles\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DBSCAN vs K-MEANS: CLUSTER SHAPE COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create datasets with different shapes\n",
    "np.random.seed(42)\n",
    "\n",
    "# 1. Moons (non-linear separable)\n",
    "X_moons, _ = make_moons(n_samples=300, noise=0.05, random_state=42)\n",
    "\n",
    "# 2. Circles (concentric)\n",
    "X_circles, _ = make_circles(n_samples=300, factor=0.5, noise=0.05, random_state=42)\n",
    "\n",
    "# 3. Blobs (well-separated spherical)\n",
    "X_blobs, _ = make_blobs(n_samples=300, centers=3, cluster_std=0.6, random_state=42)\n",
    "\n",
    "datasets = [(\"Moons\", X_moons), (\"Circles\", X_circles), (\"Blobs\", X_blobs)]\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 16))\n",
    "\n",
    "for idx, (name, X) in enumerate(datasets):\n",
    "    # Original data\n",
    "    axes[idx, 0].scatter(X[:, 0], X[:, 1], c=\"gray\", s=30, alpha=0.6, edgecolors=\"k\")\n",
    "    axes[idx, 0].set_title(f\"{name}\\n(Unlabeled)\", fontsize=12, fontweight=\"bold\")\n",
    "    axes[idx, 0].set_ylabel(name, fontsize=14, fontweight=\"bold\")\n",
    "    if idx == 0:\n",
    "        axes[idx, 0].set_xlabel(\"K-Means FAILS ‚Üí\", fontsize=11, color=\"red\")\n",
    "\n",
    "    # K-Means clustering\n",
    "    kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "    y_kmeans = kmeans.fit_predict(X)\n",
    "\n",
    "    axes[idx, 1].scatter(\n",
    "        X[:, 0], X[:, 1], c=y_kmeans, cmap=\"viridis\", s=30, alpha=0.6, edgecolors=\"k\"\n",
    "    )\n",
    "    axes[idx, 1].scatter(\n",
    "        kmeans.cluster_centers_[:, 0],\n",
    "        kmeans.cluster_centers_[:, 1],\n",
    "        c=\"red\",\n",
    "        marker=\"X\",\n",
    "        s=200,\n",
    "        edgecolors=\"black\",\n",
    "        linewidths=2,\n",
    "    )\n",
    "    axes[idx, 1].set_title(\n",
    "        f\"K-Means (k=2)\\nSilhouette: {silhouette_score(X, y_kmeans):.3f}\",\n",
    "        fontsize=12,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "    # DBSCAN clustering\n",
    "    if name == \"Moons\":\n",
    "        eps, min_samples = 0.3, 5\n",
    "    elif name == \"Circles\":\n",
    "        eps, min_samples = 0.2, 5\n",
    "    else:\n",
    "        eps, min_samples = 0.5, 5\n",
    "\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    y_dbscan = dbscan.fit_predict(X)\n",
    "\n",
    "    # Count clusters and noise\n",
    "    n_clusters = len(set(y_dbscan)) - (1 if -1 in y_dbscan else 0)\n",
    "    n_noise = list(y_dbscan).count(-1)\n",
    "\n",
    "    # Plot DBSCAN results\n",
    "    unique_labels = set(y_dbscan)\n",
    "    colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))\n",
    "\n",
    "    for k, col in zip(unique_labels, colors):\n",
    "        if k == -1:\n",
    "            # Noise points in black\n",
    "            col = \"black\"\n",
    "            markersize = 20\n",
    "        else:\n",
    "            markersize = 30\n",
    "\n",
    "        class_member_mask = y_dbscan == k\n",
    "        xy = X[class_member_mask]\n",
    "        axes[idx, 2].scatter(\n",
    "            xy[:, 0],\n",
    "            xy[:, 1],\n",
    "            c=[col],\n",
    "            s=markersize,\n",
    "            alpha=0.6,\n",
    "            edgecolors=\"k\",\n",
    "            label=f\"Cluster {k}\" if k != -1 else \"Noise\",\n",
    "        )\n",
    "\n",
    "    silh = silhouette_score(X, y_dbscan) if n_clusters > 1 else 0\n",
    "    axes[idx, 2].set_title(\n",
    "        f\"DBSCAN (Œµ={eps}, MinPts={min_samples})\\n\"\n",
    "        f\"Clusters: {n_clusters}, Noise: {n_noise}, Silhouette: {silh:.3f}\",\n",
    "        fontsize=12,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "    if idx == 0:\n",
    "        axes[idx, 2].set_xlabel(\"DBSCAN SUCCEEDS ‚Üí\", fontsize=11, color=\"green\")\n",
    "\n",
    "for ax in axes.flat:\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find optimal epsilon using k-distance plot\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINDING OPTIMAL EPSILON (Œµ)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Use moons dataset\n",
    "X = X_moons\n",
    "\n",
    "# Calculate k-distances (k=MinPts)\n",
    "k = 5\n",
    "neighbors = NearestNeighbors(n_neighbors=k)\n",
    "neighbors.fit(X)\n",
    "distances, indices = neighbors.kneighbors(X)\n",
    "\n",
    "# Sort distances (k-th nearest neighbor)\n",
    "k_distances = np.sort(distances[:, k - 1], axis=0)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot k-distance graph\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(k_distances, linewidth=2)\n",
    "plt.axhline(y=0.3, color=\"red\", linestyle=\"--\", linewidth=2, label=\"Chosen Œµ = 0.3\")\n",
    "plt.xlabel(\"Points (sorted by distance)\", fontsize=12)\n",
    "plt.ylabel(f\"{k}-th Nearest Neighbor Distance\", fontsize=12)\n",
    "plt.title('K-Distance Plot\\nLook for the \"knee\" to find optimal Œµ', fontsize=14, fontweight=\"bold\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "# Compare different epsilon values\n",
    "plt.subplot(1, 2, 2)\n",
    "eps_values = [0.2, 0.3, 0.5]\n",
    "results = []\n",
    "\n",
    "for eps in eps_values:\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=5)\n",
    "    labels = dbscan.fit_predict(X)\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise = list(labels).count(-1)\n",
    "    results.append((eps, n_clusters, n_noise))\n",
    "\n",
    "eps_list, clusters_list, noise_list = zip(*results)\n",
    "x_pos = np.arange(len(eps_list))\n",
    "\n",
    "plt.bar(x_pos - 0.2, clusters_list, 0.4, label=\"# Clusters\", color=\"steelblue\")\n",
    "plt.bar(x_pos + 0.2, noise_list, 0.4, label=\"# Noise Points\", color=\"coral\")\n",
    "plt.xlabel(\"Epsilon (Œµ)\", fontsize=12)\n",
    "plt.ylabel(\"Count\", fontsize=12)\n",
    "plt.title(\"Effect of Epsilon on Clustering\", fontsize=14, fontweight=\"bold\")\n",
    "plt.xticks(x_pos, [f\"{e}\" for e in eps_list])\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nOptimal Œµ selection:\")\n",
    "for eps, n_clusters, n_noise in results:\n",
    "    print(f\"  Œµ={eps}: {n_clusters} clusters, {n_noise} noise points\")\n",
    "\n",
    "print(\"\\n‚úì DBSCAN excels at non-spherical clusters!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hierarchical Clustering\n",
    "\n",
    "**Hierarchical Clustering** builds a tree of clusters (dendrogram) showing relationships at multiple levels.\n",
    "\n",
    "### Two Approaches\n",
    "\n",
    "**1. Agglomerative (Bottom-Up) - Most Common**\n",
    "- Start: Each point is its own cluster\n",
    "- Repeat: Merge closest clusters\n",
    "- Stop: Until one cluster remains\n",
    "\n",
    "**2. Divisive (Top-Down) - Rare**\n",
    "- Start: All points in one cluster\n",
    "- Repeat: Split clusters\n",
    "- Stop: Until each point is its own cluster\n",
    "\n",
    "### Linkage Methods\n",
    "\n",
    "How do we measure distance between clusters?\n",
    "\n",
    "| Linkage | Description | Characteristics |\n",
    "|---------|-------------|-----------------|\n",
    "| **Single** | Min distance between any two points | Tends to chain, sensitive to noise |\n",
    "| **Complete** | Max distance between any two points | Compact clusters, breaks chains |\n",
    "| **Average** | Average distance between all pairs | Balanced approach |\n",
    "| **Ward** | Minimizes within-cluster variance | Prefers equal-sized clusters (best) |\n",
    "\n",
    "### The Dendrogram\n",
    "\n",
    "A **dendrogram** is a tree diagram showing cluster hierarchy:\n",
    "\n",
    "- **Y-axis**: Distance (or dissimilarity) between clusters\n",
    "- **X-axis**: Data points\n",
    "- **Horizontal lines**: Merges (joins clusters)\n",
    "- **Height of merge**: How different clusters are\n",
    "\n",
    "**How to Read:**\n",
    "- Cut horizontally to get desired number of clusters\n",
    "- Longer vertical lines = more distinct clusters\n",
    "\n",
    "### Choosing Number of Clusters\n",
    "\n",
    "**Visual Method:**\n",
    "1. Look at dendrogram\n",
    "2. Find largest vertical gap\n",
    "3. Draw horizontal line through gap\n",
    "4. Count intersections = number of clusters\n",
    "\n",
    "**Inconsistency Method:**\n",
    "- Look for inconsistent merges (big jumps in distance)\n",
    "\n",
    "### Pros & Cons\n",
    "\n",
    "**Advantages:**\n",
    "- ‚úì No need to specify k upfront\n",
    "- ‚úì Dendrogram provides intuition\n",
    "- ‚úì Works with any distance metric\n",
    "- ‚úì Deterministic (same result every time)\n",
    "\n",
    "**Limitations:**\n",
    "- ‚úó Computationally expensive: O(n¬≥) for naive, O(n¬≤log n) optimized\n",
    "- ‚úó Not suitable for large datasets (> 10,000 points)\n",
    "- ‚úó Cannot undo previous merges\n",
    "- ‚úó Sensitive to noise and outliers\n",
    "\n",
    "### When to Use Hierarchical Clustering\n",
    "\n",
    "- **Small datasets** (< 5,000 points)\n",
    "- **Exploratory analysis** - Want to see hierarchy\n",
    "- **Taxonomy** - Biological classification\n",
    "- **Unknown k** - Dendrogram helps choose\n",
    "\n",
    "Let's create dendrograms and compare linkage methods!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical Clustering - Dendrograms and Linkage Comparison\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"HIERARCHICAL CLUSTERING WITH DENDROGRAMS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(42)\n",
    "X, y = make_blobs(n_samples=50, centers=3, cluster_std=0.7, random_state=42)\n",
    "\n",
    "print(f\"\\nDataset: {X.shape[0]} points\")\n",
    "print(\"Goal: Discover hierarchy without knowing k=3\")\n",
    "\n",
    "# Compare different linkage methods\n",
    "linkage_methods = [\"ward\", \"complete\", \"average\", \"single\"]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, method in enumerate(linkage_methods):\n",
    "    # Perform hierarchical clustering\n",
    "    Z = linkage(X, method=method)\n",
    "\n",
    "    # Create dendrogram\n",
    "    dendrogram(Z, ax=axes[idx], color_threshold=7.5 if method == \"ward\" else None)\n",
    "    axes[idx].set_title(\n",
    "        f\"Dendrogram - {method.upper()} Linkage\\n\" f'{\"(RECOMMENDED)\" if method == \"ward\" else \"\"}',\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "    axes[idx].set_xlabel(\"Data Point Index\", fontsize=12)\n",
    "    axes[idx].set_ylabel(\"Distance (Height)\", fontsize=12)\n",
    "    axes[idx].grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "    # Add horizontal line to show cut\n",
    "    if method == \"ward\":\n",
    "        axes[idx].axhline(y=7.5, color=\"red\", linestyle=\"--\", linewidth=2, label=\"Cut line (k=3)\")\n",
    "        axes[idx].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Detailed analysis with Ward linkage\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DETAILED ANALYSIS: WARD LINKAGE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "Z_ward = linkage(X, method=\"ward\")\n",
    "\n",
    "# Find optimal number of clusters\n",
    "print(\"\\nMerge distances (last 10 merges):\")\n",
    "print(\"Step | Distance | Interpretation\")\n",
    "print(\"-\" * 40)\n",
    "for i in range(max(0, len(Z_ward) - 10), len(Z_ward)):\n",
    "    print(f\"{i+1:4d} | {Z_ward[i, 2]:8.2f} | \", end=\"\")\n",
    "    if i == len(Z_ward) - 3:\n",
    "        print(\"‚Üê Big jump! Suggests k=3\")\n",
    "    else:\n",
    "        print(\"\")\n",
    "\n",
    "# Plot dendrogram with cut line\n",
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "dendrogram(Z_ward, color_threshold=7.5, above_threshold_color=\"gray\")\n",
    "plt.axhline(y=7.5, color=\"red\", linestyle=\"--\", linewidth=2, label=\"Cut for k=3\")\n",
    "plt.title(\"Dendrogram with Cut Line\\nWard Linkage\", fontsize=14, fontweight=\"bold\")\n",
    "plt.xlabel(\"Data Point Index\", fontsize=12)\n",
    "plt.ylabel(\"Distance (Ward)\", fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "# Apply clustering with different k values\n",
    "plt.subplot(1, 2, 2)\n",
    "k_values = [2, 3, 4]\n",
    "colors_list = [\"viridis\", \"plasma\", \"coolwarm\"]\n",
    "\n",
    "for i, k in enumerate(k_values):\n",
    "    hierarchical = AgglomerativeClustering(n_clusters=k, linkage=\"ward\")\n",
    "    labels = hierarchical.fit_predict(X)\n",
    "    silh = silhouette_score(X, labels)\n",
    "\n",
    "    plt.scatter(\n",
    "        X[:, 0],\n",
    "        X[:, 1],\n",
    "        c=labels,\n",
    "        cmap=colors_list[i],\n",
    "        s=100,\n",
    "        alpha=0.3,\n",
    "        edgecolors=\"k\",\n",
    "        linewidth=0.5,\n",
    "    )\n",
    "    plt.text(\n",
    "        X[:, 0].mean(),\n",
    "        X[:, 1].max() - i * 0.5,\n",
    "        f\"k={k}, Silhouette={silh:.3f}\",\n",
    "        fontsize=11,\n",
    "        bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.8),\n",
    "    )\n",
    "\n",
    "plt.title(\"Hierarchical Clustering Results\\nComparing k=2, 3, 4\", fontsize=14, fontweight=\"bold\")\n",
    "plt.xlabel(\"Feature 1\", fontsize=12)\n",
    "plt.ylabel(\"Feature 2\", fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Apply final clustering with k=3\n",
    "hierarchical_final = AgglomerativeClustering(n_clusters=3, linkage=\"ward\")\n",
    "labels_final = hierarchical_final.fit_predict(X)\n",
    "\n",
    "print(f\"\\n‚úì Hierarchical clustering complete!\")\n",
    "print(f\"Chosen k=3 based on dendrogram analysis\")\n",
    "print(f\"Silhouette Score: {silhouette_score(X, labels_final):.4f}\")\n",
    "\n",
    "# Real-world example: Customer segmentation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"REAL-WORLD APPLICATION: CUSTOMER HIERARCHY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load customer data (subset for speed)\n",
    "df_cust = pd.read_csv(\"../../data_advanced/feature_engineering.csv\").head(100)\n",
    "features = [\"age\", \"income\", \"education_years\"]\n",
    "X_cust = df_cust[features].values\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X_cust_scaled = scaler.fit_transform(X_cust)\n",
    "\n",
    "# Hierarchical clustering\n",
    "Z_cust = linkage(X_cust_scaled, method=\"ward\")\n",
    "\n",
    "plt.figure(figsize=(16, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "dendrogram(Z_cust, truncate_mode=\"lastp\", p=12, show_leaf_counts=True)\n",
    "plt.title(\n",
    "    \"Customer Segmentation Hierarchy\\n(Truncated: Last 12 Merges)\", fontsize=14, fontweight=\"bold\"\n",
    ")\n",
    "plt.xlabel(\"Cluster Size (in parentheses)\", fontsize=12)\n",
    "plt.ylabel(\"Ward Distance\", fontsize=12)\n",
    "plt.grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "# Cluster customers\n",
    "hierarchical_cust = AgglomerativeClustering(n_clusters=4, linkage=\"ward\")\n",
    "df_cust[\"segment\"] = hierarchical_cust.fit_predict(X_cust_scaled)\n",
    "\n",
    "# Analyze segments\n",
    "plt.subplot(1, 2, 2)\n",
    "segment_means = df_cust.groupby(\"segment\")[features].mean()\n",
    "segment_means_norm = (segment_means - segment_means.mean()) / segment_means.std()\n",
    "\n",
    "im = plt.imshow(segment_means_norm.T, cmap=\"RdYlGn\", aspect=\"auto\", vmin=-2, vmax=2)\n",
    "plt.colorbar(im, label=\"Standardized Mean\")\n",
    "plt.xticks(range(4), [f\"Segment {i}\" for i in range(4)])\n",
    "plt.yticks(range(len(features)), features)\n",
    "plt.title(\"Customer Segment Profiles\\n(4 Segments)\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "for i in range(4):\n",
    "    for j in range(len(features)):\n",
    "        text = plt.text(\n",
    "            i,\n",
    "            j,\n",
    "            f\"{segment_means_norm.iloc[i, j]:.2f}\",\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            color=\"black\",\n",
    "            fontsize=11,\n",
    "        )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSegment sizes:\")\n",
    "print(df_cust[\"segment\"].value_counts().sort_index())\n",
    "\n",
    "print(\"\\n‚úì Hierarchical clustering provides intuitive hierarchy visualization!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dimensionality Reduction with PCA\n",
    "\n",
    "**PCA** (Principal Component Analysis) reduces high-dimensional data to fewer dimensions while preserving maximum variance.\n",
    "\n",
    "### The Curse of Dimensionality\n",
    "\n",
    "**Problem**: High-dimensional data is sparse and hard to visualize\n",
    "\n",
    "- Distance metrics become meaningless\n",
    "- Models overfit easily\n",
    "- Visualization impossible (> 3D)\n",
    "- Computational cost increases\n",
    "\n",
    "**Solution**: Reduce dimensions intelligently\n",
    "\n",
    "### What is PCA?\n",
    "\n",
    "**Goal**: Find new axes (principal components) that:\n",
    "1. Capture maximum variance in data\n",
    "2. Are orthogonal (perpendicular) to each other\n",
    "3. Are ordered by importance\n",
    "\n",
    "**Think of it as:**\n",
    "- Finding the \"best camera angles\" to view your data\n",
    "- Compressing information with minimal loss\n",
    "\n",
    "### How PCA Works\n",
    "\n",
    "**Algorithm:**\n",
    "\n",
    "1. **Standardize** data (mean=0, variance=1)\n",
    "2. **Compute covariance matrix** (how features vary together)\n",
    "3. **Find eigenvectors** (directions of maximum variance)\n",
    "4. **Sort by eigenvalues** (variance explained by each direction)\n",
    "5. **Select top k components** (keep most important)\n",
    "6. **Transform data** to new coordinate system\n",
    "\n",
    "### Principal Components\n",
    "\n",
    "**PC1** (First Principal Component):\n",
    "- Direction of maximum variance\n",
    "- Captures most information\n",
    "- Example: In face data, might capture lighting\n",
    "\n",
    "**PC2** (Second Principal Component):\n",
    "- Orthogonal to PC1\n",
    "- Next highest variance\n",
    "- Example: Might capture face angle\n",
    "\n",
    "**And so on...**\n",
    "\n",
    "### Explained Variance\n",
    "\n",
    "**How much information does each PC contain?**\n",
    "\n",
    "- **Explained Variance Ratio**: Percentage of total variance\n",
    "- **Cumulative Variance**: Running total\n",
    "- **Rule of thumb**: Keep enough PCs to retain 95% variance\n",
    "\n",
    "### Pros & Cons\n",
    "\n",
    "**Advantages:**\n",
    "- ‚úì Reduces overfitting (fewer features)\n",
    "- ‚úì Speeds up training (less data to process)\n",
    "- ‚úì Enables visualization (2D/3D plots)\n",
    "- ‚úì Removes noise (keep signal, drop noise)\n",
    "- ‚úì Decorrelates features\n",
    "\n",
    "**Limitations:**\n",
    "- ‚úó Linear transformation only\n",
    "- ‚úó Assumes high variance = importance\n",
    "- ‚úó Components not interpretable\n",
    "- ‚úó Sensitive to scaling (must standardize)\n",
    "\n",
    "### When to Use PCA\n",
    "\n",
    "- **Visualization**: Plot high-D data in 2D/3D\n",
    "- **Preprocessing**: Before clustering or classification\n",
    "- **Compression**: Reduce storage/computation\n",
    "- **Noise reduction**: Filter out minor variations\n",
    "- **Multicollinearity**: Remove correlated features\n",
    "\n",
    "Let's reduce dimensions and visualize high-D data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA - Comprehensive Demonstration\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PRINCIPAL COMPONENT ANALYSIS (PCA)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load high-dimensional dataset (8x8 pixel images = 64 dimensions)\n",
    "digits = load_digits()\n",
    "X_digits = digits.data  # 1797 samples, 64 features\n",
    "y_digits = digits.target  # 0-9 digit labels\n",
    "\n",
    "print(f\"\\nDigits dataset: {X_digits.shape[0]} images, {X_digits.shape[1]} features (pixels)\")\n",
    "print(f\"Goal: Reduce from {X_digits.shape[1]}D to 2D for visualization\")\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_digits)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(random_state=42)\n",
    "X_pca_all = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Analyze explained variance\n",
    "explained_var = pca.explained_variance_ratio_\n",
    "cumulative_var = np.cumsum(explained_var)\n",
    "\n",
    "print(f\"\\nFirst 10 components explain:\")\n",
    "for i in range(10):\n",
    "    print(f\"  PC{i+1}: {explained_var[i]:.1%} (Cumulative: {cumulative_var[i]:.1%})\")\n",
    "\n",
    "# Find components needed for 95% variance\n",
    "n_components_95 = np.argmax(cumulative_var >= 0.95) + 1\n",
    "print(f\"\\nComponents needed for 95% variance: {n_components_95}/{X_digits.shape[1]}\")\n",
    "print(\n",
    "    f\"Dimension reduction: {X_digits.shape[1]} ‚Üí {n_components_95} ({n_components_95/X_digits.shape[1]:.1%})\"\n",
    ")\n",
    "\n",
    "# Visualize explained variance\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# 1. Scree plot (explained variance by component)\n",
    "axes[0].bar(range(1, 21), explained_var[:20], color=\"steelblue\", alpha=0.7)\n",
    "axes[0].set_xlabel(\"Principal Component\", fontsize=12)\n",
    "axes[0].set_ylabel(\"Explained Variance Ratio\", fontsize=12)\n",
    "axes[0].set_title(\n",
    "    \"Scree Plot\\nVariance Explained by Each Component\", fontsize=14, fontweight=\"bold\"\n",
    ")\n",
    "axes[0].grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "# 2. Cumulative explained variance\n",
    "axes[1].plot(range(1, len(cumulative_var) + 1), cumulative_var, \"o-\", linewidth=2, markersize=4)\n",
    "axes[1].axhline(y=0.95, color=\"red\", linestyle=\"--\", linewidth=2, label=\"95% threshold\")\n",
    "axes[1].axvline(\n",
    "    x=n_components_95,\n",
    "    color=\"green\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=f\"{n_components_95} components\",\n",
    ")\n",
    "axes[1].set_xlabel(\"Number of Components\", fontsize=12)\n",
    "axes[1].set_ylabel(\"Cumulative Explained Variance\", fontsize=12)\n",
    "axes[1].set_title(\n",
    "    \"Cumulative Variance Explained\\nHow many components to keep?\", fontsize=14, fontweight=\"bold\"\n",
    ")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Visualize principal components as images\n",
    "components_to_show = 9\n",
    "axes[2].axis(\"off\")\n",
    "axes[2].set_title(\n",
    "    \"Top 9 Principal Components\\n(Shown as 8x8 Images)\", fontsize=14, fontweight=\"bold\"\n",
    ")\n",
    "\n",
    "# Create subplot grid for components\n",
    "for i in range(components_to_show):\n",
    "    plt.subplot(3, 9, i + 7)  # Position in right third of figure\n",
    "    plt.imshow(pca.components_[i].reshape(8, 8), cmap=\"RdBu_r\", aspect=\"auto\")\n",
    "    plt.title(f\"PC{i+1}\\n{explained_var[i]:.1%}\", fontsize=9)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Reduce to 2D for visualization\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"2D VISUALIZATION WITH PCA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "pca_2d = PCA(n_components=2, random_state=42)\n",
    "X_2d = pca_2d.fit_transform(X_scaled)\n",
    "\n",
    "print(f\"\\nReduced to 2D\")\n",
    "print(f\"Variance preserved: {pca_2d.explained_variance_ratio_.sum():.1%}\")\n",
    "print(f\"  PC1: {pca_2d.explained_variance_ratio_[0]:.1%}\")\n",
    "print(f\"  PC2: {pca_2d.explained_variance_ratio_[1]:.1%}\")\n",
    "\n",
    "# Visualize digits in 2D PCA space\n",
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "scatter = plt.scatter(\n",
    "    X_2d[:, 0], X_2d[:, 1], c=y_digits, cmap=\"tab10\", s=20, alpha=0.6, edgecolors=\"k\", linewidth=0.5\n",
    ")\n",
    "plt.colorbar(scatter, label=\"Digit (0-9)\")\n",
    "plt.xlabel(f\"PC1 ({pca_2d.explained_variance_ratio_[0]:.1%} variance)\", fontsize=12)\n",
    "plt.ylabel(f\"PC2 ({pca_2d.explained_variance_ratio_[1]:.1%} variance)\", fontsize=12)\n",
    "plt.title(\"64D Data Projected to 2D\\nColors = Digit Labels (0-9)\", fontsize=14, fontweight=\"bold\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Show some actual digit images\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Sample Digit Images\\n(Original 8x8 pixels)\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "for i in range(25):\n",
    "    plt.subplot(5, 10, i + 6)  # Position in right half\n",
    "    plt.imshow(X_digits[i].reshape(8, 8), cmap=\"gray\")\n",
    "    plt.title(f\"{y_digits[i]}\", fontsize=10)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# PCA for data compression\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PCA FOR DATA COMPRESSION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Reconstruct data from reduced components\n",
    "n_components_list = [2, 5, 10, 20, 64]\n",
    "sample_idx = 0  # First digit\n",
    "\n",
    "fig, axes = plt.subplots(1, len(n_components_list), figsize=(18, 4))\n",
    "\n",
    "for idx, n_comp in enumerate(n_components_list):\n",
    "    if n_comp < 64:\n",
    "        # Reduce and reconstruct\n",
    "        pca_temp = PCA(n_components=n_comp, random_state=42)\n",
    "        X_reduced = pca_temp.fit_transform(X_scaled)\n",
    "        X_reconstructed = pca_temp.inverse_transform(X_reduced)\n",
    "\n",
    "        # Unscale\n",
    "        X_reconstructed = scaler.inverse_transform(X_reconstructed)\n",
    "\n",
    "        var_retained = pca_temp.explained_variance_ratio_.sum()\n",
    "    else:\n",
    "        # Original\n",
    "        X_reconstructed = X_digits\n",
    "        var_retained = 1.0\n",
    "\n",
    "    # Plot\n",
    "    axes[idx].imshow(X_reconstructed[sample_idx].reshape(8, 8), cmap=\"gray\")\n",
    "    axes[idx].set_title(\n",
    "        f\"{n_comp} components\\n{var_retained:.1%} variance\", fontsize=11, fontweight=\"bold\"\n",
    "    )\n",
    "    axes[idx].axis(\"off\")\n",
    "\n",
    "plt.suptitle(\n",
    "    \"Image Reconstruction with Different # of Components\\n\"\n",
    "    f\"Original Digit: {y_digits[sample_idx]}\",\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "compression_ratio = n_components_95 / X_digits.shape[1]\n",
    "print(f\"\\nCompression achieved: {X_digits.shape[1]} ‚Üí {n_components_95} components\")\n",
    "print(f\"Compression ratio: {compression_ratio:.1%}\")\n",
    "print(f\"Space saved: {1 - compression_ratio:.1%}\")\n",
    "print(f\"Information retained: 95%\")\n",
    "\n",
    "print(\"\\n‚úì PCA successfully reduced dimensions while preserving information!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. t-SNE for Visualization\n",
    "\n",
    "**t-SNE** (t-Distributed Stochastic Neighbor Embedding) is a powerful non-linear dimensionality reduction technique optimized for visualization.\n",
    "\n",
    "### PCA vs t-SNE\n",
    "\n",
    "**PCA (Linear):**\n",
    "- Fast and deterministic\n",
    "- Preserves global structure\n",
    "- Works well for linear relationships\n",
    "- Good for preprocessing\n",
    "\n",
    "**t-SNE (Non-Linear):**\n",
    "- Slow but powerful\n",
    "- Preserves local structure (clusters)\n",
    "- Captures non-linear relationships\n",
    "- Best for visualization only\n",
    "\n",
    "### How t-SNE Works\n",
    "\n",
    "**Intuition**: Keep similar points close, push dissimilar points apart\n",
    "\n",
    "**Algorithm:**\n",
    "1. **Compute pairwise similarities** in high-D space\n",
    "2. **Initialize** random low-D positions\n",
    "3. **Iteratively adjust** positions to match high-D similarities\n",
    "4. **Minimize** difference between high-D and low-D distributions\n",
    "\n",
    "### Key Parameters\n",
    "\n",
    "**perplexity** (5-50, default=30):\n",
    "- Balances local vs global structure\n",
    "- Think of it as \"expected number of neighbors\"\n",
    "- Small perplexity (5-15): Focus on local clusters\n",
    "- Large perplexity (30-50): More global structure\n",
    "- Rule: `5 < perplexity < n_samples/3`\n",
    "\n",
    "**learning_rate** (10-1000, default=200):\n",
    "- Step size for optimization\n",
    "- Too low: Stuck in local minima\n",
    "- Too high: Unstable, points overlap\n",
    "- Typical: 100-500\n",
    "\n",
    "**n_iter** (250-1000, default=1000):\n",
    "- Number of optimization iterations\n",
    "- More iterations = better convergence\n",
    "- Watch for \"KL divergence\" to stabilize\n",
    "\n",
    "### Important Caveats\n",
    "\n",
    "**‚ùå DON'T:**\n",
    "- Use t-SNE for anything except visualization\n",
    "- Interpret distances between clusters\n",
    "- Use same plot for different runs (it's stochastic)\n",
    "- Use t-SNE output as features for ML\n",
    "\n",
    "**‚úì DO:**\n",
    "- Use for exploratory data analysis\n",
    "- Try different perplexity values\n",
    "- Set random_state for reproducibility\n",
    "- Use PCA first if data > 50 dimensions\n",
    "\n",
    "### When to Use t-SNE\n",
    "\n",
    "- **Visualizing clusters**: See how data naturally groups\n",
    "- **Exploratory analysis**: Discover patterns\n",
    "- **High-dimensional data**: Where PCA fails\n",
    "- **Non-linear relationships**: Manifold structures\n",
    "\n",
    "Let's compare PCA and t-SNE on the same data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE - Comprehensive Comparison with PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import time\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"t-SNE vs PCA COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Use digits dataset (64 dimensions)\n",
    "X = X_digits[:1000]  # Subset for speed (t-SNE is slow)\n",
    "y = y_digits[:1000]\n",
    "\n",
    "print(f\"\\nDataset: {X.shape[0]} samples, {X.shape[1]} features\")\n",
    "print(\"Reducing 64D ‚Üí 2D for visualization\")\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# PCA (fast)\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"Running PCA...\")\n",
    "start_time = time.time()\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "pca_time = time.time() - start_time\n",
    "print(f\"PCA completed in {pca_time:.2f} seconds\")\n",
    "print(f\"Variance explained: {pca.explained_variance_ratio_.sum():.1%}\")\n",
    "\n",
    "# t-SNE (slow but better for visualization)\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"Running t-SNE (this takes time)...\")\n",
    "start_time = time.time()\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000, verbose=0)\n",
    "X_tsne = tsne.fit_transform(X_scaled)\n",
    "tsne_time = time.time() - start_time\n",
    "print(f\"t-SNE completed in {tsne_time:.2f} seconds\")\n",
    "print(f\"t-SNE is {tsne_time/pca_time:.1f}x slower than PCA\")\n",
    "\n",
    "# Visualize both side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "# PCA visualization\n",
    "scatter1 = axes[0].scatter(\n",
    "    X_pca[:, 0], X_pca[:, 1], c=y, cmap=\"tab10\", s=30, alpha=0.7, edgecolors=\"k\", linewidth=0.5\n",
    ")\n",
    "axes[0].set_xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]:.1%})\", fontsize=12)\n",
    "axes[0].set_ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]:.1%})\", fontsize=12)\n",
    "axes[0].set_title(\n",
    "    f\"PCA Visualization\\n\" f\"Linear, Fast ({pca_time:.2f}s), Preserves Global Structure\",\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter1, ax=axes[0], label=\"Digit\")\n",
    "\n",
    "# t-SNE visualization\n",
    "scatter2 = axes[1].scatter(\n",
    "    X_tsne[:, 0], X_tsne[:, 1], c=y, cmap=\"tab10\", s=30, alpha=0.7, edgecolors=\"k\", linewidth=0.5\n",
    ")\n",
    "axes[1].set_xlabel(\"t-SNE Dimension 1\", fontsize=12)\n",
    "axes[1].set_ylabel(\"t-SNE Dimension 2\", fontsize=12)\n",
    "axes[1].set_title(\n",
    "    f\"t-SNE Visualization\\n\" f\"Non-Linear, Slow ({tsne_time:.1f}s), Preserves Local Structure\",\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter2, ax=axes[1], label=\"Digit\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Effect of perplexity\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EFFECT OF PERPLEXITY PARAMETER\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Use smaller subset for speed\n",
    "X_small = X_scaled[:300]\n",
    "y_small = y[:300]\n",
    "\n",
    "perplexities = [5, 15, 30, 50]\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "for idx, perp in enumerate(perplexities):\n",
    "    print(f\"\\nRunning t-SNE with perplexity={perp}...\")\n",
    "    tsne_temp = TSNE(n_components=2, random_state=42, perplexity=perp, n_iter=500, verbose=0)\n",
    "    X_tsne_temp = tsne_temp.fit_transform(X_small)\n",
    "\n",
    "    scatter = axes[idx].scatter(\n",
    "        X_tsne_temp[:, 0],\n",
    "        X_tsne_temp[:, 1],\n",
    "        c=y_small,\n",
    "        cmap=\"tab10\",\n",
    "        s=40,\n",
    "        alpha=0.7,\n",
    "        edgecolors=\"k\",\n",
    "        linewidth=0.5,\n",
    "    )\n",
    "    axes[idx].set_title(\n",
    "        f\"Perplexity = {perp}\\n\"\n",
    "        f'{\"Local focus\" if perp <= 15 else \"Balanced\" if perp == 30 else \"Global focus\"}',\n",
    "        fontsize=12,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "    axes[idx].set_xlabel(\"t-SNE 1\")\n",
    "    axes[idx].set_ylabel(\"t-SNE 2\")\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(\n",
    "    \"Effect of Perplexity on t-SNE\\nLower = More local clusters, Higher = More global structure\",\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# PCA + t-SNE pipeline for high-D data\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BEST PRACTICE: PCA PREPROCESSING FOR t-SNE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nFor high-dimensional data (>50D), use PCA first:\")\n",
    "print(\"  1. PCA: 64D ‚Üí 30D (remove noise)\")\n",
    "print(\"  2. t-SNE: 30D ‚Üí 2D (visualization)\")\n",
    "\n",
    "# Apply PCA first\n",
    "pca_pre = PCA(n_components=30, random_state=42)\n",
    "X_pca_pre = pca_pre.fit_transform(X_scaled)\n",
    "\n",
    "print(f\"\\nPCA preprocessing: {X_scaled.shape[1]}D ‚Üí {X_pca_pre.shape[1]}D\")\n",
    "print(f\"Variance retained: {pca_pre.explained_variance_ratio_.sum():.1%}\")\n",
    "\n",
    "# Then t-SNE\n",
    "print(\"\\nApplying t-SNE on PCA-reduced data...\")\n",
    "start_time = time.time()\n",
    "tsne_final = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000, verbose=0)\n",
    "X_final = tsne_final.fit_transform(X_pca_pre)\n",
    "final_time = time.time() - start_time\n",
    "\n",
    "print(f\"Completed in {final_time:.2f} seconds\")\n",
    "print(f\"Faster than direct t-SNE: {tsne_time:.2f}s ‚Üí {final_time:.2f}s\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "scatter = plt.scatter(\n",
    "    X_tsne[:, 0], X_tsne[:, 1], c=y, cmap=\"tab10\", s=30, alpha=0.7, edgecolors=\"k\", linewidth=0.5\n",
    ")\n",
    "plt.xlabel(\"t-SNE 1\", fontsize=12)\n",
    "plt.ylabel(\"t-SNE 2\", fontsize=12)\n",
    "plt.title(f\"Direct t-SNE\\n64D ‚Üí 2D ({tsne_time:.2f}s)\", fontsize=14, fontweight=\"bold\")\n",
    "plt.colorbar(scatter, label=\"Digit\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "scatter = plt.scatter(\n",
    "    X_final[:, 0], X_final[:, 1], c=y, cmap=\"tab10\", s=30, alpha=0.7, edgecolors=\"k\", linewidth=0.5\n",
    ")\n",
    "plt.xlabel(\"t-SNE 1\", fontsize=12)\n",
    "plt.ylabel(\"t-SNE 2\", fontsize=12)\n",
    "plt.title(f\"PCA + t-SNE\\n64D ‚Üí 30D ‚Üí 2D ({final_time:.2f}s)\", fontsize=14, fontweight=\"bold\")\n",
    "plt.colorbar(scatter, label=\"Digit\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(\"PCA Preprocessing Speeds Up t-SNE\", fontsize=16, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Key Takeaways:\")\n",
    "print(\"  ‚Ä¢ PCA: Fast, linear, good for preprocessing\")\n",
    "print(\"  ‚Ä¢ t-SNE: Slow, non-linear, excellent for visualization\")\n",
    "print(\"  ‚Ä¢ Best practice: PCA first, then t-SNE\")\n",
    "print(\"  ‚Ä¢ Always set random_state for reproducibility\")\n",
    "print(\"  ‚Ä¢ Try different perplexity values (5-50)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Anomaly Detection\n",
    "\n",
    "**Anomaly Detection** (Outlier Detection) identifies rare items, events, or observations that differ significantly from the majority.\n",
    "\n",
    "### What are Anomalies?\n",
    "\n",
    "**Anomalies** (outliers) are data points that deviate from normal patterns:\n",
    "\n",
    "- **Point Anomalies**: Individual unusual instances\n",
    "- **Contextual Anomalies**: Unusual in specific context (e.g., high temperature in winter)\n",
    "- **Collective Anomalies**: Collection of related instances is anomalous\n",
    "\n",
    "### Why Detect Anomalies?\n",
    "\n",
    "**Real-World Applications:**\n",
    "- üîí **Fraud Detection**: Unusual credit card transactions\n",
    "- üè• **Healthcare**: Abnormal patient vitals\n",
    "- üè≠ **Manufacturing**: Defective products\n",
    "- üåê **Cybersecurity**: Network intrusions\n",
    "- üìä **Data Quality**: Corrupt or erroneous data\n",
    "\n",
    "### Unsupervised Anomaly Detection Methods\n",
    "\n",
    "**1. Isolation Forest**\n",
    "- Isolates anomalies using random splits\n",
    "- Anomalies are easier to isolate (fewer splits needed)\n",
    "- Fast and effective\n",
    "\n",
    "**2. Local Outlier Factor (LOF)**\n",
    "- Compares local density to neighbors\n",
    "- Outliers have lower density\n",
    "- Good for varying densities\n",
    "\n",
    "**3. One-Class SVM**\n",
    "- Learns decision boundary around normal data\n",
    "- Points outside boundary = anomalies\n",
    "- Effective but slower\n",
    "\n",
    "**4. Statistical Methods**\n",
    "- Z-score: Points > 3 std deviations\n",
    "- IQR: Outside 1.5 √ó IQR\n",
    "- Simple but assumes distribution\n",
    "\n",
    "### Isolation Forest - How It Works\n",
    "\n",
    "**Algorithm:**\n",
    "1. Randomly select feature and split value\n",
    "2. Recursively partition data\n",
    "3. Anomalies require fewer splits to isolate\n",
    "4. **Anomaly score** = average path length (lower = more anomalous)\n",
    "\n",
    "**Why it works**: Anomalies are \"few and different\", so they're easier to separate\n",
    "\n",
    "### Key Parameters\n",
    "\n",
    "**contamination** (0.01-0.5, default='auto'):\n",
    "- Expected proportion of outliers\n",
    "- Set based on domain knowledge\n",
    "- Example: 0.1 = expect 10% anomalies\n",
    "\n",
    "**n_estimators** (50-500, default=100):\n",
    "- Number of isolation trees\n",
    "- More trees = more stable\n",
    "- Similar to Random Forest\n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "**With labels (semi-supervised):**\n",
    "- Precision, Recall, F1-Score\n",
    "- ROC-AUC\n",
    "\n",
    "**Without labels (unsupervised):**\n",
    "- Manual inspection\n",
    "- Domain expert validation\n",
    "- Silhouette score (for clustering-based)\n",
    "\n",
    "Let's detect anomalies in sensor data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anomaly Detection - Comprehensive Implementation\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ANOMALY DETECTION WITH MULTIPLE METHODS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load sensor data\n",
    "df_sensor = pd.read_csv(\"../../data_advanced/sensor_data.csv\")\n",
    "print(f\"\\nSensor dataset: {df_sensor.shape[0]} readings, {df_sensor.shape[1]} features\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df_sensor.head())\n",
    "\n",
    "# Select features\n",
    "features = [\"temperature\", \"pressure\", \"vibration\"]\n",
    "X_sensor = df_sensor[features].values\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_sensor)\n",
    "\n",
    "# Method 1: Isolation Forest\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"METHOD 1: ISOLATION FOREST\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "iso_forest = IsolationForest(contamination=0.1, random_state=42, n_estimators=100)\n",
    "y_pred_iso = iso_forest.fit_predict(X_scaled)\n",
    "anomaly_scores_iso = iso_forest.score_samples(X_scaled)\n",
    "\n",
    "n_anomalies_iso = (y_pred_iso == -1).sum()\n",
    "print(\n",
    "    f\"\\nIsolation Forest detected: {n_anomalies_iso} anomalies ({n_anomalies_iso/len(X_sensor)*100:.1f}%)\"\n",
    ")\n",
    "\n",
    "# Method 2: Local Outlier Factor\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"METHOD 2: LOCAL OUTLIER FACTOR (LOF)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "lof = LocalOutlierFactor(n_neighbors=20, contamination=0.1)\n",
    "y_pred_lof = lof.fit_predict(X_scaled)\n",
    "anomaly_scores_lof = lof.negative_outlier_factor_\n",
    "\n",
    "n_anomalies_lof = (y_pred_lof == -1).sum()\n",
    "print(f\"\\nLOF detected: {n_anomalies_lof} anomalies ({n_anomalies_lof/len(X_sensor)*100:.1f}%)\")\n",
    "\n",
    "# Method 3: One-Class SVM\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"METHOD 3: ONE-CLASS SVM\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "oc_svm = OneClassSVM(nu=0.1, gamma=\"auto\")\n",
    "y_pred_svm = oc_svm.fit_predict(X_scaled)\n",
    "anomaly_scores_svm = oc_svm.score_samples(X_scaled)\n",
    "\n",
    "n_anomalies_svm = (y_pred_svm == -1).sum()\n",
    "print(\n",
    "    f\"\\nOne-Class SVM detected: {n_anomalies_svm} anomalies ({n_anomalies_svm/len(X_sensor)*100:.1f}%)\"\n",
    ")\n",
    "\n",
    "# Method 4: Statistical (Z-score)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"METHOD 4: STATISTICAL (Z-SCORE)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "z_scores = np.abs((X_sensor - X_sensor.mean(axis=0)) / X_sensor.std(axis=0))\n",
    "y_pred_stat = (z_scores > 3).any(axis=1).astype(int) * -2 + 1  # Convert to -1/1\n",
    "n_anomalies_stat = (y_pred_stat == -1).sum()\n",
    "\n",
    "print(\n",
    "    f\"\\nZ-score method detected: {n_anomalies_stat} anomalies ({n_anomalies_stat/len(X_sensor)*100:.1f}%)\"\n",
    ")\n",
    "print(\"(Points with any feature > 3 standard deviations)\")\n",
    "\n",
    "# Visualize all methods\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
    "\n",
    "methods = [\n",
    "    (\"Isolation Forest\", y_pred_iso, anomaly_scores_iso, axes[0, 0]),\n",
    "    (\"Local Outlier Factor\", y_pred_lof, anomaly_scores_lof, axes[0, 1]),\n",
    "    (\"One-Class SVM\", y_pred_svm, anomaly_scores_svm, axes[1, 0]),\n",
    "    (\"Z-Score (Statistical)\", y_pred_stat, z_scores.max(axis=1), axes[1, 1]),\n",
    "]\n",
    "\n",
    "for name, predictions, scores, ax in methods:\n",
    "    # Plot normal vs anomalies\n",
    "    normal_mask = predictions == 1\n",
    "    anomaly_mask = predictions == -1\n",
    "\n",
    "    ax.scatter(\n",
    "        X_sensor[normal_mask, 0],\n",
    "        X_sensor[normal_mask, 1],\n",
    "        c=\"blue\",\n",
    "        label=\"Normal\",\n",
    "        s=30,\n",
    "        alpha=0.6,\n",
    "        edgecolors=\"k\",\n",
    "        linewidth=0.5,\n",
    "    )\n",
    "    ax.scatter(\n",
    "        X_sensor[anomaly_mask, 0],\n",
    "        X_sensor[anomaly_mask, 1],\n",
    "        c=\"red\",\n",
    "        label=\"Anomaly\",\n",
    "        s=100,\n",
    "        alpha=0.8,\n",
    "        edgecolors=\"black\",\n",
    "        linewidth=1.5,\n",
    "        marker=\"X\",\n",
    "    )\n",
    "\n",
    "    ax.set_xlabel(\"Temperature\", fontsize=12)\n",
    "    ax.set_ylabel(\"Pressure\", fontsize=12)\n",
    "    ax.set_title(f\"{name}\\n{anomaly_mask.sum()} anomalies detected\", fontsize=14, fontweight=\"bold\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(\"Anomaly Detection Methods Comparison\", fontsize=16, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze anomaly scores distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Isolation Forest scores\n",
    "axes[0].hist(anomaly_scores_iso, bins=50, color=\"steelblue\", alpha=0.7, edgecolor=\"black\")\n",
    "axes[0].axvline(\n",
    "    x=iso_forest.offset_,\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=f\"Threshold = {iso_forest.offset_:.3f}\",\n",
    ")\n",
    "axes[0].set_xlabel(\"Anomaly Score\", fontsize=12)\n",
    "axes[0].set_ylabel(\"Frequency\", fontsize=12)\n",
    "axes[0].set_title(\n",
    "    \"Isolation Forest\\nScore Distribution\\n(Lower = More Anomalous)\", fontsize=12, fontweight=\"bold\"\n",
    ")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "# LOF scores\n",
    "axes[1].hist(anomaly_scores_lof, bins=50, color=\"coral\", alpha=0.7, edgecolor=\"black\")\n",
    "axes[1].axvline(x=-1.5, color=\"red\", linestyle=\"--\", linewidth=2, label=\"Typical Threshold\")\n",
    "axes[1].set_xlabel(\"LOF Score (Negative)\", fontsize=12)\n",
    "axes[1].set_ylabel(\"Frequency\", fontsize=12)\n",
    "axes[1].set_title(\n",
    "    \"Local Outlier Factor\\nScore Distribution\\n(More Negative = More Anomalous)\",\n",
    "    fontsize=12,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "# SVM scores\n",
    "axes[2].hist(anomaly_scores_svm, bins=50, color=\"lightgreen\", alpha=0.7, edgecolor=\"black\")\n",
    "axes[2].axvline(x=0, color=\"red\", linestyle=\"--\", linewidth=2, label=\"Decision Boundary\")\n",
    "axes[2].set_xlabel(\"Distance from Boundary\", fontsize=12)\n",
    "axes[2].set_ylabel(\"Frequency\", fontsize=12)\n",
    "axes[2].set_title(\n",
    "    \"One-Class SVM\\nScore Distribution\\n(Negative = Anomaly)\", fontsize=12, fontweight=\"bold\"\n",
    ")\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Agreement between methods\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"METHOD AGREEMENT ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "agreement = pd.DataFrame(\n",
    "    {\n",
    "        \"Isolation_Forest\": y_pred_iso,\n",
    "        \"LOF\": y_pred_lof,\n",
    "        \"One_Class_SVM\": y_pred_svm,\n",
    "        \"Z_Score\": y_pred_stat,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Count how many methods agree each point is anomaly\n",
    "anomaly_votes = (agreement == -1).sum(axis=1)\n",
    "\n",
    "print(f\"\\nConsensus anomalies (detected by all 4 methods): {(anomaly_votes == 4).sum()}\")\n",
    "print(f\"Detected by 3 methods: {(anomaly_votes == 3).sum()}\")\n",
    "print(f\"Detected by 2 methods: {(anomaly_votes == 2).sum()}\")\n",
    "print(f\"Detected by 1 method: {(anomaly_votes == 1).sum()}\")\n",
    "print(f\"Detected by 0 methods (all agree normal): {(anomaly_votes == 0).sum()}\")\n",
    "\n",
    "# Visualize consensus\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "scatter = plt.scatter(\n",
    "    X_sensor[:, 0],\n",
    "    X_sensor[:, 1],\n",
    "    c=anomaly_votes,\n",
    "    cmap=\"YlOrRd\",\n",
    "    s=50,\n",
    "    alpha=0.7,\n",
    "    edgecolors=\"k\",\n",
    "    linewidth=0.5,\n",
    ")\n",
    "plt.colorbar(scatter, label=\"# Methods Detecting as Anomaly\")\n",
    "plt.xlabel(\"Temperature\", fontsize=12)\n",
    "plt.ylabel(\"Pressure\", fontsize=12)\n",
    "plt.title(\n",
    "    \"Anomaly Detection Consensus\\n(Darker = More methods agree)\", fontsize=14, fontweight=\"bold\"\n",
    ")\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(\n",
    "    range(5),\n",
    "    [(anomaly_votes == i).sum() for i in range(5)],\n",
    "    color=[\"green\", \"yellow\", \"orange\", \"darkorange\", \"red\"],\n",
    "    edgecolor=\"black\",\n",
    "    linewidth=1.5,\n",
    ")\n",
    "plt.xlabel(\"Number of Methods Detecting as Anomaly\", fontsize=12)\n",
    "plt.ylabel(\"Count\", fontsize=12)\n",
    "plt.title(\"Distribution of Method Agreement\", fontsize=14, fontweight=\"bold\")\n",
    "plt.xticks(range(5), [\"0 (Normal)\", \"1\", \"2\", \"3\", \"4 (Consensus)\"])\n",
    "plt.grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Anomaly detection complete!\")\n",
    "print(\"\\nRecommendations:\")\n",
    "print(\"  ‚Ä¢ Use Isolation Forest for general purpose (fast, effective)\")\n",
    "print(\"  ‚Ä¢ Use LOF for varying density clusters\")\n",
    "print(\"  ‚Ä¢ Use One-Class SVM for high-dimensional data\")\n",
    "print(\"  ‚Ä¢ Use consensus (multiple methods) for high-confidence detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Customer Segmentation Project\n",
    "\n",
    "Let's apply everything we've learned to a **real-world customer segmentation project** using all unsupervised learning techniques!\n",
    "\n",
    "### Project Goal\n",
    "\n",
    "**Segment customers** into meaningful groups for targeted marketing using:\n",
    "- K-Means clustering\n",
    "- Hierarchical clustering for hierarchy analysis\n",
    "- PCA for dimensionality reduction\n",
    "- t-SNE for visualization\n",
    "- Anomaly detection for identifying unusual customers\n",
    "\n",
    "### Business Questions to Answer\n",
    "\n",
    "1. How many distinct customer segments exist?\n",
    "2. What characterizes each segment?\n",
    "3. Which segment should we target for:\n",
    "   - Premium products?\n",
    "   - Discount campaigns?\n",
    "   - Retention efforts?\n",
    "4. Are there any anomalous customers (potential fraud/high-value)?\n",
    "\n",
    "### Project Workflow\n",
    "\n",
    "```\n",
    "1. Data Loading & EDA\n",
    "   ‚Üì\n",
    "2. Feature Engineering & Scaling\n",
    "   ‚Üì\n",
    "3. Dimensionality Reduction (PCA)\n",
    "   ‚Üì\n",
    "4. Determine Optimal k (Elbow + Silhouette)\n",
    "   ‚Üì\n",
    "5. Apply Clustering (K-Means + Hierarchical)\n",
    "   ‚Üì\n",
    "6. Visualization (PCA + t-SNE)\n",
    "   ‚Üì\n",
    "7. Anomaly Detection\n",
    "   ‚Üì\n",
    "8. Business Insights & Recommendations\n",
    "```\n",
    "\n",
    "### Deliverables\n",
    "\n",
    "- Cluster assignments for each customer\n",
    "- Segment profiles and characteristics\n",
    "- Visualization dashboard\n",
    "- Actionable business recommendations\n",
    "\n",
    "Let's build a complete customer segmentation solution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer Segmentation - End-to-End Project\n",
    "print(\"=\" * 70)\n",
    "print(\" CUSTOMER SEGMENTATION PROJECT - END-TO-END SOLUTION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Step 1: Load and Explore Data\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 1: DATA LOADING & EXPLORATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "df_cust = pd.read_csv(\"../../data_advanced/feature_engineering.csv\")\n",
    "print(f\"\\nDataset: {df_cust.shape[0]} customers, {df_cust.shape[1]} features\")\n",
    "print(f\"\\nFeatures: {list(df_cust.columns)}\")\n",
    "print(f\"\\nBasic statistics:\")\n",
    "print(df_cust.describe())\n",
    "\n",
    "# Step 2: Feature Engineering\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 2: FEATURE SELECTION & ENGINEERING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Select relevant features for segmentation\n",
    "features = [\"age\", \"income\", \"education_years\", \"experience_years\", \"num_dependents\"]\n",
    "X = df_cust[features].copy()\n",
    "\n",
    "print(f\"\\nSelected features: {features}\")\n",
    "print(f\"Feature correlations:\")\n",
    "print(X.corr())\n",
    "\n",
    "# Standardize (crucial for clustering!)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(f\"\\n‚úì Features standardized (mean=0, std=1)\")\n",
    "\n",
    "# Step 3: Dimensionality Reduction with PCA\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 3: PCA FOR DIMENSIONALITY REDUCTION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "pca = PCA(random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Find components for 95% variance\n",
    "cumvar = np.cumsum(pca.explained_variance_ratio_)\n",
    "n_comp_95 = np.argmax(cumvar >= 0.95) + 1\n",
    "\n",
    "print(f\"\\nComponents needed for 95% variance: {n_comp_95}/{len(features)}\")\n",
    "print(f\"Explained variance by component:\")\n",
    "for i in range(len(features)):\n",
    "    print(f\"  PC{i+1}: {pca.explained_variance_ratio_[i]:.1%} (Cumulative: {cumvar[i]:.1%})\")\n",
    "\n",
    "# Use reduced dimensions\n",
    "X_reduced = X_pca[:, :n_comp_95]\n",
    "\n",
    "# Step 4: Determine Optimal k\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 4: FINDING OPTIMAL NUMBER OF CLUSTERS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "K_range = range(2, 9)\n",
    "wcss = []\n",
    "silhouette_scores = []\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_reduced)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(X_reduced, kmeans.labels_))\n",
    "\n",
    "optimal_k = 4  # Based on elbow and silhouette\n",
    "print(f\"\\n‚úì Optimal k = {optimal_k} (based on elbow method and silhouette score)\")\n",
    "\n",
    "# Step 5: Apply Clustering\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 5: CLUSTERING (K-MEANS + HIERARCHICAL)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# K-Means\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "df_cust[\"segment_kmeans\"] = kmeans.fit_predict(X_reduced)\n",
    "\n",
    "# Hierarchical (for comparison)\n",
    "hierarchical = AgglomerativeClustering(n_clusters=optimal_k, linkage=\"ward\")\n",
    "df_cust[\"segment_hier\"] = hierarchical.fit_predict(X_reduced)\n",
    "\n",
    "print(f\"K-Means Silhouette Score: {silhouette_score(X_reduced, df_cust['segment_kmeans']):.4f}\")\n",
    "print(f\"Hierarchical Silhouette Score: {silhouette_score(X_reduced, df_cust['segment_hier']):.4f}\")\n",
    "\n",
    "# Use K-Means for final segmentation\n",
    "df_cust[\"segment\"] = df_cust[\"segment_kmeans\"]\n",
    "\n",
    "# Step 6: Segment Profiling\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 6: SEGMENT PROFILING & CHARACTERIZATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for seg in range(optimal_k):\n",
    "    segment_data = df_cust[df_cust[\"segment\"] == seg]\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\n",
    "        f\"SEGMENT {seg}: {len(segment_data)} customers ({len(segment_data)/len(df_cust)*100:.1f}%)\"\n",
    "    )\n",
    "    print(f\"{'='*70}\")\n",
    "    print(segment_data[features].describe().loc[[\"mean\", \"50%\"]].T)\n",
    "\n",
    "# Create segment names based on characteristics\n",
    "segment_means = df_cust.groupby(\"segment\")[features].mean()\n",
    "\n",
    "# Name segments (simplified logic)\n",
    "segment_names = {\n",
    "    0: \"Entry-Level Young\",\n",
    "    1: \"Mid-Career Family\",\n",
    "    2: \"Senior Professionals\",\n",
    "    3: \"Experienced Elite\",\n",
    "}\n",
    "\n",
    "df_cust[\"segment_name\"] = df_cust[\"segment\"].map(segment_names)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SEGMENT NAMING & CHARACTERIZATION\")\n",
    "print(\"=\" * 70)\n",
    "for seg, name in segment_names.items():\n",
    "    print(f\"Segment {seg}: {name}\")\n",
    "\n",
    "# Step 7: Visualization Dashboard\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 7: VISUALIZATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "\n",
    "# 1. Elbow plot\n",
    "ax1 = plt.subplot(3, 3, 1)\n",
    "ax1.plot(K_range, wcss, \"bo-\", linewidth=2, markersize=8)\n",
    "ax1.axvline(x=optimal_k, color=\"red\", linestyle=\"--\", label=f\"Optimal k={optimal_k}\")\n",
    "ax1.set_xlabel(\"k\", fontsize=11)\n",
    "ax1.set_ylabel(\"WCSS\", fontsize=11)\n",
    "ax1.set_title(\"Elbow Method\", fontsize=12, fontweight=\"bold\")\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Silhouette scores\n",
    "ax2 = plt.subplot(3, 3, 2)\n",
    "ax2.plot(K_range, silhouette_scores, \"go-\", linewidth=2, markersize=8)\n",
    "ax2.axvline(x=optimal_k, color=\"red\", linestyle=\"--\", label=f\"Optimal k={optimal_k}\")\n",
    "ax2.set_xlabel(\"k\", fontsize=11)\n",
    "ax2.set_ylabel(\"Silhouette Score\", fontsize=11)\n",
    "ax2.set_title(\"Silhouette Analysis\", fontsize=12, fontweight=\"bold\")\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. PCA visualization\n",
    "ax3 = plt.subplot(3, 3, 3)\n",
    "pca_2d = PCA(n_components=2, random_state=42)\n",
    "X_pca_2d = pca_2d.fit_transform(X_scaled)\n",
    "for seg in range(optimal_k):\n",
    "    mask = df_cust[\"segment\"] == seg\n",
    "    ax3.scatter(\n",
    "        X_pca_2d[mask, 0],\n",
    "        X_pca_2d[mask, 1],\n",
    "        label=segment_names[seg],\n",
    "        s=30,\n",
    "        alpha=0.6,\n",
    "        edgecolors=\"k\",\n",
    "        linewidth=0.5,\n",
    "    )\n",
    "ax3.set_xlabel(f\"PC1 ({pca_2d.explained_variance_ratio_[0]:.1%})\", fontsize=11)\n",
    "ax3.set_ylabel(f\"PC2 ({pca_2d.explained_variance_ratio_[1]:.1%})\", fontsize=11)\n",
    "ax3.set_title(\"PCA: Segments in 2D\", fontsize=12, fontweight=\"bold\")\n",
    "ax3.legend(fontsize=9)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Segment sizes\n",
    "ax4 = plt.subplot(3, 3, 4)\n",
    "segment_counts = df_cust[\"segment\"].value_counts().sort_index()\n",
    "colors = plt.cm.Set3(range(optimal_k))\n",
    "ax4.bar(\n",
    "    [segment_names[i] for i in range(optimal_k)],\n",
    "    segment_counts,\n",
    "    color=colors,\n",
    "    edgecolor=\"black\",\n",
    "    linewidth=1.5,\n",
    ")\n",
    "ax4.set_ylabel(\"Number of Customers\", fontsize=11)\n",
    "ax4.set_title(\"Segment Distribution\", fontsize=12, fontweight=\"bold\")\n",
    "ax4.tick_params(axis=\"x\", rotation=15)\n",
    "ax4.grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "# 5. Income by segment\n",
    "ax5 = plt.subplot(3, 3, 5)\n",
    "df_cust.boxplot(column=\"income\", by=\"segment_name\", ax=ax5)\n",
    "ax5.set_xlabel(\"Segment\", fontsize=11)\n",
    "ax5.set_ylabel(\"Income\", fontsize=11)\n",
    "ax5.set_title(\"Income Distribution by Segment\", fontsize=12, fontweight=\"bold\")\n",
    "plt.sca(ax5)\n",
    "plt.xticks(rotation=15)\n",
    "ax5.get_figure().suptitle(\"\")  # Remove default title\n",
    "\n",
    "# 6. Age by segment\n",
    "ax6 = plt.subplot(3, 3, 6)\n",
    "df_cust.boxplot(column=\"age\", by=\"segment_name\", ax=ax6)\n",
    "ax6.set_xlabel(\"Segment\", fontsize=11)\n",
    "ax6.set_ylabel(\"Age\", fontsize=11)\n",
    "ax6.set_title(\"Age Distribution by Segment\", fontsize=12, fontweight=\"bold\")\n",
    "plt.sca(ax6)\n",
    "plt.xticks(rotation=15)\n",
    "ax6.get_figure().suptitle(\"\")\n",
    "\n",
    "# 7. Segment profiles heatmap\n",
    "ax7 = plt.subplot(3, 3, 7)\n",
    "segment_profiles = df_cust.groupby(\"segment\")[features].mean()\n",
    "segment_profiles_norm = (segment_profiles - segment_profiles.mean()) / segment_profiles.std()\n",
    "im = ax7.imshow(segment_profiles_norm.T, cmap=\"RdYlGn\", aspect=\"auto\", vmin=-2, vmax=2)\n",
    "ax7.set_xticks(range(optimal_k))\n",
    "ax7.set_xticklabels([segment_names[i] for i in range(optimal_k)], rotation=15)\n",
    "ax7.set_yticks(range(len(features)))\n",
    "ax7.set_yticklabels(features)\n",
    "ax7.set_title(\"Segment Feature Profiles\", fontsize=12, fontweight=\"bold\")\n",
    "plt.colorbar(im, ax=ax7, label=\"Z-Score\")\n",
    "\n",
    "# 8. Hierarchical dendrogram\n",
    "ax8 = plt.subplot(3, 3, 8)\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "Z = linkage(X_reduced[:100], method=\"ward\")  # Subset for clarity\n",
    "dendrogram(Z, ax=ax8, truncate_mode=\"lastp\", p=12, show_leaf_counts=True)\n",
    "ax8.set_title(\"Hierarchical Clustering\\n(Sample)\", fontsize=12, fontweight=\"bold\")\n",
    "ax8.set_xlabel(\"Cluster Size\", fontsize=11)\n",
    "ax8.set_ylabel(\"Distance\", fontsize=11)\n",
    "\n",
    "# 9. Anomaly detection\n",
    "ax9 = plt.subplot(3, 3, 9)\n",
    "iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "anomalies = iso_forest.fit_predict(X_scaled)\n",
    "df_cust[\"is_anomaly\"] = anomalies\n",
    "normal_mask = anomalies == 1\n",
    "anomaly_mask = anomalies == -1\n",
    "ax9.scatter(\n",
    "    X_pca_2d[normal_mask, 0], X_pca_2d[normal_mask, 1], c=\"blue\", label=\"Normal\", s=20, alpha=0.5\n",
    ")\n",
    "ax9.scatter(\n",
    "    X_pca_2d[anomaly_mask, 0],\n",
    "    X_pca_2d[anomaly_mask, 1],\n",
    "    c=\"red\",\n",
    "    label=\"Anomaly\",\n",
    "    s=100,\n",
    "    marker=\"X\",\n",
    "    edgecolors=\"black\",\n",
    "    linewidth=1.5,\n",
    ")\n",
    "ax9.set_xlabel(\"PC1\", fontsize=11)\n",
    "ax9.set_ylabel(\"PC2\", fontsize=11)\n",
    "ax9.set_title(f\"Anomaly Detection\\n{anomaly_mask.sum()} anomalies\", fontsize=12, fontweight=\"bold\")\n",
    "ax9.legend()\n",
    "ax9.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(\"Customer Segmentation Dashboard\", fontsize=16, fontweight=\"bold\", y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Step 8: Business Insights\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STEP 8: BUSINESS INSIGHTS & RECOMMENDATIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nKEY FINDINGS:\")\n",
    "print(\"-\" * 70)\n",
    "for seg in range(optimal_k):\n",
    "    seg_data = df_cust[df_cust[\"segment\"] == seg]\n",
    "    print(f\"\\n{segment_names[seg]} (n={len(seg_data)}):\")\n",
    "    print(f\"  ‚Ä¢ Average Income: ${seg_data['income'].mean():,.0f}\")\n",
    "    print(f\"  ‚Ä¢ Average Age: {seg_data['age'].mean():.1f} years\")\n",
    "    print(f\"  ‚Ä¢ Average Experience: {seg_data['experience_years'].mean():.1f} years\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ACTIONABLE RECOMMENDATIONS:\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\n1. PREMIUM PRODUCTS ‚Üí Target 'Experienced Elite' and 'Senior Professionals'\")\n",
    "print(\"   - High income, established careers\")\n",
    "print(\"   - Focus on quality and exclusivity\\n\")\n",
    "print(\"2. GROWTH PRODUCTS ‚Üí Target 'Mid-Career Family'\")\n",
    "print(\"   - Growing income, family needs\")\n",
    "print(\"   - Focus on value and family benefits\\n\")\n",
    "print(\"3. ENTRY OFFERS ‚Üí Target 'Entry-Level Young'\")\n",
    "print(\"   - Price-sensitive, career starting\")\n",
    "print(\"   - Focus on affordability and long-term relationships\\n\")\n",
    "print(f\"4. SPECIAL ATTENTION ‚Üí {anomaly_mask.sum()} anomalous customers\")\n",
    "print(\"   - Investigate for fraud OR high-value opportunities\")\n",
    "print(\"   - Manual review recommended\\n\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"‚úì CUSTOMER SEGMENTATION PROJECT COMPLETE!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Exercises\n",
    "\n",
    "Practice what you've learned with these hands-on exercises!\n",
    "\n",
    "### Exercise 1: Optimal k for Different Datasets\n",
    "Create synthetic datasets with different numbers of true clusters (2, 3, 5) using `make_blobs`. For each:\n",
    "- Apply the elbow method\n",
    "- Calculate silhouette scores for k=2 to 10\n",
    "- Compare whether the methods correctly identify the true number of clusters\n",
    "\n",
    "### Exercise 2: DBSCAN Parameter Tuning\n",
    "Load the `make_moons` dataset and:\n",
    "- Create a k-distance plot to find optimal epsilon\n",
    "- Try DBSCAN with eps values: [0.1, 0.2, 0.3, 0.4]\n",
    "- Try min_samples values: [3, 5, 10]\n",
    "- Find the combination that gives the best silhouette score\n",
    "\n",
    "### Exercise 3: PCA Reconstruction Error\n",
    "Using the digits dataset:\n",
    "- Apply PCA with different numbers of components (5, 10, 20, 30, 50)\n",
    "- For each, calculate reconstruction error: `mean((X_original - X_reconstructed)**2)`\n",
    "- Plot reconstruction error vs. number of components\n",
    "- Determine the \"sweet spot\" for compression\n",
    "\n",
    "### Exercise 4: Customer Segmentation with Different Features\n",
    "Using the customer data:\n",
    "- Try clustering with different feature combinations\n",
    "- Compare results using all features vs. only income and age\n",
    "- Does adding more features improve or worsen clustering?\n",
    "\n",
    "### Exercise 5: Anomaly Detection Comparison\n",
    "Apply all 4 anomaly detection methods (Isolation Forest, LOF, One-Class SVM, Z-score) to:\n",
    "- The customer dataset\n",
    "- Identify which customers are flagged by multiple methods\n",
    "- Manually inspect the top 10 \"most anomalous\" customers - what makes them unusual?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise Solutions - Try these yourself!\n",
    "\n",
    "# Exercise 1: Optimal k for Different Datasets\n",
    "print(\"Exercise 1: Testing elbow method on datasets with known k\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for true_k in [2, 3, 5]:\n",
    "    X_test, y_test = make_blobs(n_samples=300, centers=true_k, cluster_std=0.6, random_state=42)\n",
    "\n",
    "    # Your code here: Apply elbow method and find if it matches true_k\n",
    "    # Hint: Use range(2, 8) for k values\n",
    "    # Calculate WCSS and silhouette scores\n",
    "    # Plot results\n",
    "\n",
    "    print(f\"True k = {true_k}\")\n",
    "    print(\"  TODO: Implement elbow method and compare\\n\")\n",
    "\n",
    "# Exercise 2: DBSCAN Parameter Tuning\n",
    "print(\"\\nExercise 2: Finding optimal DBSCAN parameters\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "X_moons, _ = make_moons(n_samples=300, noise=0.05, random_state=42)\n",
    "\n",
    "# Your code here: Try different eps and min_samples combinations\n",
    "# Hint: Use nested loops to try all combinations\n",
    "# Track which combination gives best silhouette score\n",
    "\n",
    "print(\"TODO: Implement parameter grid search for DBSCAN\\n\")\n",
    "\n",
    "# Exercise 3: PCA Reconstruction Error\n",
    "print(\"\\nExercise 3: PCA reconstruction error analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Your code here: For each n_components in [5, 10, 20, 30, 50]:\n",
    "# 1. Apply PCA\n",
    "# 2. Reconstruct data\n",
    "# 3. Calculate MSE\n",
    "# 4. Plot error vs. components\n",
    "\n",
    "print(\"TODO: Calculate and plot reconstruction error\\n\")\n",
    "\n",
    "# Exercise 4: Feature Selection Impact\n",
    "print(\"\\nExercise 4: Impact of feature selection on clustering\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Your code here: Try clustering with:\n",
    "# 1. All features\n",
    "# 2. Only ['income', 'age']\n",
    "# 3. Only ['education_years', 'experience_years']\n",
    "# Compare silhouette scores\n",
    "\n",
    "print(\"TODO: Compare clustering with different feature sets\\n\")\n",
    "\n",
    "# Exercise 5: Anomaly Detection Consensus\n",
    "print(\"\\nExercise 5: Multi-method anomaly detection\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Your code here: Apply all 4 methods to customer data\n",
    "# Find customers flagged by 3+ methods\n",
    "# Print their characteristics\n",
    "\n",
    "print(\"TODO: Find high-confidence anomalies\\n\")\n",
    "\n",
    "print(\"\\n‚úì Complete these exercises to master unsupervised learning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Key Takeaways & Next Steps\n",
    "\n",
    "Congratulations! You've mastered unsupervised learning - one of the most powerful tools in machine learning for discovering hidden patterns in data.\n",
    "\n",
    "### What You've Learned\n",
    "\n",
    "#### 1. **Clustering Fundamentals**\n",
    "- ‚úì Unsupervised learning finds patterns without labels\n",
    "- ‚úì Distance metrics (Euclidean, Manhattan, Cosine)\n",
    "- ‚úì Evaluation metrics (WCSS, Silhouette Score)\n",
    "- ‚úì Use cases: customer segmentation, document clustering, image segmentation\n",
    "\n",
    "#### 2. **K-Means Clustering**\n",
    "- ‚úì Fast, scalable centroid-based clustering\n",
    "- ‚úì Elbow method for finding optimal k\n",
    "- ‚úì K-Means++ for better initialization\n",
    "- ‚úì Best for spherical, well-separated clusters\n",
    "- ‚úì **When to use**: General-purpose, known k, spherical clusters\n",
    "\n",
    "#### 3. **DBSCAN (Density-Based Clustering)**\n",
    "- ‚úì Finds arbitrary-shaped clusters\n",
    "- ‚úì No need to specify number of clusters\n",
    "- ‚úì Identifies outliers as noise\n",
    "- ‚úì Parameters: epsilon (Œµ) and MinPts\n",
    "- ‚úì **When to use**: Non-spherical clusters, unknown k, need outlier detection\n",
    "\n",
    "#### 4. **Hierarchical Clustering**\n",
    "- ‚úì Creates dendrogram showing cluster hierarchy\n",
    "- ‚úì Linkage methods: Ward (best), Complete, Average, Single\n",
    "- ‚úì Visual method for choosing k\n",
    "- ‚úì Deterministic results\n",
    "- ‚úì **When to use**: Small datasets, need hierarchy, exploratory analysis\n",
    "\n",
    "#### 5. **PCA (Principal Component Analysis)**\n",
    "- ‚úì Linear dimensionality reduction\n",
    "- ‚úì Preserves maximum variance\n",
    "- ‚úì Components are orthogonal and ordered\n",
    "- ‚úì Enables visualization and compression\n",
    "- ‚úì **When to use**: Preprocessing, visualization, noise reduction, compression\n",
    "\n",
    "#### 6. **t-SNE (t-Distributed Stochastic Neighbor Embedding)**\n",
    "- ‚úì Non-linear dimensionality reduction\n",
    "- ‚úì Excellent for visualization\n",
    "- ‚úì Preserves local structure (clusters)\n",
    "- ‚úì Key parameter: perplexity (5-50)\n",
    "- ‚úì **When to use**: Visualization ONLY, not for ML features\n",
    "\n",
    "#### 7. **Anomaly Detection**\n",
    "- ‚úì Isolation Forest: Fast, general-purpose\n",
    "- ‚úì LOF: Good for varying densities\n",
    "- ‚úì One-Class SVM: High-dimensional data\n",
    "- ‚úì Z-Score: Simple statistical method\n",
    "- ‚úì **Best practice**: Use consensus of multiple methods\n",
    "\n",
    "#### 8. **Customer Segmentation Project**\n",
    "- ‚úì End-to-end workflow: EDA ‚Üí Feature Engineering ‚Üí Clustering ‚Üí Insights\n",
    "- ‚úì Combine multiple techniques (PCA + K-Means + Anomalies)\n",
    "- ‚úì Business-focused analysis and actionable recommendations\n",
    "\n",
    "---\n",
    "\n",
    "### Quick Reference Guide\n",
    "\n",
    "| Task | Best Algorithm | Key Parameters |\n",
    "|------|----------------|----------------|\n",
    "| **General clustering** | K-Means | n_clusters, init='k-means++' |\n",
    "| **Unknown # clusters** | DBSCAN | eps, min_samples |\n",
    "| **See hierarchy** | Hierarchical | n_clusters, linkage='ward' |\n",
    "| **Reduce dimensions** | PCA | n_components (or variance) |\n",
    "| **Visualize high-D** | t-SNE | perplexity=30, n_iter=1000 |\n",
    "| **Find outliers** | Isolation Forest | contamination, n_estimators |\n",
    "\n",
    "---\n",
    "\n",
    "### Common Pitfalls & Best Practices\n",
    "\n",
    "**‚ùå Common Mistakes:**\n",
    "1. Forgetting to standardize features before clustering\n",
    "2. Using t-SNE output as features for ML (it's for visualization only!)\n",
    "3. Not validating clustering with silhouette score\n",
    "4. Ignoring the curse of dimensionality\n",
    "5. Choosing k without elbow/silhouette analysis\n",
    "\n",
    "**‚úì Best Practices:**\n",
    "1. **Always standardize** features before distance-based algorithms\n",
    "2. **Try multiple clustering methods** and compare\n",
    "3. **Validate with metrics** (silhouette, WCSS) and domain knowledge\n",
    "4. **PCA before t-SNE** for high-dimensional data (>50D)\n",
    "5. **Use consensus** for anomaly detection (multiple methods)\n",
    "6. **Profile segments** and create actionable business insights\n",
    "\n",
    "---\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "**Customer Analytics:**\n",
    "- Market segmentation\n",
    "- Churn prediction preprocessing\n",
    "- Customer lifetime value grouping\n",
    "\n",
    "**Healthcare:**\n",
    "- Patient stratification\n",
    "- Disease subtype discovery\n",
    "- Medical image segmentation\n",
    "\n",
    "**Cybersecurity:**\n",
    "- Intrusion detection\n",
    "- Malware classification\n",
    "- Fraud detection\n",
    "\n",
    "**Image Processing:**\n",
    "- Image compression\n",
    "- Object detection preprocessing\n",
    "- Facial recognition\n",
    "\n",
    "**Finance:**\n",
    "- Portfolio optimization\n",
    "- Risk assessment\n",
    "- Trading strategy grouping\n",
    "\n",
    "---\n",
    "\n",
    "### Resources for Further Learning\n",
    "\n",
    "**Documentation:**\n",
    "- [scikit-learn Clustering](https://scikit-learn.org/stable/modules/clustering.html)\n",
    "- [PCA Tutorial](https://scikit-learn.org/stable/modules/decomposition.html#pca)\n",
    "- [t-SNE FAQ](https://distill.pub/2016/misread-tsne/)\n",
    "\n",
    "**Papers:**\n",
    "- **K-Means**: MacQueen (1967) - \"Some Methods for classification and Analysis\"\n",
    "- **DBSCAN**: Ester et al. (1996) - \"A density-based algorithm\"\n",
    "- **t-SNE**: van der Maaten & Hinton (2008) - \"Visualizing Data using t-SNE\"\n",
    "- **Isolation Forest**: Liu et al. (2008) - \"Isolation Forest\"\n",
    "\n",
    "**Practical Tutorials:**\n",
    "- [Customer Segmentation with Python](https://towardsdatascience.com)\n",
    "- [Anomaly Detection in Practice](https://machinelearningmastery.com)\n",
    "- [Clustering for Beginners](https://realpython.com)\n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "**Immediate:**\n",
    "- Complete the exercises above\n",
    "- Try clustering on your own datasets\n",
    "- Experiment with different distance metrics\n",
    "\n",
    "**Next Module:**\n",
    "**Module 16**: `16_neural_networks.ipynb` - Neural Networks from Scratch\n",
    "- Build neural networks from first principles\n",
    "- Understand backpropagation\n",
    "- Introduction to deep learning frameworks\n",
    "\n",
    "**Advanced Topics:**\n",
    "- Gaussian Mixture Models (soft clustering)\n",
    "- Spectral Clustering\n",
    "- Self-Organizing Maps (SOM)\n",
    "- UMAP (alternative to t-SNE)\n",
    "- Autoencoders for dimensionality reduction\n",
    "\n",
    "---\n",
    "\n",
    "### Module Complete! üéâ\n",
    "\n",
    "You've successfully completed Module 15 on Unsupervised Learning!\n",
    "\n",
    "**You can now:**\n",
    "- ‚úì Cluster data into meaningful groups\n",
    "- ‚úì Choose the right clustering algorithm for your problem\n",
    "- ‚úì Reduce high-dimensional data for visualization\n",
    "- ‚úì Detect anomalies in datasets\n",
    "- ‚úì Build end-to-end customer segmentation solutions\n",
    "- ‚úì Create actionable business insights from clustering\n",
    "\n",
    "**Next**: `16_neural_networks.ipynb` - Deep Learning Foundations\n",
    "\n",
    "---\n",
    "\n",
    "**Remember**: Unsupervised learning is about discovering hidden patterns. Always validate results with domain expertise and business context!\n",
    "\n",
    "Keep exploring! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
