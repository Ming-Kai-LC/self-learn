{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 14: Ensemble Methods\n",
    "\n",
    "**Estimated Time**: 90 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this module, you will:\n",
    "- **Understand** the power of ensemble learning\n",
    "- **Master** bagging techniques including Random Forests\n",
    "- **Learn** boosting algorithms and when to use them\n",
    "- **Apply** XGBoost, LightGBM, and CatBoost\n",
    "- **Build** stacking and blending ensembles\n",
    "- **Implement** Kaggle-winning techniques\n",
    "- **Compare** ensemble methods on real datasets\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Modules 00-13 completed\n",
    "- Understanding of decision trees\n",
    "- Familiarity with cross-validation and hyperparameter tuning\n",
    "\n",
    "## What are Ensemble Methods?\n",
    "\n",
    "> **\"In diversity there is strength\"**\n",
    "\n",
    "**Ensemble learning** combines multiple models to create a stronger predictor than any single model alone.\n",
    "\n",
    "### The Wisdom of Crowds\n",
    "\n",
    "Just like how:\n",
    "- **Multiple doctors** provide better diagnoses than one\n",
    "- **Jury decisions** are more reliable than individual judgments  \n",
    "- **Team projects** leverage diverse skills\n",
    "\n",
    "**Ensemble methods** leverage multiple models for better predictions!\n",
    "\n",
    "### Why Ensembles Win\n",
    "\n",
    "- **Kaggle competitions**: 90%+ of winners use ensembles\n",
    "- **Production ML**: Most real-world systems use ensembles\n",
    "- **Robustness**: Less sensitive to outliers and noise\n",
    "- **Performance**: Often 5-15% better than single models\n",
    "\n",
    "### Three Main Approaches\n",
    "\n",
    "1. **Bagging** (Bootstrap Aggregating): Train same algorithm on different data subsets\n",
    "2. **Boosting**: Train models sequentially, each correcting previous errors\n",
    "3. **Stacking**: Combine different algorithms with a meta-model\n",
    "\n",
    "Let's master them all!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Ensemble methods\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    BaggingClassifier,\n",
    "    AdaBoostClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    VotingClassifier,\n",
    "    StackingClassifier,\n",
    ")\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Advanced ensemble libraries\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "\n",
    "    print(\"‚úì XGBoost available\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  XGBoost not installed. Install with: pip install xgboost\")\n",
    "    xgb = None\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "\n",
    "    print(\"‚úì LightGBM available\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  LightGBM not installed. Install with: pip install lightgbm\")\n",
    "    lgb = None\n",
    "\n",
    "try:\n",
    "    from catboost import CatBoostClassifier\n",
    "\n",
    "    print(\"‚úì CatBoost available\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  CatBoost not installed. Install with: pip install catboost\")\n",
    "    CatBoostClassifier = None\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "\n",
    "print(\"\\n‚úì Core ensemble libraries loaded!\")\n",
    "print(\"‚úì Ready to build powerful ensembles!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Ensemble Learning Introduction\n",
    "\n",
    "Ensemble methods combine predictions from multiple models to improve accuracy and robustness.\n",
    "\n",
    "### The Core Idea\n",
    "\n",
    "**Single Model:**\n",
    "```\n",
    "Data ‚Üí Model ‚Üí Prediction\n",
    "```\n",
    "\n",
    "**Ensemble:**\n",
    "```\n",
    "Data ‚Üí Model 1 ‚îê\n",
    "Data ‚Üí Model 2 ‚îú‚Üí Combine ‚Üí Better Prediction\n",
    "Data ‚Üí Model 3 ‚îò\n",
    "```\n",
    "\n",
    "### Why Ensembles Work\n",
    "\n",
    "1. **Reduce Variance** (Bagging)\n",
    "   - Train on different data subsets\n",
    "   - Average out individual model errors\n",
    "   - Example: Random Forest\n",
    "\n",
    "2. **Reduce Bias** (Boosting)\n",
    "   - Focus on hard examples\n",
    "   - Sequentially improve\n",
    "   - Example: XGBoost, AdaBoost\n",
    "\n",
    "3. **Improve Predictions** (Stacking)\n",
    "   - Combine different model types\n",
    "   - Learn optimal combination\n",
    "   - Example: Stacking classifier\n",
    "\n",
    "### Ensemble Taxonomy\n",
    "\n",
    "| Method | Type | How It Works | Best For |\n",
    "|--------|------|--------------|----------|\n",
    "| **Random Forest** | Bagging | Multiple decision trees, different data | General purpose |\n",
    "| **AdaBoost** | Boosting | Sequential weak learners, focus on errors | Binary classification |\n",
    "| **Gradient Boosting** | Boosting | Sequential trees, gradient descent | Structured data |\n",
    "| **XGBoost** | Boosting | Optimized gradient boosting | Kaggle, production |\n",
    "| **LightGBM** | Boosting | Fast gradient boosting | Large datasets |\n",
    "| **CatBoost** | Boosting | Handles categorical features | Categorical data |\n",
    "| **Voting** | Meta-ensemble | Majority vote or average | Combining models |\n",
    "| **Stacking** | Meta-ensemble | Meta-model learns combination | Maximum performance |\n",
    "\n",
    "### When to Use Ensembles\n",
    "\n",
    "‚úì **Use when:**\n",
    "- You need maximum accuracy\n",
    "- Robustness is important\n",
    "- You have computational resources\n",
    "- Competition or production setting\n",
    "\n",
    "‚úó **Don't use when:**\n",
    "- Interpretability is critical\n",
    "- Very limited compute\n",
    "- Real-time predictions needed (milliseconds)\n",
    "- Model size matters (mobile deployment)\n",
    "\n",
    "Let's load data and start building ensembles!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Prepare Data\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PREPARING DATA FOR ENSEMBLE LEARNING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load feature engineering dataset\n",
    "df = pd.read_csv(\"../../data_advanced/feature_engineering.csv\")\n",
    "\n",
    "# Prepare features\n",
    "le_city = LabelEncoder()\n",
    "le_job = LabelEncoder()\n",
    "df[\"city_encoded\"] = le_city.fit_transform(df[\"city\"])\n",
    "df[\"job_encoded\"] = le_job.fit_transform(df[\"job_category\"])\n",
    "\n",
    "# Select features\n",
    "features = [\n",
    "    \"age\",\n",
    "    \"income\",\n",
    "    \"education_years\",\n",
    "    \"experience_years\",\n",
    "    \"num_dependents\",\n",
    "    \"city_encoded\",\n",
    "    \"job_encoded\",\n",
    "]\n",
    "\n",
    "X = df[features]\n",
    "y = df[\"loan_approved\"]\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset: {X.shape[0]} samples, {X.shape[1]} features\")\n",
    "print(f\"Train set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(y.value_counts())\n",
    "print(f\"Approval rate: {y.mean():.2%}\")\n",
    "\n",
    "# Baseline: Single Decision Tree\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BASELINE: Single Decision Tree\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "baseline_tree = DecisionTreeClassifier(random_state=42)\n",
    "baseline_tree.fit(X_train, y_train)\n",
    "\n",
    "train_score = baseline_tree.score(X_train, y_train)\n",
    "test_score = baseline_tree.score(X_test, y_test)\n",
    "\n",
    "print(f\"\\nBaseline Decision Tree:\")\n",
    "print(f\"  Train Accuracy: {train_score:.4f}\")\n",
    "print(f\"  Test Accuracy:  {test_score:.4f}\")\n",
    "print(f\"  Overfitting:    {train_score - test_score:.4f}\")\n",
    "\n",
    "if train_score - test_score > 0.1:\n",
    "    print(\"\\n‚ö†Ô∏è  High overfitting! Decision trees overfit easily.\")\n",
    "    print(\"üí° Ensembles will help reduce this!\")\n",
    "\n",
    "print(\"\\n‚úì Data prepared and baseline established!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bagging and Random Forests\n",
    "\n",
    "**Bagging** = **B**ootstrap **Agg**regat**ing**\n",
    "\n",
    "### How Bagging Works\n",
    "\n",
    "1. **Bootstrap**: Create N random samples (with replacement) from training data\n",
    "2. **Train**: Train a model on each sample\n",
    "3. **Aggregate**: Combine predictions (vote for classification, average for regression)\n",
    "\n",
    "### Why It Reduces Variance\n",
    "\n",
    "- Each model sees slightly different data\n",
    "- Individual models make different errors\n",
    "- Errors cancel out when averaged\n",
    "- Final prediction is more stable\n",
    "\n",
    "### Random Forest = Bagging + Random Feature Selection\n",
    "\n",
    "**Standard Bagging:**\n",
    "- Bootstrap samples\n",
    "- All features considered\n",
    "\n",
    "**Random Forest:**\n",
    "- Bootstrap samples\n",
    "- Random subset of features at each split\n",
    "- Even more diversity!\n",
    "\n",
    "### Key Hyperparameters\n",
    "\n",
    "| Parameter | Description | Typical Values |\n",
    "|-----------|-------------|----------------|\n",
    "| `n_estimators` | Number of trees | 100-500 |\n",
    "| `max_depth` | Tree depth | 10-30 or None |\n",
    "| `min_samples_split` | Min samples to split | 2-10 |\n",
    "| `max_features` | Features per split | 'sqrt', 'log2' |\n",
    "| `max_samples` | Bootstrap sample size | 0.5-1.0 |\n",
    "\n",
    "### Advantages\n",
    "\n",
    "‚úì Reduces overfitting  \n",
    "‚úì Handles high-dimensional data  \n",
    "‚úì Provides feature importance  \n",
    "‚úì Robust to outliers  \n",
    "‚úì Parallelizable  \n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "‚úó Can be slow  \n",
    "‚úó Memory intensive  \n",
    "‚úó Less interpretable  \n",
    "\n",
    "Let's compare Bagging vs Random Forest!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging and Random Forests Implementation\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BAGGING VS RANDOM FOREST\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Standard Bagging\n",
    "print(\"\\n1. Bagging Classifier\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "bagging = BaggingClassifier(\n",
    "    estimator=DecisionTreeClassifier(),\n",
    "    n_estimators=100,\n",
    "    max_samples=0.8,\n",
    "    max_features=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "bagging.fit(X_train, y_train)\n",
    "bag_train = bagging.score(X_train, y_train)\n",
    "bag_test = bagging.score(X_test, y_test)\n",
    "\n",
    "print(f\"Train Accuracy: {bag_train:.4f}\")\n",
    "print(f\"Test Accuracy:  {bag_test:.4f}\")\n",
    "print(f\"Overfitting:    {bag_train - bag_test:.4f}\")\n",
    "\n",
    "# 2. Random Forest\n",
    "print(\"\\n2. Random Forest Classifier\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=5,\n",
    "    max_features=\"sqrt\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "rf_train = rf.score(X_train, y_train)\n",
    "rf_test = rf.score(X_test, y_test)\n",
    "\n",
    "print(f\"Train Accuracy: {rf_train:.4f}\")\n",
    "print(f\"Test Accuracy:  {rf_test:.4f}\")\n",
    "print(f\"Overfitting:    {rf_train - rf_test:.4f}\")\n",
    "\n",
    "# 3. Comparison\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COMPARISON SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "comparison = pd.DataFrame(\n",
    "    {\n",
    "        \"Model\": [\"Single Tree\", \"Bagging\", \"Random Forest\"],\n",
    "        \"Train Acc\": [train_score, bag_train, rf_train],\n",
    "        \"Test Acc\": [test_score, bag_test, rf_test],\n",
    "        \"Overfitting\": [train_score - test_score, bag_train - bag_test, rf_train - rf_test],\n",
    "    }\n",
    ")\n",
    "\n",
    "display(comparison)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Accuracy comparison\n",
    "x = np.arange(len(comparison))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x - width / 2, comparison[\"Train Acc\"], width, label=\"Train\", alpha=0.8)\n",
    "axes[0].bar(x + width / 2, comparison[\"Test Acc\"], width, label=\"Test\", alpha=0.8)\n",
    "axes[0].set_ylabel(\"Accuracy\")\n",
    "axes[0].set_title(\"Bagging Methods Comparison\", fontweight=\"bold\")\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(comparison[\"Model\"])\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# Overfitting comparison\n",
    "colors = [\"red\" if x > 0.05 else \"green\" for x in comparison[\"Overfitting\"]]\n",
    "axes[1].barh(comparison[\"Model\"], comparison[\"Overfitting\"], color=colors, alpha=0.7)\n",
    "axes[1].axvline(x=0.05, color=\"orange\", linestyle=\"--\", label=\"5% threshold\")\n",
    "axes[1].set_xlabel(\"Overfitting (Train - Test)\")\n",
    "axes[1].set_title(\"Overfitting Reduction\", fontweight=\"bold\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(axis=\"x\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature importance from Random Forest\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FEATURE IMPORTANCE (Random Forest)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "importance_df = pd.DataFrame(\n",
    "    {\"Feature\": features, \"Importance\": rf.feature_importances_}\n",
    ").sort_values(\"Importance\", ascending=False)\n",
    "\n",
    "print(\"\\n\" + importance_df.to_string(index=False))\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df[\"Feature\"], importance_df[\"Importance\"], color=\"steelblue\", alpha=0.8)\n",
    "plt.xlabel(\"Importance\", fontweight=\"bold\")\n",
    "plt.title(\"Random Forest Feature Importance\", fontweight=\"bold\", fontsize=14)\n",
    "plt.grid(axis=\"x\", alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Insights:\")\n",
    "print(\"   ‚Ä¢ Bagging reduces overfitting significantly\")\n",
    "print(\"   ‚Ä¢ Random Forest adds feature randomization\")\n",
    "print(\"   ‚Ä¢ Both outperform single decision tree\")\n",
    "print(\"   ‚Ä¢ Feature importance helps interpretability\")\n",
    "print(\"\\n‚úì Bagging and Random Forests demonstrated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Boosting Fundamentals\n",
    "\n",
    "**Boosting** trains models sequentially, each one correcting errors of the previous models.\n",
    "\n",
    "### Key Difference: Bagging vs Boosting\n",
    "\n",
    "**Bagging:**\n",
    "```\n",
    "Train in parallel ‚Üí Independent models ‚Üí Average predictions\n",
    "```\n",
    "\n",
    "**Boosting:**\n",
    "```\n",
    "Train sequentially ‚Üí Each model corrects previous errors ‚Üí Weighted combination\n",
    "```\n",
    "\n",
    "### How Boosting Works\n",
    "\n",
    "1. **Start**: Train initial model on data\n",
    "2. **Identify errors**: Find samples where model performs poorly\n",
    "3. **Weight**: Give more importance to misclassified samples\n",
    "4. **Train next model**: Focus on hard examples\n",
    "5. **Combine**: Weight models by performance\n",
    "6. **Repeat**: Until reaching desired number of models\n",
    "\n",
    "### Popular Boosting Algorithms\n",
    "\n",
    "| Algorithm | Year | Key Features |\n",
    "|-----------|------|--------------|\n",
    "| **AdaBoost** | 1996 | Adaptive boosting, weights samples |\n",
    "| **Gradient Boosting** | 1999 | Uses gradient descent, builds on residuals |\n",
    "| **XGBoost** | 2014 | Extreme gradient boosting, regularization |\n",
    "| **LightGBM** | 2017 | Fast, memory efficient, leaf-wise growth |\n",
    "| **CatBoost** | 2017 | Categorical features, ordered boosting |\n",
    "\n",
    "### AdaBoost Algorithm\n",
    "\n",
    "**Step-by-step:**\n",
    "1. Initialize sample weights: w = 1/N for all samples\n",
    "2. For each iteration:\n",
    "   - Train weak learner on weighted data\n",
    "   - Calculate error rate\n",
    "   - Calculate model weight (higher for better models)\n",
    "   - Update sample weights (increase for misclassified)\n",
    "3. Final prediction: Weighted vote\n",
    "\n",
    "### Gradient Boosting Algorithm\n",
    "\n",
    "**Key idea**: Each tree predicts the *residuals* (errors) of previous trees\n",
    "\n",
    "```\n",
    "F‚ÇÄ(x) = initial prediction\n",
    "F‚ÇÅ(x) = F‚ÇÄ(x) + Œ±‚ÇÅ ¬∑ h‚ÇÅ(x)  # Add tree predicting residuals\n",
    "F‚ÇÇ(x) = F‚ÇÅ(x) + Œ±‚ÇÇ ¬∑ h‚ÇÇ(x)  # Add tree predicting remaining residuals\n",
    "...\n",
    "F‚Çò(x) = Final prediction\n",
    "```\n",
    "\n",
    "### Why Boosting Works\n",
    "\n",
    "‚úì **Reduces bias**: Focuses on hard examples\n",
    "‚úì **Powerful**: Often best performance on tabular data\n",
    "‚úì **Flexible**: Many tuning options\n",
    "\n",
    "### Challenges\n",
    "\n",
    "‚úó **Overfitting risk**: Can memorize training data\n",
    "‚úó **Sensitive to noise**: Outliers get high weights\n",
    "‚úó **Sequential**: Cannot parallelize easily\n",
    "‚úó **Requires tuning**: Many hyperparameters\n",
    "\n",
    "Let's see AdaBoost and Gradient Boosting in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosting Algorithms Implementation\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BOOSTING ALGORITHMS COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. AdaBoost\n",
    "print(\"\\n1. AdaBoost Classifier\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "adaboost = AdaBoostClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=1),  # Weak learners (stumps)\n",
    "    n_estimators=100,\n",
    "    learning_rate=1.0,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "adaboost.fit(X_train, y_train)\n",
    "ada_train = adaboost.score(X_train, y_train)\n",
    "ada_test = adaboost.score(X_test, y_test)\n",
    "\n",
    "print(f\"Train Accuracy: {ada_train:.4f}\")\n",
    "print(f\"Test Accuracy:  {ada_test:.4f}\")\n",
    "print(f\"Overfitting:    {ada_train - ada_test:.4f}\")\n",
    "\n",
    "# 2. Gradient Boosting\n",
    "print(\"\\n2. Gradient Boosting Classifier\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "gb = GradientBoostingClassifier(\n",
    "    n_estimators=100, learning_rate=0.1, max_depth=3, min_samples_split=5, random_state=42\n",
    ")\n",
    "\n",
    "gb.fit(X_train, y_train)\n",
    "gb_train = gb.score(X_train, y_train)\n",
    "gb_test = gb.score(X_test, y_test)\n",
    "\n",
    "print(f\"Train Accuracy: {gb_train:.4f}\")\n",
    "print(f\"Test Accuracy:  {gb_test:.4f}\")\n",
    "print(f\"Overfitting:    {gb_train - gb_test:.4f}\")\n",
    "\n",
    "# 3. Comparison: Bagging vs Boosting\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BAGGING VS BOOSTING COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "all_models = pd.DataFrame(\n",
    "    {\n",
    "        \"Model\": [\"Single Tree\", \"Bagging\", \"Random Forest\", \"AdaBoost\", \"Gradient Boosting\"],\n",
    "        \"Type\": [\"Baseline\", \"Bagging\", \"Bagging\", \"Boosting\", \"Boosting\"],\n",
    "        \"Train Acc\": [train_score, bag_train, rf_train, ada_train, gb_train],\n",
    "        \"Test Acc\": [test_score, bag_test, rf_test, ada_test, gb_test],\n",
    "        \"Overfitting\": [\n",
    "            train_score - test_score,\n",
    "            bag_train - bag_test,\n",
    "            rf_train - rf_test,\n",
    "            ada_train - ada_test,\n",
    "            gb_train - gb_test,\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "display(all_models)\n",
    "\n",
    "# Visualize comprehensive comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Accuracy comparison\n",
    "x = np.arange(len(all_models))\n",
    "width = 0.35\n",
    "\n",
    "axes[0, 0].bar(\n",
    "    x - width / 2, all_models[\"Train Acc\"], width, label=\"Train\", alpha=0.8, color=\"skyblue\"\n",
    ")\n",
    "axes[0, 0].bar(x + width / 2, all_models[\"Test Acc\"], width, label=\"Test\", alpha=0.8, color=\"coral\")\n",
    "axes[0, 0].set_ylabel(\"Accuracy\", fontweight=\"bold\")\n",
    "axes[0, 0].set_title(\"All Ensemble Methods - Accuracy\", fontweight=\"bold\")\n",
    "axes[0, 0].set_xticks(x)\n",
    "axes[0, 0].set_xticklabels(all_models[\"Model\"], rotation=45, ha=\"right\")\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# 2. Test accuracy ranking\n",
    "sorted_models = all_models.sort_values(\"Test Acc\")\n",
    "colors_gradient = plt.cm.RdYlGn(sorted_models[\"Test Acc\"])\n",
    "axes[0, 1].barh(sorted_models[\"Model\"], sorted_models[\"Test Acc\"], color=colors_gradient, alpha=0.8)\n",
    "axes[0, 1].set_xlabel(\"Test Accuracy\", fontweight=\"bold\")\n",
    "axes[0, 1].set_title(\"Ranking by Test Performance\", fontweight=\"bold\")\n",
    "axes[0, 1].grid(axis=\"x\", alpha=0.3)\n",
    "\n",
    "for i, (model, acc) in enumerate(zip(sorted_models[\"Model\"], sorted_models[\"Test Acc\"])):\n",
    "    axes[0, 1].text(acc, i, f\" {acc:.4f}\", va=\"center\")\n",
    "\n",
    "# 3. Overfitting analysis\n",
    "colors = [\"red\" if x > 0.05 else \"green\" for x in all_models[\"Overfitting\"]]\n",
    "axes[1, 0].barh(all_models[\"Model\"], all_models[\"Overfitting\"], color=colors, alpha=0.7)\n",
    "axes[1, 0].axvline(x=0.05, color=\"orange\", linestyle=\"--\", linewidth=2, label=\"5% threshold\")\n",
    "axes[1, 0].set_xlabel(\"Overfitting (Train - Test)\", fontweight=\"bold\")\n",
    "axes[1, 0].set_title(\"Overfitting Analysis\", fontweight=\"bold\")\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(axis=\"x\", alpha=0.3)\n",
    "\n",
    "# 4. Bagging vs Boosting\n",
    "bagging_models = all_models[all_models[\"Type\"].isin([\"Bagging\"])]\n",
    "boosting_models = all_models[all_models[\"Type\"] == \"Boosting\"]\n",
    "\n",
    "methods = [\"Bagging\\n(Avg)\", \"Boosting\\n(Avg)\"]\n",
    "avg_acc = [bagging_models[\"Test Acc\"].mean(), boosting_models[\"Test Acc\"].mean()]\n",
    "colors_bar = [\"#3498db\", \"#e74c3c\"]\n",
    "\n",
    "bars = axes[1, 1].bar(methods, avg_acc, color=colors_bar, alpha=0.8, edgecolor=\"black\", linewidth=2)\n",
    "axes[1, 1].set_ylabel(\"Average Test Accuracy\", fontweight=\"bold\")\n",
    "axes[1, 1].set_title(\"Bagging vs Boosting Performance\", fontweight=\"bold\")\n",
    "axes[1, 1].set_ylim(0, 1)\n",
    "axes[1, 1].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "for bar, acc in zip(bars, avg_acc):\n",
    "    height = bar.get_height()\n",
    "    axes[1, 1].text(\n",
    "        bar.get_x() + bar.get_width() / 2.0,\n",
    "        height,\n",
    "        f\"{acc:.4f}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=12,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Learning curves comparison\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BOOSTING FEATURE IMPORTANCE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Gradient Boosting feature importance\n",
    "gb_importance = pd.DataFrame(\n",
    "    {\"Feature\": features, \"Importance\": gb.feature_importances_}\n",
    ").sort_values(\"Importance\", ascending=False)\n",
    "\n",
    "print(\"\\nGradient Boosting Feature Importance:\")\n",
    "print(gb_importance.to_string(index=False))\n",
    "\n",
    "print(\"\\nüí° Key Insights:\")\n",
    "print(\"   ‚Ä¢ Boosting often achieves higher test accuracy\")\n",
    "print(\"   ‚Ä¢ AdaBoost uses weak learners (shallow trees)\")\n",
    "print(\"   ‚Ä¢ Gradient Boosting more flexible, often performs better\")\n",
    "print(\"   ‚Ä¢ Boosting can overfit if not tuned properly\")\n",
    "print(\"\\n‚úì Boosting fundamentals demonstrated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. XGBoost Deep Dive\n",
    "\n",
    "**XGBoost** (eXtreme Gradient Boosting) is the most popular ML algorithm for structured data.\n",
    "\n",
    "### Why XGBoost Dominates\n",
    "\n",
    "üèÜ **Kaggle**: Used in 90%+ of winning solutions  \n",
    "üöÄ **Speed**: 10x faster than sklearn's Gradient Boosting  \n",
    "üéØ **Performance**: State-of-the-art accuracy  \n",
    "üîß **Features**: Built-in regularization, handling missing values, parallel processing  \n",
    "\n",
    "### Key Innovations\n",
    "\n",
    "1. **Regularization**: L1 (Lasso) and L2 (Ridge) penalties prevent overfitting\n",
    "2. **Tree Pruning**: Max depth first, then prune backward\n",
    "3. **Handling Missing Values**: Learns best direction for missing data\n",
    "4. **Parallel Processing**: Tree construction parallelized\n",
    "5. **Cache Optimization**: Efficient memory usage\n",
    "\n",
    "### XGBoost vs Gradient Boosting\n",
    "\n",
    "| Feature | sklearn GBM | XGBoost |\n",
    "|---------|-------------|---------|\n",
    "| Speed | Slow | 10x faster |\n",
    "| Regularization | No | Yes (L1 + L2) |\n",
    "| Missing values | Requires imputation | Handles automatically |\n",
    "| Parallel | No | Yes |\n",
    "| Early stopping | No | Yes |\n",
    "| Custom objectives | No | Yes |\n",
    "\n",
    "### Important Hyperparameters\n",
    "\n",
    "**Tree Parameters:**\n",
    "- `max_depth`: Max tree depth (3-10)\n",
    "- `min_child_weight`: Minimum samples in leaf (1-10)\n",
    "- `gamma`: Minimum loss reduction (0-5)\n",
    "\n",
    "**Boosting Parameters:**\n",
    "- `n_estimators`: Number of trees (100-1000)\n",
    "- `learning_rate`: Shrinkage (0.01-0.3)\n",
    "- `subsample`: Row sampling ratio (0.5-1.0)\n",
    "- `colsample_bytree`: Column sampling (0.5-1.0)\n",
    "\n",
    "**Regularization:**\n",
    "- `reg_alpha`: L1 penalty (0-1)\n",
    "- `reg_lambda`: L2 penalty (0-1)\n",
    "\n",
    "### Tuning Strategy\n",
    "\n",
    "1. **Start**: Fix learning_rate=0.1, tune tree params\n",
    "2. **Then**: Tune n_estimators with early stopping\n",
    "3. **Finally**: Lower learning_rate, increase n_estimators\n",
    "\n",
    "Let's see XGBoost in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Implementation\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"XGBOOST - THE KAGGLE CHAMPION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if xgb is not None:\n",
    "    # XGBoost with default parameters\n",
    "    print(\"\\n1. XGBoost with Default Parameters\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    xgb_default = xgb.XGBClassifier(random_state=42, eval_metric=\"logloss\", use_label_encoder=False)\n",
    "\n",
    "    xgb_default.fit(X_train, y_train)\n",
    "    xgb_default_train = xgb_default.score(X_train, y_train)\n",
    "    xgb_default_test = xgb_default.score(X_test, y_test)\n",
    "\n",
    "    print(f\"Train Accuracy: {xgb_default_train:.4f}\")\n",
    "    print(f\"Test Accuracy:  {xgb_default_test:.4f}\")\n",
    "\n",
    "    # XGBoost with tuned parameters\n",
    "    print(\"\\n2. XGBoost with Tuned Parameters\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    xgb_tuned = xgb.XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.1,  # L1 regularization\n",
    "        reg_lambda=1.0,  # L2 regularization\n",
    "        random_state=42,\n",
    "        eval_metric=\"logloss\",\n",
    "        use_label_encoder=False,\n",
    "    )\n",
    "\n",
    "    # Train with early stopping\n",
    "    xgb_tuned.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)\n",
    "\n",
    "    xgb_tuned_train = xgb_tuned.score(X_train, y_train)\n",
    "    xgb_tuned_test = xgb_tuned.score(X_test, y_test)\n",
    "\n",
    "    print(f\"Train Accuracy: {xgb_tuned_train:.4f}\")\n",
    "    print(f\"Test Accuracy:  {xgb_tuned_test:.4f}\")\n",
    "    print(\n",
    "        f\"Best iteration: {xgb_tuned.best_iteration if hasattr(xgb_tuned, 'best_iteration') else 'N/A'}\"\n",
    "    )\n",
    "\n",
    "    # Comparison with other methods\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"COMPREHENSIVE ENSEMBLE COMPARISON\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    final_comparison = pd.DataFrame(\n",
    "        {\n",
    "            \"Model\": [\n",
    "                \"Single Tree\",\n",
    "                \"Random Forest\",\n",
    "                \"Gradient Boosting\",\n",
    "                \"XGBoost Default\",\n",
    "                \"XGBoost Tuned\",\n",
    "            ],\n",
    "            \"Train Acc\": [train_score, rf_train, gb_train, xgb_default_train, xgb_tuned_train],\n",
    "            \"Test Acc\": [test_score, rf_test, gb_test, xgb_default_test, xgb_tuned_test],\n",
    "            \"Overfitting\": [\n",
    "                train_score - test_score,\n",
    "                rf_train - rf_test,\n",
    "                gb_train - gb_test,\n",
    "                xgb_default_train - xgb_default_test,\n",
    "                xgb_tuned_train - xgb_tuned_test,\n",
    "            ],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    display(final_comparison)\n",
    "\n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    # Test accuracy comparison\n",
    "    sorted_final = final_comparison.sort_values(\"Test Acc\")\n",
    "    colors = plt.cm.RdYlGn(sorted_final[\"Test Acc\"])\n",
    "\n",
    "    axes[0].barh(\n",
    "        sorted_final[\"Model\"], sorted_final[\"Test Acc\"], color=colors, alpha=0.8, edgecolor=\"black\"\n",
    "    )\n",
    "    axes[0].set_xlabel(\"Test Accuracy\", fontweight=\"bold\")\n",
    "    axes[0].set_title(\"Final Model Comparison\", fontweight=\"bold\", fontsize=14)\n",
    "    axes[0].grid(axis=\"x\", alpha=0.3)\n",
    "\n",
    "    for i, (model, acc) in enumerate(zip(sorted_final[\"Model\"], sorted_final[\"Test Acc\"])):\n",
    "        axes[0].text(acc, i, f\" {acc:.4f}\", va=\"center\", fontweight=\"bold\")\n",
    "\n",
    "    # Feature importance from XGBoost\n",
    "    xgb_importance = pd.DataFrame(\n",
    "        {\"Feature\": features, \"Importance\": xgb_tuned.feature_importances_}\n",
    "    ).sort_values(\"Importance\", ascending=True)\n",
    "\n",
    "    axes[1].barh(\n",
    "        xgb_importance[\"Feature\"],\n",
    "        xgb_importance[\"Importance\"],\n",
    "        color=\"steelblue\",\n",
    "        alpha=0.8,\n",
    "        edgecolor=\"black\",\n",
    "    )\n",
    "    axes[1].set_xlabel(\"Importance\", fontweight=\"bold\")\n",
    "    axes[1].set_title(\"XGBoost Feature Importance\", fontweight=\"bold\", fontsize=14)\n",
    "    axes[1].grid(axis=\"x\", alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Training history (if available)\n",
    "    if hasattr(xgb_tuned, \"evals_result\"):\n",
    "        results = xgb_tuned.evals_result()\n",
    "        if \"validation_0\" in results and \"logloss\" in results[\"validation_0\"]:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(results[\"validation_0\"][\"logloss\"], label=\"Test Loss\", linewidth=2)\n",
    "            plt.xlabel(\"Iteration\", fontweight=\"bold\")\n",
    "            plt.ylabel(\"Log Loss\", fontweight=\"bold\")\n",
    "            plt.title(\"XGBoost Training History\", fontweight=\"bold\", fontsize=14)\n",
    "            plt.legend()\n",
    "            plt.grid(alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    print(\"\\nüí° Key Insights:\")\n",
    "    print(\"   ‚Ä¢ XGBoost typically achieves best performance\")\n",
    "    print(\"   ‚Ä¢ Regularization prevents overfitting\")\n",
    "    print(\"   ‚Ä¢ Early stopping saves computation\")\n",
    "    print(\"   ‚Ä¢ Feature importance aids interpretation\")\n",
    "\n",
    "    print(\"\\n‚úì XGBoost demonstrated!\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  XGBoost not installed\")\n",
    "    print(\"Install with: pip install xgboost\")\n",
    "    print(\"\\nXGBoost is the gold standard for tabular data!\")\n",
    "    print(\"Used in 90%+ of Kaggle winning solutions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. LightGBM for Large Datasets\n",
    "\n",
    "**LightGBM** = Light Gradient Boosting Machine (Microsoft)\n",
    "\n",
    "### Key Advantages\n",
    "\n",
    "üöÄ **Speed**: 10x faster than XGBoost on large data  \n",
    "üíæ **Memory**: Uses less memory  \n",
    "üìä **Performance**: Often matches or beats XGBoost  \n",
    "\n",
    "### Unique Features\n",
    "\n",
    "1. **Leaf-wise Growth**: Grows trees by best leaf (not level-wise)\n",
    "2. **Histogram-based**: Bins continuous features\n",
    "3. **GOSS**: Gradient-based One-Side Sampling\n",
    "4. **EFB**: Exclusive Feature Bundling\n",
    "\n",
    "### When to Use LightGBM\n",
    "\n",
    "‚úì Large datasets (>10K rows)  \n",
    "‚úì High-dimensional data (>100 features)  \n",
    "‚úì Speed is critical  \n",
    "‚úì Limited memory  \n",
    "\n",
    "### XGBoost vs LightGBM\n",
    "\n",
    "| Aspect | XGBoost | LightGBM |\n",
    "|--------|---------|----------|\n",
    "| Tree growth | Level-wise | Leaf-wise |\n",
    "| Speed | Fast | Faster |\n",
    "| Memory | Moderate | Lower |\n",
    "| Small data | Better | Can overfit |\n",
    "| Large data | Good | Better |\n",
    "\n",
    "**Pro tip**: For datasets <10K rows, use XGBoost. For larger, try LightGBM!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM Implementation (if available)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LIGHTGBM - FAST AND EFFICIENT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if lgb is not None:\n",
    "    lgb_model = lgb.LGBMClassifier(\n",
    "        n_estimators=200, max_depth=5, learning_rate=0.1, num_leaves=31, random_state=42, verbose=-1\n",
    "    )\n",
    "\n",
    "    lgb_model.fit(X_train, y_train)\n",
    "    lgb_test = lgb_model.score(X_test, y_test)\n",
    "\n",
    "    print(f\"LightGBM Test Accuracy: {lgb_test:.4f}\")\n",
    "    print(\"‚úì LightGBM trained successfully!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  LightGBM not installed\")\n",
    "    print(\"Install with: pip install lightgbm\")\n",
    "    lgb_test = None\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ADVANCED BOOSTING LIBRARIES SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "summary_data = []\n",
    "if xgb is not None:\n",
    "    summary_data.append(\n",
    "        {\n",
    "            \"Library\": \"XGBoost\",\n",
    "            \"Test Accuracy\": xgb_tuned_test,\n",
    "            \"Best For\": \"General purpose, Kaggle\",\n",
    "        }\n",
    "    )\n",
    "if lgb is not None:\n",
    "    summary_data.append(\n",
    "        {\"Library\": \"LightGBM\", \"Test Accuracy\": lgb_test, \"Best For\": \"Large datasets, speed\"}\n",
    "    )\n",
    "\n",
    "if summary_data:\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    display(summary_df)\n",
    "\n",
    "    print(\"\\nüí° Recommendations:\")\n",
    "    print(\"   ‚Ä¢ XGBoost: Best all-around choice\")\n",
    "    print(\"   ‚Ä¢ LightGBM: Use for large datasets (>100K rows)\")\n",
    "    print(\"   ‚Ä¢ CatBoost: Best for categorical features\")\n",
    "else:\n",
    "    print(\"\\nInstall advanced libraries:\")\n",
    "    print(\"  pip install xgboost lightgbm catboost\")\n",
    "\n",
    "print(\"\\n‚úì Advanced gradient boosting libraries covered!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. CatBoost for Categorical Data\n",
    "\n",
    "**CatBoost** = Categorical Boosting (Yandex)\n",
    "\n",
    "### Key Advantage: Built-in Categorical Handling\n",
    "\n",
    "**Most libraries**: Require manual encoding (one-hot, label)  \n",
    "**CatBoost**: Handles categorical features automatically!\n",
    "\n",
    "### Unique Features\n",
    "\n",
    "1. **Ordered Boosting**: Prevents target leakage\n",
    "2. **Categorical Features**: No preprocessing needed\n",
    "3. **Symmetric Trees**: Faster prediction\n",
    "4. **Robust**: Handles missing values, outliers\n",
    "\n",
    "### When to Use CatBoost\n",
    "\n",
    "‚úì Many categorical features  \n",
    "‚úì Don't want to engineer encodings  \n",
    "‚úì Need robust model  \n",
    "‚úì Production deployment  \n",
    "\n",
    "### Quick Comparison\n",
    "\n",
    "```python\n",
    "# Other libraries\n",
    "X['category'] = LabelEncoder().fit_transform(X['category'])\n",
    "\n",
    "# CatBoost\n",
    "model.fit(X, y, cat_features=['category'])  # That's it!\n",
    "```\n",
    "\n",
    "**Note**: CatBoost handles categories automatically, making it great for datasets with many categorical columns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost Implementation (if available)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CATBOOST - CATEGORICAL FEATURES EXPERT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if CatBoostClassifier is not None:\n",
    "    cat_model = CatBoostClassifier(\n",
    "        iterations=200, depth=5, learning_rate=0.1, random_seed=42, verbose=False\n",
    "    )\n",
    "\n",
    "    cat_model.fit(X_train, y_train)\n",
    "    cat_test = cat_model.score(X_test, y_test)\n",
    "\n",
    "    print(f\"CatBoost Test Accuracy: {cat_test:.4f}\")\n",
    "    print(\"‚úì CatBoost trained successfully!\")\n",
    "    print(\"\\nüí° CatBoost excels with categorical features\")\n",
    "    print(\"   Use cat_features parameter for categorical columns\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  CatBoost not installed\")\n",
    "    print(\"Install with: pip install catboost\")\n",
    "\n",
    "print(\"\\n‚úì CatBoost covered!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Stacking and Blending\n",
    "\n",
    "**Stacking** combines different types of models using a meta-learner.\n",
    "\n",
    "### How Stacking Works\n",
    "\n",
    "```\n",
    "Base Models:\n",
    "  Model 1 (Random Forest) ‚Üí Predictions 1\n",
    "  Model 2 (XGBoost)       ‚Üí Predictions 2  \n",
    "  Model 3 (Logistic Reg)  ‚Üí Predictions 3\n",
    "                              ‚Üì\n",
    "                    Meta-Model (combines all)\n",
    "                              ‚Üì\n",
    "                      Final Prediction\n",
    "```\n",
    "\n",
    "### Stacking vs Blending\n",
    "\n",
    "| Aspect | Stacking | Blending |\n",
    "|--------|----------|----------|\n",
    "| Method | Cross-validation | Holdout set |\n",
    "| Data usage | All training data | Split data |\n",
    "| Complexity | Higher | Simpler |\n",
    "| Performance | Better | Good |\n",
    "| Overfitting risk | Lower | Higher |\n",
    "\n",
    "### Why Stacking Works\n",
    "\n",
    "‚úì **Diversity**: Different models make different errors  \n",
    "‚úì **Strength**: Combines best of each model  \n",
    "‚úì **Flexibility**: Can use any meta-learner  \n",
    "‚úì **Performance**: Often best single-model alternative  \n",
    "\n",
    "### Common Stacking Strategies\n",
    "\n",
    "1. **Voting**: Simple averaging or majority vote\n",
    "2. **Weighted Voting**: Weight models by performance\n",
    "3. **Meta-Model**: Train model on predictions\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. Use diverse base models (different algorithms)\n",
    "2. Use cross-validation to generate meta-features\n",
    "3. Simple meta-model (Logistic Regression, Ridge)\n",
    "4. Don't overfit meta-model\n",
    "5. Keep validation set separate\n",
    "\n",
    "Let's build a stacking ensemble!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking and Blending Implementation\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STACKING ENSEMBLE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Voting Classifier (Simple Ensemble)\n",
    "print(\"\\n1. Voting Classifier (Simple Average)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        (\"rf\", RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "        (\"gb\", GradientBoostingClassifier(n_estimators=100, random_state=42)),\n",
    "        (\"lr\", LogisticRegression(random_state=42, max_iter=1000)),\n",
    "    ],\n",
    "    voting=\"soft\",  # Use probability averaging\n",
    ")\n",
    "\n",
    "voting_clf.fit(X_train, y_train)\n",
    "voting_test = voting_clf.score(X_test, y_test)\n",
    "\n",
    "print(f\"Voting Classifier Test Accuracy: {voting_test:.4f}\")\n",
    "\n",
    "# 2. Stacking Classifier (Meta-Model)\n",
    "print(\"\\n2. Stacking Classifier (Meta-Model)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Base models (diverse algorithms)\n",
    "base_models = [\n",
    "    (\"rf\", RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)),\n",
    "    (\"gb\", GradientBoostingClassifier(n_estimators=100, max_depth=3, random_state=42)),\n",
    "    (\"lr\", LogisticRegression(random_state=42, max_iter=1000)),\n",
    "]\n",
    "\n",
    "# Meta-model (simple model on base predictions)\n",
    "meta_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "\n",
    "# Stacking classifier\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=base_models,\n",
    "    final_estimator=meta_model,\n",
    "    cv=5,  # Use 5-fold CV to generate meta-features\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "stacking_clf.fit(X_train, y_train)\n",
    "stacking_test = stacking_clf.score(X_test, y_test)\n",
    "\n",
    "print(f\"Stacking Classifier Test Accuracy: {stacking_test:.4f}\")\n",
    "\n",
    "# 3. Compare All Ensemble Methods\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL ENSEMBLE COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "final_results = {\n",
    "    \"Single Tree\": test_score,\n",
    "    \"Random Forest\": rf_test,\n",
    "    \"Gradient Boosting\": gb_test,\n",
    "    \"Voting Ensemble\": voting_test,\n",
    "    \"Stacking Ensemble\": stacking_test,\n",
    "}\n",
    "\n",
    "# Add XGBoost if available\n",
    "if xgb is not None:\n",
    "    final_results[\"XGBoost\"] = xgb_tuned_test\n",
    "\n",
    "results_final_df = pd.DataFrame(\n",
    "    {\"Model\": list(final_results.keys()), \"Test Accuracy\": list(final_results.values())}\n",
    ").sort_values(\"Test Accuracy\", ascending=False)\n",
    "\n",
    "display(results_final_df)\n",
    "\n",
    "# Visualize final comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "colors = plt.cm.RdYlGn(results_final_df[\"Test Accuracy\"])\n",
    "bars = plt.barh(\n",
    "    results_final_df[\"Model\"],\n",
    "    results_final_df[\"Test Accuracy\"],\n",
    "    color=colors,\n",
    "    alpha=0.8,\n",
    "    edgecolor=\"black\",\n",
    "    linewidth=2,\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Test Accuracy\", fontweight=\"bold\", fontsize=12)\n",
    "plt.title(\"Complete Ensemble Methods Comparison\", fontweight=\"bold\", fontsize=14)\n",
    "plt.grid(axis=\"x\", alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, (model, acc) in enumerate(zip(results_final_df[\"Model\"], results_final_df[\"Test Accuracy\"])):\n",
    "    plt.text(acc, i, f\" {acc:.4f}\", va=\"center\", fontweight=\"bold\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Best performer\n",
    "best_model = results_final_df.iloc[0]\n",
    "improvement = ((best_model[\"Test Accuracy\"] - test_score) / test_score) * 100\n",
    "\n",
    "print(f\"\\nüèÜ Best Model: {best_model['Model']}\")\n",
    "print(f\"   Accuracy: {best_model['Test Accuracy']:.4f}\")\n",
    "print(f\"   Improvement over single tree: {improvement:.1f}%\")\n",
    "\n",
    "print(\"\\nüí° Key Insights:\")\n",
    "print(\"   ‚Ä¢ Stacking combines diverse models\")\n",
    "print(\"   ‚Ä¢ Voting is simpler, often nearly as good\")\n",
    "print(\"   ‚Ä¢ Ensembles typically beat single models\")\n",
    "print(\"   ‚Ä¢ Diminishing returns after certain point\")\n",
    "\n",
    "print(\"\\n‚úì Stacking and blending demonstrated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Kaggle Competition Strategy\n",
    "\n",
    "Ensemble methods dominate Kaggle competitions. Here's the winning playbook.\n",
    "\n",
    "### Typical Kaggle Winning Solution\n",
    "\n",
    "```\n",
    "Level 1: Diverse Base Models\n",
    "‚îú‚îÄ XGBoost (multiple configs)\n",
    "‚îú‚îÄ LightGBM (multiple configs)\n",
    "‚îú‚îÄ CatBoost\n",
    "‚îú‚îÄ Neural Networks\n",
    "‚îî‚îÄ Random Forest\n",
    "\n",
    "Level 2: Stacking\n",
    "‚îî‚îÄ Combine with Ridge/Logistic Regression\n",
    "\n",
    "Level 3: Blending\n",
    "‚îî‚îÄ Weighted average of best models\n",
    "```\n",
    "\n",
    "### Kaggle Best Practices\n",
    "\n",
    "1. **Feature Engineering** (most important!)\n",
    "2. **Cross-Validation**: 5-10 fold stratified\n",
    "3. **Multiple Models**: XGBoost + LightGBM + CatBoost\n",
    "4. **Hyperparameter Tuning**: Bayesian optimization\n",
    "5. **Ensemble Diversity**: Different algorithms, features, seeds\n",
    "6. **Stacking**: 2-3 levels maximum\n",
    "7. **Blending**: Weight by CV score\n",
    "\n",
    "### Common Mistakes to Avoid\n",
    "\n",
    "‚ùå **Overfitting to leaderboard**: Trust your CV score  \n",
    "‚ùå **Too complex ensembles**: Diminishing returns  \n",
    "‚ùå **Not enough diversity**: Similar models don't help  \n",
    "‚ùå **Ignoring features**: Best ensemble won't beat better features  \n",
    "\n",
    "### Quick Kaggle Template\n",
    "\n",
    "```python\n",
    "# 1. Strong single models\n",
    "xgb_model = xgb.XGBClassifier(params)\n",
    "lgb_model = lgb.LGBMClassifier(params)\n",
    "\n",
    "# 2. Different random seeds\n",
    "models = []\n",
    "for seed in [42, 123, 456]:\n",
    "    model = xgb.XGBClassifier(random_state=seed)\n",
    "    models.append(model)\n",
    "\n",
    "# 3. Stack or blend\n",
    "final_pred = np.mean([m.predict_proba(X) for m in models], axis=0)\n",
    "```\n",
    "\n",
    "### Resources\n",
    "\n",
    "- Kaggle Learn: Free courses\n",
    "- Past competition solutions: Study winners\n",
    "- Kaggle Forums: Ask questions\n",
    "\n",
    "**Pro tip**: Start with Titanic or House Prices competitions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle-Style Ensemble Example\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"KAGGLE-STYLE ENSEMBLE DEMO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Simulate Kaggle approach: Multiple models with different seeds\n",
    "print(\"\\nBuilding ensemble with different random seeds...\")\n",
    "\n",
    "ensemble_predictions = []\n",
    "\n",
    "# Train same model with different seeds for diversity\n",
    "for seed in [42, 123, 456, 789]:\n",
    "    model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=seed, n_jobs=-1)\n",
    "    model.fit(X_train, y_train)\n",
    "    pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    ensemble_predictions.append(pred_proba)\n",
    "\n",
    "# Blend predictions (simple average)\n",
    "blended_proba = np.mean(ensemble_predictions, axis=0)\n",
    "blended_pred = (blended_proba >= 0.5).astype(int)\n",
    "blended_accuracy = accuracy_score(y_test, blended_pred)\n",
    "\n",
    "# Compare single model vs ensemble\n",
    "single_model_acc = (\n",
    "    RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "    .fit(X_train, y_train)\n",
    "    .score(X_test, y_test)\n",
    ")\n",
    "\n",
    "print(f\"\\nSingle Model (seed=42):    {single_model_acc:.4f}\")\n",
    "print(f\"Ensemble (4 seeds):        {blended_accuracy:.4f}\")\n",
    "print(f\"Improvement:               {(blended_accuracy - single_model_acc):.4f}\")\n",
    "\n",
    "print(\"\\nüí° Even simple ensembles improve performance!\")\n",
    "print(\"   Kaggle winners use 10-50+ models\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"QUICK KAGGLE WORKFLOW\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "workflow = \"\"\"\n",
    "1. ‚úì Exploratory Data Analysis (EDA)\n",
    "2. ‚úì Feature Engineering (MOST IMPORTANT!)\n",
    "3. ‚úì Baseline Model (Random Forest / XGBoost)\n",
    "4. ‚úì Cross-Validation Setup (Stratified K-Fold)\n",
    "5. ‚úì Hyperparameter Tuning (Grid/Random/Bayesian)\n",
    "6. ‚úì Try Multiple Algorithms\n",
    "7. ‚úì Build Ensemble (Stacking/Blending)\n",
    "8. ‚úì Submit and Iterate\n",
    "\n",
    "üìä Feature Engineering > Ensembles > Hyperparameter Tuning\n",
    "\"\"\"\n",
    "\n",
    "print(workflow)\n",
    "\n",
    "print(\"‚úì Kaggle strategy demonstrated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Hands-On Exercises\n",
    "\n",
    "Practice ensemble methods to master them!\n",
    "\n",
    "### Exercise 1: Bagging Comparison\n",
    "Compare BaggingClassifier with different base estimators:\n",
    "- Decision Tree\n",
    "- Logistic Regression\n",
    "- SVM\n",
    "\n",
    "Which works best? Why?\n",
    "\n",
    "### Exercise 2: XGBoost Tuning\n",
    "Tune XGBoost on the customer_reviews dataset:\n",
    "- Grid search over: n_estimators, max_depth, learning_rate\n",
    "- Use early stopping\n",
    "- Compare with default parameters\n",
    "\n",
    "### Exercise 3: Build a Stacking Ensemble\n",
    "Create a stacking ensemble with:\n",
    "- Base: Random Forest, XGBoost, Logistic Regression\n",
    "- Meta: Ridge Regression\n",
    "- Evaluate with cross-validation\n",
    "\n",
    "### Exercise 4: Kaggle Simulation\n",
    "1. Load Titanic or House Prices dataset\n",
    "2. Build 3+ different models\n",
    "3. Create stacking/blending ensemble\n",
    "4. Beat single best model by 2%+\n",
    "\n",
    "### Exercise 5: Feature Importance Comparison\n",
    "Compare feature importance from:\n",
    "- Random Forest\n",
    "- Gradient Boosting\n",
    "- XGBoost\n",
    "\n",
    "Do they agree? What does this tell you?\n",
    "\n",
    "Ready to practice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise Workspace\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EXERCISES - Master Ensemble Methods!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Exercise 1: Bagging Comparison\n",
    "print(\"\\nExercise 1: Bagging with Different Base Estimators\")\n",
    "print(\"-\" * 60)\n",
    "print(\"TODO: Create BaggingClassifier with different base models\")\n",
    "print(\"TODO: Compare performance\\n\")\n",
    "\n",
    "# Exercise 2: XGBoost Tuning\n",
    "print(\"Exercise 2: XGBoost Hyperparameter Tuning\")\n",
    "print(\"-\" * 60)\n",
    "print(\"TODO: Define parameter grid\")\n",
    "print(\"TODO: Use GridSearchCV\")\n",
    "print(\"TODO: Evaluate with cross-validation\\n\")\n",
    "\n",
    "# Exercise 3: Stacking Ensemble\n",
    "print(\"Exercise 3: Build Custom Stacking Ensemble\")\n",
    "print(\"-\" * 60)\n",
    "print(\"TODO: Define base models\")\n",
    "print(\"TODO: Create StackingClassifier\")\n",
    "print(\"TODO: Compare with individual models\\n\")\n",
    "\n",
    "# Exercise 4: Kaggle Simulation\n",
    "print(\"Exercise 4: Kaggle Competition Simulation\")\n",
    "print(\"-\" * 60)\n",
    "print(\"TODO: Load competition dataset\")\n",
    "print(\"TODO: Feature engineering\")\n",
    "print(\"TODO: Build multiple models\")\n",
    "print(\"TODO: Create ensemble\\n\")\n",
    "\n",
    "# Your code here for all exercises\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TIPS FOR SUCCESS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"‚Ä¢ Start simple, then add complexity\")\n",
    "print(\"‚Ä¢ Always use cross-validation\")\n",
    "print(\"‚Ä¢ Compare against baseline\")\n",
    "print(\"‚Ä¢ Visualize results\")\n",
    "print(\"‚Ä¢ Document what works\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Key Takeaways & Next Steps\n",
    "\n",
    "Congratulations! You've mastered ensemble methods - the key to winning ML competitions!\n",
    "\n",
    "### What You've Learned\n",
    "\n",
    "#### 1. **Ensemble Fundamentals**\n",
    "- ‚úì Why ensembles work: diversity + aggregation = better predictions\n",
    "- ‚úì Three main approaches: Bagging, Boosting, Stacking\n",
    "- ‚úì When to use ensembles vs single models\n",
    "\n",
    "#### 2. **Bagging Methods**\n",
    "- ‚úì Bootstrap aggregating reduces variance\n",
    "- ‚úì Random Forest = Bagging + feature randomization\n",
    "- ‚úì Parallel training, robust to overfitting\n",
    "- ‚úì Feature importance for interpretation\n",
    "\n",
    "#### 3. **Boosting Methods**\n",
    "- ‚úì AdaBoost: Adaptive boosting with weak learners\n",
    "- ‚úì Gradient Boosting: Sequential error correction\n",
    "- ‚úì Reduces bias, focuses on hard examples\n",
    "- ‚úì Requires careful tuning to avoid overfitting\n",
    "\n",
    "#### 4. **Advanced Boosting Libraries**\n",
    "- ‚úì **XGBoost**: Regularization, speed, Kaggle champion\n",
    "- ‚úì **LightGBM**: Fast for large datasets, leaf-wise growth\n",
    "- ‚úì **CatBoost**: Automatic categorical handling\n",
    "\n",
    "#### 5. **Meta-Ensembles**\n",
    "- ‚úì Voting: Simple averaging or majority vote\n",
    "- ‚úì Stacking: Meta-model learns optimal combination\n",
    "- ‚úì Blending: Holdout set predictions\n",
    "- ‚úì Diversity is key to ensemble success\n",
    "\n",
    "#### 6. **Kaggle Strategies**\n",
    "- ‚úì Feature engineering > Ensembles > Hyperparameters\n",
    "- ‚úì XGBoost + LightGBM + CatBoost standard combo\n",
    "- ‚úì Cross-validation essential\n",
    "- ‚úì Stack 2-3 levels maximum\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "> **\"Ensemble methods win 90%+ of Kaggle competitions\"**\n",
    "\n",
    "**Performance Hierarchy:**\n",
    "1. **Feature Engineering** (50%+ improvement possible)\n",
    "2. **Algorithm Selection** (10-20% improvement)\n",
    "3. **Hyperparameter Tuning** (5-10% improvement)\n",
    "4. **Ensembles** (5-15% improvement)\n",
    "\n",
    "**Best ROI**: Feature engineering + XGBoost!\n",
    "\n",
    "### Quick Reference Guide\n",
    "\n",
    "| Algorithm | When to Use | Key Parameters |\n",
    "|-----------|-------------|----------------|\n",
    "| **Random Forest** | General purpose, baseline | n_estimators, max_depth |\n",
    "| **XGBoost** | Tabular data, competitions | learning_rate, max_depth, reg_alpha/lambda |\n",
    "| **LightGBM** | Large datasets (>100K rows) | num_leaves, learning_rate |\n",
    "| **CatBoost** | Many categorical features | iterations, depth |\n",
    "| **Stacking** | Maximum performance | base_models, meta_model |\n",
    "\n",
    "### Common Pitfalls\n",
    "\n",
    "1. **Over-ensembling**: Diminishing returns after 5-10 models\n",
    "2. **Lack of diversity**: Similar models don't help\n",
    "3. **Overfitting**: Trust CV score, not test score\n",
    "4. **Ignoring features**: Best ensemble < Good features + simple model\n",
    "5. **Too complex**: Can't debug or deploy easily\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "Ensemble methods power:\n",
    "- **E-commerce**: Recommendation systems, fraud detection\n",
    "- **Finance**: Credit scoring, risk assessment\n",
    "- **Healthcare**: Disease prediction, treatment outcomes\n",
    "- **Marketing**: Customer churn, conversion prediction\n",
    "- **Autonomous vehicles**: Sensor fusion, decision making\n",
    "\n",
    "### Production Considerations\n",
    "\n",
    "**Advantages:**\n",
    "‚úì Superior accuracy\n",
    "‚úì Robust to outliers\n",
    "‚úì Feature importance\n",
    "‚úì Handles missing data (XGBoost/LightGBM)\n",
    "\n",
    "**Challenges:**\n",
    "‚úó Longer training time\n",
    "‚úó Larger model size\n",
    "‚úó Slower predictions\n",
    "‚úó Harder to interpret\n",
    "‚úó More hyperparameters\n",
    "\n",
    "**Deploy When:**\n",
    "- Accuracy is critical\n",
    "- Can afford compute\n",
    "- Batch predictions OK\n",
    "- Model monitoring in place\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "#### Module 15: Unsupervised Learning\n",
    "- Clustering (K-Means, DBSCAN, Hierarchical)\n",
    "- Dimensionality Reduction (PCA, t-SNE)\n",
    "- Anomaly Detection\n",
    "- Customer Segmentation\n",
    "\n",
    "#### Practice Projects\n",
    "1. **Titanic** (Kaggle): Classification with ensembles\n",
    "2. **House Prices** (Kaggle): Regression with XGBoost\n",
    "3. **Credit Card Fraud**: Imbalanced data with boosting\n",
    "4. **Customer Segmentation**: Clustering + classification\n",
    "\n",
    "#### Advanced Topics\n",
    "- **Deep Ensembles**: Neural network ensembles\n",
    "- **Snapshot Ensembles**: Single training run, multiple models\n",
    "- **AutoML**: Automated ensemble creation (Auto-sklearn, H2O)\n",
    "- **Model Interpretation**: SHAP values for ensembles\n",
    "\n",
    "### Recommended Practice\n",
    "\n",
    "Spend **3-4 hours**:\n",
    "1. Complete all 5 exercises\n",
    "2. Enter Titanic competition on Kaggle\n",
    "3. Build production ensemble pipeline\n",
    "4. Compare all methods on your own dataset\n",
    "\n",
    "### Resources\n",
    "\n",
    "**Documentation:**\n",
    "- [XGBoost Docs](https://xgboost.readthedocs.io/)\n",
    "- [LightGBM Docs](https://lightgbm.readthedocs.io/)\n",
    "- [CatBoost Docs](https://catboost.ai/)\n",
    "\n",
    "**Kaggle:**\n",
    "- Titanic Competition\n",
    "- House Prices Competition\n",
    "- Past competition winning solutions\n",
    "\n",
    "**Books:**\n",
    "- \"Ensemble Methods for Machine Learning\" by Gautam Kunapuli\n",
    "- \"Hands-On Machine Learning\" Ch. 7 (Ensemble Learning)\n",
    "\n",
    "**Tools:**\n",
    "- SHAP for model interpretation\n",
    "- MLflow for experiment tracking\n",
    "- Optuna for hyperparameter optimization\n",
    "\n",
    "### Final Wisdom\n",
    "\n",
    "> **\"The best model is often an ensemble of good models with good features\"**\n",
    "\n",
    "**Ensemble Philosophy:**\n",
    "- **Diversity beats individual genius**\n",
    "- **Aggregate intelligence > single expert**\n",
    "- **Simple combination often works best**\n",
    "\n",
    "**For Production:**\n",
    "1. Start with XGBoost (good default)\n",
    "2. Add LightGBM if large data\n",
    "3. Simple voting/averaging if needed\n",
    "4. Complex stacking only if accuracy critical\n",
    "\n",
    "**For Competitions:**\n",
    "1. Feature engineering first\n",
    "2. Multiple XGBoost/LightGBM/CatBoost\n",
    "3. Stack everything\n",
    "4. Blend top models\n",
    "\n",
    "Keep experimenting, keep ensemble-ing!\n",
    "\n",
    "---\n",
    "\n",
    "### Module Complete! üéâ\n",
    "\n",
    "**Time invested**: ~90 minutes  \n",
    "**Skills gained**: Kaggle-level ensemble methods  \n",
    "**Confidence**: Competition-ready!\n",
    "\n",
    "**Next Module**: `15_unsupervised_learning.ipynb` - Discover patterns without labels!\n",
    "\n",
    "---\n",
    "\n",
    "*Built with Claude Code | Module 14: Ensemble Methods*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
