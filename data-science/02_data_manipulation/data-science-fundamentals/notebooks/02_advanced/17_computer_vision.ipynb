{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 17: Computer Vision with CNNs\n",
    "\n",
    "**Estimated Time**: 90 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this module, you will master computer vision with cnns.\n",
    "\n",
    "Topics covered:\n",
    "- Computer Vision Introduction\n",
    "- Convolutional Neural Networks\n",
    "- Pooling and Padding\n",
    "- Building CNN Architecture\n",
    "- Transfer Learning with Pre-trained Models\n",
    "- Image Classification Project\n",
    "- Data Augmentation\n",
    "- Model Deployment\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Modules 00-11 completed\n",
    "- Intermediate Python and ML knowledge\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Computer Vision Introduction\n",
    "\n",
    "**Computer Vision** (CV) enables machines to understand and interpret visual information from the world.\n",
    "\n",
    "### What is Computer Vision?\n",
    "\n",
    "> **\"Teaching computers to see and understand images like humans do\"**\n",
    "\n",
    "**Goal**: Extract meaningful information from digital images and videos\n",
    "\n",
    "**Examples:**\n",
    "- ðŸ“¸ **Photo tagging**: Facebook recognizes faces\n",
    "- ðŸš— **Self-driving cars**: Detect pedestrians, traffic signs\n",
    "- ðŸ¥ **Medical imaging**: Diagnose diseases from X-rays, MRIs\n",
    "- ðŸ“± **Face unlock**: iPhone Face ID\n",
    "- ðŸ›’ **Product search**: Google Lens\n",
    "- ðŸŽ¨ **Art generation**: DALL-E, Midjourney\n",
    "\n",
    "### The Challenge: Images are Just Numbers\n",
    "\n",
    "**Human sees**: Cat, dog, tree, car\n",
    "**Computer sees**: Matrix of pixel values (0-255)\n",
    "\n",
    "```\n",
    "Image (28Ã—28 grayscale) = 784 numbers\n",
    "Image (224Ã—224 RGB) = 150,528 numbers!\n",
    "```\n",
    "\n",
    "**Problem**: How do we learn patterns from millions of pixel values?\n",
    "**Solution**: Convolutional Neural Networks (CNNs)\n",
    "\n",
    "### Why Regular Neural Networks Fail for Images\n",
    "\n",
    "**Example**: 224Ã—224 RGB image = 150,528 input neurons\n",
    "\n",
    "**Problems:**\n",
    "1. **Too many parameters**: Hidden layer with 1000 neurons = 150M weights!\n",
    "2. **No spatial awareness**: Nearby pixels are related, but fully connected networks don't know this\n",
    "3. **Not translation invariant**: Cat in top-left â‰  Cat in bottom-right\n",
    "4. **Computationally expensive**: Training is impossibly slow\n",
    "\n",
    "### CNNs: The Solution\n",
    "\n",
    "**Key Ideas:**\n",
    "1. **Local connectivity**: Neurons only connect to small regions\n",
    "2. **Parameter sharing**: Same filters applied across entire image\n",
    "3. **Translation invariance**: Detect patterns anywhere in image\n",
    "4. **Hierarchical features**: Low-level (edges) â†’ High-level (objects)\n",
    "\n",
    "### How CNNs Learn Features\n",
    "\n",
    "**Traditional ML**: Hand-craft features (SIFT, HOG, etc.)\n",
    "**CNNs**: Learn features automatically!\n",
    "\n",
    "**Layer 1**: Detects edges, colors\n",
    "**Layer 2**: Detects textures, patterns\n",
    "**Layer 3**: Detects parts (eyes, wheels, leaves)\n",
    "**Layer 4**: Detects objects (faces, cars, trees)\n",
    "\n",
    "### Image Representation\n",
    "\n",
    "**Grayscale Image:**\n",
    "- Shape: (height, width)\n",
    "- Values: 0 (black) to 255 (white)\n",
    "- Example: MNIST digits (28Ã—28)\n",
    "\n",
    "**RGB Color Image:**\n",
    "- Shape: (height, width, 3)\n",
    "- 3 channels: Red, Green, Blue\n",
    "- Each pixel: (R, G, B) where each is 0-255\n",
    "- Example: CIFAR-10 (32Ã—32Ã—3)\n",
    "\n",
    "### Computer Vision Tasks\n",
    "\n",
    "| Task | Description | Output |\n",
    "|------|-------------|--------|\n",
    "| **Image Classification** | What is in this image? | Single label (cat, dog, car) |\n",
    "| **Object Detection** | Where are objects? | Bounding boxes + labels |\n",
    "| **Semantic Segmentation** | Classify each pixel | Pixel-wise labels |\n",
    "| **Instance Segmentation** | Separate object instances | Individual object masks |\n",
    "| **Image Generation** | Create new images | Synthetic image |\n",
    "\n",
    "### Historical Milestones\n",
    "\n",
    "- **2012**: AlexNet wins ImageNet (CNNs revolution begins)\n",
    "- **2014**: VGGNet, GoogLeNet (deeper networks)\n",
    "- **2015**: ResNet (152 layers with skip connections!)\n",
    "- **2017**: Transformers for vision (ViT)\n",
    "- **2020s**: Diffusion models (DALL-E, Stable Diffusion)\n",
    "\n",
    "Let's visualize how computers see images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computer Vision Introduction - How Computers See Images\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"HOW COMPUTERS SEE IMAGES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load MNIST dataset (handwritten digits)\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "X_digits = digits.data\n",
    "y_digits = digits.target\n",
    "\n",
    "print(f\"\\nMNIST Digits dataset loaded:\")\n",
    "print(f\"  Shape: {digits.images.shape}\")\n",
    "print(f\"  Images: {len(digits.images)} samples of 8Ã—8 pixels\")\n",
    "print(f\"  Classes: {np.unique(y_digits).tolist()} (digits 0-9)\")\n",
    "\n",
    "# Visualize: Human view vs Computer view\n",
    "fig, axes = plt.subplots(2, 5, figsize=(16, 7))\n",
    "\n",
    "for i in range(5):\n",
    "    idx = i * 100  # Sample every 100th image\n",
    "\n",
    "    # Top row: Human view (image)\n",
    "    axes[0, i].imshow(digits.images[idx], cmap=\"gray\")\n",
    "    axes[0, i].set_title(f'Human Sees:\\nDigit \"{y_digits[idx]}\"', fontsize=12, fontweight=\"bold\")\n",
    "    axes[0, i].axis(\"off\")\n",
    "\n",
    "    # Bottom row: Computer view (numbers)\n",
    "    im = axes[1, i].imshow(digits.images[idx], cmap=\"viridis\", interpolation=\"none\")\n",
    "    axes[1, i].set_title(\n",
    "        f\"Computer Sees:\\n64 numbers (8Ã—8)\", fontsize=12, fontweight=\"bold\", color=\"red\"\n",
    "    )\n",
    "\n",
    "    # Add pixel values as text\n",
    "    for row in range(8):\n",
    "        for col in range(8):\n",
    "            value = int(digits.images[idx][row, col])\n",
    "            color = \"white\" if value > 8 else \"black\"\n",
    "            if row % 2 == 0 and col % 2 == 0:  # Show every other value to avoid clutter\n",
    "                axes[1, i].text(\n",
    "                    col, row, str(value), ha=\"center\", va=\"center\", fontsize=7, color=color\n",
    "                )\n",
    "\n",
    "    axes[1, i].set_xticks([])\n",
    "    axes[1, i].set_yticks([])\n",
    "\n",
    "plt.colorbar(im, ax=axes[1, :], label=\"Pixel Intensity (0-16)\", shrink=0.8)\n",
    "plt.suptitle(\"Human Perception vs Computer Representation\", fontsize=16, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# RGB Color channels demonstration\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RGB COLOR CHANNELS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a simple colored image\n",
    "img_rgb = np.zeros((100, 100, 3), dtype=np.uint8)\n",
    "img_rgb[:, :33, 0] = 255  # Red channel (left third)\n",
    "img_rgb[:, 33:66, 1] = 255  # Green channel (middle third)\n",
    "img_rgb[:, 66:, 2] = 255  # Blue channel (right third)\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "# Original RGB image\n",
    "axes[0].imshow(img_rgb)\n",
    "axes[0].set_title(\"RGB Image\\n(What we see)\", fontsize=12, fontweight=\"bold\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "# Red channel\n",
    "axes[1].imshow(img_rgb[:, :, 0], cmap=\"Reds\")\n",
    "axes[1].set_title(\"Red Channel\\n(R values)\", fontsize=12, fontweight=\"bold\")\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "# Green channel\n",
    "axes[2].imshow(img_rgb[:, :, 1], cmap=\"Greens\")\n",
    "axes[2].set_title(\"Green Channel\\n(G values)\", fontsize=12, fontweight=\"bold\")\n",
    "axes[2].axis(\"off\")\n",
    "\n",
    "# Blue channel\n",
    "axes[3].imshow(img_rgb[:, :, 2], cmap=\"Blues\")\n",
    "axes[3].set_title(\"Blue Channel\\n(B values)\", fontsize=12, fontweight=\"bold\")\n",
    "axes[3].axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"RGB Image Decomposition: 3 Separate Matrices\", fontsize=14, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nRGB Image shape: (height, width, 3)\")\n",
    "print(f\"  Example: {img_rgb.shape}\")\n",
    "print(f\"  Total values: {img_rgb.size} numbers\")\n",
    "print(f\"  Memory: {img_rgb.nbytes} bytes\")\n",
    "\n",
    "# Visualize feature hierarchy\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CNN FEATURE HIERARCHY (Conceptual)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(18, 4))\n",
    "\n",
    "# Layer 1: Edges\n",
    "edge_img = np.zeros((50, 50))\n",
    "edge_img[20:30, :] = 1  # Horizontal edge\n",
    "axes[0].imshow(edge_img, cmap=\"gray\")\n",
    "axes[0].set_title(\"Layer 1\\nDetects EDGES\\n(Low-level features)\", fontsize=11, fontweight=\"bold\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "# Layer 2: Textures\n",
    "texture = np.random.rand(50, 50)\n",
    "axes[1].imshow(texture, cmap=\"gray\")\n",
    "axes[1].set_title(\"Layer 2\\nDetects TEXTURES\\n(Mid-level features)\", fontsize=11, fontweight=\"bold\")\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "# Layer 3: Parts\n",
    "axes[2].text(\n",
    "    0.5,\n",
    "    0.5,\n",
    "    \"Eyes\\nWheels\\nLeaves\",\n",
    "    ha=\"center\",\n",
    "    va=\"center\",\n",
    "    fontsize=14,\n",
    "    bbox=dict(boxstyle=\"round\", facecolor=\"lightblue\"),\n",
    ")\n",
    "axes[2].set_xlim(0, 1)\n",
    "axes[2].set_ylim(0, 1)\n",
    "axes[2].set_title(\"Layer 3\\nDetects PARTS\\n(Object components)\", fontsize=11, fontweight=\"bold\")\n",
    "axes[2].axis(\"off\")\n",
    "\n",
    "# Layer 4: Objects\n",
    "axes[3].text(\n",
    "    0.5,\n",
    "    0.5,\n",
    "    \"Cat\\nCar\\nTree\",\n",
    "    ha=\"center\",\n",
    "    va=\"center\",\n",
    "    fontsize=16,\n",
    "    fontweight=\"bold\",\n",
    "    bbox=dict(boxstyle=\"round\", facecolor=\"lightgreen\"),\n",
    ")\n",
    "axes[3].set_xlim(0, 1)\n",
    "axes[3].set_ylim(0, 1)\n",
    "axes[3].set_title(\"Layer 4\\nDetects OBJECTS\\n(High-level concepts)\", fontsize=11, fontweight=\"bold\")\n",
    "axes[3].axis(\"off\")\n",
    "\n",
    "plt.suptitle(\n",
    "    \"How CNNs Build Understanding: Hierarchical Feature Learning\", fontsize=14, fontweight=\"bold\"\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Computer Vision fundamentals visualized!\")\n",
    "print(\"  â€¢ Images are matrices of numbers (pixels)\")\n",
    "print(\"  â€¢ RGB images have 3 channels (Red, Green, Blue)\")\n",
    "print(\"  â€¢ CNNs learn hierarchical features automatically\")\n",
    "print(\"  â€¢ Low-level (edges) â†’ High-level (objects)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Convolutional Neural Networks (CNNs)\n",
    "\n",
    "**CNNs** are neural networks specifically designed for processing grid-like data (images).\n",
    "\n",
    "### The Convolution Operation\n",
    "\n",
    "**Convolution** slides a small filter (kernel) across the image to detect patterns.\n",
    "\n",
    "**Analogy**: Like a flashlight scanning a dark room - you see one small area at a time.\n",
    "\n",
    "### How Convolution Works\n",
    "\n",
    "**Step-by-step:**\n",
    "1. Take a **filter** (e.g., 3Ã—3 matrix of weights)\n",
    "2. **Slide** it across the image\n",
    "3. At each position, compute **element-wise multiplication** and **sum**\n",
    "4. Result: **Feature map** (activation map)\n",
    "\n",
    "**Example**:\n",
    "```\n",
    "Image (5Ã—5)        Filter (3Ã—3)      Output (3Ã—3)\n",
    "[1 2 3 4 5]       [1 0 -1]\n",
    "[0 1 2 3 4]   *   [1 0 -1]    =    Feature Map\n",
    "[5 6 7 8 9]       [1 0 -1]\n",
    "[2 3 4 5 6]\n",
    "[1 2 3 4 5]\n",
    "```\n",
    "\n",
    "### Filters Detect Features\n",
    "\n",
    "Different filters detect different patterns:\n",
    "\n",
    "**Vertical Edge Detector:**\n",
    "```\n",
    "[1  0  -1]\n",
    "[1  0  -1]\n",
    "[1  0  -1]\n",
    "```\n",
    "\n",
    "**Horizontal Edge Detector:**\n",
    "```\n",
    "[ 1   1   1]\n",
    "[ 0   0   0]\n",
    "[-1  -1  -1]\n",
    "```\n",
    "\n",
    "**Blur Filter:**\n",
    "```\n",
    "[1/9  1/9  1/9]\n",
    "[1/9  1/9  1/9]\n",
    "[1/9  1/9  1/9]\n",
    "```\n",
    "\n",
    "### Key Parameters\n",
    "\n",
    "**Filter Size (Kernel Size):**\n",
    "- Common: 3Ã—3, 5Ã—5, 7Ã—7\n",
    "- Larger = more context, fewer parameters\n",
    "- **Default: 3Ã—3** (most common)\n",
    "\n",
    "**Stride:**\n",
    "- How many pixels to move filter each step\n",
    "- Stride=1: Move 1 pixel (default)\n",
    "- Stride=2: Move 2 pixels (downsample)\n",
    "\n",
    "**Padding:**\n",
    "- Add zeros around image border\n",
    "- **Valid**: No padding (output smaller)\n",
    "- **Same**: Pad so output = input size\n",
    "\n",
    "### Output Size Formula\n",
    "\n",
    "```\n",
    "Output size = (Input size - Filter size + 2Ã—Padding) / Stride + 1\n",
    "```\n",
    "\n",
    "**Example**:\n",
    "- Input: 32Ã—32, Filter: 5Ã—5, Stride: 1, Padding: 0\n",
    "- Output: (32 - 5 + 0) / 1 + 1 = 28Ã—28\n",
    "\n",
    "### Multiple Filters = Multiple Feature Maps\n",
    "\n",
    "**One layer typically has many filters:**\n",
    "- 32 filters â†’ 32 feature maps\n",
    "- 64 filters â†’ 64 feature maps\n",
    "- Each filter learns different patterns!\n",
    "\n",
    "### Why CNNs Work\n",
    "\n",
    "**1. Parameter Sharing:**\n",
    "- Same filter used across entire image\n",
    "- Detect \"cat ear\" anywhere in image\n",
    "- Dramatically reduces parameters\n",
    "\n",
    "**2. Local Connectivity:**\n",
    "- Each neuron looks at small region\n",
    "- Captures spatial relationships\n",
    "- More efficient than fully connected\n",
    "\n",
    "**3. Translation Invariance:**\n",
    "- Detect patterns regardless of position\n",
    "- Cat in corner or center â†’ still detected\n",
    "\n",
    "### CNN vs Fully Connected\n",
    "\n",
    "**Fully Connected (28Ã—28 image, 100 hidden):**\n",
    "- Parameters: 28Ã—28Ã—100 = 78,400\n",
    "\n",
    "**Conv Layer (32 filters of 3Ã—3):**\n",
    "- Parameters: 3Ã—3Ã—32 = 288 only!\n",
    "- **272Ã— fewer parameters!**\n",
    "\n",
    "Let's visualize convolution in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional Neural Networks - Convolution in Action\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CONVOLUTION OPERATION DEMONSTRATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# Manual convolution implementation\n",
    "def convolve2d(image, kernel, stride=1, padding=0):\n",
    "    \"\"\"\n",
    "    Perform 2D convolution on image with kernel.\n",
    "\n",
    "    Parameters:\n",
    "    - image: 2D numpy array\n",
    "    - kernel: 2D numpy array (filter)\n",
    "    - stride: step size for sliding window\n",
    "    - padding: zeros to add around border\n",
    "\n",
    "    Returns:\n",
    "    - feature_map: convolution output\n",
    "    \"\"\"\n",
    "    # Add padding\n",
    "    if padding > 0:\n",
    "        image = np.pad(image, padding, mode=\"constant\", constant_values=0)\n",
    "\n",
    "    # Get dimensions\n",
    "    img_h, img_w = image.shape\n",
    "    ker_h, ker_w = kernel.shape\n",
    "\n",
    "    # Calculate output size\n",
    "    out_h = (img_h - ker_h) // stride + 1\n",
    "    out_w = (img_w - ker_w) // stride + 1\n",
    "\n",
    "    # Initialize output\n",
    "    feature_map = np.zeros((out_h, out_w))\n",
    "\n",
    "    # Slide kernel across image\n",
    "    for i in range(0, out_h):\n",
    "        for j in range(0, out_w):\n",
    "            # Extract region\n",
    "            region = image[i * stride : i * stride + ker_h, j * stride : j * stride + ker_w]\n",
    "            # Element-wise multiplication and sum\n",
    "            feature_map[i, j] = np.sum(region * kernel)\n",
    "\n",
    "    return feature_map\n",
    "\n",
    "\n",
    "# Create sample image (simple pattern)\n",
    "image = np.array(\n",
    "    [\n",
    "        [0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 1, 1, 1, 1, 1, 0],\n",
    "        [0, 1, 1, 1, 1, 1, 0],\n",
    "        [0, 1, 1, 1, 1, 1, 0],\n",
    "        [0, 1, 1, 1, 1, 1, 0],\n",
    "        [0, 1, 1, 1, 1, 1, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0],\n",
    "    ],\n",
    "    dtype=float,\n",
    ")\n",
    "\n",
    "# Define different filters\n",
    "filters = {\n",
    "    \"Vertical Edge\": np.array([[1, 0, -1], [1, 0, -1], [1, 0, -1]]),\n",
    "    \"Horizontal Edge\": np.array([[1, 1, 1], [0, 0, 0], [-1, -1, -1]]),\n",
    "    \"Blur\": np.array([[1 / 9, 1 / 9, 1 / 9], [1 / 9, 1 / 9, 1 / 9], [1 / 9, 1 / 9, 1 / 9]]),\n",
    "    \"Sharpen\": np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]]),\n",
    "}\n",
    "\n",
    "print(\"\\n1. DEMONSTRATING DIFFERENT FILTERS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Apply filters and visualize\n",
    "fig, axes = plt.subplots(2, 5, figsize=(18, 8))\n",
    "\n",
    "# Original image\n",
    "axes[0, 0].imshow(image, cmap=\"gray\", vmin=-3, vmax=3)\n",
    "axes[0, 0].set_title(\"Original Image\\n(7Ã—7)\", fontsize=11, fontweight=\"bold\")\n",
    "axes[0, 0].axis(\"off\")\n",
    "\n",
    "axes[1, 0].text(\n",
    "    0.5,\n",
    "    0.5,\n",
    "    \"Input\\nImage\",\n",
    "    ha=\"center\",\n",
    "    va=\"center\",\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    "    bbox=dict(boxstyle=\"round\", facecolor=\"lightblue\"),\n",
    ")\n",
    "axes[1, 0].set_xlim(0, 1)\n",
    "axes[1, 0].set_ylim(0, 1)\n",
    "axes[1, 0].axis(\"off\")\n",
    "\n",
    "# Apply each filter\n",
    "for idx, (name, kernel) in enumerate(filters.items(), 1):\n",
    "    # Compute convolution\n",
    "    feature_map = convolve2d(image, kernel, stride=1, padding=0)\n",
    "\n",
    "    # Show filter\n",
    "    axes[0, idx].imshow(kernel, cmap=\"RdBu\", vmin=-1, vmax=1)\n",
    "    axes[0, idx].set_title(f\"{name} Filter\\n(3Ã—3)\", fontsize=11, fontweight=\"bold\")\n",
    "    axes[0, idx].axis(\"off\")\n",
    "\n",
    "    # Add kernel values\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            value = kernel[i, j]\n",
    "            color = \"white\" if abs(value) > 0.5 else \"black\"\n",
    "            axes[0, idx].text(\n",
    "                j,\n",
    "                i,\n",
    "                f\"{value:.1f}\",\n",
    "                ha=\"center\",\n",
    "                va=\"center\",\n",
    "                fontsize=9,\n",
    "                color=color,\n",
    "                fontweight=\"bold\",\n",
    "            )\n",
    "\n",
    "    # Show feature map\n",
    "    axes[1, idx].imshow(feature_map, cmap=\"RdBu\", vmin=-3, vmax=3)\n",
    "    axes[1, idx].set_title(\n",
    "        f\"Feature Map\\n({feature_map.shape[0]}Ã—{feature_map.shape[1]})\",\n",
    "        fontsize=11,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "    axes[1, idx].axis(\"off\")\n",
    "\n",
    "plt.suptitle(\n",
    "    \"Convolution: Different Filters Detect Different Features\", fontsize=14, fontweight=\"bold\"\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ“ Applied 4 different filters to {image.shape} image\")\n",
    "print(f\"  Filter size: 3Ã—3\")\n",
    "print(f\"  Output size: {feature_map.shape} (no padding, stride=1)\")\n",
    "\n",
    "# Demonstrate convolution step-by-step\n",
    "print(\"\\n2. STEP-BY-STEP CONVOLUTION PROCESS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Use a digit image for realistic example\n",
    "sample_digit = digits.images[0]  # First digit\n",
    "vertical_filter = np.array([[1, 0, -1], [1, 0, -1], [1, 0, -1]])\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "# Show original image\n",
    "axes[0, 0].imshow(sample_digit, cmap=\"gray\")\n",
    "axes[0, 0].set_title(\"Original Digit\\n(8Ã—8 image)\", fontsize=11, fontweight=\"bold\")\n",
    "axes[0, 0].axis(\"off\")\n",
    "\n",
    "# Show filter\n",
    "axes[1, 0].imshow(vertical_filter, cmap=\"RdBu\", vmin=-1, vmax=1)\n",
    "axes[1, 0].set_title(\"Vertical Edge\\nFilter (3Ã—3)\", fontsize=11, fontweight=\"bold\")\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        axes[1, 0].text(\n",
    "            j, i, vertical_filter[i, j], ha=\"center\", va=\"center\", fontsize=10, fontweight=\"bold\"\n",
    "        )\n",
    "axes[1, 0].axis(\"off\")\n",
    "\n",
    "# Show 3 intermediate steps\n",
    "positions = [(0, 0), (0, 3), (3, 3)]\n",
    "for idx, (row, col) in enumerate(positions, 1):\n",
    "    # Extract 3Ã—3 region\n",
    "    region = sample_digit[row : row + 3, col : col + 3]\n",
    "\n",
    "    # Show region\n",
    "    axes[0, idx].imshow(sample_digit, cmap=\"gray\", alpha=0.3)\n",
    "    # Highlight region with rectangle\n",
    "    from matplotlib.patches import Rectangle\n",
    "\n",
    "    rect = Rectangle((col - 0.5, row - 0.5), 3, 3, linewidth=3, edgecolor=\"red\", facecolor=\"none\")\n",
    "    axes[0, idx].add_patch(rect)\n",
    "    axes[0, idx].set_title(\n",
    "        f\"Step {idx}: Position ({row},{col})\", fontsize=11, fontweight=\"bold\", color=\"red\"\n",
    "    )\n",
    "    axes[0, idx].axis(\"off\")\n",
    "\n",
    "    # Show computation\n",
    "    result = np.sum(region * vertical_filter)\n",
    "    axes[1, idx].imshow(region, cmap=\"gray\")\n",
    "    axes[1, idx].set_title(f\"Region Ã— Filter\\n= {result:.1f}\", fontsize=11, fontweight=\"bold\")\n",
    "    axes[1, idx].axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Convolution: Sliding Window Process\", fontsize=14, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ“ Demonstrated sliding window at 3 positions\")\n",
    "print(f\"  At each position: element-wise multiply + sum\")\n",
    "\n",
    "# Effect of stride and padding\n",
    "print(\"\\n3. EFFECT OF STRIDE AND PADDING\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "test_image = digits.images[5]  # Use a digit\n",
    "edge_filter = filters[\"Vertical Edge\"]\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(18, 8))\n",
    "\n",
    "# Original\n",
    "axes[0, 0].imshow(test_image, cmap=\"gray\")\n",
    "axes[0, 0].set_title(\"Original\\n8Ã—8\", fontsize=11, fontweight=\"bold\")\n",
    "axes[0, 0].axis(\"off\")\n",
    "\n",
    "# Configurations\n",
    "configs = [(\"Stride=1, Pad=0\", 1, 0), (\"Stride=2, Pad=0\", 2, 0), (\"Stride=1, Pad=1\", 1, 1)]\n",
    "\n",
    "for idx, (title, stride, padding) in enumerate(configs, 1):\n",
    "    # Compute convolution\n",
    "    result = convolve2d(test_image, edge_filter, stride=stride, padding=padding)\n",
    "\n",
    "    # Show result\n",
    "    axes[0, idx].imshow(result, cmap=\"RdBu\")\n",
    "    axes[0, idx].set_title(\n",
    "        f\"{title}\\nOutput: {result.shape[0]}Ã—{result.shape[1]}\", fontsize=11, fontweight=\"bold\"\n",
    "    )\n",
    "    axes[0, idx].axis(\"off\")\n",
    "\n",
    "    # Show formula\n",
    "    input_size = test_image.shape[0]\n",
    "    filter_size = edge_filter.shape[0]\n",
    "    output_size = (input_size - filter_size + 2 * padding) // stride + 1\n",
    "\n",
    "    formula_text = f\"Output = (I - F + 2P) / S + 1\\n\"\n",
    "    formula_text += f\"= ({input_size} - {filter_size} + 2Ã—{padding}) / {stride} + 1\\n\"\n",
    "    formula_text += f\"= {output_size}Ã—{output_size}\"\n",
    "\n",
    "    axes[1, idx].text(\n",
    "        0.5,\n",
    "        0.5,\n",
    "        formula_text,\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        fontsize=10,\n",
    "        bbox=dict(boxstyle=\"round\", facecolor=\"lightyellow\"),\n",
    "        family=\"monospace\",\n",
    "    )\n",
    "    axes[1, idx].set_xlim(0, 1)\n",
    "    axes[1, idx].set_ylim(0, 1)\n",
    "    axes[1, idx].set_title(\"Formula\", fontsize=11, fontweight=\"bold\")\n",
    "    axes[1, idx].axis(\"off\")\n",
    "\n",
    "# Explanation\n",
    "explanation = \"Stride: Controls downsampling\\n\\n\"\n",
    "explanation += \"Padding: Preserves border info\\n\\n\"\n",
    "explanation += \"I=Input, F=Filter,\\nP=Padding, S=Stride\"\n",
    "axes[1, 0].text(\n",
    "    0.5,\n",
    "    0.5,\n",
    "    explanation,\n",
    "    ha=\"center\",\n",
    "    va=\"center\",\n",
    "    fontsize=11,\n",
    "    bbox=dict(boxstyle=\"round\", facecolor=\"lightblue\"),\n",
    ")\n",
    "axes[1, 0].set_xlim(0, 1)\n",
    "axes[1, 0].set_ylim(0, 1)\n",
    "axes[1, 0].axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Stride and Padding Control Output Dimensions\", fontsize=14, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ“ Output size formula: (Input - Filter + 2Ã—Padding) / Stride + 1\")\n",
    "print(f\"  Stride=1, Pad=0: 8Ã—8 â†’ 6Ã—6\")\n",
    "print(f\"  Stride=2, Pad=0: 8Ã—8 â†’ 3Ã—3 (downsampled)\")\n",
    "print(f\"  Stride=1, Pad=1: 8Ã—8 â†’ 8Ã—8 (same size)\")\n",
    "\n",
    "# Real-world example with MNIST\n",
    "print(\"\\n4. MULTIPLE FILTERS ON REAL DATA\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Load multiple digits\n",
    "sample_indices = [0, 10, 20, 30]\n",
    "\n",
    "fig, axes = plt.subplots(len(sample_indices), 5, figsize=(18, 14))\n",
    "\n",
    "for row_idx, digit_idx in enumerate(sample_indices):\n",
    "    digit_img = digits.images[digit_idx]\n",
    "    digit_label = y_digits[digit_idx]\n",
    "\n",
    "    # Original\n",
    "    axes[row_idx, 0].imshow(digit_img, cmap=\"gray\")\n",
    "    axes[row_idx, 0].set_title(\n",
    "        f\"Digit {digit_label}\" if row_idx == 0 else \"\", fontsize=11, fontweight=\"bold\"\n",
    "    )\n",
    "    if row_idx == 0:\n",
    "        axes[row_idx, 0].set_ylabel(\"Original\", fontsize=11, fontweight=\"bold\")\n",
    "    axes[row_idx, 0].set_xticks([])\n",
    "    axes[row_idx, 0].set_yticks([])\n",
    "\n",
    "    # Apply 4 filters\n",
    "    filter_names = [\"Vertical Edge\", \"Horizontal Edge\", \"Blur\", \"Sharpen\"]\n",
    "    for col_idx, filter_name in enumerate(filter_names, 1):\n",
    "        feature_map = convolve2d(digit_img, filters[filter_name], stride=1, padding=0)\n",
    "\n",
    "        axes[row_idx, col_idx].imshow(feature_map, cmap=\"RdBu\")\n",
    "        if row_idx == 0:\n",
    "            axes[row_idx, col_idx].set_title(filter_name, fontsize=11, fontweight=\"bold\")\n",
    "        axes[row_idx, col_idx].set_xticks([])\n",
    "        axes[row_idx, col_idx].set_yticks([])\n",
    "\n",
    "plt.suptitle(\n",
    "    \"Multiple Filters Extract Different Features from Same Image\", fontsize=14, fontweight=\"bold\"\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ“ Applied 4 filters to 4 different digit images\")\n",
    "print(f\"  Each filter detects different patterns\")\n",
    "print(f\"  â€¢ Vertical Edge: Detects vertical boundaries\")\n",
    "print(f\"  â€¢ Horizontal Edge: Detects horizontal boundaries\")\n",
    "print(f\"  â€¢ Blur: Smooths the image\")\n",
    "print(f\"  â€¢ Sharpen: Enhances edges\")\n",
    "\n",
    "# Parameter comparison\n",
    "print(\"\\n5. PARAMETER EFFICIENCY: CNN vs FULLY CONNECTED\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Calculate parameters\n",
    "image_size = 28 * 28  # MNIST full size\n",
    "hidden_units = 100\n",
    "\n",
    "# Fully connected\n",
    "fc_params = image_size * hidden_units\n",
    "print(f\"\\nFully Connected Layer:\")\n",
    "print(f\"  Input: {image_size} (28Ã—28 image)\")\n",
    "print(f\"  Hidden: {hidden_units} neurons\")\n",
    "print(f\"  Parameters: {image_size} Ã— {hidden_units} = {fc_params:,}\")\n",
    "\n",
    "# Convolutional\n",
    "num_filters = 32\n",
    "filter_size = 3\n",
    "conv_params = num_filters * (filter_size * filter_size + 1)  # +1 for bias\n",
    "print(f\"\\nConvolutional Layer:\")\n",
    "print(f\"  Filters: {num_filters}\")\n",
    "print(f\"  Filter size: {filter_size}Ã—{filter_size}\")\n",
    "print(f\"  Parameters: {num_filters} Ã— ({filter_size}Ã—{filter_size} + 1) = {conv_params}\")\n",
    "\n",
    "reduction = fc_params / conv_params\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"PARAMETER REDUCTION: {reduction:.0f}Ã— fewer parameters!\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Visualize comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "categories = [\"Fully Connected\", \"Convolutional\"]\n",
    "params = [fc_params, conv_params]\n",
    "colors = [\"#ff6b6b\", \"#4ecdc4\"]\n",
    "\n",
    "bars = ax.bar(categories, params, color=colors, edgecolor=\"black\", linewidth=2)\n",
    "\n",
    "# Add value labels\n",
    "for bar, param in zip(bars, params):\n",
    "    height = bar.get_height()\n",
    "    ax.text(\n",
    "        bar.get_x() + bar.get_width() / 2.0,\n",
    "        height,\n",
    "        f\"{param:,}\\nparameters\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=12,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "ax.set_ylabel(\"Number of Parameters\", fontsize=13, fontweight=\"bold\")\n",
    "ax.set_title(\n",
    "    f\"Parameter Efficiency: CNN vs Fully Connected\\n(CNN uses {reduction:.0f}Ã— fewer parameters!)\",\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "ax.set_ylim(0, fc_params * 1.2)\n",
    "ax.grid(axis=\"y\", alpha=0.3, linestyle=\"--\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Convolution operation demonstrated!\")\n",
    "print(\"  â€¢ Filters slide across image to detect patterns\")\n",
    "print(\"  â€¢ Different filters detect different features\")\n",
    "print(\"  â€¢ Stride and padding control output dimensions\")\n",
    "print(\"  â€¢ Massive parameter reduction vs fully connected layers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pooling and Padding\n",
    "\n",
    "After convolution, we often apply **pooling** to reduce spatial dimensions and **padding** to control output size.\n",
    "\n",
    "### What is Pooling?\n",
    "\n",
    "**Pooling** (downsampling) reduces the size of feature maps while retaining important information.\n",
    "\n",
    "**Purpose:**\n",
    "1. Reduce computational cost (fewer parameters in next layer)\n",
    "2. Make features more robust to small translations\n",
    "3. Increase receptive field (neurons see larger regions)\n",
    "4. Prevent overfitting\n",
    "\n",
    "### Max Pooling\n",
    "\n",
    "**Most common type**: Take the maximum value in each region.\n",
    "\n",
    "**Example (2Ã—2 Max Pooling):**\n",
    "```\n",
    "Input (4Ã—4)          Output (2Ã—2)\n",
    "[1  3  2  4]         [3  4]\n",
    "[5  6  1  2]    â†’    [8  7]\n",
    "[7  8  3  1]\n",
    "[4  2  7  5]\n",
    "```\n",
    "\n",
    "**How it works:**\n",
    "- Slide a 2Ã—2 window with stride=2\n",
    "- At each position, take max value\n",
    "- Output: 1/4 the size (16 â†’ 4 pixels)\n",
    "\n",
    "### Average Pooling\n",
    "\n",
    "Take the average value instead of max.\n",
    "\n",
    "**When to use:**\n",
    "- Max Pooling: Most common, works well for CNNs\n",
    "- Average Pooling: Sometimes used in later layers, preserves more info\n",
    "\n",
    "### Common Pooling Configurations\n",
    "\n",
    "**2Ã—2 pooling, stride=2**: Most common (halves dimensions)\n",
    "**3Ã—3 pooling, stride=2**: More aggressive downsampling\n",
    "\n",
    "### Pooling Effects\n",
    "\n",
    "**Before pooling**: 224Ã—224Ã—64 feature maps\n",
    "**After 2Ã—2 pooling**: 112Ã—112Ã—64 feature maps\n",
    "**Result**: 75% reduction in size! (50,176 â†’ 12,544 pixels per channel)\n",
    "\n",
    "### What is Padding?\n",
    "\n",
    "**Padding** adds border pixels (usually zeros) around the image.\n",
    "\n",
    "**Types:**\n",
    "\n",
    "**Valid Padding** (no padding):\n",
    "- Output smaller than input\n",
    "- Border info lost\n",
    "\n",
    "**Same Padding** (zero padding):\n",
    "- Output same size as input (with stride=1)\n",
    "- Preserves border information\n",
    "- Most common in modern CNNs\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Original (3Ã—3)     Padded (5Ã—5)\n",
    "[1 2 3]           [0 0 0 0 0]\n",
    "[4 5 6]     â†’     [0 1 2 3 0]\n",
    "[7 8 9]           [0 4 5 6 0]\n",
    "                  [0 7 8 9 0]\n",
    "                  [0 0 0 0 0]\n",
    "```\n",
    "\n",
    "### Why Padding Matters\n",
    "\n",
    "**Without padding:**\n",
    "- Output shrinks with each layer\n",
    "- Border pixels processed less\n",
    "- Deep networks lose too much spatial info\n",
    "\n",
    "**With padding:**\n",
    "- Maintain spatial dimensions\n",
    "- All pixels processed equally\n",
    "- Build deeper networks\n",
    "\n",
    "### Output Size with Padding & Pooling\n",
    "\n",
    "**After Convolution:**\n",
    "```\n",
    "Output = (Input - Filter + 2Ã—Padding) / Stride + 1\n",
    "```\n",
    "\n",
    "**After Pooling:**\n",
    "```\n",
    "Output = Input / Pool_size (typically)\n",
    "```\n",
    "\n",
    "**Example Pipeline:**\n",
    "```\n",
    "Input: 224Ã—224\n",
    "Conv (3Ã—3, pad=1, stride=1): 224Ã—224\n",
    "Max Pool (2Ã—2, stride=2): 112Ã—112\n",
    "Conv (3Ã—3, pad=1, stride=1): 112Ã—112\n",
    "Max Pool (2Ã—2, stride=2): 56Ã—56\n",
    "```\n",
    "\n",
    "### Typical CNN Pattern\n",
    "\n",
    "**CONV â†’ ReLU â†’ POOL â†’ CONV â†’ ReLU â†’ POOL â†’ FC**\n",
    "\n",
    "Each CONV-POOL pair:\n",
    "- CONV: Detects features\n",
    "- ReLU: Non-linearity\n",
    "- POOL: Reduces dimensions\n",
    "\n",
    "### Pooling Advantages & Disadvantages\n",
    "\n",
    "**Advantages:**\n",
    "- âœ“ Reduces computation\n",
    "- âœ“ Translation invariance\n",
    "- âœ“ Reduces overfitting\n",
    "- âœ“ Increases receptive field\n",
    "\n",
    "**Disadvantages:**\n",
    "- âœ— Loses spatial information\n",
    "- âœ— Not learnable (fixed operation)\n",
    "- âœ— May discard useful details\n",
    "\n",
    "**Modern trend:** Some architectures avoid pooling, use strided convolutions instead!\n",
    "\n",
    "Let's visualize pooling and padding operations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pooling and Padding - Demonstrations\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"POOLING AND PADDING OPERATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# Implement pooling operations\n",
    "def max_pool2d(image, pool_size=2, stride=2):\n",
    "    \"\"\"Max pooling operation.\"\"\"\n",
    "    h, w = image.shape\n",
    "    out_h = (h - pool_size) // stride + 1\n",
    "    out_w = (w - pool_size) // stride + 1\n",
    "\n",
    "    output = np.zeros((out_h, out_w))\n",
    "\n",
    "    for i in range(out_h):\n",
    "        for j in range(out_w):\n",
    "            region = image[i * stride : i * stride + pool_size, j * stride : j * stride + pool_size]\n",
    "            output[i, j] = np.max(region)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def avg_pool2d(image, pool_size=2, stride=2):\n",
    "    \"\"\"Average pooling operation.\"\"\"\n",
    "    h, w = image.shape\n",
    "    out_h = (h - pool_size) // stride + 1\n",
    "    out_w = (w - pool_size) // stride + 1\n",
    "\n",
    "    output = np.zeros((out_h, out_w))\n",
    "\n",
    "    for i in range(out_h):\n",
    "        for j in range(out_w):\n",
    "            region = image[i * stride : i * stride + pool_size, j * stride : j * stride + pool_size]\n",
    "            output[i, j] = np.mean(region)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "print(\"\\n1. MAX POOLING vs AVERAGE POOLING\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Use a digit for demonstration\n",
    "test_digit = digits.images[8]  # Pick an interesting digit\n",
    "\n",
    "# Apply pooling\n",
    "max_pooled = max_pool2d(test_digit, pool_size=2, stride=2)\n",
    "avg_pooled = avg_pool2d(test_digit, pool_size=2, stride=2)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Original\n",
    "axes[0].imshow(test_digit, cmap=\"gray\")\n",
    "axes[0].set_title(\n",
    "    f\"Original Image\\n{test_digit.shape[0]}Ã—{test_digit.shape[1]} pixels\",\n",
    "    fontsize=12,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "# Max pooled\n",
    "axes[1].imshow(max_pooled, cmap=\"gray\")\n",
    "axes[1].set_title(\n",
    "    f\"Max Pooling (2Ã—2)\\n{max_pooled.shape[0]}Ã—{max_pooled.shape[1]} pixels\\n(75% size reduction)\",\n",
    "    fontsize=12,\n",
    "    fontweight=\"bold\",\n",
    "    color=\"red\",\n",
    ")\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "# Average pooled\n",
    "axes[2].imshow(avg_pooled, cmap=\"gray\")\n",
    "axes[2].set_title(\n",
    "    f\"Average Pooling (2Ã—2)\\n{avg_pooled.shape[0]}Ã—{avg_pooled.shape[1]} pixels\\n(75% size reduction)\",\n",
    "    fontsize=12,\n",
    "    fontweight=\"bold\",\n",
    "    color=\"blue\",\n",
    ")\n",
    "axes[2].axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Pooling: Downsampling Feature Maps\", fontsize=14, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ“ Original: {test_digit.shape} â†’ Pooled: {max_pooled.shape}\")\n",
    "print(f\"  Size reduction: {(1 - max_pooled.size / test_digit.size) * 100:.1f}%\")\n",
    "print(f\"  Max pooling preserves strong activations\")\n",
    "print(f\"  Avg pooling smooths the feature map\")\n",
    "\n",
    "# Step-by-step max pooling\n",
    "print(\"\\n2. MAX POOLING STEP-BY-STEP\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Create a simple example for clarity\n",
    "simple_image = np.array(\n",
    "    [\n",
    "        [1, 3, 2, 4, 1, 2],\n",
    "        [5, 6, 1, 2, 3, 1],\n",
    "        [7, 8, 3, 1, 2, 4],\n",
    "        [4, 2, 7, 5, 1, 3],\n",
    "        [3, 1, 4, 6, 2, 5],\n",
    "        [2, 5, 1, 3, 7, 4],\n",
    "    ],\n",
    "    dtype=float,\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(18, 9))\n",
    "\n",
    "# Original image\n",
    "axes[0, 0].imshow(simple_image, cmap=\"viridis\", vmin=0, vmax=8)\n",
    "axes[0, 0].set_title(\"Original\\n6Ã—6 Image\", fontsize=11, fontweight=\"bold\")\n",
    "for i in range(6):\n",
    "    for j in range(6):\n",
    "        axes[0, 0].text(\n",
    "            j,\n",
    "            i,\n",
    "            int(simple_image[i, j]),\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            color=\"white\",\n",
    "            fontsize=10,\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "axes[0, 0].set_xticks([])\n",
    "axes[0, 0].set_yticks([])\n",
    "\n",
    "# Draw grid lines\n",
    "for i in range(0, 7, 2):\n",
    "    axes[0, 0].axhline(i - 0.5, color=\"red\", linewidth=2)\n",
    "    axes[0, 0].axvline(i - 0.5, color=\"red\", linewidth=2)\n",
    "\n",
    "# Show 3 pooling steps\n",
    "positions = [(0, 0), (0, 2), (2, 2)]\n",
    "for idx, (row, col) in enumerate(positions, 1):\n",
    "    # Highlight region\n",
    "    axes[0, idx].imshow(simple_image, cmap=\"viridis\", alpha=0.3, vmin=0, vmax=8)\n",
    "\n",
    "    # Draw all values\n",
    "    for i in range(6):\n",
    "        for j in range(6):\n",
    "            axes[0, idx].text(\n",
    "                j, i, int(simple_image[i, j]), ha=\"center\", va=\"center\", color=\"gray\", fontsize=9\n",
    "            )\n",
    "\n",
    "    # Highlight current 2Ã—2 region\n",
    "    from matplotlib.patches import Rectangle\n",
    "\n",
    "    rect = Rectangle((col - 0.5, row - 0.5), 2, 2, linewidth=4, edgecolor=\"red\", facecolor=\"none\")\n",
    "    axes[0, idx].add_patch(rect)\n",
    "\n",
    "    # Highlight values in region\n",
    "    region = simple_image[row : row + 2, col : col + 2]\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            axes[0, idx].text(\n",
    "                col + j,\n",
    "                row + i,\n",
    "                int(region[i, j]),\n",
    "                ha=\"center\",\n",
    "                va=\"center\",\n",
    "                color=\"red\",\n",
    "                fontsize=12,\n",
    "                fontweight=\"bold\",\n",
    "            )\n",
    "\n",
    "    max_val = np.max(region)\n",
    "    axes[0, idx].set_title(\n",
    "        f\"Step {idx}\\nRegion ({row}:{row+2}, {col}:{col+2})\\nMax = {int(max_val)}\",\n",
    "        fontsize=11,\n",
    "        fontweight=\"bold\",\n",
    "        color=\"red\",\n",
    "    )\n",
    "    axes[0, idx].set_xticks([])\n",
    "    axes[0, idx].set_yticks([])\n",
    "\n",
    "# Final result\n",
    "pooled_result = max_pool2d(simple_image, pool_size=2, stride=2)\n",
    "axes[1, 0].imshow(pooled_result, cmap=\"viridis\", vmin=0, vmax=8)\n",
    "axes[1, 0].set_title(\"Pooled Output\\n3Ã—3 Result\", fontsize=11, fontweight=\"bold\")\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        axes[1, 0].text(\n",
    "            j,\n",
    "            i,\n",
    "            int(pooled_result[i, j]),\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            color=\"white\",\n",
    "            fontsize=12,\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "axes[1, 0].set_xticks([])\n",
    "axes[1, 0].set_yticks([])\n",
    "\n",
    "# Show the 3 extracted maxima\n",
    "for idx in range(1, 4):\n",
    "    row, col = positions[idx - 1]\n",
    "    region = simple_image[row : row + 2, col : col + 2]\n",
    "    max_val = np.max(region)\n",
    "\n",
    "    axes[1, idx].text(\n",
    "        0.5,\n",
    "        0.5,\n",
    "        f\"Position ({row//2},{col//2})\\n\\n\"\n",
    "        f\"2Ã—2 Region:\\n{region.astype(int).tolist()}\\n\\n\"\n",
    "        f\"Max = {int(max_val)}\",\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        fontsize=10,\n",
    "        bbox=dict(boxstyle=\"round\", facecolor=\"lightcoral\"),\n",
    "        family=\"monospace\",\n",
    "    )\n",
    "    axes[1, idx].set_xlim(0, 1)\n",
    "    axes[1, idx].set_ylim(0, 1)\n",
    "    axes[1, idx].axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Max Pooling: Take Maximum from Each Region\", fontsize=14, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ“ Max pooling with 2Ã—2 filter, stride=2\")\n",
    "print(f\"  Input: 6Ã—6 â†’ Output: 3Ã—3\")\n",
    "print(f\"  Each output pixel = max of 2Ã—2 input region\")\n",
    "\n",
    "# Padding demonstration\n",
    "print(\"\\n3. PADDING DEMONSTRATION\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Use a small image for clarity\n",
    "small_img = digits.images[0][:5, :5]  # 5Ã—5 region\n",
    "\n",
    "# Different padding amounts\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "padding_amounts = [0, 1, 2, 3]\n",
    "\n",
    "for idx, pad in enumerate(padding_amounts):\n",
    "    # Apply padding\n",
    "    if pad > 0:\n",
    "        padded = np.pad(small_img, pad, mode=\"constant\", constant_values=0)\n",
    "    else:\n",
    "        padded = small_img.copy()\n",
    "\n",
    "    # Show padded image\n",
    "    axes[0, idx].imshow(padded, cmap=\"gray\")\n",
    "    axes[0, idx].set_title(\n",
    "        f\"Padding = {pad}\\nSize: {padded.shape[0]}Ã—{padded.shape[1]}\",\n",
    "        fontsize=11,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "    axes[0, idx].axis(\"off\")\n",
    "\n",
    "    # Highlight original region if padded\n",
    "    if pad > 0:\n",
    "        from matplotlib.patches import Rectangle\n",
    "\n",
    "        rect = Rectangle(\n",
    "            (pad - 0.5, pad - 0.5),\n",
    "            small_img.shape[1],\n",
    "            small_img.shape[0],\n",
    "            linewidth=3,\n",
    "            edgecolor=\"red\",\n",
    "            facecolor=\"none\",\n",
    "        )\n",
    "        axes[0, idx].add_patch(rect)\n",
    "\n",
    "    # Show formula\n",
    "    original_size = small_img.shape[0]\n",
    "    new_size = padded.shape[0]\n",
    "    formula = f\"Padded Size = Original + 2Ã—Padding\\n\"\n",
    "    formula += f\"= {original_size} + 2Ã—{pad}\\n\"\n",
    "    formula += f\"= {new_size}Ã—{new_size}\"\n",
    "\n",
    "    axes[1, idx].text(\n",
    "        0.5,\n",
    "        0.5,\n",
    "        formula,\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        fontsize=10,\n",
    "        bbox=dict(boxstyle=\"round\", facecolor=\"lightyellow\"),\n",
    "        family=\"monospace\",\n",
    "    )\n",
    "    axes[1, idx].set_xlim(0, 1)\n",
    "    axes[1, idx].set_ylim(0, 1)\n",
    "    axes[1, idx].axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Padding: Add Zeros Around Border (shown in red box)\", fontsize=14, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ“ Padding adds border pixels (usually zeros)\")\n",
    "print(f\"  Padding=0: {small_img.shape} (no change)\")\n",
    "print(f\"  Padding=1: {np.pad(small_img, 1, 'constant').shape}\")\n",
    "print(f\"  Padding=2: {np.pad(small_img, 2, 'constant').shape}\")\n",
    "\n",
    "# Effect of padding on convolution output size\n",
    "print(\"\\n4. PADDING EFFECT ON OUTPUT SIZE\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "test_img = digits.images[3]\n",
    "kernel_3x3 = filters[\"Vertical Edge\"]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "padding_configs = [(\"Valid (Pad=0)\", 0), (\"Same (Pad=1)\", 1), (\"Full (Pad=2)\", 2)]\n",
    "\n",
    "for idx, (name, pad) in enumerate(padding_configs):\n",
    "    # Apply convolution with padding\n",
    "    conv_result = convolve2d(test_img, kernel_3x3, stride=1, padding=pad)\n",
    "\n",
    "    # Show result\n",
    "    axes[0, idx].imshow(conv_result, cmap=\"RdBu\")\n",
    "\n",
    "    size_change = \"Same size!\" if conv_result.shape == test_img.shape else \"Size changed\"\n",
    "    axes[0, idx].set_title(\n",
    "        f\"{name}\\nInput: {test_img.shape[0]}Ã—{test_img.shape[1]} â†’ Output: {conv_result.shape[0]}Ã—{conv_result.shape[1]}\\n{size_change}\",\n",
    "        fontsize=11,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "    axes[0, idx].axis(\"off\")\n",
    "\n",
    "    # Show explanation\n",
    "    formula = f\"Output Size Formula:\\n\"\n",
    "    formula += f\"(Input - Kernel + 2Ã—Pad) / Stride + 1\\n\\n\"\n",
    "    formula += f\"({test_img.shape[0]} - 3 + 2Ã—{pad}) / 1 + 1\\n\"\n",
    "    formula += f\"= {conv_result.shape[0]}Ã—{conv_result.shape[1]}\\n\\n\"\n",
    "\n",
    "    if pad == 0:\n",
    "        desc = \"No padding\\nâ€¢ Output shrinks\\nâ€¢ Border info lost\"\n",
    "    elif pad == 1:\n",
    "        desc = \"'Same' padding\\nâ€¢ Output = Input\\nâ€¢ Preserves dimensions\"\n",
    "    else:\n",
    "        desc = \"Extra padding\\nâ€¢ Output larger\\nâ€¢ Adds extra border\"\n",
    "\n",
    "    axes[1, idx].text(\n",
    "        0.5,\n",
    "        0.5,\n",
    "        formula + desc,\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        fontsize=10,\n",
    "        bbox=dict(boxstyle=\"round\", facecolor=\"lightblue\"),\n",
    "        family=\"monospace\",\n",
    "    )\n",
    "    axes[1, idx].set_xlim(0, 1)\n",
    "    axes[1, idx].set_ylim(0, 1)\n",
    "    axes[1, idx].axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Padding Controls Output Dimensions After Convolution\", fontsize=14, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ“ Padding strategy affects network depth\")\n",
    "print(f\"  Valid (pad=0): Output shrinks, limits network depth\")\n",
    "print(f\"  Same (pad=1): Output = Input, enables deep networks\")\n",
    "\n",
    "# Full CNN pipeline visualization\n",
    "print(\"\\n5. COMPLETE CNN LAYER PIPELINE\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Start with original image\n",
    "original = digits.images[5]\n",
    "\n",
    "# Apply conv + pooling pipeline\n",
    "conv_out = convolve2d(original, filters[\"Vertical Edge\"], stride=1, padding=0)\n",
    "relu_out = np.maximum(0, conv_out)  # ReLU activation\n",
    "pool_out = max_pool2d(relu_out, pool_size=2, stride=2)\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 5))\n",
    "\n",
    "# Step 1: Original\n",
    "axes[0].imshow(original, cmap=\"gray\")\n",
    "axes[0].set_title(\n",
    "    f\"1. Input Image\\n{original.shape[0]}Ã—{original.shape[1]}\", fontsize=11, fontweight=\"bold\"\n",
    ")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "# Arrow\n",
    "axes[0].text(\n",
    "    1.1,\n",
    "    0.5,\n",
    "    \"â†’\\nConv\\n3Ã—3\",\n",
    "    transform=axes[0].transAxes,\n",
    "    fontsize=10,\n",
    "    ha=\"center\",\n",
    "    va=\"center\",\n",
    "    bbox=dict(boxstyle=\"round\", facecolor=\"lightgreen\"),\n",
    ")\n",
    "\n",
    "# Step 2: After Convolution\n",
    "axes[1].imshow(conv_out, cmap=\"RdBu\")\n",
    "axes[1].set_title(\n",
    "    f\"2. After Conv\\n{conv_out.shape[0]}Ã—{conv_out.shape[1]}\", fontsize=11, fontweight=\"bold\"\n",
    ")\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "axes[1].text(\n",
    "    1.1,\n",
    "    0.5,\n",
    "    \"â†’\\nReLU\",\n",
    "    transform=axes[1].transAxes,\n",
    "    fontsize=10,\n",
    "    ha=\"center\",\n",
    "    va=\"center\",\n",
    "    bbox=dict(boxstyle=\"round\", facecolor=\"lightyellow\"),\n",
    ")\n",
    "\n",
    "# Step 3: After ReLU\n",
    "axes[2].imshow(relu_out, cmap=\"RdBu\")\n",
    "axes[2].set_title(\n",
    "    f\"3. After ReLU\\n{relu_out.shape[0]}Ã—{relu_out.shape[1]}\\n(negative â†’ 0)\",\n",
    "    fontsize=11,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "axes[2].axis(\"off\")\n",
    "\n",
    "axes[2].text(\n",
    "    1.1,\n",
    "    0.5,\n",
    "    \"â†’\\nPool\\n2Ã—2\",\n",
    "    transform=axes[2].transAxes,\n",
    "    fontsize=10,\n",
    "    ha=\"center\",\n",
    "    va=\"center\",\n",
    "    bbox=dict(boxstyle=\"round\", facecolor=\"lightcoral\"),\n",
    ")\n",
    "\n",
    "# Step 4: After Pooling\n",
    "axes[3].imshow(pool_out, cmap=\"RdBu\")\n",
    "axes[3].set_title(\n",
    "    f\"4. After MaxPool\\n{pool_out.shape[0]}Ã—{pool_out.shape[1]}\\n(50% size)\",\n",
    "    fontsize=11,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "axes[3].axis(\"off\")\n",
    "\n",
    "# Summary\n",
    "axes[4].text(\n",
    "    0.5,\n",
    "    0.5,\n",
    "    \"Complete Layer:\\n\\n\"\n",
    "    \"8Ã—8 â†’ Conv â†’ 6Ã—6\\n\"\n",
    "    \"â†“ ReLU (no size change)\\n\"\n",
    "    \"6Ã—6 â†’ Pool â†’ 3Ã—3\\n\\n\"\n",
    "    f\"Total reduction:\\n\"\n",
    "    f\"{original.size} â†’ {pool_out.size} pixels\\n\"\n",
    "    f\"({(1-pool_out.size/original.size)*100:.0f}% smaller)\",\n",
    "    ha=\"center\",\n",
    "    va=\"center\",\n",
    "    fontsize=11,\n",
    "    bbox=dict(boxstyle=\"round\", facecolor=\"lightblue\"),\n",
    "    family=\"monospace\",\n",
    ")\n",
    "axes[4].set_xlim(0, 1)\n",
    "axes[4].set_ylim(0, 1)\n",
    "axes[4].axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Typical CNN Layer: CONV â†’ ReLU â†’ POOL\", fontsize=14, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ“ Complete CNN layer pipeline demonstrated!\")\n",
    "print(f\"  1. Convolution: Detect features (8Ã—8 â†’ 6Ã—6)\")\n",
    "print(f\"  2. ReLU: Non-linearity (6Ã—6 â†’ 6Ã—6)\")\n",
    "print(f\"  3. MaxPool: Downsample (6Ã—6 â†’ 3Ã—3)\")\n",
    "print(\n",
    "    f\"  Total: {original.size} â†’ {pool_out.size} pixels ({(1-pool_out.size/original.size)*100:.0f}% reduction)\"\n",
    ")\n",
    "\n",
    "# Compare different pooling sizes\n",
    "print(\"\\n6. EFFECT OF DIFFERENT POOL SIZES\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "test_image = digits.images[7]\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 5))\n",
    "\n",
    "# Original\n",
    "axes[0].imshow(test_image, cmap=\"gray\")\n",
    "axes[0].set_title(\n",
    "    f\"Original\\n{test_image.shape[0]}Ã—{test_image.shape[1]}\", fontsize=11, fontweight=\"bold\"\n",
    ")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "# Different pool sizes\n",
    "pool_configs = [(2, 2), (2, 4), (4, 4)]\n",
    "for idx, (pool_size, stride) in enumerate(pool_configs, 1):\n",
    "    pooled = max_pool2d(test_image, pool_size=pool_size, stride=stride)\n",
    "\n",
    "    axes[idx].imshow(pooled, cmap=\"gray\")\n",
    "    reduction = (1 - pooled.size / test_image.size) * 100\n",
    "    axes[idx].set_title(\n",
    "        f\"Pool {pool_size}Ã—{pool_size}\\n{pooled.shape[0]}Ã—{pooled.shape[1]} ({reduction:.0f}% reduction)\",\n",
    "        fontsize=11,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "    axes[idx].axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Larger Pooling = More Aggressive Downsampling\", fontsize=14, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ“ Pooling size controls downsampling rate\")\n",
    "print(f\"  2Ã—2 pool: Most common (4Ã— reduction per layer)\")\n",
    "print(f\"  4Ã—4 pool: Aggressive (16Ã— reduction per layer)\")\n",
    "print(f\"  Trade-off: Computation vs information loss\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"POOLING & PADDING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(\"âœ“ Pooling reduces spatial dimensions (downsampling)\")\n",
    "print(\"  â€¢ Max pooling: Most common, preserves strong features\")\n",
    "print(\"  â€¢ Typically 2Ã—2 with stride=2 (50% reduction)\")\n",
    "print(\"\\nâœ“ Padding controls output size\")\n",
    "print(\"  â€¢ Valid (pad=0): Output shrinks\")\n",
    "print(\"  â€¢ Same (pad=1): Output = Input size\")\n",
    "print(\"  â€¢ Enables building deep networks\")\n",
    "print(\"\\nâœ“ Typical pattern: CONV â†’ ReLU â†’ POOL\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building CNN Architecture\n",
    "\n",
    "Now let's build a complete CNN from scratch and with Keras!\n",
    "\n",
    "### Typical CNN Architecture\n",
    "\n",
    "**Standard pattern:**\n",
    "```\n",
    "INPUT â†’ [CONV â†’ ReLU â†’ POOL] Ã— N â†’ FC â†’ FC â†’ OUTPUT\n",
    "```\n",
    "\n",
    "**Example (LeNet-5 style for MNIST):**\n",
    "```\n",
    "Input (28Ã—28Ã—1)\n",
    "    â†“\n",
    "Conv1: 32 filters (3Ã—3) â†’ 28Ã—28Ã—32\n",
    "ReLU â†’ 28Ã—28Ã—32\n",
    "MaxPool (2Ã—2) â†’ 14Ã—14Ã—32\n",
    "    â†“\n",
    "Conv2: 64 filters (3Ã—3) â†’ 14Ã—14Ã—64\n",
    "ReLU â†’ 14Ã—14Ã—64\n",
    "MaxPool (2Ã—2) â†’ 7Ã—7Ã—64\n",
    "    â†“\n",
    "Flatten â†’ 3136 neurons\n",
    "Dense1 â†’ 128 neurons\n",
    "ReLU\n",
    "Dropout (0.5)\n",
    "    â†“\n",
    "Dense2 (Output) â†’ 10 neurons (softmax)\n",
    "```\n",
    "\n",
    "### CNN Layer Types\n",
    "\n",
    "**1. Convolutional Layers (Feature Extraction)**\n",
    "- Detect patterns (edges, textures, objects)\n",
    "- Parameters: # filters, filter size, stride, padding\n",
    "- Example: `Conv2D(32, (3,3), padding='same')`\n",
    "\n",
    "**2. Activation Layers (Non-linearity)**\n",
    "- ReLU most common: `f(x) = max(0, x)`\n",
    "- Enables learning complex patterns\n",
    "- No parameters (fixed function)\n",
    "\n",
    "**3. Pooling Layers (Downsampling)**\n",
    "- Reduce spatial dimensions\n",
    "- MaxPooling most common\n",
    "- Example: `MaxPooling2D((2,2))`\n",
    "\n",
    "**4. Flatten Layer (Reshape)**\n",
    "- Convert 2D feature maps â†’ 1D vector\n",
    "- Example: 7Ã—7Ã—64 â†’ 3136 neurons\n",
    "- Required before fully connected layers\n",
    "\n",
    "**5. Dense (Fully Connected) Layers**\n",
    "- Traditional neural network layers\n",
    "- Final classification/regression\n",
    "- Example: `Dense(10, activation='softmax')`\n",
    "\n",
    "**6. Dropout (Regularization)**\n",
    "- Randomly drop neurons during training\n",
    "- Prevents overfitting\n",
    "- Example: `Dropout(0.5)` drops 50%\n",
    "\n",
    "### Famous CNN Architectures\n",
    "\n",
    "| Architecture | Year | Depth | Key Innovation |\n",
    "|--------------|------|-------|----------------|\n",
    "| **LeNet-5** | 1998 | 5 | First successful CNN (digits) |\n",
    "| **AlexNet** | 2012 | 8 | ReLU + GPU training (ImageNet winner) |\n",
    "| **VGGNet** | 2014 | 16-19 | Deeper with 3Ã—3 filters |\n",
    "| **GoogLeNet** | 2014 | 22 | Inception modules (1Ã—1 convs) |\n",
    "| **ResNet** | 2015 | 50-152 | Skip connections (residual learning) |\n",
    "| **MobileNet** | 2017 | - | Efficient for mobile devices |\n",
    "| **EfficientNet** | 2019 | - | Optimized scaling |\n",
    "\n",
    "### Calculating Parameters\n",
    "\n",
    "**Convolutional Layer:**\n",
    "```\n",
    "Params = (filter_height Ã— filter_width Ã— input_channels + 1) Ã— num_filters\n",
    "```\n",
    "\n",
    "**Example:**\n",
    "- Input: 28Ã—28Ã—1\n",
    "- Conv: 32 filters of 3Ã—3\n",
    "- Params = (3 Ã— 3 Ã— 1 + 1) Ã— 32 = **320 parameters**\n",
    "\n",
    "**Dense Layer:**\n",
    "```\n",
    "Params = (input_size + 1) Ã— output_size\n",
    "```\n",
    "\n",
    "**Example:**\n",
    "- Input: 3136 neurons\n",
    "- Output: 128 neurons\n",
    "- Params = (3136 + 1) Ã— 128 = **401,536 parameters**\n",
    "\n",
    "**Notice**: Most parameters are in Dense layers!\n",
    "\n",
    "### Receptive Field\n",
    "\n",
    "**Receptive field** = Region of input that affects one output neuron\n",
    "\n",
    "**Grows with depth:**\n",
    "- After Conv1 (3Ã—3): 3Ã—3 receptive field\n",
    "- After Pool1 (2Ã—2): 6Ã—6 receptive field\n",
    "- After Conv2 (3Ã—3): 10Ã—10 receptive field\n",
    "- After Pool2 (2Ã—2): 20Ã—20 receptive field\n",
    "\n",
    "**Deeper networks see more context!**\n",
    "\n",
    "### Design Principles\n",
    "\n",
    "**1. Start Small, Go Deep**\n",
    "- More layers better than fewer wide layers\n",
    "- 3Ã—3 filters are sweet spot\n",
    "\n",
    "**2. Double Filters, Halve Dimensions**\n",
    "- Conv1: 32 filters, 28Ã—28\n",
    "- Pool1: 32 filters, 14Ã—14\n",
    "- Conv2: 64 filters, 14Ã—14\n",
    "- Pool2: 64 filters, 7Ã—7\n",
    "\n",
    "**3. Use Batch Normalization**\n",
    "- Normalize activations between layers\n",
    "- Faster training, better performance\n",
    "\n",
    "**4. Add Dropout Before Dense Layers**\n",
    "- Prevent overfitting\n",
    "- Typically 0.25-0.5\n",
    "\n",
    "**5. Data Augmentation**\n",
    "- Rotate, flip, zoom images\n",
    "- Dramatically improves generalization\n",
    "\n",
    "### Common Mistakes\n",
    "\n",
    "âŒ **Too many parameters in Dense layers**\n",
    "â†’ Add more CONV/POOL, reduce Dense size\n",
    "\n",
    "âŒ **Too deep too fast**\n",
    "â†’ Gradually increase depth\n",
    "\n",
    "âŒ **No regularization**\n",
    "â†’ Add Dropout, L2 regularization\n",
    "\n",
    "âŒ **Wrong output activation**\n",
    "â†’ Binary: sigmoid, Multi-class: softmax\n",
    "\n",
    "Let's build CNNs from scratch and with Keras!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building CNN Architecture - Complete Implementation\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BUILDING CNN WITH KERAS/TENSORFLOW\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Import TensorFlow/Keras\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers, models\n",
    "    from tensorflow.keras.datasets import mnist\n",
    "    from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "    print(f\"\\nâœ“ TensorFlow {tf.__version__} loaded successfully!\")\n",
    "    has_tf = True\n",
    "except ImportError:\n",
    "    print(\"\\nâš  TensorFlow not available. Install with: pip install tensorflow\")\n",
    "    has_tf = False\n",
    "\n",
    "if has_tf:\n",
    "    # Load and prepare MNIST data\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"1. LOADING AND PREPARING MNIST DATA\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Load dataset\n",
    "    (X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "    # Normalize pixel values to [0, 1]\n",
    "    X_train_full = X_train_full.astype(\"float32\") / 255.0\n",
    "    X_test = X_test.astype(\"float32\") / 255.0\n",
    "\n",
    "    # Reshape for CNN (add channel dimension)\n",
    "    X_train_full = X_train_full.reshape(-1, 28, 28, 1)\n",
    "    X_test = X_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "    # Split into train and validation\n",
    "    X_train = X_train_full[:50000]\n",
    "    y_train = y_train_full[:50000]\n",
    "    X_val = X_train_full[50000:]\n",
    "    y_val = y_train_full[50000:]\n",
    "\n",
    "    # Convert labels to one-hot encoding\n",
    "    y_train_cat = to_categorical(y_train, 10)\n",
    "    y_val_cat = to_categorical(y_val, 10)\n",
    "    y_test_cat = to_categorical(y_test, 10)\n",
    "\n",
    "    print(f\"\\nâœ“ Data loaded and preprocessed:\")\n",
    "    print(f\"  Training: {X_train.shape}, Labels: {y_train_cat.shape}\")\n",
    "    print(f\"  Validation: {X_val.shape}, Labels: {y_val_cat.shape}\")\n",
    "    print(f\"  Test: {X_test.shape}, Labels: {y_test_cat.shape}\")\n",
    "    print(f\"  Pixel range: [{X_train.min():.1f}, {X_train.max():.1f}]\")\n",
    "    print(f\"  Classes: {np.unique(y_train).tolist()}\")\n",
    "\n",
    "    # Build CNN model\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"2. BUILDING CNN ARCHITECTURE\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    model = models.Sequential(\n",
    "        [\n",
    "            # First Conv Block\n",
    "            layers.Conv2D(\n",
    "                32, (3, 3), activation=\"relu\", padding=\"same\", input_shape=(28, 28, 1), name=\"conv1\"\n",
    "            ),\n",
    "            layers.MaxPooling2D((2, 2), name=\"pool1\"),\n",
    "            # Second Conv Block\n",
    "            layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\", name=\"conv2\"),\n",
    "            layers.MaxPooling2D((2, 2), name=\"pool2\"),\n",
    "            # Flatten and Dense layers\n",
    "            layers.Flatten(name=\"flatten\"),\n",
    "            layers.Dense(128, activation=\"relu\", name=\"dense1\"),\n",
    "            layers.Dropout(0.5, name=\"dropout\"),\n",
    "            layers.Dense(10, activation=\"softmax\", name=\"output\"),\n",
    "        ],\n",
    "        name=\"MNIST_CNN\",\n",
    "    )\n",
    "\n",
    "    # Display architecture\n",
    "    print(\"\\nâœ“ CNN Model Architecture:\")\n",
    "    print(\"-\" * 60)\n",
    "    model.summary()\n",
    "\n",
    "    # Visualize architecture with text diagram\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ARCHITECTURE VISUALIZATION\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    architecture_text = \"\"\"\n",
    "    Input Image (28Ã—28Ã—1)\n",
    "           â†“\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚ Conv2D (32 filters) â”‚ â†’ 28Ã—28Ã—32\n",
    "    â”‚ 3Ã—3, padding='same' â”‚\n",
    "    â”‚ ReLU activation     â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "           â†“\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚ MaxPooling2D (2Ã—2)  â”‚ â†’ 14Ã—14Ã—32\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "           â†“\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚ Conv2D (64 filters) â”‚ â†’ 14Ã—14Ã—64\n",
    "    â”‚ 3Ã—3, padding='same' â”‚\n",
    "    â”‚ ReLU activation     â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "           â†“\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚ MaxPooling2D (2Ã—2)  â”‚ â†’ 7Ã—7Ã—64\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "           â†“\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚ Flatten             â”‚ â†’ 3136\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "           â†“\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚ Dense (128)         â”‚ â†’ 128\n",
    "    â”‚ ReLU activation     â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "           â†“\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚ Dropout (0.5)       â”‚ â†’ 128\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "           â†“\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚ Dense (10)          â”‚ â†’ 10\n",
    "    â”‚ Softmax activation  â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "           â†“\n",
    "    Output Probabilities (10 classes)\n",
    "    \"\"\"\n",
    "    print(architecture_text)\n",
    "\n",
    "    # Count parameters by layer\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"PARAMETER BREAKDOWN\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    total_params = 0\n",
    "    for layer in model.layers:\n",
    "        params = layer.count_params()\n",
    "        total_params += params\n",
    "        print(f\"{layer.name:12s}: {params:>10,} parameters\")\n",
    "\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Total':12s}: {total_params:>10,} parameters\")\n",
    "\n",
    "    # Compile model\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"3. COMPILING MODEL\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    print(\"\\nâœ“ Model compiled with:\")\n",
    "    print(\"  Optimizer: Adam\")\n",
    "    print(\"  Loss: Categorical Crossentropy\")\n",
    "    print(\"  Metrics: Accuracy\")\n",
    "\n",
    "    # Train model\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"4. TRAINING MODEL\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\nTraining CNN on MNIST (this may take 1-2 minutes)...\")\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train_cat,\n",
    "        epochs=5,\n",
    "        batch_size=128,\n",
    "        validation_data=(X_val, y_val_cat),\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    # Evaluate on test set\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"5. EVALUATING MODEL\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test_cat, verbose=0)\n",
    "\n",
    "    print(f\"\\nâœ“ Test Results:\")\n",
    "    print(f\"  Loss: {test_loss:.4f}\")\n",
    "    print(f\"  Accuracy: {test_accuracy*100:.2f}%\")\n",
    "\n",
    "    # Visualize training history\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"6. TRAINING HISTORY VISUALIZATION\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    # Plot accuracy\n",
    "    axes[0].plot(history.history[\"accuracy\"], \"b-\", label=\"Training\", linewidth=2)\n",
    "    axes[0].plot(history.history[\"val_accuracy\"], \"r-\", label=\"Validation\", linewidth=2)\n",
    "    axes[0].set_title(\"Model Accuracy\", fontsize=13, fontweight=\"bold\")\n",
    "    axes[0].set_xlabel(\"Epoch\", fontsize=11)\n",
    "    axes[0].set_ylabel(\"Accuracy\", fontsize=11)\n",
    "    axes[0].legend(fontsize=10)\n",
    "    axes[0].grid(alpha=0.3)\n",
    "\n",
    "    # Plot loss\n",
    "    axes[1].plot(history.history[\"loss\"], \"b-\", label=\"Training\", linewidth=2)\n",
    "    axes[1].plot(history.history[\"val_loss\"], \"r-\", label=\"Validation\", linewidth=2)\n",
    "    axes[1].set_title(\"Model Loss\", fontsize=13, fontweight=\"bold\")\n",
    "    axes[1].set_xlabel(\"Epoch\", fontsize=11)\n",
    "    axes[1].set_ylabel(\"Loss\", fontsize=11)\n",
    "    axes[1].legend(fontsize=10)\n",
    "    axes[1].grid(alpha=0.3)\n",
    "\n",
    "    plt.suptitle(\"CNN Training Progress\", fontsize=14, fontweight=\"bold\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Make predictions\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"7. PREDICTION EXAMPLES\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Get predictions for first 12 test images\n",
    "    predictions = model.predict(X_test[:12], verbose=0)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    true_classes = y_test[:12]\n",
    "\n",
    "    fig, axes = plt.subplots(2, 6, figsize=(18, 6))\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    for i in range(12):\n",
    "        axes[i].imshow(X_test[i].reshape(28, 28), cmap=\"gray\")\n",
    "\n",
    "        pred_class = predicted_classes[i]\n",
    "        true_class = true_classes[i]\n",
    "        confidence = predictions[i][pred_class] * 100\n",
    "\n",
    "        # Color code: green if correct, red if wrong\n",
    "        color = \"green\" if pred_class == true_class else \"red\"\n",
    "\n",
    "        axes[i].set_title(\n",
    "            f\"Pred: {pred_class}\\nTrue: {true_class}\\n({confidence:.1f}%)\",\n",
    "            fontsize=10,\n",
    "            color=color,\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "        axes[i].axis(\"off\")\n",
    "\n",
    "    plt.suptitle(\"CNN Predictions (Green=Correct, Red=Wrong)\", fontsize=14, fontweight=\"bold\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    correct_predictions = np.sum(predicted_classes == true_classes)\n",
    "    print(f\"\\nâœ“ Predictions on 12 samples:\")\n",
    "    print(f\"  Correct: {correct_predictions}/12 ({correct_predictions/12*100:.1f}%)\")\n",
    "\n",
    "    # Visualize learned filters\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"8. VISUALIZING LEARNED FILTERS\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Get first convolutional layer weights\n",
    "    conv1_weights = model.layers[0].get_weights()[0]  # Shape: (3, 3, 1, 32)\n",
    "\n",
    "    # Visualize first 16 filters\n",
    "    fig, axes = plt.subplots(4, 8, figsize=(16, 8))\n",
    "\n",
    "    for i in range(16):\n",
    "        ax = axes[i // 8, i % 8]\n",
    "\n",
    "        # Extract filter (3Ã—3)\n",
    "        filter_img = conv1_weights[:, :, 0, i]\n",
    "\n",
    "        # Normalize for visualization\n",
    "        filter_img = (filter_img - filter_img.min()) / (filter_img.max() - filter_img.min() + 1e-8)\n",
    "\n",
    "        ax.imshow(filter_img, cmap=\"viridis\")\n",
    "        ax.set_title(f\"Filter {i+1}\", fontsize=9)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    # Hide unused subplots\n",
    "    for i in range(16, 32):\n",
    "        axes[i // 8, i % 8].axis(\"off\")\n",
    "\n",
    "    plt.suptitle(\"Learned Filters from First Conv Layer (3Ã—3 each)\", fontsize=14, fontweight=\"bold\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\nâœ“ Visualized 16 of 32 learned filters\")\n",
    "    print(f\"  Each filter: 3Ã—3 pixels\")\n",
    "    print(f\"  These filters detect different edge patterns!\")\n",
    "\n",
    "    # Feature map visualization\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"9. FEATURE MAP VISUALIZATION\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Create model that outputs intermediate layers\n",
    "    layer_outputs = [layer.output for layer in model.layers[:3]]  # conv1, pool1, conv2\n",
    "    activation_model = models.Model(inputs=model.input, outputs=layer_outputs)\n",
    "\n",
    "    # Get activations for one test image\n",
    "    test_img = X_test[0:1]  # First test image\n",
    "    activations = activation_model.predict(test_img, verbose=0)\n",
    "\n",
    "    # Visualize\n",
    "    layer_names = [\"Conv1 (32 filters)\", \"Pool1\", \"Conv2 (64 filters)\"]\n",
    "\n",
    "    for layer_idx, (activation, layer_name) in enumerate(zip(activations, layer_names)):\n",
    "        num_features = min(8, activation.shape[-1])  # Show 8 feature maps\n",
    "\n",
    "        fig, axes = plt.subplots(1, num_features + 1, figsize=(18, 3))\n",
    "\n",
    "        # Show original image\n",
    "        axes[0].imshow(test_img[0].reshape(28, 28), cmap=\"gray\")\n",
    "        axes[0].set_title(\"Input\", fontsize=10, fontweight=\"bold\")\n",
    "        axes[0].axis(\"off\")\n",
    "\n",
    "        # Show feature maps\n",
    "        for i in range(num_features):\n",
    "            axes[i + 1].imshow(activation[0, :, :, i], cmap=\"viridis\")\n",
    "            axes[i + 1].set_title(f\"Feature {i+1}\", fontsize=10)\n",
    "            axes[i + 1].axis(\"off\")\n",
    "\n",
    "        plt.suptitle(f\"{layer_name} - Feature Maps\", fontsize=12, fontweight=\"bold\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    print(f\"\\nâœ“ Feature maps visualized for 3 layers\")\n",
    "    print(f\"  Different filters detect different patterns\")\n",
    "    print(f\"  Early layers: edges, Late layers: complex patterns\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"CNN BUILDING SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"âœ“ Built CNN with {total_params:,} parameters\")\n",
    "    print(f\"âœ“ Trained for 5 epochs\")\n",
    "    print(f\"âœ“ Achieved {test_accuracy*100:.2f}% test accuracy\")\n",
    "    print(f\"âœ“ Architecture: Convâ†’Poolâ†’Convâ†’Poolâ†’Denseâ†’Output\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "else:\n",
    "    print(\"\\nSkipping CNN building (TensorFlow not available)\")\n",
    "    print(\"Install TensorFlow to run this section:\")\n",
    "    print(\"  pip install tensorflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Transfer Learning with Pre-trained Models\n",
    "\n",
    "**Transfer Learning**: Use models pre-trained on large datasets for your own tasks!\n",
    "\n",
    "### What is Transfer Learning?\n",
    "\n",
    "Instead of training from scratch, start with a model already trained on millions of images.\n",
    "\n",
    "**Analogy**: Like hiring an experienced artist instead of teaching someone from scratch.\n",
    "\n",
    "### Why Transfer Learning?\n",
    "\n",
    "**Advantages:**\n",
    "1. **Less data needed**: Works with small datasets (100s instead of millions)\n",
    "2. **Faster training**: Hours instead of days/weeks\n",
    "3. **Better performance**: Pre-trained features are powerful\n",
    "4. **Lower cost**: No need for expensive GPUs for weeks\n",
    "\n",
    "**When to use:**\n",
    "- âœ… Small dataset (< 10,000 images)\n",
    "- âœ… Limited computational resources\n",
    "- âœ… Similar to ImageNet classes (objects, animals, scenes)\n",
    "- âœ… Need quick prototype\n",
    "\n",
    "### Popular Pre-trained Models\n",
    "\n",
    "| Model | Year | Size | ImageNet Accuracy | Parameters |\n",
    "|-------|------|------|-------------------|------------|\n",
    "| **VGG16** | 2014 | 528 MB | 71.3% | 138M |\n",
    "| **ResNet50** | 2015 | 98 MB | 74.9% | 25.6M |\n",
    "| **InceptionV3** | 2015 | 92 MB | 77.9% | 23.8M |\n",
    "| **MobileNetV2** | 2018 | 14 MB | 71.3% | 3.5M (mobile!) |\n",
    "| **EfficientNetB0** | 2019 | 29 MB | 77.1% | 5.3M |\n",
    "\n",
    "**ImageNet**: 1.2M images, 1000 classes (dogs, cats, cars, etc.)\n",
    "\n",
    "### Two Transfer Learning Strategies\n",
    "\n",
    "**1. Feature Extraction**\n",
    "- Freeze pre-trained layers (don't update weights)\n",
    "- Add new classifier on top\n",
    "- Only train new layers\n",
    "\n",
    "```python\n",
    "base_model.trainable = False  # Freeze all layers\n",
    "```\n",
    "\n",
    "**Use when:**\n",
    "- Very small dataset (< 1000 images)\n",
    "- Very similar to ImageNet\n",
    "\n",
    "**2. Fine-Tuning**\n",
    "- Unfreeze some top layers\n",
    "- Train entire model with small learning rate\n",
    "- Allows adaptation to your specific data\n",
    "\n",
    "```python\n",
    "base_model.trainable = True  # Unfreeze\n",
    "# Freeze early layers, train later layers\n",
    "```\n",
    "\n",
    "**Use when:**\n",
    "- Medium dataset (1000-10,000 images)\n",
    "- Somewhat different from ImageNet\n",
    "\n",
    "### Transfer Learning Workflow\n",
    "\n",
    "**Step 1**: Load pre-trained model (without top classification layer)\n",
    "```python\n",
    "base_model = tf.keras.applications.MobileNetV2(\n",
    "    include_top=False,  # Remove classification layer\n",
    "    weights='imagenet',  # Use ImageNet weights\n",
    "    input_shape=(224, 224, 3)\n",
    ")\n",
    "```\n",
    "\n",
    "**Step 2**: Freeze base model\n",
    "```python\n",
    "base_model.trainable = False\n",
    "```\n",
    "\n",
    "**Step 3**: Add custom classifier\n",
    "```python\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "```\n",
    "\n",
    "**Step 4**: Train\n",
    "```python\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "model.fit(X_train, y_train, epochs=10)\n",
    "```\n",
    "\n",
    "**Step 5** (Optional): Fine-tune\n",
    "```python\n",
    "base_model.trainable = True\n",
    "# Freeze first 100 layers\n",
    "for layer in base_model.layers[:100]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "# Recompile with small learning rate\n",
    "model.compile(optimizer=Adam(1e-5), loss='categorical_crossentropy')\n",
    "model.fit(X_train, y_train, epochs=10)\n",
    "```\n",
    "\n",
    "### Data Preprocessing for Pre-trained Models\n",
    "\n",
    "**Important**: Different models trained with different preprocessing!\n",
    "\n",
    "```python\n",
    "# VGG16, ResNet\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "# MobileNet\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "\n",
    "# Always use model's specific preprocessing!\n",
    "X_processed = preprocess_input(X)\n",
    "```\n",
    "\n",
    "### Input Size Requirements\n",
    "\n",
    "**Most models expect specific sizes:**\n",
    "- VGG16, ResNet: 224Ã—224\n",
    "- InceptionV3: 299Ã—299\n",
    "- EfficientNet: varies (224-600)\n",
    "\n",
    "**Resize images:**\n",
    "```python\n",
    "tf.keras.layers.Resizing(224, 224)\n",
    "```\n",
    "\n",
    "### Example Performance Comparison\n",
    "\n",
    "| Approach | Training Time | Accuracy (1000 images) |\n",
    "|----------|---------------|------------------------|\n",
    "| From Scratch | 2 hours | 65% |\n",
    "| Transfer Learning (frozen) | 10 minutes | 85% |\n",
    "| Transfer Learning (fine-tuned) | 30 minutes | 92% |\n",
    "\n",
    "**Transfer learning is 12Ã— faster and 27% more accurate!**\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "**1. Start with feature extraction**\n",
    "- Train only classifier first\n",
    "- Verify it works before fine-tuning\n",
    "\n",
    "**2. Use small learning rate for fine-tuning**\n",
    "- 10-100Ã— smaller than normal\n",
    "- Prevents destroying pre-trained weights\n",
    "\n",
    "**3. Unfreeze gradually**\n",
    "- Don't unfreeze all layers at once\n",
    "- Start with last few layers\n",
    "\n",
    "**4. Monitor validation accuracy**\n",
    "- Stop if overfitting\n",
    "- Use early stopping\n",
    "\n",
    "**5. Data augmentation**\n",
    "- Even with transfer learning!\n",
    "- Helps prevent overfitting\n",
    "\n",
    "Let's implement transfer learning with MobileNetV2!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer Learning - Implementation with MobileNetV2\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TRANSFER LEARNING DEMONSTRATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if has_tf:\n",
    "    from tensorflow.keras.applications import MobileNetV2\n",
    "    from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "    from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    import time\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"1. PREPARING DATA FOR TRANSFER LEARNING\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Note: MobileNetV2 expects 3-channel RGB images (224Ã—224Ã—3)\n",
    "    # We'll convert MNIST grayscale to RGB by repeating channels\n",
    "\n",
    "    # Take a subset for faster demonstration\n",
    "    X_train_subset = X_train[:5000]\n",
    "    y_train_subset = y_train_cat[:5000]\n",
    "    X_val_subset = X_val[:1000]\n",
    "    y_val_subset = y_val_cat[:1000]\n",
    "\n",
    "    # Resize MNIST from 28Ã—28 to 32Ã—32 (MobileNet minimum)\n",
    "    # Convert grayscale to RGB (repeat channel 3 times)\n",
    "    def prepare_for_transfer_learning(X):\n",
    "        # Resize to 32Ã—32\n",
    "        X_resized = tf.image.resize(X, [32, 32])\n",
    "        # Convert to RGB (repeat grayscale channel)\n",
    "        X_rgb = tf.image.grayscale_to_rgb(X_resized)\n",
    "        return X_rgb.numpy()\n",
    "\n",
    "    print(\"\\nPreparing images for MobileNetV2...\")\n",
    "    X_train_rgb = prepare_for_transfer_learning(X_train_subset)\n",
    "    X_val_rgb = prepare_for_transfer_learning(X_val_subset)\n",
    "\n",
    "    # Apply MobileNetV2 preprocessing\n",
    "    X_train_preprocessed = preprocess_input(X_train_rgb.copy())\n",
    "    X_val_preprocessed = preprocess_input(X_val_rgb.copy())\n",
    "\n",
    "    print(f\"\\nâœ“ Data prepared:\")\n",
    "    print(f\"  Training shape: {X_train_preprocessed.shape}\")\n",
    "    print(f\"  Validation shape: {X_val_preprocessed.shape}\")\n",
    "    print(f\"  Converted: 28Ã—28Ã—1 â†’ 32Ã—32Ã—3 (RGB)\")\n",
    "\n",
    "    # Build transfer learning model\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"2. BUILDING TRANSFER LEARNING MODEL\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Load pre-trained MobileNetV2\n",
    "    print(\"\\nLoading MobileNetV2 with ImageNet weights...\")\n",
    "    base_model = MobileNetV2(\n",
    "        include_top=False,  # Exclude classification layer\n",
    "        weights=\"imagenet\",  # Use ImageNet weights\n",
    "        input_shape=(32, 32, 3),\n",
    "    )\n",
    "\n",
    "    # Freeze base model\n",
    "    base_model.trainable = False\n",
    "\n",
    "    print(f\"\\nâœ“ MobileNetV2 loaded:\")\n",
    "    print(f\"  Trained on: ImageNet (1.2M images, 1000 classes)\")\n",
    "    print(f\"  Parameters: {base_model.count_params():,}\")\n",
    "    print(f\"  Frozen: Yes (we'll only train classifier)\")\n",
    "\n",
    "    # Build full model\n",
    "    transfer_model = models.Sequential(\n",
    "        [\n",
    "            base_model,\n",
    "            GlobalAveragePooling2D(name=\"global_pool\"),\n",
    "            layers.Dense(128, activation=\"relu\", name=\"dense1\"),\n",
    "            layers.Dropout(0.5, name=\"dropout\"),\n",
    "            layers.Dense(10, activation=\"softmax\", name=\"output\"),\n",
    "        ],\n",
    "        name=\"Transfer_Learning_Model\",\n",
    "    )\n",
    "\n",
    "    print(\"\\nâœ“ Complete model architecture:\")\n",
    "    print(\"-\" * 60)\n",
    "    transfer_model.summary()\n",
    "\n",
    "    # Count trainable vs non-trainable\n",
    "    total_params = transfer_model.count_params()\n",
    "    trainable_params = sum([tf.size(w).numpy() for w in transfer_model.trainable_weights])\n",
    "    non_trainable_params = total_params - trainable_params\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"PARAMETER BREAKDOWN\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total parameters:        {total_params:>12,}\")\n",
    "    print(\n",
    "        f\"Trainable parameters:    {trainable_params:>12,}  ({trainable_params/total_params*100:.1f}%)\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Non-trainable parameters:{non_trainable_params:>12,}  ({non_trainable_params/total_params*100:.1f}%)\"\n",
    "    )\n",
    "    print(\"\\nâœ“ Only training {:.1f}% of parameters!\".format(trainable_params / total_params * 100))\n",
    "\n",
    "    # Compile model\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"3. COMPILING TRANSFER LEARNING MODEL\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    transfer_model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    print(\"\\nâœ“ Model compiled with:\")\n",
    "    print(\"  Optimizer: Adam (lr=0.001)\")\n",
    "    print(\"  Loss: Categorical Crossentropy\")\n",
    "\n",
    "    # Train model\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"4. TRAINING (FEATURE EXTRACTION)\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\nTraining only the classifier (base model frozen)...\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    history_transfer = transfer_model.fit(\n",
    "        X_train_preprocessed,\n",
    "        y_train_subset,\n",
    "        epochs=5,\n",
    "        batch_size=64,\n",
    "        validation_data=(X_val_preprocessed, y_val_subset),\n",
    "        verbose=1,\n",
    "    )\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    print(f\"\\nâœ“ Training completed in {training_time:.1f} seconds\")\n",
    "\n",
    "    # Evaluate\n",
    "    val_loss, val_acc = transfer_model.evaluate(X_val_preprocessed, y_val_subset, verbose=0)\n",
    "    print(f\"  Validation Accuracy: {val_acc*100:.2f}%\")\n",
    "\n",
    "    # Compare with custom CNN\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"5. PERFORMANCE COMPARISON\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(\"\\nðŸ“Š Transfer Learning vs Custom CNN:\\n\")\n",
    "    print(f\"{'Metric':<25} {'Custom CNN':>15} {'Transfer Learning':>20}\")\n",
    "    print(\"-\" * 62)\n",
    "\n",
    "    # Training time comparison (approximate)\n",
    "    custom_time = 120  # Approximate from earlier\n",
    "    print(f\"{'Training Time':<25} {custom_time:>12.0f}s {training_time:>18.1f}s\")\n",
    "\n",
    "    # Accuracy comparison\n",
    "    custom_acc = test_accuracy * 100  # From earlier CNN\n",
    "    transfer_acc = val_acc * 100\n",
    "    print(f\"{'Accuracy':<25} {custom_acc:>12.1f}% {transfer_acc:>17.1f}%\")\n",
    "\n",
    "    # Parameters comparison\n",
    "    custom_params = model.count_params()\n",
    "    transfer_trainable = trainable_params\n",
    "    print(f\"{'Trainable Params':<25} {custom_params:>12,} {transfer_trainable:>18,}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    speedup = custom_time / training_time\n",
    "    print(f\"âš¡ Transfer learning is {speedup:.1f}Ã— faster!\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Visualize training history\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"6. TRAINING HISTORY\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    # Accuracy\n",
    "    axes[0].plot(history_transfer.history[\"accuracy\"], \"b-\", label=\"Training\", linewidth=2)\n",
    "    axes[0].plot(history_transfer.history[\"val_accuracy\"], \"r-\", label=\"Validation\", linewidth=2)\n",
    "    axes[0].set_title(\"Transfer Learning: Accuracy\", fontsize=13, fontweight=\"bold\")\n",
    "    axes[0].set_xlabel(\"Epoch\", fontsize=11)\n",
    "    axes[0].set_ylabel(\"Accuracy\", fontsize=11)\n",
    "    axes[0].legend(fontsize=10)\n",
    "    axes[0].grid(alpha=0.3)\n",
    "\n",
    "    # Loss\n",
    "    axes[1].plot(history_transfer.history[\"loss\"], \"b-\", label=\"Training\", linewidth=2)\n",
    "    axes[1].plot(history_transfer.history[\"val_loss\"], \"r-\", label=\"Validation\", linewidth=2)\n",
    "    axes[1].set_title(\"Transfer Learning: Loss\", fontsize=13, fontweight=\"bold\")\n",
    "    axes[1].set_xlabel(\"Epoch\", fontsize=11)\n",
    "    axes[1].set_ylabel(\"Loss\", fontsize=11)\n",
    "    axes[1].legend(fontsize=10)\n",
    "    axes[1].grid(alpha=0.3)\n",
    "\n",
    "    plt.suptitle(\"Transfer Learning Training Progress\", fontsize=14, fontweight=\"bold\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Make predictions\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"7. PREDICTIONS WITH TRANSFER LEARNING\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Prepare test images\n",
    "    X_test_rgb = prepare_for_transfer_learning(X_test[:12])\n",
    "    X_test_preprocessed = preprocess_input(X_test_rgb.copy())\n",
    "\n",
    "    predictions_transfer = transfer_model.predict(X_test_preprocessed, verbose=0)\n",
    "    predicted_classes_transfer = np.argmax(predictions_transfer, axis=1)\n",
    "    true_classes_transfer = y_test[:12]\n",
    "\n",
    "    fig, axes = plt.subplots(2, 6, figsize=(18, 6))\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    for i in range(12):\n",
    "        axes[i].imshow(X_test[i].reshape(28, 28), cmap=\"gray\")\n",
    "\n",
    "        pred_class = predicted_classes_transfer[i]\n",
    "        true_class = true_classes_transfer[i]\n",
    "        confidence = predictions_transfer[i][pred_class] * 100\n",
    "\n",
    "        color = \"green\" if pred_class == true_class else \"red\"\n",
    "\n",
    "        axes[i].set_title(\n",
    "            f\"Pred: {pred_class}\\nTrue: {true_class}\\n({confidence:.1f}%)\",\n",
    "            fontsize=10,\n",
    "            color=color,\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "        axes[i].axis(\"off\")\n",
    "\n",
    "    plt.suptitle(\n",
    "        \"Transfer Learning Predictions (Green=Correct, Red=Wrong)\", fontsize=14, fontweight=\"bold\"\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    correct = np.sum(predicted_classes_transfer == true_classes_transfer)\n",
    "    print(f\"\\nâœ“ Accuracy on 12 samples: {correct}/12 ({correct/12*100:.1f}%)\")\n",
    "\n",
    "    # Fine-tuning demonstration\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"8. FINE-TUNING (OPTIONAL ADVANCED STEP)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(\"\\nFine-tuning: Unfreeze last few layers and retrain with small LR\")\n",
    "    print(\"\\nArchitecture of base model:\")\n",
    "    print(f\"  Total layers: {len(base_model.layers)}\")\n",
    "    print(f\"  Currently all frozen: {not base_model.trainable}\")\n",
    "\n",
    "    # Unfreeze last 20 layers\n",
    "    base_model.trainable = True\n",
    "    fine_tune_at = len(base_model.layers) - 20\n",
    "\n",
    "    for layer in base_model.layers[:fine_tune_at]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Count trainable params after unfreezing\n",
    "    trainable_after = sum([tf.size(w).numpy() for w in transfer_model.trainable_weights])\n",
    "\n",
    "    print(f\"\\nâœ“ Fine-tuning setup:\")\n",
    "    print(f\"  Unfrozen layers: Last {len(base_model.layers) - fine_tune_at} layers\")\n",
    "    print(f\"  Frozen layers: First {fine_tune_at} layers\")\n",
    "    print(f\"  Trainable parameters: {trainable_after:,} (was {trainable_params:,})\")\n",
    "\n",
    "    # Recompile with smaller learning rate\n",
    "    transfer_model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0001),  # 10Ã— smaller LR\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    print(\"\\nâœ“ Recompiled with smaller learning rate:\")\n",
    "    print(\"  LR: 0.0001 (10Ã— smaller to preserve pre-trained weights)\")\n",
    "\n",
    "    print(\"\\nðŸ’¡ Fine-tuning tips:\")\n",
    "    print(\"  â€¢ Use much smaller learning rate (0.0001 vs 0.001)\")\n",
    "    print(\"  â€¢ Unfreeze gradually (last layers first)\")\n",
    "    print(\"  â€¢ Monitor validation loss carefully\")\n",
    "    print(\"  â€¢ Stop if overfitting occurs\")\n",
    "\n",
    "    # Visualize architecture\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"9. TRANSFER LEARNING ARCHITECTURE\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    architecture_viz = \"\"\"\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚         INPUT IMAGE (32Ã—32Ã—3)        â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                     â†“\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚      MobileNetV2 Base Model         â”‚\n",
    "    â”‚     (Pre-trained on ImageNet)       â”‚\n",
    "    â”‚                                      â”‚\n",
    "    â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚\n",
    "    â”‚   â”‚ Convolutional Blocks      â”‚     â”‚\n",
    "    â”‚   â”‚ (155 layers)              â”‚     â”‚\n",
    "    â”‚   â”‚                           â”‚     â”‚\n",
    "    â”‚   â”‚ Learned features:         â”‚     â”‚\n",
    "    â”‚   â”‚  â€¢ Edges & textures       â”‚     â”‚\n",
    "    â”‚   â”‚  â€¢ Patterns & shapes      â”‚     â”‚\n",
    "    â”‚   â”‚  â€¢ Object parts           â”‚     â”‚\n",
    "    â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚\n",
    "    â”‚                                      â”‚\n",
    "    â”‚   ðŸ”’ FROZEN (weights not updated)   â”‚\n",
    "    â”‚   2.2M parameters                    â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                     â†“\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚     Global Average Pooling 2D       â”‚\n",
    "    â”‚          (Reduce spatial dims)      â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                     â†“\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚          Dense (128, ReLU)          â”‚\n",
    "    â”‚      ðŸ”“ TRAINABLE                   â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                     â†“\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚          Dropout (0.5)              â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                     â†“\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚      Dense (10, Softmax)            â”‚\n",
    "    â”‚      ðŸ”“ TRAINABLE                   â”‚\n",
    "    â”‚      (Our custom classifier)        â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                     â†“\n",
    "           OUTPUT (10 classes)\n",
    "    \n",
    "    Key:\n",
    "    ðŸ”’ = Frozen (use pre-trained weights)\n",
    "    ðŸ”“ = Trainable (learn from our data)\n",
    "    \"\"\"\n",
    "\n",
    "    ax.text(\n",
    "        0.5,\n",
    "        0.5,\n",
    "        architecture_viz,\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        fontsize=10,\n",
    "        family=\"monospace\",\n",
    "        bbox=dict(boxstyle=\"round\", facecolor=\"lightblue\", alpha=0.8),\n",
    "    )\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(\"Transfer Learning Architecture\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TRANSFER LEARNING SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"âœ“ Used pre-trained MobileNetV2 from ImageNet\")\n",
    "    print(f\"âœ“ Froze base model ({non_trainable_params:,} parameters)\")\n",
    "    print(f\"âœ“ Trained only classifier ({trainable_params:,} parameters)\")\n",
    "    print(f\"âœ“ Achieved {val_acc*100:.2f}% accuracy in {training_time:.1f}s\")\n",
    "    print(f\"âœ“ {speedup:.1f}Ã— faster than training from scratch!\")\n",
    "    print(\"\\nðŸ’¡ Transfer learning ideal for:\")\n",
    "    print(\"  â€¢ Small datasets (< 10K images)\")\n",
    "    print(\"  â€¢ Limited GPU resources\")\n",
    "    print(\"  â€¢ Quick prototypes\")\n",
    "    print(\"  â€¢ Similar to ImageNet classes\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "else:\n",
    "    print(\"\\nSkipping transfer learning (TensorFlow not available)\")\n",
    "    print(\"Install TensorFlow to run this section\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Image Classification Project\n",
    "\n",
    "Put everything together in a complete image classification pipeline!\n",
    "\n",
    "### Project Overview\n",
    "\n",
    "**Goal**: Build an end-to-end image classifier\n",
    "\n",
    "**Steps:**\n",
    "1. Load and explore data\n",
    "2. Preprocess images\n",
    "3. Build/train model\n",
    "4. Evaluate performance\n",
    "5. Make predictions\n",
    "6. Analyze errors\n",
    "\n",
    "### Real-World Classification Tasks\n",
    "\n",
    "**Medical Imaging:**\n",
    "- Detect tumors in X-rays/MRIs\n",
    "- Classify skin lesions (melanoma detection)\n",
    "- Diagnose diabetic retinopathy from eye scans\n",
    "\n",
    "**Agriculture:**\n",
    "- Identify plant diseases from leaf images\n",
    "- Count crops from drone images\n",
    "- Detect pests\n",
    "\n",
    "**Manufacturing:**\n",
    "- Quality control (detect defects)\n",
    "- Sort products by type\n",
    "- Safety monitoring\n",
    "\n",
    "**Security:**\n",
    "- Face recognition\n",
    "- License plate reading\n",
    "- Surveillance analysis\n",
    "\n",
    "### Key Components of Classification Pipeline\n",
    "\n",
    "**1. Data Loading & Exploration**\n",
    "```python\n",
    "# Load data\n",
    "train_images, train_labels = load_dataset()\n",
    "\n",
    "# Explore\n",
    "print(f\"Samples: {len(train_images)}\")\n",
    "print(f\"Image shape: {train_images[0].shape}\")\n",
    "print(f\"Classes: {np.unique(train_labels)}\")\n",
    "```\n",
    "\n",
    "**2. Data Preprocessing**\n",
    "```python\n",
    "# Normalize pixels to [0, 1]\n",
    "X = X.astype('float32') / 255.0\n",
    "\n",
    "# Reshape for CNN if needed\n",
    "X = X.reshape(-1, height, width, channels)\n",
    "\n",
    "# One-hot encode labels\n",
    "y = to_categorical(y, num_classes)\n",
    "```\n",
    "\n",
    "**3. Data Splitting**\n",
    "```python\n",
    "# Split into train/validation/test\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5)\n",
    "```\n",
    "\n",
    "**4. Model Building**\n",
    "```python\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=input_shape),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "```\n",
    "\n",
    "**5. Training**\n",
    "```python\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val, y_val)\n",
    ")\n",
    "```\n",
    "\n",
    "**6. Evaluation**\n",
    "```python\n",
    "# Test set evaluation\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred_classes)\n",
    "print(classification_report(y_true, y_pred_classes))\n",
    "```\n",
    "\n",
    "### Performance Metrics\n",
    "\n",
    "**Accuracy**: Overall correct predictions\n",
    "```\n",
    "Accuracy = (TP + TN) / Total\n",
    "```\n",
    "\n",
    "**Precision**: Of positive predictions, how many correct?\n",
    "```\n",
    "Precision = TP / (TP + FP)\n",
    "```\n",
    "\n",
    "**Recall**: Of actual positives, how many found?\n",
    "```\n",
    "Recall = TP / (TP + FN)\n",
    "```\n",
    "\n",
    "**F1-Score**: Harmonic mean of precision and recall\n",
    "```\n",
    "F1 = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)\n",
    "```\n",
    "\n",
    "### Common Issues & Solutions\n",
    "\n",
    "**Problem: Overfitting**\n",
    "- Validation accuracy lower than training\n",
    "- Solution: Add Dropout, L2 regularization, data augmentation\n",
    "\n",
    "**Problem: Underfitting**\n",
    "- Both train and validation accuracy low\n",
    "- Solution: Increase model capacity, train longer\n",
    "\n",
    "**Problem: Class Imbalance**\n",
    "- Some classes have many more samples\n",
    "- Solution: Class weights, oversampling, focal loss\n",
    "\n",
    "**Problem: Low Accuracy**\n",
    "- Model not learning\n",
    "- Solution: Check data quality, try transfer learning, tune hyperparameters\n",
    "\n",
    "Let's build a complete classification project!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Classification Project - Complete Pipeline\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPLETE IMAGE CLASSIFICATION PROJECT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if has_tf:\n",
    "    from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "    # Use our already-trained model from Section 4\n",
    "    print(\"\\nâœ“ Using CNN model trained earlier\")\n",
    "    print(f\"  Model: {model.name}\")\n",
    "    print(f\"  Test Accuracy: {test_accuracy*100:.2f}%\")\n",
    "\n",
    "    # Make predictions on test set\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"1. COMPREHENSIVE EVALUATION\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Get predictions\n",
    "    y_pred_probs = model.predict(X_test, verbose=0)\n",
    "    y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
    "    y_true_classes = y_test\n",
    "\n",
    "    # Classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(\n",
    "        classification_report(\n",
    "            y_true_classes, y_pred_classes, target_names=[str(i) for i in range(10)]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Confusion matrix\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"2. CONFUSION MATRIX\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "    im = ax.imshow(cm, cmap=\"Blues\")\n",
    "\n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(im, ax=ax)\n",
    "    cbar.set_label(\"Count\", fontsize=12, fontweight=\"bold\")\n",
    "\n",
    "    # Set ticks\n",
    "    ax.set_xticks(np.arange(10))\n",
    "    ax.set_yticks(np.arange(10))\n",
    "    ax.set_xticklabels(range(10), fontsize=11)\n",
    "    ax.set_yticklabels(range(10), fontsize=11)\n",
    "\n",
    "    # Add labels\n",
    "    ax.set_xlabel(\"Predicted Label\", fontsize=13, fontweight=\"bold\")\n",
    "    ax.set_ylabel(\"True Label\", fontsize=13, fontweight=\"bold\")\n",
    "    ax.set_title(\"Confusion Matrix: MNIST Digit Classification\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "    # Add text annotations\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            text_color = \"white\" if cm[i, j] > cm.max() / 2 else \"black\"\n",
    "            ax.text(\n",
    "                j,\n",
    "                i,\n",
    "                cm[i, j],\n",
    "                ha=\"center\",\n",
    "                va=\"center\",\n",
    "                color=text_color,\n",
    "                fontsize=11,\n",
    "                fontweight=\"bold\",\n",
    "            )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Per-class accuracy\n",
    "    print(\"\\nâœ“ Confusion Matrix Analysis:\")\n",
    "    class_accuracies = cm.diagonal() / cm.sum(axis=1)\n",
    "    for digit in range(10):\n",
    "        print(\n",
    "            f\"  Digit {digit}: {class_accuracies[digit]*100:.2f}% accuracy ({cm[digit, digit]}/{cm[digit].sum()} correct)\"\n",
    "        )\n",
    "\n",
    "    # Error analysis\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"3. ERROR ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Find misclassified examples\n",
    "    misclassified_idx = np.where(y_pred_classes != y_true_classes)[0]\n",
    "\n",
    "    print(\n",
    "        f\"\\nâœ“ Total errors: {len(misclassified_idx)}/{len(y_test)} ({len(misclassified_idx)/len(y_test)*100:.2f}%)\"\n",
    "    )\n",
    "\n",
    "    # Show worst predictions (lowest confidence on wrong predictions)\n",
    "    misclassified_probs = y_pred_probs[misclassified_idx]\n",
    "    misclassified_confidences = np.max(misclassified_probs, axis=1)\n",
    "\n",
    "    # Sort by confidence (most confident errors first)\n",
    "    sorted_idx = np.argsort(-misclassified_confidences)[:12]\n",
    "    worst_errors = misclassified_idx[sorted_idx]\n",
    "\n",
    "    fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    for idx, error_idx in enumerate(worst_errors):\n",
    "        axes[idx].imshow(X_test[error_idx].reshape(28, 28), cmap=\"gray\")\n",
    "\n",
    "        true_label = y_true_classes[error_idx]\n",
    "        pred_label = y_pred_classes[error_idx]\n",
    "        confidence = y_pred_probs[error_idx][pred_label] * 100\n",
    "\n",
    "        axes[idx].set_title(\n",
    "            f\"True: {true_label} | Pred: {pred_label}\\nConfidence: {confidence:.1f}%\",\n",
    "            fontsize=11,\n",
    "            fontweight=\"bold\",\n",
    "            color=\"red\",\n",
    "        )\n",
    "        axes[idx].axis(\"off\")\n",
    "\n",
    "    plt.suptitle(\"Most Confident Misclassifications\", fontsize=14, fontweight=\"bold\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nâœ“ Showing 12 most confident misclassifications\")\n",
    "    print(\"  These are errors where model was very confident but wrong!\")\n",
    "\n",
    "    # Per-class error analysis\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"4. MOST CONFUSED DIGIT PAIRS\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Find most common confusions (off-diagonal elements)\n",
    "    confusion_pairs = []\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            if i != j and cm[i, j] > 0:\n",
    "                confusion_pairs.append((i, j, cm[i, j]))\n",
    "\n",
    "    # Sort by count\n",
    "    confusion_pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    print(\"\\nTop 10 most common misclassifications:\\n\")\n",
    "    print(f\"{'True':<6} {'Predicted':<12} {'Count':<8} {'% of true class'}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    for true_label, pred_label, count in confusion_pairs[:10]:\n",
    "        pct = count / cm[true_label].sum() * 100\n",
    "        print(f\"{true_label:<6} {pred_label:<12} {count:<8} {pct:>6.2f}%\")\n",
    "\n",
    "    # Prediction confidence distribution\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"5. PREDICTION CONFIDENCE ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Get max probabilities for all predictions\n",
    "    all_confidences = np.max(y_pred_probs, axis=1)\n",
    "\n",
    "    # Separate correct vs incorrect\n",
    "    correct_mask = y_pred_classes == y_true_classes\n",
    "    correct_confidences = all_confidences[correct_mask]\n",
    "    incorrect_confidences = all_confidences[~correct_mask]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "    # Histogram of confidences\n",
    "    axes[0].hist(\n",
    "        correct_confidences, bins=50, alpha=0.7, label=\"Correct\", color=\"green\", edgecolor=\"black\"\n",
    "    )\n",
    "    axes[0].hist(\n",
    "        incorrect_confidences, bins=50, alpha=0.7, label=\"Incorrect\", color=\"red\", edgecolor=\"black\"\n",
    "    )\n",
    "    axes[0].set_xlabel(\"Confidence\", fontsize=12, fontweight=\"bold\")\n",
    "    axes[0].set_ylabel(\"Count\", fontsize=12, fontweight=\"bold\")\n",
    "    axes[0].set_title(\"Prediction Confidence Distribution\", fontsize=13, fontweight=\"bold\")\n",
    "    axes[0].legend(fontsize=11)\n",
    "    axes[0].grid(alpha=0.3)\n",
    "\n",
    "    # Box plot\n",
    "    axes[1].boxplot(\n",
    "        [correct_confidences, incorrect_confidences],\n",
    "        labels=[\"Correct\", \"Incorrect\"],\n",
    "        patch_artist=True,\n",
    "        boxprops=dict(facecolor=\"lightblue\", edgecolor=\"black\", linewidth=2),\n",
    "        medianprops=dict(color=\"red\", linewidth=2),\n",
    "    )\n",
    "    axes[1].set_ylabel(\"Confidence\", fontsize=12, fontweight=\"bold\")\n",
    "    axes[1].set_title(\"Confidence: Correct vs Incorrect\", fontsize=13, fontweight=\"bold\")\n",
    "    axes[1].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\nâœ“ Confidence statistics:\")\n",
    "    print(f\"  Correct predictions:\")\n",
    "    print(f\"    Mean: {correct_confidences.mean():.4f}\")\n",
    "    print(f\"    Median: {np.median(correct_confidences):.4f}\")\n",
    "    print(f\"    Std: {correct_confidences.std():.4f}\")\n",
    "    print(f\"\\n  Incorrect predictions:\")\n",
    "    print(f\"    Mean: {incorrect_confidences.mean():.4f}\")\n",
    "    print(f\"    Median: {np.median(incorrect_confidences):.4f}\")\n",
    "    print(f\"    Std: {incorrect_confidences.std():.4f}\")\n",
    "\n",
    "    # Model calibration\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"6. MODEL CALIBRATION\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Bin predictions by confidence\n",
    "    bins = [0.0, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 1.0]\n",
    "    bin_names = [\"0-50%\", \"50-60%\", \"60-70%\", \"70-80%\", \"80-90%\", \"90-95%\", \"95-100%\"]\n",
    "\n",
    "    print(\"\\nCalibration: Does 80% confidence = 80% accuracy?\\n\")\n",
    "    print(f\"{'Confidence Range':<15} {'Count':<10} {'Actual Accuracy':<18} {'Gap'}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for i in range(len(bins) - 1):\n",
    "        mask = (all_confidences >= bins[i]) & (all_confidences < bins[i + 1])\n",
    "        count = mask.sum()\n",
    "        if count > 0:\n",
    "            accuracy = correct_mask[mask].mean()\n",
    "            expected = (bins[i] + bins[i + 1]) / 2\n",
    "            gap = accuracy - expected\n",
    "            print(f\"{bin_names[i]:<15} {count:<10} {accuracy*100:>15.2f}% {gap*100:>10.2f}%\")\n",
    "\n",
    "    print(\"\\nâœ“ Well-calibrated model: Actual accuracy matches confidence\")\n",
    "    print(\"  Large gap = overconfident or underconfident\")\n",
    "\n",
    "    # Project summary\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"PROJECT SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nâœ“ Dataset: MNIST (handwritten digits)\")\n",
    "    print(f\"âœ“ Training samples: {len(X_train)}\")\n",
    "    print(f\"âœ“ Test samples: {len(X_test)}\")\n",
    "    print(f\"âœ“ Classes: 10 (digits 0-9)\")\n",
    "    print(f\"\\nâœ“ Model Architecture:\")\n",
    "    print(f\"  â€¢ Conv2D (32 filters, 3Ã—3) â†’ MaxPool\")\n",
    "    print(f\"  â€¢ Conv2D (64 filters, 3Ã—3) â†’ MaxPool\")\n",
    "    print(f\"  â€¢ Dense (128) â†’ Dropout â†’ Dense (10)\")\n",
    "    print(f\"  â€¢ Total parameters: {model.count_params():,}\")\n",
    "    print(f\"\\nâœ“ Performance:\")\n",
    "    print(f\"  â€¢ Test Accuracy: {test_accuracy*100:.2f}%\")\n",
    "    print(f\"  â€¢ Total errors: {len(misclassified_idx)}/{len(y_test)}\")\n",
    "    print(f\"  â€¢ Average confidence (correct): {correct_confidences.mean():.4f}\")\n",
    "    print(f\"  â€¢ Average confidence (incorrect): {incorrect_confidences.mean():.4f}\")\n",
    "    print(\"\\nâœ“ Key insights:\")\n",
    "    print(\n",
    "        f\"  â€¢ Most confused pairs: {confusion_pairs[0][0]}â†’{confusion_pairs[0][1]} ({confusion_pairs[0][2]} times)\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  â€¢ Best recognized digit: {class_accuracies.argmax()} ({class_accuracies.max()*100:.2f}%)\"\n",
    "    )\n",
    "    print(f\"  â€¢ Hardest digit: {class_accuracies.argmin()} ({class_accuracies.min()*100:.2f}%)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "else:\n",
    "    print(\"\\nSkipping project (TensorFlow not available)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Augmentation\n",
    "\n",
    "**Data Augmentation**: Artificially increase dataset size by creating modified versions of images.\n",
    "\n",
    "### Why Data Augmentation?\n",
    "\n",
    "**Problems it solves:**\n",
    "1. **Overfitting**: Model memorizes training data\n",
    "2. **Small datasets**: Need more training examples\n",
    "3. **Poor generalization**: Model fails on new data\n",
    "\n",
    "**Benefits:**\n",
    "- âœ… 2-10Ã— larger effective dataset\n",
    "- âœ… Better generalization\n",
    "- âœ… More robust to variations\n",
    "- âœ… Reduces overfitting\n",
    "- âœ… No additional data collection needed!\n",
    "\n",
    "### Common Augmentation Techniques\n",
    "\n",
    "**1. Geometric Transformations**\n",
    "- **Rotation**: Rotate image by degrees\n",
    "- **Flip**: Horizontal/vertical flip\n",
    "- **Zoom**: Scale image in/out\n",
    "- **Shift**: Translate left/right/up/down\n",
    "- **Shear**: Slant image\n",
    "\n",
    "**2. Color Transformations**\n",
    "- **Brightness**: Lighter or darker\n",
    "- **Contrast**: Increase/decrease contrast\n",
    "- **Saturation**: More/less colorful\n",
    "- **Hue**: Shift colors\n",
    "\n",
    "**3. Noise & Distortion**\n",
    "- **Gaussian noise**: Add random noise\n",
    "- **Blur**: Gaussian blur\n",
    "- **Cutout**: Remove random patches\n",
    "- **Mixup**: Blend two images\n",
    "\n",
    "### Augmentation Examples\n",
    "\n",
    "**Original â†’ Augmented**\n",
    "```\n",
    "Cat image â†’ Rotated 15Â° â†’ Flipped â†’ Zoomed â†’ Brighter\n",
    "```\n",
    "\n",
    "### When to Use Augmentation\n",
    "\n",
    "**Always use for:**\n",
    "- Small datasets (< 10K images)\n",
    "- Image classification\n",
    "- Object detection\n",
    "\n",
    "**Use carefully for:**\n",
    "- Medical imaging (only realistic transforms)\n",
    "- Document analysis (no rotation if orientation matters)\n",
    "\n",
    "**Avoid for:**\n",
    "- Text recognition (if augmentation distorts text)\n",
    "- Tasks where transforms change meaning\n",
    "\n",
    "### Keras ImageDataGenerator\n",
    "\n",
    "**Training with augmentation:**\n",
    "```python\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,          # Rotate 0-20 degrees\n",
    "    width_shift_range=0.2,      # Shift left/right 20%\n",
    "    height_shift_range=0.2,     # Shift up/down 20%\n",
    "    horizontal_flip=True,       # Random horizontal flip\n",
    "    zoom_range=0.2,             # Zoom in/out 20%\n",
    "    fill_mode='nearest'         # Fill empty pixels\n",
    ")\n",
    "\n",
    "# Fit on training data\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# Train with augmented data\n",
    "model.fit(\n",
    "    datagen.flow(X_train, y_train, batch_size=32),\n",
    "    epochs=50\n",
    ")\n",
    "```\n",
    "\n",
    "### Modern Approach: Keras Layers\n",
    "\n",
    "**Built into model (preferred):**\n",
    "```python\n",
    "model = Sequential([\n",
    "    # Augmentation layers (only active during training)\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "    tf.keras.layers.RandomRotation(0.1),\n",
    "    tf.keras.layers.RandomZoom(0.1),\n",
    "    \n",
    "    # Regular layers\n",
    "    Conv2D(32, (3,3), activation='relu'),\n",
    "    ...\n",
    "])\n",
    "```\n",
    "\n",
    "**Advantages:**\n",
    "- Integrated into model\n",
    "- GPU accelerated\n",
    "- Automatic train/test mode\n",
    "\n",
    "### Augmentation Best Practices\n",
    "\n",
    "**1. Start Simple**\n",
    "- Begin with flip + rotation\n",
    "- Add more if needed\n",
    "\n",
    "**2. Keep Realistic**\n",
    "- Don't rotate documents 90Â°\n",
    "- Don't flip text\n",
    "- Medical images: only valid transforms\n",
    "\n",
    "**3. Monitor Performance**\n",
    "- Check if augmentation helps\n",
    "- Too much augmentation can hurt!\n",
    "\n",
    "**4. Balance**\n",
    "- Light augmentation: rotation Â±15Â°, flip\n",
    "- Medium: + zoom, shift\n",
    "- Heavy: + color, noise, cutout\n",
    "\n",
    "### Example Impact\n",
    "\n",
    "| Scenario | Without Augmentation | With Augmentation |\n",
    "|----------|---------------------|-------------------|\n",
    "| **500 images** | 75% accuracy | 88% accuracy |\n",
    "| **1,000 images** | 82% accuracy | 92% accuracy |\n",
    "| **5,000 images** | 91% accuracy | 95% accuracy |\n",
    "\n",
    "**Biggest gains on small datasets!**\n",
    "\n",
    "Let's implement and visualize data augmentation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation - Implementation and Visualization\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA AUGMENTATION TECHNIQUES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if has_tf:\n",
    "    from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "    from scipy import ndimage\n",
    "\n",
    "    # Select a sample image\n",
    "    sample_img = X_test[0].reshape(28, 28, 1)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"1. BASIC AUGMENTATION TECHNIQUES\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "\n",
    "    # Original\n",
    "    axes[0, 0].imshow(sample_img.reshape(28, 28), cmap=\"gray\")\n",
    "    axes[0, 0].set_title(\"Original\", fontsize=12, fontweight=\"bold\")\n",
    "    axes[0, 0].axis(\"off\")\n",
    "\n",
    "    # 1. Rotation\n",
    "    rotated = ndimage.rotate(sample_img.reshape(28, 28), angle=15, reshape=False)\n",
    "    axes[0, 1].imshow(rotated, cmap=\"gray\")\n",
    "    axes[0, 1].set_title(\"Rotated (+15Â°)\", fontsize=12, fontweight=\"bold\")\n",
    "    axes[0, 1].axis(\"off\")\n",
    "\n",
    "    # 2. Horizontal flip\n",
    "    flipped_h = np.fliplr(sample_img.reshape(28, 28))\n",
    "    axes[0, 2].imshow(flipped_h, cmap=\"gray\")\n",
    "    axes[0, 2].set_title(\"Horizontal Flip\", fontsize=12, fontweight=\"bold\")\n",
    "    axes[0, 2].axis(\"off\")\n",
    "\n",
    "    # 3. Vertical flip\n",
    "    flipped_v = np.flipud(sample_img.reshape(28, 28))\n",
    "    axes[0, 3].imshow(flipped_v, cmap=\"gray\")\n",
    "    axes[0, 3].set_title(\"Vertical Flip\", fontsize=12, fontweight=\"bold\")\n",
    "    axes[0, 3].axis(\"off\")\n",
    "\n",
    "    # 4. Zoom (simple crop and resize)\n",
    "    zoom_factor = 0.8\n",
    "    h, w = sample_img.shape[:2]\n",
    "    h_new, w_new = int(h * zoom_factor), int(w * zoom_factor)\n",
    "    start_h, start_w = (h - h_new) // 2, (w - w_new) // 2\n",
    "    zoomed = sample_img[start_h : start_h + h_new, start_w : start_w + w_new]\n",
    "    zoomed = tf.image.resize(zoomed, [28, 28]).numpy()\n",
    "    axes[1, 0].imshow(zoomed.reshape(28, 28), cmap=\"gray\")\n",
    "    axes[1, 0].set_title(\"Zoom In (0.8Ã—)\", fontsize=12, fontweight=\"bold\")\n",
    "    axes[1, 0].axis(\"off\")\n",
    "\n",
    "    # 5. Brightness adjustment\n",
    "    brighter = np.clip(sample_img * 1.5, 0, 1)\n",
    "    axes[1, 1].imshow(brighter.reshape(28, 28), cmap=\"gray\")\n",
    "    axes[1, 1].set_title(\"Brighter (1.5Ã—)\", fontsize=12, fontweight=\"bold\")\n",
    "    axes[1, 1].axis(\"off\")\n",
    "\n",
    "    # 6. Darker\n",
    "    darker = sample_img * 0.6\n",
    "    axes[1, 2].imshow(darker.reshape(28, 28), cmap=\"gray\")\n",
    "    axes[1, 2].set_title(\"Darker (0.6Ã—)\", fontsize=12, fontweight=\"bold\")\n",
    "    axes[1, 2].axis(\"off\")\n",
    "\n",
    "    # 7. Gaussian noise\n",
    "    noise = np.random.normal(0, 0.1, sample_img.shape)\n",
    "    noisy = np.clip(sample_img + noise, 0, 1)\n",
    "    axes[1, 3].imshow(noisy.reshape(28, 28), cmap=\"gray\")\n",
    "    axes[1, 3].set_title(\"+ Gaussian Noise\", fontsize=12, fontweight=\"bold\")\n",
    "    axes[1, 3].axis(\"off\")\n",
    "\n",
    "    # 8. Shift right\n",
    "    shifted = np.roll(sample_img, shift=5, axis=1)\n",
    "    axes[2, 0].imshow(shifted.reshape(28, 28), cmap=\"gray\")\n",
    "    axes[2, 0].set_title(\"Shift Right\", fontsize=12, fontweight=\"bold\")\n",
    "    axes[2, 0].axis(\"off\")\n",
    "\n",
    "    # 9. Shift down\n",
    "    shifted_down = np.roll(sample_img, shift=5, axis=0)\n",
    "    axes[2, 1].imshow(shifted_down.reshape(28, 28), cmap=\"gray\")\n",
    "    axes[2, 1].set_title(\"Shift Down\", fontsize=12, fontweight=\"bold\")\n",
    "    axes[2, 1].axis(\"off\")\n",
    "\n",
    "    # 10. Combination (rotate + flip)\n",
    "    combo = ndimage.rotate(flipped_h, angle=-10, reshape=False)\n",
    "    axes[2, 2].imshow(combo, cmap=\"gray\")\n",
    "    axes[2, 2].set_title(\"Rotate + Flip\", fontsize=12, fontweight=\"bold\")\n",
    "    axes[2, 2].axis(\"off\")\n",
    "\n",
    "    # 11. Combination (zoom + noise)\n",
    "    combo2 = np.clip(zoomed + noise, 0, 1)\n",
    "    axes[2, 3].imshow(combo2.reshape(28, 28), cmap=\"gray\")\n",
    "    axes[2, 3].set_title(\"Zoom + Noise\", fontsize=12, fontweight=\"bold\")\n",
    "    axes[2, 3].axis(\"off\")\n",
    "\n",
    "    plt.suptitle(\"Data Augmentation Techniques\", fontsize=14, fontweight=\"bold\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nâœ“ Demonstrated 11 augmentation techniques\")\n",
    "    print(\"  Each creates a new training example from 1 image!\")\n",
    "\n",
    "    # ImageDataGenerator demonstration\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"2. KERAS IMAGEDATAGENERATOR\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Create augmentation generator\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=False,  # Don't flip digits\n",
    "        fill_mode=\"nearest\",\n",
    "    )\n",
    "\n",
    "    # Generate augmented images\n",
    "    sample_batch = sample_img.reshape(1, 28, 28, 1)\n",
    "\n",
    "    fig, axes = plt.subplots(3, 5, figsize=(18, 11))\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    # Original\n",
    "    axes[0].imshow(sample_img.reshape(28, 28), cmap=\"gray\")\n",
    "    axes[0].set_title(\"Original\", fontsize=12, fontweight=\"bold\", color=\"blue\")\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    # Generate 14 augmented versions\n",
    "    aug_iter = datagen.flow(sample_batch, batch_size=1)\n",
    "    for i in range(1, 15):\n",
    "        aug_img = next(aug_iter)[0]\n",
    "        axes[i].imshow(aug_img.reshape(28, 28), cmap=\"gray\")\n",
    "        axes[i].set_title(f\"Augmented {i}\", fontsize=11)\n",
    "        axes[i].axis(\"off\")\n",
    "\n",
    "    plt.suptitle(\n",
    "        \"ImageDataGenerator: 14 Augmented Versions of Same Image\", fontsize=14, fontweight=\"bold\"\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nâœ“ Generated 14 augmented versions using ImageDataGenerator\")\n",
    "    print(\"  Settings: rotation (Â±20Â°), shift (Â±20%), zoom (Â±20%)\")\n",
    "\n",
    "    # Show effect on multiple images\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"3. AUGMENTATION ON MULTIPLE IMAGES\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    fig, axes = plt.subplots(5, 6, figsize=(18, 15))\n",
    "\n",
    "    for row in range(5):\n",
    "        # Original\n",
    "        img_idx = row * 10\n",
    "        original = X_test[img_idx].reshape(1, 28, 28, 1)\n",
    "        axes[row, 0].imshow(original.reshape(28, 28), cmap=\"gray\")\n",
    "        axes[row, 0].set_title(f\"Digit {y_test[img_idx]}\", fontsize=11, fontweight=\"bold\")\n",
    "        axes[row, 0].axis(\"off\")\n",
    "\n",
    "        # 5 augmented versions\n",
    "        aug_iter = datagen.flow(original, batch_size=1)\n",
    "        for col in range(1, 6):\n",
    "            aug_img = next(aug_iter)[0]\n",
    "            axes[row, col].imshow(aug_img.reshape(28, 28), cmap=\"gray\")\n",
    "            if row == 0:\n",
    "                axes[row, col].set_title(f\"Aug {col}\", fontsize=11)\n",
    "            axes[row, col].axis(\"off\")\n",
    "\n",
    "    plt.suptitle(\"Data Augmentation: Creating Training Variations\", fontsize=14, fontweight=\"bold\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nâœ“ Applied augmentation to 5 different digits\")\n",
    "    print(\"  Each digit â†’ 5 augmented versions\")\n",
    "    print(\"  Effective dataset size: 6Ã— larger!\")\n",
    "\n",
    "    # Modern Keras layers approach\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"4. MODERN APPROACH: KERAS AUGMENTATION LAYERS\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Create model with built-in augmentation\n",
    "    augmentation_layers = keras.Sequential(\n",
    "        [\n",
    "            layers.RandomFlip(\"horizontal\"),\n",
    "            layers.RandomRotation(0.1),  # Â±10%\n",
    "            layers.RandomZoom(0.1),  # Â±10%\n",
    "        ],\n",
    "        name=\"augmentation\",\n",
    "    )\n",
    "\n",
    "    print(\"\\nâœ“ Created augmentation pipeline:\")\n",
    "    print(\"  â€¢ RandomFlip (horizontal)\")\n",
    "    print(\"  â€¢ RandomRotation (Â±10%)\")\n",
    "    print(\"  â€¢ RandomZoom (Â±10%)\")\n",
    "    print(\"\\nðŸ’¡ These layers are built into the model\")\n",
    "    print(\"  â€¢ Only active during training\")\n",
    "    print(\"  â€¢ GPU accelerated\")\n",
    "    print(\"  â€¢ No preprocessing needed!\")\n",
    "\n",
    "    # Visualize modern augmentation\n",
    "    fig, axes = plt.subplots(2, 6, figsize=(18, 6))\n",
    "\n",
    "    test_img = X_test[5].reshape(1, 28, 28, 1)\n",
    "\n",
    "    # Original\n",
    "    axes[0, 0].imshow(test_img.reshape(28, 28), cmap=\"gray\")\n",
    "    axes[0, 0].set_title(\"Original\", fontsize=12, fontweight=\"bold\", color=\"blue\")\n",
    "    axes[0, 0].axis(\"off\")\n",
    "\n",
    "    # Apply augmentation 11 times\n",
    "    for idx in range(11):\n",
    "        row = idx // 6\n",
    "        col = (idx + 1) % 6\n",
    "\n",
    "        augmented = augmentation_layers(test_img, training=True)\n",
    "        axes[row, col].imshow(augmented[0].numpy().reshape(28, 28), cmap=\"gray\")\n",
    "        axes[row, col].set_title(f\"Aug {idx+1}\", fontsize=11)\n",
    "        axes[row, col].axis(\"off\")\n",
    "\n",
    "    plt.suptitle(\"Modern Keras Augmentation Layers\", fontsize=14, fontweight=\"bold\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nâœ“ Generated 11 augmented versions using Keras layers\")\n",
    "\n",
    "    # Comparison of dataset sizes\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"5. EFFECTIVE DATASET SIZE WITH AUGMENTATION\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    original_size = 50000  # MNIST training set\n",
    "    augmentations_per_image = [0, 1, 2, 5, 10, 20]\n",
    "    effective_sizes = [original_size * (1 + aug) for aug in augmentations_per_image]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    bars = ax.bar(\n",
    "        range(len(augmentations_per_image)),\n",
    "        effective_sizes,\n",
    "        color=[\"red\", \"orange\", \"yellow\", \"lightgreen\", \"green\", \"darkgreen\"],\n",
    "        edgecolor=\"black\",\n",
    "        linewidth=2,\n",
    "    )\n",
    "\n",
    "    # Add value labels\n",
    "    for idx, (bar, size) in enumerate(zip(bars, effective_sizes)):\n",
    "        height = bar.get_height()\n",
    "        multiplier = augmentations_per_image[idx] + 1\n",
    "        ax.text(\n",
    "            bar.get_x() + bar.get_width() / 2.0,\n",
    "            height,\n",
    "            f\"{size:,}\\n({multiplier}Ã— original)\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontsize=11,\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "\n",
    "    ax.set_xticks(range(len(augmentations_per_image)))\n",
    "    ax.set_xticklabels([f\"{aug} aug/img\" for aug in augmentations_per_image], fontsize=11)\n",
    "    ax.set_xlabel(\"Augmentations per Image\", fontsize=13, fontweight=\"bold\")\n",
    "    ax.set_ylabel(\"Effective Dataset Size\", fontsize=13, fontweight=\"bold\")\n",
    "    ax.set_title(\n",
    "        \"Data Augmentation Increases Effective Dataset Size\", fontsize=14, fontweight=\"bold\"\n",
    "    )\n",
    "    ax.grid(axis=\"y\", alpha=0.3, linestyle=\"--\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nâœ“ Dataset size multiplication:\")\n",
    "    for aug, size in zip(augmentations_per_image, effective_sizes):\n",
    "        multiplier = aug + 1\n",
    "        print(f\"  {aug} aug/image: {original_size:,} â†’ {size:,} ({multiplier}Ã— original)\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"DATA AUGMENTATION SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"âœ“ Artificially increases dataset size\")\n",
    "    print(\"  â€¢ Apply random transforms to training images\")\n",
    "    print(\"  â€¢ Each epoch sees different variations\")\n",
    "    print(\"\\nâœ“ Benefits:\")\n",
    "    print(\"  â€¢ Reduces overfitting\")\n",
    "    print(\"  â€¢ Improves generalization\")\n",
    "    print(\"  â€¢ No additional data collection!\")\n",
    "    print(\"\\nâœ“ Implementation:\")\n",
    "    print(\"  â€¢ Classic: ImageDataGenerator\")\n",
    "    print(\"  â€¢ Modern: Keras augmentation layers (preferred)\")\n",
    "    print(\"\\nâœ“ Best practices:\")\n",
    "    print(\"  â€¢ Use realistic transforms only\")\n",
    "    print(\"  â€¢ Start simple (flip, rotate, zoom)\")\n",
    "    print(\"  â€¢ Monitor validation performance\")\n",
    "    print(\"  â€¢ Biggest gains on small datasets\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "else:\n",
    "    print(\"\\nSkipping data augmentation (TensorFlow not available)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Deployment\n",
    "\n",
    "**Deployment**: Making your trained model available for real-world use!\n",
    "\n",
    "### Why Deployment Matters\n",
    "\n",
    "**Training â‰  Production**\n",
    "\n",
    "You've built a great model, but now what?\n",
    "- How do users interact with it?\n",
    "- How do you serve predictions?\n",
    "- How to handle 1000s of requests?\n",
    "\n",
    "### Deployment Options\n",
    "\n",
    "**1. Save Model for Later Use**\n",
    "```python\n",
    "# Save entire model\n",
    "model.save('my_model.h5')\n",
    "\n",
    "# Load later\n",
    "loaded_model = tf.keras.models.load_model('my_model.h5')\n",
    "\n",
    "# Make predictions\n",
    "predictions = loaded_model.predict(new_images)\n",
    "```\n",
    "\n",
    "**2. REST API (Flask/FastAPI)**\n",
    "- Build web API\n",
    "- Receive images via HTTP\n",
    "- Return predictions as JSON\n",
    "- Good for: Web apps, mobile apps\n",
    "\n",
    "**3. Cloud Deployment**\n",
    "- **AWS SageMaker**: Managed ML platform\n",
    "- **Google Cloud AI Platform**: Auto-scaling\n",
    "- **Azure ML**: Enterprise integration\n",
    "- Good for: Production systems\n",
    "\n",
    "**4. Edge Deployment**\n",
    "- **TensorFlow Lite**: Mobile/IoT devices\n",
    "- **ONNX**: Cross-platform format\n",
    "- **TensorFlow.js**: Browser deployment\n",
    "- Good for: Mobile apps, offline use\n",
    "\n",
    "**5. Batch Processing**\n",
    "- Process many images at once\n",
    "- Schedule jobs (daily/weekly)\n",
    "- Good for: Reports, batch analysis\n",
    "\n",
    "### Deployment Checklist\n",
    "\n",
    "**Before deploying:**\n",
    "- âœ… Model trained and validated\n",
    "- âœ… Acceptable accuracy on test set\n",
    "- âœ… Error analysis completed\n",
    "- âœ… Model saved in portable format\n",
    "- âœ… Input preprocessing documented\n",
    "- âœ… Output format defined\n",
    "\n",
    "**Production considerations:**\n",
    "- ðŸ” **Monitoring**: Track prediction accuracy over time\n",
    "- âš¡ **Performance**: Response time < 100ms?\n",
    "- ðŸ”’ **Security**: Authentication, rate limiting\n",
    "- ðŸ“Š **Logging**: Save predictions for analysis\n",
    "- ðŸ”„ **Versioning**: Track model versions\n",
    "- ðŸš¨ **Alerts**: Notify if accuracy drops\n",
    "\n",
    "### Simple Flask API Example\n",
    "\n",
    "```python\n",
    "from flask import Flask, request, jsonify\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "app = Flask(__name__)\n",
    "model = tf.keras.models.load_model('digit_classifier.h5')\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    # Get image from request\n",
    "    file = request.files['image']\n",
    "    img = load_and_preprocess(file)\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict(img)\n",
    "    digit = int(np.argmax(prediction))\n",
    "    confidence = float(np.max(prediction))\n",
    "    \n",
    "    # Return JSON\n",
    "    return jsonify({\n",
    "        'digit': digit,\n",
    "        'confidence': confidence\n",
    "    })\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000)\n",
    "```\n",
    "\n",
    "**Usage:**\n",
    "```bash\n",
    "curl -X POST -F \"image=@digit.png\" http://localhost:5000/predict\n",
    "```\n",
    "\n",
    "### TensorFlow Lite (Mobile)\n",
    "\n",
    "**Convert to TFLite:**\n",
    "```python\n",
    "# Convert model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save\n",
    "with open('model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- 10-100Ã— smaller model\n",
    "- Faster inference\n",
    "- Runs on mobile/embedded devices\n",
    "\n",
    "### Model Serving Best Practices\n",
    "\n",
    "**1. Preprocessing Pipeline**\n",
    "- Document all preprocessing steps\n",
    "- Package with model\n",
    "- Ensure consistency\n",
    "\n",
    "**2. Versioning**\n",
    "```\n",
    "models/\n",
    "  â”œâ”€â”€ digit_classifier_v1.h5\n",
    "  â”œâ”€â”€ digit_classifier_v2.h5\n",
    "  â””â”€â”€ digit_classifier_v3.h5  (current)\n",
    "```\n",
    "\n",
    "**3. A/B Testing**\n",
    "- Deploy new model to 10% of users\n",
    "- Compare with old model\n",
    "- Gradually increase if better\n",
    "\n",
    "**4. Fallback Strategy**\n",
    "- If model fails, return default\n",
    "- Log errors for debugging\n",
    "- Don't crash application!\n",
    "\n",
    "**5. Monitoring**\n",
    "```python\n",
    "# Track metrics\n",
    "latency = time_end - time_start\n",
    "prediction_confidence = max(predictions)\n",
    "\n",
    "# Log\n",
    "logger.info(f\"Prediction: {digit}, Confidence: {confidence:.2f}, Latency: {latency:.3f}s\")\n",
    "\n",
    "# Alert if confidence low\n",
    "if confidence < 0.5:\n",
    "    send_alert(\"Low confidence prediction!\")\n",
    "```\n",
    "\n",
    "### Deployment Comparison\n",
    "\n",
    "| Method | Difficulty | Cost | Scalability | Use Case |\n",
    "|--------|-----------|------|-------------|----------|\n",
    "| **Local Script** | Easy | Free | Low | Development |\n",
    "| **Flask API** | Medium | Low | Medium | Small apps |\n",
    "| **Cloud (AWS/GCP)** | Hard | High | High | Production |\n",
    "| **TFLite Mobile** | Medium | Low | N/A | Mobile apps |\n",
    "| **TF.js Browser** | Medium | Low | High | Web apps |\n",
    "\n",
    "### Common Deployment Issues\n",
    "\n",
    "**Problem: Slow predictions**\n",
    "- Solution: Use GPU, batch requests, optimize model\n",
    "\n",
    "**Problem: Model file too large**\n",
    "- Solution: Quantization, pruning, distillation\n",
    "\n",
    "**Problem: Different results in production**\n",
    "- Solution: Check preprocessing, library versions\n",
    "\n",
    "**Problem: Out of memory**\n",
    "- Solution: Reduce batch size, use smaller model\n",
    "\n",
    "Let's save and deploy our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Deployment - Saving and Loading Models\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL DEPLOYMENT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if has_tf:\n",
    "    import os\n",
    "    import json\n",
    "\n",
    "    # Define save paths\n",
    "    model_dir = \"notebooks/outputs\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    model_path = os.path.join(model_dir, \"digit_classifier.h5\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"1. SAVING THE MODEL\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Save model\n",
    "    model.save(model_path)\n",
    "\n",
    "    file_size_mb = os.path.getsize(model_path) / (1024 * 1024)\n",
    "\n",
    "    print(f\"\\nâœ“ Model saved successfully!\")\n",
    "    print(f\"  Path: {model_path}\")\n",
    "    print(f\"  Size: {file_size_mb:.2f} MB\")\n",
    "    print(f\"  Format: Keras HDF5\")\n",
    "\n",
    "    # Also save as SavedModel format (TensorFlow native)\n",
    "    savedmodel_dir = os.path.join(model_dir, \"digit_classifier_savedmodel\")\n",
    "    model.save(savedmodel_dir, save_format=\"tf\")\n",
    "    print(f\"\\nâœ“ Also saved in TensorFlow SavedModel format\")\n",
    "    print(f\"  Path: {savedmodel_dir}/\")\n",
    "\n",
    "    # Load model back\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"2. LOADING THE MODEL\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    loaded_model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "    print(f\"\\nâœ“ Model loaded successfully!\")\n",
    "    print(f\"  Architecture: {len(loaded_model.layers)} layers\")\n",
    "    print(f\"  Parameters: {loaded_model.count_params():,}\")\n",
    "\n",
    "    # Verify loaded model works\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"3. VERIFY LOADED MODEL\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Make predictions with loaded model\n",
    "    test_samples = X_test[:5]\n",
    "\n",
    "    original_preds = model.predict(test_samples, verbose=0)\n",
    "    loaded_preds = loaded_model.predict(test_samples, verbose=0)\n",
    "\n",
    "    # Check if predictions match\n",
    "    predictions_match = np.allclose(original_preds, loaded_preds)\n",
    "\n",
    "    print(f\"\\nâœ“ Verification:\")\n",
    "    print(f\"  Predictions match: {predictions_match}\")\n",
    "    print(f\"  Max difference: {np.max(np.abs(original_preds - loaded_preds)):.10f}\")\n",
    "\n",
    "    # Create deployment package\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"4. CREATE DEPLOYMENT PACKAGE\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Save preprocessing info\n",
    "    preprocessing_info = {\n",
    "        \"input_shape\": [28, 28, 1],\n",
    "        \"pixel_range\": [0.0, 1.0],\n",
    "        \"normalization\": \"divide by 255\",\n",
    "        \"color_mode\": \"grayscale\",\n",
    "        \"classes\": list(range(10)),\n",
    "        \"class_names\": [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"],\n",
    "    }\n",
    "\n",
    "    info_path = os.path.join(model_dir, \"model_info.json\")\n",
    "    with open(info_path, \"w\") as f:\n",
    "        json.dump(preprocessing_info, f, indent=2)\n",
    "\n",
    "    print(\"\\nâœ“ Deployment package created:\")\n",
    "    print(f\"  Model: {model_path}\")\n",
    "    print(f\"  Info: {info_path}\")\n",
    "    print(\"\\nPreprocessing requirements:\")\n",
    "    for key, value in preprocessing_info.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "    # Demonstrate inference function\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"5. PRODUCTION INFERENCE FUNCTION\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    def predict_digit(image, model_path, return_confidence=True):\n",
    "        \"\"\"\n",
    "        Production-ready inference function.\n",
    "\n",
    "        Args:\n",
    "            image: NumPy array of shape (28, 28) or (28, 28, 1)\n",
    "            model_path: Path to saved model\n",
    "            return_confidence: Whether to return confidence score\n",
    "\n",
    "        Returns:\n",
    "            dict with 'digit' and optionally 'confidence'\n",
    "        \"\"\"\n",
    "        import time\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Load model (in production, load once at startup)\n",
    "        model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "        # Preprocess\n",
    "        if image.ndim == 2:\n",
    "            image = image.reshape(1, 28, 28, 1)\n",
    "        elif image.ndim == 3:\n",
    "            image = image.reshape(1, 28, 28, 1)\n",
    "\n",
    "        # Ensure float32 and normalized\n",
    "        image = image.astype(\"float32\")\n",
    "        if image.max() > 1.0:\n",
    "            image = image / 255.0\n",
    "\n",
    "        # Predict\n",
    "        prediction = model.predict(image, verbose=0)\n",
    "        digit = int(np.argmax(prediction))\n",
    "        confidence = float(np.max(prediction))\n",
    "\n",
    "        latency = time.time() - start_time\n",
    "\n",
    "        result = {\"digit\": digit, \"latency_ms\": round(latency * 1000, 2)}\n",
    "\n",
    "        if return_confidence:\n",
    "            result[\"confidence\"] = round(confidence, 4)\n",
    "            result[\"all_probabilities\"] = prediction[0].tolist()\n",
    "\n",
    "        return result\n",
    "\n",
    "    # Test inference function\n",
    "    test_image = X_test[0].reshape(28, 28)\n",
    "    true_label = y_test[0]\n",
    "\n",
    "    result = predict_digit(test_image, model_path)\n",
    "\n",
    "    print(\"\\nâœ“ Inference function test:\")\n",
    "    print(f\"  True label: {true_label}\")\n",
    "    print(f\"  Predicted: {result['digit']}\")\n",
    "    print(f\"  Confidence: {result['confidence']:.2%}\")\n",
    "    print(f\"  Latency: {result['latency_ms']:.2f} ms\")\n",
    "    print(f\"  Correct: {'âœ“' if result['digit'] == true_label else 'âœ—'}\")\n",
    "\n",
    "    # Batch prediction performance\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"6. BATCH PREDICTION PERFORMANCE\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    import time\n",
    "\n",
    "    batch_sizes = [1, 10, 50, 100]\n",
    "\n",
    "    print(f\"\\n{'Batch Size':<12} {'Total Time':<12} {'Time/Image':<15} {'Throughput'}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for batch_size in batch_sizes:\n",
    "        batch = X_test[:batch_size]\n",
    "\n",
    "        start = time.time()\n",
    "        preds = loaded_model.predict(batch, verbose=0)\n",
    "        elapsed = time.time() - start\n",
    "\n",
    "        time_per_image = (elapsed / batch_size) * 1000  # ms\n",
    "        throughput = batch_size / elapsed  # images/sec\n",
    "\n",
    "        print(\n",
    "            f\"{batch_size:<12} {elapsed:>10.3f}s {time_per_image:>12.2f} ms {throughput:>12.1f} img/s\"\n",
    "        )\n",
    "\n",
    "    print(\"\\nâœ“ Batch processing is more efficient!\")\n",
    "    print(\"  Larger batches = better GPU utilization\")\n",
    "\n",
    "    # Model conversion demonstration\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"7. MODEL CONVERSION (TensorFlow Lite)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    try:\n",
    "        # Convert to TFLite\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "        tflite_model = converter.convert()\n",
    "\n",
    "        # Save TFLite model\n",
    "        tflite_path = os.path.join(model_dir, \"digit_classifier.tflite\")\n",
    "        with open(tflite_path, \"wb\") as f:\n",
    "            f.write(tflite_model)\n",
    "\n",
    "        # Compare sizes\n",
    "        h5_size = os.path.getsize(model_path) / 1024  # KB\n",
    "        tflite_size = os.path.getsize(tflite_path) / 1024  # KB\n",
    "        compression_ratio = h5_size / tflite_size\n",
    "\n",
    "        print(\"\\nâœ“ TensorFlow Lite model created!\")\n",
    "        print(f\"  Original (H5): {h5_size:.1f} KB\")\n",
    "        print(f\"  TFLite: {tflite_size:.1f} KB\")\n",
    "        print(f\"  Compression: {compression_ratio:.1f}Ã— smaller!\")\n",
    "        print(f\"\\nðŸ’¡ TFLite benefits:\")\n",
    "        print(\"  â€¢ Optimized for mobile/embedded devices\")\n",
    "        print(\"  â€¢ Faster inference\")\n",
    "        print(\"  â€¢ Smaller file size\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâš  TFLite conversion failed: {e}\")\n",
    "        print(\"  (This is normal in some environments)\")\n",
    "\n",
    "    # Deployment summary\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"DEPLOYMENT SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(\"\\nâœ“ Model ready for deployment!\")\n",
    "    print(f\"\\nFiles created:\")\n",
    "    print(f\"  1. {model_path}\")\n",
    "    print(f\"  2. {savedmodel_dir}/\")\n",
    "    print(f\"  3. {info_path}\")\n",
    "    if os.path.exists(os.path.join(model_dir, \"digit_classifier.tflite\")):\n",
    "        print(f\"  4. {os.path.join(model_dir, 'digit_classifier.tflite')}\")\n",
    "\n",
    "    print(\"\\nâœ“ Next steps for deployment:\")\n",
    "    print(\"  1. Choose deployment method (API, cloud, mobile)\")\n",
    "    print(\"  2. Set up serving infrastructure\")\n",
    "    print(\"  3. Implement monitoring and logging\")\n",
    "    print(\"  4. Test with real data\")\n",
    "    print(\"  5. Deploy to production!\")\n",
    "\n",
    "    print(\"\\nâœ“ Example deployment commands:\")\n",
    "    print(\"\\n  Flask API:\")\n",
    "    print(\"    python app.py\")\n",
    "    print(\"\\n  Docker:\")\n",
    "    print(\"    docker build -t digit-classifier .\")\n",
    "    print(\"    docker run -p 5000:5000 digit-classifier\")\n",
    "    print(\"\\n  Cloud (AWS):\")\n",
    "    print(\"    aws sagemaker create-model ...\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ðŸ’¡ DEPLOYMENT BEST PRACTICES\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\n1. Versioning:\")\n",
    "    print(\"  â€¢ Use semantic versioning (v1.0.0, v1.1.0, v2.0.0)\")\n",
    "    print(\"  â€¢ Keep old versions for rollback\")\n",
    "    print(\"\\n2. Monitoring:\")\n",
    "    print(\"  â€¢ Track prediction latency\")\n",
    "    print(\"  â€¢ Monitor confidence scores\")\n",
    "    print(\"  â€¢ Log errors and edge cases\")\n",
    "    print(\"\\n3. Security:\")\n",
    "    print(\"  â€¢ Add authentication (API keys)\")\n",
    "    print(\"  â€¢ Rate limiting (prevent abuse)\")\n",
    "    print(\"  â€¢ Input validation (check image format)\")\n",
    "    print(\"\\n4. Performance:\")\n",
    "    print(\"  â€¢ Use batching for multiple requests\")\n",
    "    print(\"  â€¢ Cache frequent predictions\")\n",
    "    print(\"  â€¢ Use GPU for high throughput\")\n",
    "    print(\"\\n5. Testing:\")\n",
    "    print(\"  â€¢ A/B test new models\")\n",
    "    print(\"  â€¢ Canary deployments (gradual rollout)\")\n",
    "    print(\"  â€¢ Monitor performance metrics\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "else:\n",
    "    print(\"\\nSkipping deployment (TensorFlow not available)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Exercises\n",
    "\n",
    "Apply your Computer Vision and CNN knowledge with these hands-on exercises!\n",
    "\n",
    "### Exercise 1: Build a Simple CNN (Easy)\n",
    "**Task**: Create a CNN for MNIST with the following architecture:\n",
    "- Conv2D (16 filters, 3Ã—3) â†’ ReLU â†’ MaxPool\n",
    "- Conv2D (32 filters, 3Ã—3) â†’ ReLU â†’ MaxPool\n",
    "- Dense (64) â†’ Dropout (0.3) â†’ Dense (10)\n",
    "\n",
    "**Goals**:\n",
    "- Achieve >95% test accuracy\n",
    "- Count total parameters\n",
    "- Compare with a fully connected network\n",
    "\n",
    "**Hints**:\n",
    "- Use padding='same' to preserve dimensions\n",
    "- Train for 10 epochs\n",
    "- Use Adam optimizer\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise 2: Data Augmentation Experiment (Medium)\n",
    "**Task**: Train two models on a subset (1000 images) of MNIST:\n",
    "1. Without augmentation\n",
    "2. With augmentation (rotation, zoom, shift)\n",
    "\n",
    "**Goals**:\n",
    "- Compare test accuracies\n",
    "- Plot training curves\n",
    "- Analyze which benefits more from augmentation\n",
    "\n",
    "**Hints**:\n",
    "- Use ImageDataGenerator or Keras augmentation layers\n",
    "- Train both for same number of epochs\n",
    "- Use same architecture for fair comparison\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise 3: Transfer Learning (Medium)\n",
    "**Task**: Use MobileNetV2 for a custom classification task:\n",
    "- Load pre-trained MobileNetV2 (ImageNet weights)\n",
    "- Freeze base layers\n",
    "- Add custom classifier\n",
    "- Train on CIFAR-10 (or subset of MNIST)\n",
    "\n",
    "**Goals**:\n",
    "- Achieve better accuracy than training from scratch\n",
    "- Compare training time\n",
    "- Visualize learned features\n",
    "\n",
    "**Hints**:\n",
    "- Resize images to 32Ã—32 minimum\n",
    "- Convert grayscale to RGB if needed\n",
    "- Use small learning rate\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise 4: Error Analysis (Medium)\n",
    "**Task**: Analyze misclassifications from your trained model:\n",
    "- Find all misclassified examples\n",
    "- Group by confusion pairs (e.g., 3â†’8, 5â†’3)\n",
    "- Visualize most common errors\n",
    "- Propose improvements\n",
    "\n",
    "**Goals**:\n",
    "- Understand model weaknesses\n",
    "- Identify systematic errors\n",
    "- Create error report\n",
    "\n",
    "**Hints**:\n",
    "- Use confusion matrix\n",
    "- Sort by frequency\n",
    "- Look for patterns in errors\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise 5: Custom Dataset (Hard)\n",
    "**Task**: Create a CNN for a custom image classification task:\n",
    "- Collect/download a small dataset (100+ images, 3+ classes)\n",
    "- Preprocess images (resize, normalize)\n",
    "- Build and train CNN\n",
    "- Evaluate and deploy\n",
    "\n",
    "**Suggestions**:\n",
    "- Dog breeds (3-5 breeds)\n",
    "- Fruits (apple, banana, orange)\n",
    "- Hand gestures (rock, paper, scissors)\n",
    "- Traffic signs\n",
    "\n",
    "**Goals**:\n",
    "- Complete end-to-end pipeline\n",
    "- Handle real-world data\n",
    "- Deploy working model\n",
    "\n",
    "**Hints**:\n",
    "- Start with small dataset\n",
    "- Use data augmentation heavily\n",
    "- Consider transfer learning\n",
    "- Save model for reuse\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise 6: Model Optimization (Hard)\n",
    "**Task**: Optimize your CNN for production:\n",
    "- Reduce model size by 50%+\n",
    "- Maintain >90% of original accuracy\n",
    "- Measure inference speed\n",
    "\n",
    "**Techniques to try**:\n",
    "- Reduce filters per layer\n",
    "- Use fewer layers\n",
    "- Pruning\n",
    "- Quantization (TFLite)\n",
    "- Knowledge distillation\n",
    "\n",
    "**Goals**:\n",
    "- Smaller model file\n",
    "- Faster inference\n",
    "- Minimal accuracy loss\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise 7: Real-time Deployment (Advanced)\n",
    "**Task**: Deploy your digit classifier as a web app:\n",
    "- Create Flask/FastAPI endpoint\n",
    "- Accept image uploads\n",
    "- Return predictions\n",
    "- Add visualization\n",
    "\n",
    "**Features**:\n",
    "- Upload image (PNG/JPG)\n",
    "- Display predicted digit\n",
    "- Show confidence score\n",
    "- Visualize top-3 predictions\n",
    "\n",
    "**Goals**:\n",
    "- Working web service\n",
    "- Handle edge cases\n",
    "- Production-ready code\n",
    "\n",
    "**Hints**:\n",
    "- Use Flask for simplicity\n",
    "- Handle image formats correctly\n",
    "- Add error handling\n",
    "- Test with various inputs\n",
    "\n",
    "---\n",
    "\n",
    "### Bonus Challenges\n",
    "\n",
    "**Challenge 1**: Build VGG-style network\n",
    "- Stack multiple 3Ã—3 convolutions\n",
    "- Compare with simpler CNN\n",
    "\n",
    "**Challenge 2**: Implement Grad-CAM\n",
    "- Visualize which parts of image the CNN focuses on\n",
    "- Interpret model decisions\n",
    "\n",
    "**Challenge 3**: Multi-task Learning\n",
    "- Predict both digit class AND whether it's even/odd\n",
    "- Single model, two outputs\n",
    "\n",
    "**Challenge 4**: Ensemble Methods\n",
    "- Train 5 different CNNs\n",
    "- Combine predictions (voting or averaging)\n",
    "- Beat single model performance\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise Solutions\n",
    "\n",
    "Solutions for these exercises can be found in:\n",
    "- `notebooks/solutions/17_cnn_exercises_solutions.ipynb`\n",
    "\n",
    "**Remember**: Try exercises yourself first before checking solutions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise Workspace\n",
    "\n",
    "# Choose an exercise and implement it here!\n",
    "\n",
    "# Exercise 1: Build a Simple CNN\n",
    "# ---------------------------------\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "# Exercise 2: Data Augmentation Experiment\n",
    "# -----------------------------------------\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "# Exercise 3: Transfer Learning\n",
    "# ------------------------------\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "# Exercise 4: Error Analysis\n",
    "# ---------------------------\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "# Exercise 5: Custom Dataset\n",
    "# ---------------------------\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "# Exercise 6: Model Optimization\n",
    "# -------------------------------\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "# Exercise 7: Real-time Deployment\n",
    "# ---------------------------------\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Key Takeaways\n",
    "\n",
    "Congratulations on completing Module 17: Computer Vision with CNNs!\n",
    "\n",
    "### Core Concepts Mastered\n",
    "\n",
    "**1. Computer Vision Fundamentals**\n",
    "- Images are matrices of pixel values (0-255)\n",
    "- RGB images have 3 channels (Red, Green, Blue)\n",
    "- CNNs automatically learn hierarchical features\n",
    "- Traditional ML requires hand-crafted features\n",
    "\n",
    "**2. Convolutional Neural Networks (CNNs)**\n",
    "- Convolution operation: slide filters across image\n",
    "- Each filter detects different patterns (edges, textures, objects)\n",
    "- Parameters = (filter_sizeÂ² Ã— input_channels + 1) Ã— num_filters\n",
    "- Stride controls how much filter moves\n",
    "- Padding preserves spatial dimensions\n",
    "\n",
    "**3. Pooling and Padding**\n",
    "- Max pooling: downsample by taking maximum in each region\n",
    "- Reduces computational cost and overfitting\n",
    "- Padding adds zeros around border to preserve dimensions\n",
    "- Typical pattern: CONV â†’ ReLU â†’ POOL\n",
    "\n",
    "**4. CNN Architecture**\n",
    "- Standard: INPUT â†’ [CONV â†’ ReLU â†’ POOL]Ã—N â†’ FC â†’ OUTPUT\n",
    "- Most parameters in Dense layers, not Conv layers\n",
    "- Receptive field grows with depth\n",
    "- Famous architectures: LeNet, AlexNet, VGG, ResNet\n",
    "\n",
    "**5. Transfer Learning**\n",
    "- Use pre-trained models (ImageNet weights)\n",
    "- Freeze base model, train only classifier\n",
    "- 10-100Ã— faster training, better accuracy on small datasets\n",
    "- Fine-tuning: unfreeze last layers with small learning rate\n",
    "\n",
    "**6. Data Augmentation**\n",
    "- Artificially increase dataset size (2-10Ã—)\n",
    "- Techniques: rotation, flip, zoom, shift, brightness\n",
    "- Biggest gains on small datasets\n",
    "- Modern approach: Keras augmentation layers\n",
    "\n",
    "**7. Model Evaluation**\n",
    "- Confusion matrix shows class-wise errors\n",
    "- Classification report: precision, recall, F1-score\n",
    "- Error analysis: understand systematic failures\n",
    "- Confidence calibration: does 80% confidence = 80% accuracy?\n",
    "\n",
    "**8. Model Deployment**\n",
    "- Save model: .h5 (Keras), SavedModel (TensorFlow)\n",
    "- Deployment options: Flask API, cloud (AWS/GCP), mobile (TFLite)\n",
    "- Production considerations: monitoring, versioning, security\n",
    "- Batch processing more efficient than single predictions\n",
    "\n",
    "### Key Formulas\n",
    "\n",
    "**Output Size After Convolution:**\n",
    "```\n",
    "Output = (Input - Filter + 2Ã—Padding) / Stride + 1\n",
    "```\n",
    "\n",
    "**Example**: 28Ã—28 input, 3Ã—3 filter, stride=1, padding=0\n",
    "- Output = (28 - 3 + 0) / 1 + 1 = 26Ã—26\n",
    "\n",
    "**Conv Layer Parameters:**\n",
    "```\n",
    "Params = (filter_h Ã— filter_w Ã— in_channels + 1) Ã— num_filters\n",
    "```\n",
    "\n",
    "**Example**: 32 filters of 3Ã—3, input has 1 channel\n",
    "- Params = (3 Ã— 3 Ã— 1 + 1) Ã— 32 = 320\n",
    "\n",
    "### Performance Tips\n",
    "\n",
    "**For Better Accuracy:**\n",
    "- Use data augmentation (rotation, zoom, flip)\n",
    "- Try transfer learning (pre-trained models)\n",
    "- Add more Conv layers (go deeper)\n",
    "- Increase filters (32 â†’ 64 â†’ 128)\n",
    "- Use batch normalization\n",
    "- Tune learning rate\n",
    "\n",
    "**For Faster Training:**\n",
    "- Use transfer learning (freeze base model)\n",
    "- Smaller batch size if memory limited\n",
    "- Use GPU if available\n",
    "- Reduce image resolution\n",
    "- Fewer filters per layer\n",
    "\n",
    "**For Smaller Models:**\n",
    "- Reduce number of filters\n",
    "- Use global average pooling instead of FC layers\n",
    "- Convert to TFLite (quantization)\n",
    "- Knowledge distillation\n",
    "\n",
    "### Common Mistakes to Avoid\n",
    "\n",
    "âŒ **Too many parameters in Dense layers**\n",
    "â†’ Add more CONV/POOL layers, use Global Average Pooling\n",
    "\n",
    "âŒ **No data augmentation on small datasets**\n",
    "â†’ Always augment when < 10K images\n",
    "\n",
    "âŒ **Training from scratch with small dataset**\n",
    "â†’ Use transfer learning instead\n",
    "\n",
    "âŒ **Wrong input preprocessing**\n",
    "â†’ Normalize pixels to [0,1], use correct preprocessing for pre-trained models\n",
    "\n",
    "âŒ **Not monitoring validation loss**\n",
    "â†’ Track both train and validation metrics, stop if overfitting\n",
    "\n",
    "âŒ **Ignoring class imbalance**\n",
    "â†’ Use class weights, balanced sampling, or focal loss\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "**Medical Imaging:**\n",
    "- Detect tumors in X-rays/CT scans\n",
    "- Classify skin lesions (melanoma detection)\n",
    "- Diagnose diabetic retinopathy\n",
    "\n",
    "**Autonomous Vehicles:**\n",
    "- Detect pedestrians, traffic signs, vehicles\n",
    "- Lane detection\n",
    "- Semantic segmentation of road scenes\n",
    "\n",
    "**Security & Surveillance:**\n",
    "- Face recognition\n",
    "- Anomaly detection\n",
    "- License plate reading\n",
    "\n",
    "**Agriculture:**\n",
    "- Crop disease detection\n",
    "- Yield estimation from drone imagery\n",
    "- Weed detection\n",
    "\n",
    "**E-commerce:**\n",
    "- Visual search\n",
    "- Product recommendations\n",
    "- Quality control\n",
    "\n",
    "### Tools & Libraries Learned\n",
    "\n",
    "- **TensorFlow/Keras**: Model building and training\n",
    "- **sklearn**: Data splitting, metrics, evaluation\n",
    "- **NumPy**: Array operations, manual convolution\n",
    "- **Matplotlib/Seaborn**: Visualization\n",
    "- **ImageDataGenerator**: Data augmentation\n",
    "- **TFLite**: Mobile deployment\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "**To deepen your Computer Vision skills:**\n",
    "\n",
    "1. **Advanced Architectures:**\n",
    "   - ResNet (residual connections)\n",
    "   - Inception (multi-scale features)\n",
    "   - EfficientNet (compound scaling)\n",
    "   - Vision Transformers (ViT)\n",
    "\n",
    "2. **Object Detection:**\n",
    "   - YOLO, Faster R-CNN\n",
    "   - Bounding box prediction\n",
    "   - Non-max suppression\n",
    "\n",
    "3. **Semantic Segmentation:**\n",
    "   - U-Net, Mask R-CNN\n",
    "   - Pixel-wise classification\n",
    "   - Medical image segmentation\n",
    "\n",
    "4. **Generative Models:**\n",
    "   - GANs (Generative Adversarial Networks)\n",
    "   - Stable Diffusion\n",
    "   - Image-to-image translation\n",
    "\n",
    "5. **Advanced Topics:**\n",
    "   - Self-supervised learning\n",
    "   - Few-shot learning\n",
    "   - Neural architecture search\n",
    "\n",
    "### Recommended Resources\n",
    "\n",
    "**Books:**\n",
    "- \"Deep Learning for Computer Vision\" by Rajalingappaa Shanmugamani\n",
    "- \"Hands-On Computer Vision with TensorFlow 2\" by Planche & Andres\n",
    "\n",
    "**Courses:**\n",
    "- Stanford CS231n: Convolutional Neural Networks\n",
    "- Fast.ai: Practical Deep Learning for Coders\n",
    "\n",
    "**Datasets to Practice:**\n",
    "- CIFAR-10/100 (60K images, 10/100 classes)\n",
    "- ImageNet (1.2M images, 1000 classes)\n",
    "- COCO (object detection, segmentation)\n",
    "- Kaggle competitions (real-world challenges)\n",
    "\n",
    "**Communities:**\n",
    "- r/computervision (Reddit)\n",
    "- Papers with Code (latest research)\n",
    "- Kaggle forums (practical tips)\n",
    "\n",
    "---\n",
    "\n",
    "## Module Complete!\n",
    "\n",
    "You've learned:\n",
    "âœ… Computer vision fundamentals\n",
    "âœ… How CNNs work (convolution, pooling, padding)\n",
    "âœ… Building CNN architectures\n",
    "âœ… Transfer learning with pre-trained models\n",
    "âœ… Data augmentation techniques\n",
    "âœ… Complete classification pipeline\n",
    "âœ… Model deployment strategies\n",
    "\n",
    "**Next Module**: `18_nlp.ipynb` - Natural Language Processing\n",
    "\n",
    "---\n",
    "\n",
    "### Final Project Idea\n",
    "\n",
    "**Build an Image Classifier for Your Own Domain!**\n",
    "\n",
    "Choose a topic you're interested in:\n",
    "- Medical images (COVID-19 X-rays)\n",
    "- Wildlife (animal species)\n",
    "- Fashion (clothing classification)\n",
    "- Food (recipe suggestions)\n",
    "- Art (style classification)\n",
    "\n",
    "**Steps:**\n",
    "1. Collect/download dataset (100+ images per class)\n",
    "2. Preprocess and augment data\n",
    "3. Try both custom CNN and transfer learning\n",
    "4. Evaluate and analyze errors\n",
    "5. Deploy as web app\n",
    "6. Share your results!\n",
    "\n",
    "Keep practicing and building! Computer vision is one of the most impactful areas of AI. ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
