{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 13: Model Selection & Hyperparameter Tuning\n",
    "\n",
    "**Estimated Time**: 75 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this module, you will:\n",
    "- **Understand** how to choose the right model for your problem\n",
    "- **Master** cross-validation techniques to prevent overfitting\n",
    "- **Apply** Grid Search and Random Search for hyperparameter tuning\n",
    "- **Use** Bayesian Optimization for efficient parameter search\n",
    "- **Build** robust model comparison frameworks\n",
    "- **Interpret** learning curves to diagnose model performance\n",
    "- **Implement** advanced validation strategies\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Modules 00-12 completed\n",
    "- Understanding of ML algorithms (classification, regression)\n",
    "- Familiarity with scikit-learn\n",
    "\n",
    "## What is Model Selection & Hyperparameter Tuning?\n",
    "\n",
    "Every ML algorithm has two types of parameters:\n",
    "\n",
    "1. **Model Parameters** - Learned from data (e.g., weights in linear regression)\n",
    "2. **Hyperparameters** - Set before training (e.g., learning rate, tree depth)\n",
    "\n",
    "**Model selection** = Choosing the right algorithm  \n",
    "**Hyperparameter tuning** = Optimizing the algorithm's settings\n",
    "\n",
    "### Why It Matters\n",
    "\n",
    "- Default hyperparameters rarely give best performance\n",
    "- Can improve accuracy by **5-20%** or more\n",
    "- Critical for winning Kaggle competitions\n",
    "- Essential for production ML systems\n",
    "\n",
    "Let's master these skills!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_val_score,\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    learning_curve,\n",
    "    validation_curve,\n",
    "    KFold,\n",
    "    StratifiedKFold,\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "\n",
    "print(\"‚úì All libraries loaded successfully!\")\n",
    "print(\"‚úì Ready for model selection and tuning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Selection Strategies\n",
    "\n",
    "Choosing the right algorithm is crucial. Different algorithms have different strengths and weaknesses.\n",
    "\n",
    "### Common ML Algorithms & When to Use Them\n",
    "\n",
    "| Algorithm | Best For | Pros | Cons |\n",
    "|-----------|----------|------|------|\n",
    "| **Logistic Regression** | Binary classification, baseline | Fast, interpretable, no tuning | Assumes linearity |\n",
    "| **Decision Trees** | Non-linear patterns, interpretability | Easy to understand | Overfits easily |\n",
    "| **Random Forest** | General purpose, robust | Handles non-linearity, robust | Black box, slower |\n",
    "| **SVM** | Small datasets, high dimensions | Effective in high dims | Slow on large data |\n",
    "| **KNN** | Small datasets, simple patterns | Simple, no training | Slow prediction, sensitive to scale |\n",
    "| **XGBoost/LightGBM** | Kaggle, tabular data | State-of-the-art performance | Requires tuning |\n",
    "| **Neural Networks** | Images, text, complex patterns | Very flexible | Needs lots of data, hard to tune |\n",
    "\n",
    "### Decision Framework\n",
    "\n",
    "```\n",
    "Start here:\n",
    "‚îÇ\n",
    "‚îú‚îÄ Linear separable? ‚Üí Logistic Regression / SVM\n",
    "‚îÇ\n",
    "‚îú‚îÄ Small dataset (<1000 samples)? ‚Üí SVM / KNN\n",
    "‚îÇ\n",
    "‚îú‚îÄ Need interpretability? ‚Üí Decision Tree / Logistic Regression\n",
    "‚îÇ\n",
    "‚îú‚îÄ Tabular data? ‚Üí Random Forest / XGBoost\n",
    "‚îÇ\n",
    "‚îú‚îÄ Images/Text? ‚Üí Neural Networks (CNNs/RNNs)\n",
    "‚îÇ\n",
    "‚îî‚îÄ Not sure? ‚Üí Try Random Forest (good baseline)\n",
    "```\n",
    "\n",
    "### The No Free Lunch Theorem\n",
    "\n",
    "> \"No single algorithm works best for all problems.\"\n",
    "\n",
    "**Implication**: Always try multiple algorithms and compare!\n",
    "\n",
    "Let's load data and compare different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Selection - Compare Multiple Algorithms\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPARING MULTIPLE ML ALGORITHMS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"../../data_advanced/feature_engineering.csv\")\n",
    "\n",
    "# Prepare features\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le_city = LabelEncoder()\n",
    "le_job = LabelEncoder()\n",
    "\n",
    "df[\"city_encoded\"] = le_city.fit_transform(df[\"city\"])\n",
    "df[\"job_encoded\"] = le_job.fit_transform(df[\"job_category\"])\n",
    "\n",
    "# Select features\n",
    "features = [\n",
    "    \"age\",\n",
    "    \"income\",\n",
    "    \"education_years\",\n",
    "    \"experience_years\",\n",
    "    \"num_dependents\",\n",
    "    \"city_encoded\",\n",
    "    \"job_encoded\",\n",
    "]\n",
    "X = df[features]\n",
    "y = df[\"loan_approved\"]\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Scale features (important for some algorithms)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\nDataset: {X.shape[0]} samples, {X.shape[1]} features\")\n",
    "print(f\"Train set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"Class distribution: {y.value_counts().to_dict()}\")\n",
    "\n",
    "# Define models to compare\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=42, max_iter=1000),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    \"SVM\": SVC(random_state=42),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "}\n",
    "\n",
    "# Compare models\n",
    "results = []\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING AND EVALUATING MODELS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Time the training\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Train model (use scaled data for all)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred_train = model.predict(X_train_scaled)\n",
    "    y_pred_test = model.predict(X_test_scaled)\n",
    "\n",
    "    # Metrics\n",
    "    train_acc = accuracy_score(y_train, y_pred_train)\n",
    "    test_acc = accuracy_score(y_test, y_pred_test)\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    results.append(\n",
    "        {\n",
    "            \"Model\": name,\n",
    "            \"Train Accuracy\": train_acc,\n",
    "            \"Test Accuracy\": test_acc,\n",
    "            \"Overfitting\": train_acc - test_acc,\n",
    "            \"Time (s)\": training_time,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Train Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"  Test Accuracy:  {test_acc:.4f}\")\n",
    "    print(f\"  Overfitting:    {train_acc - test_acc:.4f}\")\n",
    "    print(f\"  Training Time:  {training_time:.4f}s\")\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results).sort_values(\"Test Accuracy\", ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SUMMARY COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "display(results_df)\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Train vs Test Accuracy\n",
    "x = np.arange(len(results_df))\n",
    "width = 0.35\n",
    "\n",
    "axes[0, 0].bar(\n",
    "    x - width / 2, results_df[\"Train Accuracy\"], width, label=\"Train\", alpha=0.8, color=\"skyblue\"\n",
    ")\n",
    "axes[0, 0].bar(\n",
    "    x + width / 2, results_df[\"Test Accuracy\"], width, label=\"Test\", alpha=0.8, color=\"coral\"\n",
    ")\n",
    "axes[0, 0].set_xlabel(\"Model\")\n",
    "axes[0, 0].set_ylabel(\"Accuracy\")\n",
    "axes[0, 0].set_title(\"Train vs Test Accuracy\", fontweight=\"bold\")\n",
    "axes[0, 0].set_xticks(x)\n",
    "axes[0, 0].set_xticklabels(results_df[\"Model\"], rotation=45, ha=\"right\")\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# 2. Overfitting Analysis\n",
    "colors = [\"red\" if x > 0.05 else \"green\" for x in results_df[\"Overfitting\"]]\n",
    "axes[0, 1].barh(results_df[\"Model\"], results_df[\"Overfitting\"], color=colors, alpha=0.7)\n",
    "axes[0, 1].axvline(x=0.05, color=\"orange\", linestyle=\"--\", label=\"5% threshold\")\n",
    "axes[0, 1].set_xlabel(\"Overfitting (Train - Test)\")\n",
    "axes[0, 1].set_title(\"Overfitting Analysis\", fontweight=\"bold\")\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(axis=\"x\", alpha=0.3)\n",
    "\n",
    "# 3. Training Time\n",
    "axes[1, 0].bar(\n",
    "    results_df[\"Model\"], results_df[\"Time (s)\"], color=\"lightgreen\", alpha=0.8, edgecolor=\"black\"\n",
    ")\n",
    "axes[1, 0].set_xlabel(\"Model\")\n",
    "axes[1, 0].set_ylabel(\"Time (seconds)\")\n",
    "axes[1, 0].set_title(\"Training Time Comparison\", fontweight=\"bold\")\n",
    "axes[1, 0].tick_params(axis=\"x\", rotation=45)\n",
    "plt.setp(axes[1, 0].xaxis.get_majorticklabels(), rotation=45, ha=\"right\")\n",
    "axes[1, 0].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# 4. Test Accuracy Ranking\n",
    "sorted_by_acc = results_df.sort_values(\"Test Accuracy\")\n",
    "colors_gradient = plt.cm.RdYlGn(sorted_by_acc[\"Test Accuracy\"])\n",
    "axes[1, 1].barh(\n",
    "    sorted_by_acc[\"Model\"], sorted_by_acc[\"Test Accuracy\"], color=colors_gradient, alpha=0.8\n",
    ")\n",
    "axes[1, 1].set_xlabel(\"Test Accuracy\")\n",
    "axes[1, 1].set_title(\"Model Ranking by Test Accuracy\", fontweight=\"bold\")\n",
    "axes[1, 1].grid(axis=\"x\", alpha=0.3)\n",
    "\n",
    "for i, (model, acc) in enumerate(zip(sorted_by_acc[\"Model\"], sorted_by_acc[\"Test Accuracy\"])):\n",
    "    axes[1, 1].text(acc, i, f\" {acc:.4f}\", va=\"center\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Winner\n",
    "best_model = results_df.iloc[0]\n",
    "print(\"\\nüèÜ Best Model:\", best_model[\"Model\"])\n",
    "print(f\"   Test Accuracy: {best_model['Test Accuracy']:.4f}\")\n",
    "print(f\"   Overfitting: {best_model['Overfitting']:.4f}\")\n",
    "print(\"\\n‚úì Model comparison complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cross-Validation Techniques\n",
    "\n",
    "A single train/test split can be misleading. Cross-validation provides more robust evaluation.\n",
    "\n",
    "### Why Cross-Validation?\n",
    "\n",
    "**Problem with single split:**\n",
    "- Results depend on which samples are in train vs test\n",
    "- Could get lucky (or unlucky) with the split\n",
    "- Wastes data (can't use test set for training)\n",
    "\n",
    "**Cross-validation solution:**\n",
    "- Use all data for both training and testing\n",
    "- Average performance across multiple splits\n",
    "- More reliable performance estimate\n",
    "\n",
    "### Common CV Methods\n",
    "\n",
    "1. **K-Fold Cross-Validation**\n",
    "   - Split data into K folds\n",
    "   - Train on K-1 folds, test on 1 fold\n",
    "   - Repeat K times\n",
    "   - Average the results\n",
    "\n",
    "2. **Stratified K-Fold**\n",
    "   - Like K-Fold but preserves class distribution\n",
    "   - **Use this for classification!**\n",
    "\n",
    "3. **Leave-One-Out (LOO)**\n",
    "   - K = number of samples\n",
    "   - Expensive but uses maximum data\n",
    "\n",
    "4. **Time Series Split**\n",
    "   - Respects temporal order\n",
    "   - Use for time series data\n",
    "\n",
    "### Typical K Values\n",
    "\n",
    "- K=5: Fast, good for large datasets\n",
    "- K=10: Standard choice, good balance\n",
    "- K=number of samples: Maximum accuracy estimate (slow)\n",
    "\n",
    "Let's compare single split vs cross-validation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Validation in Action\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CROSS-VALIDATION DEMONSTRATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Use our best model from previous comparison\n",
    "model = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "\n",
    "# Method 1: Single train/test split (what we did before)\n",
    "print(\"\\nMethod 1: Single Train/Test Split\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "model.fit(X_train_scaled, y_train)\n",
    "single_score = model.score(X_test_scaled, y_test)\n",
    "print(f\"Accuracy: {single_score:.4f}\")\n",
    "\n",
    "# Method 2: 5-Fold Cross-Validation\n",
    "print(\"\\nMethod 2: 5-Fold Cross-Validation\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "cv_scores_5 = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring=\"accuracy\")\n",
    "print(f\"Fold scores: {cv_scores_5}\")\n",
    "print(f\"Mean CV Accuracy: {cv_scores_5.mean():.4f} (+/- {cv_scores_5.std() * 2:.4f})\")\n",
    "\n",
    "# Method 3: 10-Fold Cross-Validation\n",
    "print(\"\\nMethod 3: 10-Fold Cross-Validation\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "cv_scores_10 = cross_val_score(model, X_train_scaled, y_train, cv=10, scoring=\"accuracy\")\n",
    "print(f\"Fold scores: {cv_scores_10}\")\n",
    "print(f\"Mean CV Accuracy: {cv_scores_10.mean():.4f} (+/- {cv_scores_10.std() * 2:.4f})\")\n",
    "\n",
    "# Method 4: Stratified K-Fold (preserves class distribution)\n",
    "print(\"\\nMethod 4: Stratified 5-Fold Cross-Validation\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores_stratified = cross_val_score(model, X_train_scaled, y_train, cv=skf, scoring=\"accuracy\")\n",
    "print(f\"Fold scores: {cv_scores_stratified}\")\n",
    "print(\n",
    "    f\"Mean CV Accuracy: {cv_scores_stratified.mean():.4f} (+/- {cv_scores_stratified.std() * 2:.4f})\"\n",
    ")\n",
    "\n",
    "# Visualize CV results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# 1. Box plot of CV scores\n",
    "cv_data = {\n",
    "    \"5-Fold\": cv_scores_5,\n",
    "    \"10-Fold\": cv_scores_10,\n",
    "    \"Stratified\\n5-Fold\": cv_scores_stratified,\n",
    "}\n",
    "\n",
    "axes[0].boxplot(cv_data.values(), labels=cv_data.keys())\n",
    "axes[0].axhline(y=single_score, color=\"red\", linestyle=\"--\", label=\"Single Split\")\n",
    "axes[0].set_ylabel(\"Accuracy\")\n",
    "axes[0].set_title(\"Cross-Validation Score Distribution\", fontweight=\"bold\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# 2. Fold-by-fold comparison\n",
    "x = np.arange(5)\n",
    "width = 0.25\n",
    "\n",
    "axes[1].bar(x - width, cv_scores_5, width, label=\"5-Fold\", alpha=0.8)\n",
    "axes[1].bar(x, cv_scores_stratified, width, label=\"Stratified 5-Fold\", alpha=0.8)\n",
    "axes[1].bar(x + width, cv_scores_10[:5], width, label=\"10-Fold (first 5)\", alpha=0.8)\n",
    "\n",
    "axes[1].axhline(y=single_score, color=\"red\", linestyle=\"--\", label=\"Single Split\", linewidth=2)\n",
    "axes[1].set_xlabel(\"Fold Number\")\n",
    "axes[1].set_ylabel(\"Accuracy\")\n",
    "axes[1].set_title(\"Fold-by-Fold Accuracy Comparison\", fontweight=\"bold\")\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels([f\"Fold {i+1}\" for i in range(5)])\n",
    "axes[1].legend()\n",
    "axes[1].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary comparison\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COMPARISON SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "comparison_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Method\": [\"Single Split\", \"5-Fold CV\", \"10-Fold CV\", \"Stratified 5-Fold CV\"],\n",
    "        \"Mean Accuracy\": [\n",
    "            single_score,\n",
    "            cv_scores_5.mean(),\n",
    "            cv_scores_10.mean(),\n",
    "            cv_scores_stratified.mean(),\n",
    "        ],\n",
    "        \"Std Dev\": [0, cv_scores_5.std(), cv_scores_10.std(), cv_scores_stratified.std()],\n",
    "        \"95% CI\": [\n",
    "            0,\n",
    "            cv_scores_5.std() * 2,\n",
    "            cv_scores_10.std() * 2,\n",
    "            cv_scores_stratified.std() * 2,\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "display(comparison_df)\n",
    "\n",
    "print(\"\\nüí° Key Insights:\")\n",
    "print(\"   ‚Ä¢ CV provides confidence intervals (mean +/- 2*std)\")\n",
    "print(\"   ‚Ä¢ More folds = more reliable estimate (but slower)\")\n",
    "print(\"   ‚Ä¢ Stratified K-Fold preserves class balance\")\n",
    "print(\"   ‚Ä¢ Single split can be misleading!\")\n",
    "print(\"\\n‚úì Cross-validation demonstrated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Grid Search\n",
    "\n",
    "Grid Search exhaustively tries all combinations of hyperparameters.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. Define a grid of hyperparameter values\n",
    "2. Try every possible combination  \n",
    "3. Evaluate each using cross-validation\n",
    "4. Return the best combination\n",
    "\n",
    "### Pros & Cons\n",
    "\n",
    "**Pros:**\n",
    "- Guaranteed to find best in grid\n",
    "- Easy to implement\n",
    "- Parallelizable\n",
    "\n",
    "**Cons:**\n",
    "- Exponentially slow\n",
    "- Wastes time on bad combos\n",
    "- Limited to discrete values\n",
    "\n",
    "### Example\n",
    "\n",
    "With 3 params (3, 4, 5 values) and 5-fold CV = **3 √ó 4 √ó 5 √ó 5 = 300 fits!**\n",
    "\n",
    "Let's tune a Random Forest!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search Implementation\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"GRID SEARCH HYPERPARAMETER TUNING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Smaller grid for demonstration\n",
    "param_grid = {\"n_estimators\": [50, 100], \"max_depth\": [5, 10, None], \"min_samples_split\": [2, 5]}\n",
    "\n",
    "print(\"\\nParameter Grid:\")\n",
    "for param, values in param_grid.items():\n",
    "    print(f\"  {param}: {values}\")\n",
    "\n",
    "total_combos = 2 * 3 * 2\n",
    "print(f\"\\nCombinations: {total_combos}\")\n",
    "print(f\"With 5-fold CV: {total_combos * 5} fits\")\n",
    "\n",
    "# Grid Search\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf, param_grid=param_grid, cv=5, scoring=\"accuracy\", n_jobs=-1, verbose=0\n",
    ")\n",
    "\n",
    "print(\"\\nRunning Grid Search...\")\n",
    "start = time.time()\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"\\n‚úì Complete in {elapsed:.2f}s\")\n",
    "print(f\"Best params: {grid_search.best_params_}\")\n",
    "print(f\"Best CV score: {grid_search.best_score_:.4f}\")\n",
    "print(f\"Test score: {grid_search.score(X_test_scaled, y_test):.4f}\")\n",
    "\n",
    "# Show all results\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "summary = results_df[[\"params\", \"mean_test_score\", \"rank_test_score\"]].sort_values(\n",
    "    \"rank_test_score\"\n",
    ")\n",
    "print(\"\\nAll combinations ranked:\")\n",
    "display(summary.head(10))\n",
    "\n",
    "print(\"\\n‚úì Grid Search complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Random Search\n",
    "\n",
    "Random Search samples random combinations instead of trying all.\n",
    "\n",
    "### Grid Search vs Random Search\n",
    "\n",
    "| Aspect | Grid Search | Random Search |\n",
    "|--------|-------------|---------------|\n",
    "| Coverage | All combinations | Random sample |\n",
    "| Speed | Slow | Fast |\n",
    "| Best for | Small grids | Large search spaces |\n",
    "| Guarantee | Finds best in grid | May miss best |\n",
    "\n",
    "### Why Random Search Works\n",
    "\n",
    "- Most parameters don't matter much\n",
    "- Random sampling explores more of the space\n",
    "- Often finds good solutions faster\n",
    "\n",
    "### When to Use\n",
    "\n",
    "- **Grid Search**: ‚â§3 parameters, small grids\n",
    "- **Random Search**: Many parameters, large ranges\n",
    "\n",
    "Let's compare!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Search Implementation\n",
    "\n",
    "from scipy.stats import randint\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"RANDOM SEARCH vs GRID SEARCH\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Random Search parameter distributions\n",
    "param_dist = {\n",
    "    \"n_estimators\": randint(50, 200),\n",
    "    \"max_depth\": [5, 10, 15, 20, None],\n",
    "    \"min_samples_split\": randint(2, 11),\n",
    "    \"min_samples_leaf\": randint(1, 5),\n",
    "}\n",
    "\n",
    "# Random Search\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=42),\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,  # Try 20 random combinations\n",
    "    cv=5,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "print(f\"\\nRandom Search: {20} random combinations\")\n",
    "print(f\"Grid Search would try: {2*5*10*4} = 400 combinations!\\n\")\n",
    "\n",
    "start = time.time()\n",
    "random_search.fit(X_train_scaled, y_train)\n",
    "rs_time = time.time() - start\n",
    "\n",
    "print(f\"Random Search time: {rs_time:.2f}s\")\n",
    "print(f\"Best params: {random_search.best_params_}\")\n",
    "print(f\"Best CV score: {random_search.best_score_:.4f}\")\n",
    "print(f\"Test score: {random_search.score(X_test_scaled, y_test):.4f}\")\n",
    "\n",
    "print(f\"\\n‚úì Random Search: {20} fits in {rs_time:.2f}s\")\n",
    "print(f\"‚úì Grid Search: {total_combos*5} fits in {elapsed:.2f}s\")\n",
    "print(f\"‚úì Speedup: {elapsed/rs_time:.1f}x faster!\")\n",
    "print(\"\\n‚úì Random Search complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Bayesian Optimization with Optuna\n",
    "\n",
    "Bayesian Optimization uses past trials to inform future searches - smarter than random!\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. Try a few random combinations\n",
    "2. Build a model of performance\n",
    "3. Use model to pick promising next trial\n",
    "4. Repeat\n",
    "\n",
    "### Advantages\n",
    "\n",
    "- Much faster than Grid/Random Search\n",
    "- Explores intelligently\n",
    "- Works with continuous parameters\n",
    "\n",
    "### When to Use\n",
    "\n",
    "- Expensive models (deep learning)\n",
    "- Many hyperparameters\n",
    "- Limited time budget\n",
    "\n",
    "**Note**: Requires `optuna` library. Install with: `pip install optuna`\n",
    "\n",
    "For now, Grid Search and Random Search are sufficient for most tabular data problems!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian Optimization - Conceptual Example\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BAYESIAN OPTIMIZATION (Conceptual)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nBayesian Optimization Workflow:\")\n",
    "print(\"1. Start with random trials\")\n",
    "print(\"2. Fit surrogate model (Gaussian Process)\")\n",
    "print(\"3. Use acquisition function to pick next trial\")\n",
    "print(\"4. Evaluate and update model\")\n",
    "print(\"5. Repeat until budget exhausted\\n\")\n",
    "\n",
    "print(\"Example libraries:\")\n",
    "print(\"  ‚Ä¢ Optuna (recommended)\")\n",
    "print(\"  ‚Ä¢ Hyperopt\")\n",
    "print(\"  ‚Ä¢ Scikit-Optimize\")\n",
    "\n",
    "print(\"\\nüí° For deep learning, Bayesian optimization can save days of tuning!\")\n",
    "print(\"‚úì Concept explained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Comparison Framework\n",
    "\n",
    "Build a systematic framework to compare multiple models fairly.\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Use same train/test split** for all models\n",
    "2. **Use cross-validation** for robust estimates\n",
    "3. **Track multiple metrics** (accuracy, precision, recall, F1)\n",
    "4. **Measure training time**\n",
    "5. **Test on unseen data**\n",
    "\n",
    "### Comparison Checklist\n",
    "\n",
    "‚úì Baseline model (simplest)  \n",
    "‚úì Multiple algorithms  \n",
    "‚úì Default hyperparameters first  \n",
    "‚úì Tune best performing models  \n",
    "‚úì Statistical significance tests  \n",
    "‚úì Document all results  \n",
    "\n",
    "We've already built this framework in Section 1!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Comparison Summary\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPLETE MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Compare: Baseline vs Tuned\n",
    "baseline_rf = RandomForestClassifier(random_state=42)\n",
    "tuned_rf = grid_search.best_estimator_\n",
    "\n",
    "models_final = {\n",
    "    \"Baseline Random Forest\": baseline_rf,\n",
    "    \"Grid Search Tuned RF\": grid_search.best_estimator_,\n",
    "    \"Random Search Tuned RF\": random_search.best_estimator_,\n",
    "}\n",
    "\n",
    "print(\"\\nFinal Comparison:\")\n",
    "for name, model in models_final.items():\n",
    "    if \"Baseline\" in name:\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "    cv_score = cross_val_score(model, X_train_scaled, y_train, cv=5).mean()\n",
    "    test_score = model.score(X_test_scaled, y_test)\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  CV Score: {cv_score:.4f}\")\n",
    "    print(f\"  Test Score: {test_score:.4f}\")\n",
    "\n",
    "print(\"\\n‚úì Model comparison framework complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Learning Curves\n",
    "\n",
    "Learning curves show how performance changes with training data size.\n",
    "\n",
    "### What They Tell Us\n",
    "\n",
    "1. **Underfitting**: Both curves low, converged\n",
    "   - Solution: More complex model\n",
    "\n",
    "2. **Overfitting**: Large gap between curves\n",
    "   - Solution: More data, regularization\n",
    "\n",
    "3. **Good Fit**: Small gap, high performance\n",
    "   - Solution: You're done!\n",
    "\n",
    "### How to Use\n",
    "\n",
    "- Plot train/validation score vs training size\n",
    "- Diagnose model issues\n",
    "- Decide if more data would help\n",
    "\n",
    "Let's visualize!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Curves Visualization\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LEARNING CURVES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate learning curve\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    scoring=\"accuracy\",\n",
    ")\n",
    "\n",
    "# Calculate mean and std\n",
    "train_mean = train_scores.mean(axis=1)\n",
    "train_std = train_scores.std(axis=1)\n",
    "val_mean = val_scores.mean(axis=1)\n",
    "val_std = val_scores.std(axis=1)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_sizes, train_mean, label=\"Training score\", marker=\"o\", linewidth=2)\n",
    "plt.plot(train_sizes, val_mean, label=\"Validation score\", marker=\"s\", linewidth=2)\n",
    "\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.15)\n",
    "plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.15)\n",
    "\n",
    "plt.xlabel(\"Training Size\", fontweight=\"bold\")\n",
    "plt.ylabel(\"Accuracy\", fontweight=\"bold\")\n",
    "plt.title(\"Learning Curves\", fontweight=\"bold\", fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nDiagnosis:\")\n",
    "print(f\"  Final training score: {train_mean[-1]:.4f}\")\n",
    "print(f\"  Final validation score: {val_mean[-1]:.4f}\")\n",
    "print(f\"  Gap: {train_mean[-1] - val_mean[-1]:.4f}\")\n",
    "\n",
    "if train_mean[-1] - val_mean[-1] > 0.05:\n",
    "    print(\"  ‚ö†Ô∏è  Overfitting detected - consider more data or regularization\")\n",
    "elif val_mean[-1] < 0.7:\n",
    "    print(\"  ‚ö†Ô∏è  Underfitting - consider more complex model\")\n",
    "else:\n",
    "    print(\"  ‚úì Good fit!\")\n",
    "\n",
    "print(\"\\n‚úì Learning curves complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Validation Strategies\n",
    "\n",
    "Choose the right validation strategy for your data type.\n",
    "\n",
    "### Common Strategies\n",
    "\n",
    "| Data Type | Strategy | Why |\n",
    "|-----------|----------|-----|\n",
    "| **Standard** | Stratified K-Fold | Preserves class balance |\n",
    "| **Time Series** | Time Series Split | Respects temporal order |\n",
    "| **Small dataset** | Leave-One-Out | Uses maximum data |\n",
    "| **Imbalanced** | Stratified K-Fold | Maintains class ratio |\n",
    "| **Grouped data** | Group K-Fold | Keeps groups together |\n",
    "\n",
    "### Time Series Split Example\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "# Ensures test data is always after train data\n",
    "```\n",
    "\n",
    "### Key Rule\n",
    "\n",
    "> **Never use future data to predict the past!**\n",
    "\n",
    "‚úì Validation strategies covered!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Strategies Example\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"VALIDATION STRATEGY COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Compare different validation strategies\n",
    "strategies = {\n",
    "    \"Stratified 5-Fold\": StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    \"Regular 5-Fold\": KFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    \"Stratified 10-Fold\": StratifiedKFold(n_splits=10, shuffle=True, random_state=42),\n",
    "}\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "\n",
    "for name, strategy in strategies.items():\n",
    "    scores = cross_val_score(model, X_train_scaled, y_train, cv=strategy, scoring=\"accuracy\")\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Scores: {scores}\")\n",
    "    print(f\"  Mean: {scores.mean():.4f} (+/- {scores.std()*2:.4f})\")\n",
    "\n",
    "print(\"\\n‚úì For classification, use Stratified K-Fold!\")\n",
    "print(\"‚úì Validation strategies complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Hands-On Exercises\n",
    "\n",
    "Practice model selection and tuning!\n",
    "\n",
    "### Exercise 1: Model Selection\n",
    "Compare Logistic Regression, SVM, and KNN on the customer_reviews dataset. Which performs best?\n",
    "\n",
    "### Exercise 2: Grid Search\n",
    "Tune a Decision Tree using Grid Search. Parameters: max_depth, min_samples_split, min_samples_leaf.\n",
    "\n",
    "### Exercise 3: Random Search\n",
    "Use Random Search to tune a Random Forest with 5+ parameters. Compare time vs Grid Search.\n",
    "\n",
    "### Exercise 4: Learning Curves\n",
    "Generate learning curves for an overfitting model. Diagnose the problem.\n",
    "\n",
    "### Exercise 5: Complete Pipeline\n",
    "Build an end-to-end pipeline:\n",
    "1. Load data\n",
    "2. Compare 3+ algorithms\n",
    "3. Tune best model with Grid/Random Search\n",
    "4. Evaluate with cross-validation\n",
    "5. Plot learning curves\n",
    "\n",
    "Ready to practice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise Workspace\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EXERCISES - Practice Your Skills!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Exercise 1: Model Selection\n",
    "print(\"\\nExercise 1: Model Selection\")\n",
    "print(\"-\" * 60)\n",
    "print(\"TODO: Load customer_reviews.csv\")\n",
    "print(\"TODO: Compare 3+ algorithms\")\n",
    "print(\"TODO: Report best model and scores\\n\")\n",
    "\n",
    "# Exercise 2: Grid Search\n",
    "print(\"Exercise 2: Grid Search\")\n",
    "print(\"-\" * 60)\n",
    "print(\"TODO: Define parameter grid for Decision Tree\")\n",
    "print(\"TODO: Run GridSearchCV\")\n",
    "print(\"TODO: Report best parameters\\n\")\n",
    "\n",
    "# Exercise 3: Random Search\n",
    "print(\"Exercise 3: Random Search\")\n",
    "print(\"-\" * 60)\n",
    "print(\"TODO: Define parameter distributions\")\n",
    "print(\"TODO: Run RandomizedSearchCV\")\n",
    "print(\"TODO: Compare time with Grid Search\\n\")\n",
    "\n",
    "# Your code here for exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Key Takeaways & Next Steps\n",
    "\n",
    "Excellent work mastering model selection and hyperparameter tuning!\n",
    "\n",
    "### What You've Learned\n",
    "\n",
    "#### 1. **Model Selection**\n",
    "- ‚úì How to choose the right algorithm\n",
    "- ‚úì Comparing multiple models systematically\n",
    "- ‚úì Baseline ‚Üí Multiple algorithms ‚Üí Tune best\n",
    "- ‚úì No Free Lunch Theorem\n",
    "\n",
    "#### 2. **Cross-Validation**\n",
    "- ‚úì K-Fold and Stratified K-Fold\n",
    "- ‚úì Why single split can be misleading\n",
    "- ‚úì Confidence intervals from CV scores\n",
    "- ‚úì When to use which strategy\n",
    "\n",
    "#### 3. **Hyperparameter Tuning**\n",
    "- ‚úì Grid Search - exhaustive but slow\n",
    "- ‚úì Random Search - fast and effective\n",
    "- ‚úì Bayesian Optimization - intelligent search\n",
    "- ‚úì When to use each method\n",
    "\n",
    "#### 4. **Model Diagnostics**\n",
    "- ‚úì Learning curves for overfitting detection\n",
    "- ‚úì Validation strategies for different data types\n",
    "- ‚úì Model comparison frameworks\n",
    "- ‚úì Performance metrics tracking\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "> **\"Hyperparameter tuning can improve performance by 5-20%, but good features can improve it by 50%+\"**\n",
    "\n",
    "**Tuning Hierarchy:**\n",
    "1. **Feature engineering** (biggest impact)\n",
    "2. **Algorithm selection** (moderate impact)\n",
    "3. **Hyperparameter tuning** (smaller but important)\n",
    "\n",
    "### Best Practices Checklist\n",
    "\n",
    "‚úì Always start with baseline model  \n",
    "‚úì Use cross-validation (not single split)  \n",
    "‚úì Try multiple algorithms before tuning  \n",
    "‚úì Use Stratified K-Fold for classification  \n",
    "‚úì Grid Search for ‚â§3 parameters, Random Search for more  \n",
    "‚úì Plot learning curves to diagnose issues  \n",
    "‚úì Test final model on held-out test set  \n",
    "‚úì Document all experiments  \n",
    "\n",
    "### Quick Reference Table\n",
    "\n",
    "| Task | Method | Code |\n",
    "|------|--------|------|\n",
    "| Compare models | cross_val_score | `cross_val_score(model, X, y, cv=5)` |\n",
    "| Tune parameters | GridSearchCV | `GridSearchCV(model, param_grid, cv=5)` |\n",
    "| Fast tuning | RandomizedSearchCV | `RandomizedSearchCV(model, params, n_iter=20)` |\n",
    "| Learning curves | learning_curve | `learning_curve(model, X, y, cv=5)` |\n",
    "\n",
    "### Common Pitfalls\n",
    "\n",
    "1. **Tuning on test set** - Always use CV on training data only!\n",
    "2. **Data leakage** - Fit scalers on training data, transform test data\n",
    "3. **Ignoring baseline** - Always compare against simple model\n",
    "4. **Over-tuning** - Diminishing returns, focus on features instead\n",
    "5. **Wrong CV strategy** - Use Stratified for classification, Time Series Split for temporal data\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "Model selection and tuning is essential in:\n",
    "- **Healthcare**: Optimizing disease prediction models\n",
    "- **Finance**: Credit scoring and fraud detection\n",
    "- **E-commerce**: Recommendation system optimization\n",
    "- **Manufacturing**: Predictive maintenance tuning\n",
    "- **Competitions**: Kaggle leaderboard climbing\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "#### Module 14: Ensemble Methods\n",
    "- XGBoost, LightGBM, CatBoost\n",
    "- Stacking and blending\n",
    "- Kaggle-winning techniques\n",
    "\n",
    "#### Practice Projects\n",
    "1. Titanic competition (Kaggle)\n",
    "2. House Prices competition\n",
    "3. Your own dataset\n",
    "\n",
    "#### Advanced Topics\n",
    "- Nested cross-validation\n",
    "- Time series cross-validation\n",
    "- Bayesian optimization deep dive\n",
    "- AutoML tools (Auto-sklearn, H2O)\n",
    "\n",
    "### Recommended Practice\n",
    "\n",
    "Spend **2-3 hours**:\n",
    "1. Complete all exercises\n",
    "2. Apply to your own dataset\n",
    "3. Participate in a Kaggle competition\n",
    "4. Build a model comparison template\n",
    "\n",
    "### Resources\n",
    "\n",
    "**Documentation:**\n",
    "- [scikit-learn Model Selection](https://scikit-learn.org/stable/model_selection.html)\n",
    "- [GridSearchCV Guide](https://scikit-learn.org/stable/modules/grid_search.html)\n",
    "\n",
    "**Books:**\n",
    "- \"Hands-On Machine Learning\" by Aur√©lien G√©ron (Chapter 2)\n",
    "- \"Python Machine Learning\" by Sebastian Raschka\n",
    "\n",
    "**Tools:**\n",
    "- Optuna for Bayesian Optimization\n",
    "- Weights & Biases for experiment tracking\n",
    "- MLflow for model management\n",
    "\n",
    "---\n",
    "\n",
    "### Module Complete! üéâ\n",
    "\n",
    "**Skills Gained:**\n",
    "- Model selection strategies\n",
    "- Cross-validation mastery\n",
    "- Hyperparameter tuning (Grid, Random, Bayesian)\n",
    "- Learning curve interpretation\n",
    "- Production-ready validation pipelines\n",
    "\n",
    "**Next Module**: `14_ensemble_methods.ipynb` - Learn XGBoost and win Kaggle!\n",
    "\n",
    "---\n",
    "\n",
    "*Built with Claude Code | Module 13: Model Selection & Hyperparameter Tuning*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
