{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 12: Feature Engineering Mastery\n",
    "\n",
    "**Estimated Time**: 75 minutes\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this module, you will:\n",
    "- **Understand** why feature engineering is crucial for ML success\n",
    "- **Master** techniques for transforming numerical and categorical features\n",
    "- **Extract** meaningful features from datetime data\n",
    "- **Create** polynomial and interaction features\n",
    "- **Apply** feature selection methods to improve model performance\n",
    "- **Analyze** feature importance to understand model decisions\n",
    "- **Build** automated feature engineering pipelines\n",
    "- **Practice** end-to-end feature engineering on real datasets\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Modules 00-11 completed (especially pandas, scikit-learn)\n",
    "- Understanding of basic ML concepts\n",
    "- Familiarity with supervised learning algorithms\n",
    "\n",
    "## What is Feature Engineering?\n",
    "\n",
    "> \"Coming up with features is difficult, time-consuming, requires expert knowledge. 'Applied machine learning' is basically feature engineering.\" ‚Äî **Andrew Ng**\n",
    "\n",
    "**Feature engineering** is the process of using domain knowledge to create features (input variables) that make machine learning algorithms work better. It's often the difference between a mediocre model and a winning solution.\n",
    "\n",
    "### Why It Matters\n",
    "\n",
    "- Can improve model accuracy by **10-50%** or more\n",
    "- Often more impactful than algorithm choice\n",
    "- Requires creativity and domain understanding\n",
    "- Key skill that separates good from great data scientists\n",
    "\n",
    "### The Feature Engineering Process\n",
    "\n",
    "```\n",
    "Raw Data ‚Üí Feature Creation ‚Üí Feature Transformation ‚Üí Feature Selection ‚Üí Model Training\n",
    "```\n",
    "\n",
    "Let's master each step!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler,\n",
    "    MinMaxScaler,\n",
    "    LabelEncoder,\n",
    "    OneHotEncoder,\n",
    "    PolynomialFeatures,\n",
    ")\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE, SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Set style for better visualizations\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "\n",
    "print(\"‚úì All libraries loaded successfully!\")\n",
    "print(\"‚úì Ready for feature engineering!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Feature Engineering\n",
    "\n",
    "Feature engineering is the art and science of transforming raw data into features that better represent the underlying problem to the predictive models.\n",
    "\n",
    "### Types of Features\n",
    "\n",
    "1. **Numerical Features**\n",
    "   - Continuous: height, weight, price\n",
    "   - Discrete: count of items, number of clicks\n",
    "   \n",
    "2. **Categorical Features**\n",
    "   - Nominal: color, city, product category\n",
    "   - Ordinal: education level, satisfaction rating\n",
    "   \n",
    "3. **Datetime Features**\n",
    "   - Timestamps, dates, time periods\n",
    "   - Can extract: year, month, day, hour, day_of_week, etc.\n",
    "   \n",
    "4. **Text Features**\n",
    "   - Descriptions, reviews, comments\n",
    "   - Requires special processing (covered in NLP module)\n",
    "\n",
    "### Common Feature Engineering Techniques\n",
    "\n",
    "| Technique | Description | When to Use |\n",
    "|-----------|-------------|-------------|\n",
    "| **Scaling** | Normalize feature ranges | Tree-based models don't need, linear models do |\n",
    "| **Encoding** | Convert categorical to numerical | All ML models require numerical input |\n",
    "| **Binning** | Group continuous values into bins | Create categorical from numerical |\n",
    "| **Transformation** | Log, sqrt, power transforms | Handle skewed distributions |\n",
    "| **Interaction** | Combine multiple features | Capture feature relationships |\n",
    "| **Polynomial** | Create higher-degree features | Capture non-linear patterns |\n",
    "\n",
    "### Load the Dataset\n",
    "\n",
    "We'll use the feature engineering dataset created in `data_advanced/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the feature engineering dataset\n",
    "df = pd.read_csv(\"../../data_advanced/feature_engineering.csv\")\n",
    "\n",
    "print(f\"Dataset loaded: {df.shape[0]} rows √ó {df.shape[1]} columns\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Display first few rows\n",
    "display(df.head(10))\n",
    "\n",
    "# Data types and info\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DATA TYPES\")\n",
    "print(\"=\" * 60)\n",
    "print(df.dtypes)\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"NUMERICAL FEATURES - STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "display(df.describe())\n",
    "\n",
    "# Target variable distribution\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TARGET VARIABLE (loan_approved)\")\n",
    "print(\"=\" * 60)\n",
    "print(df[\"loan_approved\"].value_counts())\n",
    "print(f\"\\nApproval Rate: {df['loan_approved'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Numerical Feature Engineering\n",
    "\n",
    "Numerical features often require transformation to improve model performance.\n",
    "\n",
    "### Key Techniques for Numerical Features\n",
    "\n",
    "1. **Scaling/Normalization**\n",
    "   - **StandardScaler**: Mean=0, Std=1 (assumes normal distribution)\n",
    "   - **MinMaxScaler**: Scale to [0, 1] range\n",
    "   - **RobustScaler**: Resistant to outliers (uses median, IQR)\n",
    "\n",
    "2. **Transformations**\n",
    "   - **Log transform**: For right-skewed data\n",
    "   - **Square root**: For moderate skewness\n",
    "   - **Box-Cox**: Automatic optimal transformation\n",
    "\n",
    "3. **Binning/Discretization**\n",
    "   - Convert continuous ‚Üí categorical\n",
    "   - Useful for age groups, income brackets, etc.\n",
    "\n",
    "4. **Creating Derived Features**\n",
    "   - Ratios: income_per_dependent = income / num_dependents\n",
    "   - Differences: experience_gap = age - education_years - 18\n",
    "   - Aggregations: total, average, etc.\n",
    "\n",
    "### When to Use Which Scaler?\n",
    "\n",
    "| Scaler | Use When | Example |\n",
    "|--------|----------|---------|\n",
    "| **StandardScaler** | Normal distribution | Heights, weights |\n",
    "| **MinMaxScaler** | Bounded range needed | Neural networks, image pixels |\n",
    "| **RobustScaler** | Outliers present | Financial data, real estate |\n",
    "| **No scaling** | Tree-based models | Random Forest, XGBoost |\n",
    "\n",
    "Let's apply these techniques!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical Feature Engineering Examples\n",
    "\n",
    "# 1. SCALING - Compare different scalers\n",
    "print(\"=\" * 60)\n",
    "print(\"1. FEATURE SCALING COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Select numerical features\n",
    "numerical_features = [\"age\", \"income\", \"education_years\", \"experience_years\"]\n",
    "sample_data = df[numerical_features].head()\n",
    "\n",
    "print(\"\\nOriginal Values:\")\n",
    "display(sample_data)\n",
    "\n",
    "# Standard Scaler\n",
    "scaler_standard = StandardScaler()\n",
    "scaled_standard = pd.DataFrame(\n",
    "    scaler_standard.fit_transform(sample_data), columns=[f\"{col}_std\" for col in numerical_features]\n",
    ")\n",
    "\n",
    "print(\"\\nStandardScaler (mean=0, std=1):\")\n",
    "display(scaled_standard)\n",
    "\n",
    "# MinMax Scaler\n",
    "scaler_minmax = MinMaxScaler()\n",
    "scaled_minmax = pd.DataFrame(\n",
    "    scaler_minmax.fit_transform(sample_data),\n",
    "    columns=[f\"{col}_minmax\" for col in numerical_features],\n",
    ")\n",
    "\n",
    "print(\"\\nMinMaxScaler (range=[0,1]):\")\n",
    "display(scaled_minmax)\n",
    "\n",
    "# 2. TRANSFORMATIONS - Handle skewed data\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"2. HANDLING SKEWED DISTRIBUTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check skewness of income\n",
    "print(f\"\\nIncome Skewness: {df['income'].skew():.2f}\")\n",
    "print(\"(Skewness > 1 or < -1 indicates high skewness)\")\n",
    "\n",
    "# Log transformation for right-skewed data\n",
    "df[\"income_log\"] = np.log1p(df[\"income\"])  # log1p = log(1 + x) to handle zeros\n",
    "\n",
    "# Visualize before and after\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(df[\"income\"], bins=30, edgecolor=\"black\", alpha=0.7)\n",
    "axes[0].set_title(f'Original Income\\nSkewness: {df[\"income\"].skew():.2f}')\n",
    "axes[0].set_xlabel(\"Income ($)\")\n",
    "axes[0].set_ylabel(\"Frequency\")\n",
    "\n",
    "axes[1].hist(df[\"income_log\"], bins=30, edgecolor=\"black\", alpha=0.7, color=\"green\")\n",
    "axes[1].set_title(f'Log-Transformed Income\\nSkewness: {df[\"income_log\"].skew():.2f}')\n",
    "axes[1].set_xlabel(\"Log(Income)\")\n",
    "axes[1].set_ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. BINNING - Create categorical from numerical\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"3. BINNING NUMERICAL FEATURES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create age groups\n",
    "df[\"age_group\"] = pd.cut(\n",
    "    df[\"age\"], bins=[0, 25, 35, 50, 100], labels=[\"Young\", \"Early Career\", \"Mid Career\", \"Senior\"]\n",
    ")\n",
    "\n",
    "print(\"\\nAge Group Distribution:\")\n",
    "print(df[\"age_group\"].value_counts().sort_index())\n",
    "\n",
    "# 4. DERIVED FEATURES - Create new features\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"4. CREATING DERIVED FEATURES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Income per dependent\n",
    "df[\"income_per_dependent\"] = df[\"income\"] / (df[\"num_dependents\"] + 1)\n",
    "\n",
    "# Years of experience per education year (efficiency metric)\n",
    "df[\"experience_efficiency\"] = df[\"experience_years\"] / (df[\"education_years\"] + 1)\n",
    "\n",
    "# Age when started working\n",
    "df[\"work_start_age\"] = df[\"age\"] - df[\"experience_years\"]\n",
    "\n",
    "print(\"\\nNew Derived Features Created:\")\n",
    "print(\"‚úì income_per_dependent\")\n",
    "print(\"‚úì experience_efficiency\")\n",
    "print(\"‚úì work_start_age\")\n",
    "\n",
    "print(\"\\nSample of new features:\")\n",
    "display(\n",
    "    df[\n",
    "        [\n",
    "            \"income\",\n",
    "            \"num_dependents\",\n",
    "            \"income_per_dependent\",\n",
    "            \"experience_years\",\n",
    "            \"education_years\",\n",
    "            \"experience_efficiency\",\n",
    "        ]\n",
    "    ].head()\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Numerical feature engineering complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Categorical Encoding Strategies\n",
    "\n",
    "Most machine learning algorithms require numerical input. We need to convert categorical variables into numbers.\n",
    "\n",
    "### Common Encoding Methods\n",
    "\n",
    "1. **Label Encoding**\n",
    "   - Assigns each category a unique integer\n",
    "   - Best for: Ordinal data (Low < Medium < High)\n",
    "   - Warning: Creates unintended ordinality\n",
    "\n",
    "2. **One-Hot Encoding**\n",
    "   - Creates binary column for each category\n",
    "   - Best for: Nominal data (no natural order)\n",
    "   - Warning: Can create too many features (high cardinality)\n",
    "\n",
    "3. **Target Encoding (Mean Encoding)**\n",
    "   - Replaces category with mean of target variable\n",
    "   - Best for: High cardinality features\n",
    "   - Warning: Can cause overfitting (use cross-validation)\n",
    "\n",
    "4. **Frequency Encoding**\n",
    "   - Replaces category with its frequency/count\n",
    "   - Best for: When frequency matters\n",
    "   \n",
    "5. **Binary Encoding**\n",
    "   - Converts to binary representation\n",
    "   - Best for: High cardinality with memory constraints\n",
    "\n",
    "### Choosing the Right Encoding\n",
    "\n",
    "| Scenario | Recommended Encoding |\n",
    "|----------|---------------------|\n",
    "| Ordinal data (Education: HS < Bachelor < Master) | **Label Encoding** |\n",
    "| Nominal data (Color: Red, Blue, Green) | **One-Hot Encoding** |\n",
    "| High cardinality (1000+ unique cities) | **Target/Frequency Encoding** |\n",
    "| Tree-based models | **Label/Target Encoding** |\n",
    "| Linear models | **One-Hot Encoding** |\n",
    "\n",
    "Let's see examples of each!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical Encoding Examples\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CATEGORICAL FEATURES IN OUR DATASET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "categorical_features = [\"city\", \"job_category\"]\n",
    "print(f\"\\nCategorical columns: {categorical_features}\\n\")\n",
    "\n",
    "for col in categorical_features:\n",
    "    print(f\"{col}:\")\n",
    "    print(df[col].value_counts())\n",
    "    print()\n",
    "\n",
    "# 1. LABEL ENCODING\n",
    "print(\"=\" * 60)\n",
    "print(\"1. LABEL ENCODING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Apply label encoding to job_category\n",
    "le = LabelEncoder()\n",
    "df[\"job_category_label\"] = le.fit_transform(df[\"job_category\"])\n",
    "\n",
    "print(\"\\nOriginal vs Label Encoded:\")\n",
    "comparison = (\n",
    "    df[[\"job_category\", \"job_category_label\"]].drop_duplicates().sort_values(\"job_category_label\")\n",
    ")\n",
    "display(comparison)\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  Note: Numbers don't imply order (Education=0 doesn't mean it's 'less than' Finance=1)\")\n",
    "\n",
    "# 2. ONE-HOT ENCODING\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"2. ONE-HOT ENCODING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create dummy variables for city\n",
    "city_dummies = pd.get_dummies(df[\"city\"], prefix=\"city\")\n",
    "\n",
    "print(f\"\\nOriginal feature: 1 column (city)\")\n",
    "print(f\"One-hot encoded: {city_dummies.shape[1]} columns\")\n",
    "print(f\"\\nNew columns created:\")\n",
    "print(city_dummies.columns.tolist())\n",
    "\n",
    "print(\"\\nSample of one-hot encoded data:\")\n",
    "display(pd.concat([df[[\"city\"]].head(), city_dummies.head()], axis=1))\n",
    "\n",
    "print(\"\\nüí° Insight: Each city gets its own binary column\")\n",
    "\n",
    "# 3. FREQUENCY ENCODING\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"3. FREQUENCY ENCODING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate frequency for each city\n",
    "city_counts = df[\"city\"].value_counts()\n",
    "df[\"city_frequency\"] = df[\"city\"].map(city_counts)\n",
    "\n",
    "print(\"\\nCity Frequency Mapping:\")\n",
    "print(city_counts)\n",
    "\n",
    "print(\"\\nSample of frequency encoded data:\")\n",
    "display(df[[\"city\", \"city_frequency\"]].head(10))\n",
    "\n",
    "# 4. TARGET ENCODING\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"4. TARGET ENCODING (Mean Encoding)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate mean of target variable for each city\n",
    "city_target_mean = df.groupby(\"city\")[\"loan_approved\"].mean()\n",
    "df[\"city_target_encoded\"] = df[\"city\"].map(city_target_mean)\n",
    "\n",
    "print(\"\\nTarget Encoding Mapping:\")\n",
    "print(\"(Mean loan approval rate per city)\")\n",
    "for city, mean_approval in city_target_mean.items():\n",
    "    print(f\"{city:15s}: {mean_approval:.2%}\")\n",
    "\n",
    "print(\"\\nSample of target encoded data:\")\n",
    "display(df[[\"city\", \"loan_approved\", \"city_target_encoded\"]].head(10))\n",
    "\n",
    "print(\"\\nüí° Cities with higher approval rates get higher encoded values\")\n",
    "\n",
    "# 5. COMPARISON OF ALL ENCODINGS\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"5. ENCODING COMPARISON SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Original\": df[\"city\"].head(8),\n",
    "        \"Label_Encoded\": le.fit_transform(df[\"city\"].head(8)),\n",
    "        \"Frequency\": df[\"city_frequency\"].head(8),\n",
    "        \"Target_Encoded\": df[\"city_target_encoded\"].head(8).round(3),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Add one-hot columns\n",
    "for col in city_dummies.columns:\n",
    "    comparison_df[col] = city_dummies[col].head(8).values\n",
    "\n",
    "print(\"\\nSide-by-side comparison:\")\n",
    "display(comparison_df)\n",
    "\n",
    "print(\"\\n‚úì Categorical encoding strategies demonstrated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Datetime Feature Extraction\n",
    "\n",
    "Datetime features contain rich information that can be extracted into multiple useful features.\n",
    "\n",
    "### What Can We Extract from Dates?\n",
    "\n",
    "From a single datetime column, you can create:\n",
    "\n",
    "1. **Temporal Components**\n",
    "   - Year, Month, Day, Hour, Minute, Second\n",
    "   - Day of week, Day of year, Week of year\n",
    "   - Quarter, Semester\n",
    "\n",
    "2. **Cyclical Features**\n",
    "   - Is weekend?, Is holiday?, Is business hour?\n",
    "   - Season (Spring, Summer, Fall, Winter)\n",
    "   - Beginning/End of month\n",
    "\n",
    "3. **Time-Based Features**\n",
    "   - Time since reference date\n",
    "   - Age, tenure, days until event\n",
    "   - Time between events\n",
    "\n",
    "4. **Cyclical Encoding**\n",
    "   - Convert cyclical features (month, day_of_week) to sin/cos\n",
    "   - Preserves cycl nature: December (12) is close to January (1)\n",
    "\n",
    "### Example: Customer Registration Date\n",
    "\n",
    "Let's create a sample datetime feature and extract useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datetime Feature Extraction Examples\n",
    "\n",
    "# Create sample registration dates for our dataset\n",
    "np.random.seed(42)\n",
    "base_date = pd.to_datetime(\"2020-01-01\")\n",
    "random_days = np.random.randint(0, 1460, size=len(df))  # 4 years of dates\n",
    "df[\"registration_date\"] = base_date + pd.to_timedelta(random_days, unit=\"D\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATETIME FEATURE EXTRACTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nOriginal datetime column:\")\n",
    "print(df[\"registration_date\"].head(10))\n",
    "\n",
    "# 1. BASIC TEMPORAL COMPONENTS\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"1. EXTRACTING TEMPORAL COMPONENTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "df[\"reg_year\"] = df[\"registration_date\"].dt.year\n",
    "df[\"reg_month\"] = df[\"registration_date\"].dt.month\n",
    "df[\"reg_day\"] = df[\"registration_date\"].dt.day\n",
    "df[\"reg_day_of_week\"] = df[\"registration_date\"].dt.dayofweek  # Monday=0, Sunday=6\n",
    "df[\"reg_day_name\"] = df[\"registration_date\"].dt.day_name()\n",
    "df[\"reg_quarter\"] = df[\"registration_date\"].dt.quarter\n",
    "df[\"reg_week_of_year\"] = df[\"registration_date\"].dt.isocalendar().week\n",
    "\n",
    "print(\"\\nExtracted features:\")\n",
    "temporal_features = df[\n",
    "    [\n",
    "        \"registration_date\",\n",
    "        \"reg_year\",\n",
    "        \"reg_month\",\n",
    "        \"reg_day\",\n",
    "        \"reg_day_of_week\",\n",
    "        \"reg_day_name\",\n",
    "        \"reg_quarter\",\n",
    "    ]\n",
    "].head(10)\n",
    "display(temporal_features)\n",
    "\n",
    "# 2. BOOLEAN FLAGS\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"2. CREATING BOOLEAN FLAGS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "df[\"is_weekend\"] = (df[\"reg_day_of_week\"] >= 5).astype(int)\n",
    "df[\"is_month_start\"] = df[\"registration_date\"].dt.is_month_start.astype(int)\n",
    "df[\"is_month_end\"] = df[\"registration_date\"].dt.is_month_end.astype(int)\n",
    "df[\"is_quarter_start\"] = df[\"registration_date\"].dt.is_quarter_start.astype(int)\n",
    "\n",
    "\n",
    "# Determine season\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return \"Winter\"\n",
    "    elif month in [3, 4, 5]:\n",
    "        return \"Spring\"\n",
    "    elif month in [6, 7, 8]:\n",
    "        return \"Summer\"\n",
    "    else:\n",
    "        return \"Fall\"\n",
    "\n",
    "\n",
    "df[\"season\"] = df[\"reg_month\"].apply(get_season)\n",
    "\n",
    "print(\"\\nBoolean flags created:\")\n",
    "flag_sample = df[\n",
    "    [\"registration_date\", \"is_weekend\", \"is_month_start\", \"is_month_end\", \"season\"]\n",
    "].head(10)\n",
    "display(flag_sample)\n",
    "\n",
    "# 3. TIME-BASED FEATURES\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"3. TIME-BASED CALCULATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Days since registration (tenure)\n",
    "reference_date = pd.to_datetime(\"2024-01-01\")\n",
    "df[\"days_since_registration\"] = (reference_date - df[\"registration_date\"]).dt.days\n",
    "df[\"years_since_registration\"] = df[\"days_since_registration\"] / 365.25\n",
    "\n",
    "print(f\"\\nReference date: {reference_date.date()}\")\n",
    "print(\"\\nTenure calculation:\")\n",
    "tenure_sample = df[\n",
    "    [\"registration_date\", \"days_since_registration\", \"years_since_registration\"]\n",
    "].head(10)\n",
    "display(tenure_sample)\n",
    "\n",
    "# 4. CYCLICAL ENCODING\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"4. CYCLICAL ENCODING (Sin/Cos Transformation)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Encode month cyclically\n",
    "df[\"month_sin\"] = np.sin(2 * np.pi * df[\"reg_month\"] / 12)\n",
    "df[\"month_cos\"] = np.cos(2 * np.pi * df[\"reg_month\"] / 12)\n",
    "\n",
    "# Encode day of week cyclically\n",
    "df[\"day_of_week_sin\"] = np.sin(2 * np.pi * df[\"reg_day_of_week\"] / 7)\n",
    "df[\"day_of_week_cos\"] = np.cos(2 * np.pi * df[\"reg_day_of_week\"] / 7)\n",
    "\n",
    "print(\"\\nCyclical encoding preserves the circular nature:\")\n",
    "print(\"December (12) and January (1) are now mathematically close!\\n\")\n",
    "\n",
    "cyclical_sample = (\n",
    "    df[[\"reg_month\", \"month_sin\", \"month_cos\"]].drop_duplicates().sort_values(\"reg_month\")\n",
    ")\n",
    "display(cyclical_sample)\n",
    "\n",
    "# Visualize cyclical encoding\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Month encoding\n",
    "months = np.arange(1, 13)\n",
    "month_sin = np.sin(2 * np.pi * months / 12)\n",
    "month_cos = np.cos(2 * np.pi * months / 12)\n",
    "\n",
    "axes[0].plot(months, month_sin, \"o-\", label=\"sin(month)\", linewidth=2)\n",
    "axes[0].plot(months, month_cos, \"s-\", label=\"cos(month)\", linewidth=2)\n",
    "axes[0].set_xlabel(\"Month\")\n",
    "axes[0].set_ylabel(\"Encoded Value\")\n",
    "axes[0].set_title(\"Cyclical Encoding of Months\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_xticks(months)\n",
    "\n",
    "# Polar plot showing cyclical nature\n",
    "theta = 2 * np.pi * months / 12\n",
    "axes[1] = plt.subplot(122, projection=\"polar\")\n",
    "axes[1].plot(theta, np.ones_like(theta), \"o-\", markersize=10, linewidth=2)\n",
    "for i, month in enumerate(\n",
    "    [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n",
    "):\n",
    "    axes[1].text(theta[i], 1.1, month, ha=\"center\", va=\"center\")\n",
    "axes[1].set_title(\"Months as Cyclical Feature\\n(December is close to January)\", pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Datetime feature extraction complete!\")\n",
    "print(\n",
    "    f\"‚úì Created {sum(df.columns.str.contains('reg_|month_|day_of_week_|is_|season|days_since|years_since'))} new features from 1 datetime column!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Polynomial and Interaction Features\n",
    "\n",
    "Linear models can only capture linear relationships. Polynomial and interaction features help capture non-linear patterns.\n",
    "\n",
    "### Polynomial Features\n",
    "\n",
    "Transform features into higher-degree polynomials:\n",
    "- **Degree 2**: x, x¬≤, x*y, y, y¬≤\n",
    "- **Degree 3**: x, x¬≤, x¬≥, x*y, x¬≤*y, x*y¬≤, y, y¬≤, y¬≥\n",
    "\n",
    "**When to use:**\n",
    "- Linear models (Linear Regression, Logistic Regression)\n",
    "- When you suspect non-linear relationships\n",
    "- Can significantly improve model performance\n",
    "\n",
    "**Warning:**\n",
    "- Creates many features (can cause overfitting)\n",
    "- Increases computational cost\n",
    "- Use feature selection after creating polynomials\n",
    "\n",
    "### Interaction Features\n",
    "\n",
    "Multiply pairs of features to capture their combined effect:\n",
    "- income * education_years\n",
    "- age * num_dependents\n",
    "\n",
    "**Examples of useful interactions:**\n",
    "- Price √ó Quantity = Total Value\n",
    "- Hours Worked √ó Hourly Rate = Earnings\n",
    "- Bedroom Count √ó Square Footage = Spaciousness Score\n",
    "\n",
    "Let's create these features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial and Interaction Features\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"POLYNOMIAL & INTERACTION FEATURES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Select a few numerical features for demonstration\n",
    "base_features = [\"age\", \"income\", \"education_years\"]\n",
    "X_sample = df[base_features].head(5)\n",
    "\n",
    "print(\"\\nOriginal features:\")\n",
    "print(f\"Shape: {X_sample.shape}\")\n",
    "display(X_sample)\n",
    "\n",
    "# 1. POLYNOMIAL FEATURES (Degree 2)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"1. CREATING POLYNOMIAL FEATURES (Degree 2)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly.fit_transform(X_sample)\n",
    "\n",
    "# Get feature names\n",
    "poly_feature_names = poly.get_feature_names_out(base_features)\n",
    "\n",
    "print(f\"\\nOriginal features: {len(base_features)}\")\n",
    "print(f\"Polynomial features (degree=2): {len(poly_feature_names)}\")\n",
    "print(f\"\\nNew features created:\")\n",
    "for i, name in enumerate(poly_feature_names):\n",
    "    print(f\"  {i+1}. {name}\")\n",
    "\n",
    "print(\"\\nPolynomial features (first 3 rows):\")\n",
    "poly_df = pd.DataFrame(X_poly, columns=poly_feature_names)\n",
    "display(poly_df.head(3))\n",
    "\n",
    "print(\"\\nüí° Notice:\")\n",
    "print(\"   - Original features: age, income, education_years\")\n",
    "print(\"   - Added squares: age¬≤, income¬≤, education_years¬≤\")\n",
    "print(\"   - Added interactions: age√óincome, age√óeducation_years, income√óeducation_years\")\n",
    "\n",
    "# 2. MANUAL INTERACTION FEATURES\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"2. CREATING CUSTOM INTERACTION FEATURES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create meaningful interactions for our loan approval problem\n",
    "df[\"income_education_interaction\"] = df[\"income\"] * df[\"education_years\"]\n",
    "df[\"age_dependents_interaction\"] = df[\"age\"] * df[\"num_dependents\"]\n",
    "df[\"income_experience_interaction\"] = df[\"income\"] * df[\"experience_years\"]\n",
    "\n",
    "print(\"\\nCreated custom interactions:\")\n",
    "print(\"‚úì income √ó education_years ‚Üí income_education_interaction\")\n",
    "print(\"‚úì age √ó num_dependents ‚Üí age_dependents_interaction\")\n",
    "print(\"‚úì income √ó experience_years ‚Üí income_experience_interaction\")\n",
    "\n",
    "interaction_sample = df[\n",
    "    [\n",
    "        \"income\",\n",
    "        \"education_years\",\n",
    "        \"income_education_interaction\",\n",
    "        \"age\",\n",
    "        \"num_dependents\",\n",
    "        \"age_dependents_interaction\",\n",
    "    ]\n",
    "].head()\n",
    "display(interaction_sample)\n",
    "\n",
    "# 3. DEMONSTRATE IMPACT ON MODEL PERFORMANCE\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"3. IMPACT OF POLYNOMIAL FEATURES ON MODEL PERFORMANCE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Prepare data WITHOUT polynomial features\n",
    "features_simple = [\"age\", \"income\", \"education_years\", \"experience_years\", \"num_dependents\"]\n",
    "X_simple = df[features_simple].fillna(df[features_simple].mean())\n",
    "y = df[\"loan_approved\"]\n",
    "\n",
    "# Train/test split\n",
    "X_train_simple, X_test_simple, y_train, y_test = train_test_split(\n",
    "    X_simple, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_simple_scaled = scaler.fit_transform(X_train_simple)\n",
    "X_test_simple_scaled = scaler.transform(X_test_simple)\n",
    "\n",
    "# Train logistic regression WITHOUT polynomial features\n",
    "lr_simple = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr_simple.fit(X_train_simple_scaled, y_train)\n",
    "score_simple = lr_simple.score(X_test_simple_scaled, y_test)\n",
    "\n",
    "print(f\"\\nüìä Logistic Regression WITHOUT polynomial features:\")\n",
    "print(f\"   Accuracy: {score_simple:.4f}\")\n",
    "\n",
    "# Prepare data WITH polynomial features\n",
    "poly_transform = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train_poly = poly_transform.fit_transform(X_train_simple_scaled)\n",
    "X_test_poly = poly_transform.transform(X_test_simple_scaled)\n",
    "\n",
    "# Train logistic regression WITH polynomial features\n",
    "lr_poly = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr_poly.fit(X_train_poly, y_train)\n",
    "score_poly = lr_poly.score(X_test_poly, y_test)\n",
    "\n",
    "print(f\"\\nüìä Logistic Regression WITH polynomial features:\")\n",
    "print(f\"   Features: {X_train_simple.shape[1]} ‚Üí {X_train_poly.shape[1]}\")\n",
    "print(f\"   Accuracy: {score_poly:.4f}\")\n",
    "print(\n",
    "    f\"   Improvement: {(score_poly - score_simple):.4f} ({((score_poly - score_simple)/score_simple)*100:.1f}%)\"\n",
    ")\n",
    "\n",
    "# Visualize comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "models = [\"Without\\nPolynomial Features\", \"With\\nPolynomial Features\"]\n",
    "accuracies = [score_simple, score_poly]\n",
    "colors = [\"lightblue\", \"lightgreen\"]\n",
    "\n",
    "bars = ax.bar(models, accuracies, color=colors, edgecolor=\"black\", linewidth=2, alpha=0.7)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    height = bar.get_height()\n",
    "    ax.text(\n",
    "        bar.get_x() + bar.get_width() / 2.0,\n",
    "        height,\n",
    "        f\"{acc:.4f}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "ax.set_ylabel(\"Accuracy\", fontsize=12)\n",
    "ax.set_title(\"Impact of Polynomial Features on Model Performance\", fontsize=14, fontweight=\"bold\")\n",
    "ax.set_ylim(0, 1)\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Polynomial and interaction features demonstrated!\")\n",
    "print(f\"‚úì Polynomial features increased model performance!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Selection Methods\n",
    "\n",
    "Not all features are useful. Some are redundant, some are irrelevant, and some may even hurt model performance.\n",
    "\n",
    "### Why Feature Selection?\n",
    "\n",
    "1. **Reduces overfitting** - Less redundant data means less opportunity to make decisions based on noise\n",
    "2. **Improves accuracy** - Less misleading data means better model performance\n",
    "3. **Reduces training time** - Fewer features = faster computation\n",
    "4. **Improves interpretability** - Simpler models are easier to explain\n",
    "\n",
    "### Feature Selection Methods\n",
    "\n",
    "1. **Filter Methods**\n",
    "   - Statistical tests (correlation, chi-square, ANOVA)\n",
    "   - SelectKBest, SelectPercentile\n",
    "   - Fast but don't consider feature interactions\n",
    "\n",
    "2. **Wrapper Methods**\n",
    "   - Recursive Feature Elimination (RFE)\n",
    "   - Forward/Backward selection\n",
    "   - Slow but consider feature interactions\n",
    "\n",
    "3. **Embedded Methods**\n",
    "   - Lasso (L1) regularization\n",
    "   - Tree-based feature importance\n",
    "   - Built into model training\n",
    "\n",
    "### Comparison\n",
    "\n",
    "| Method | Speed | Accuracy | Considers Interactions |\n",
    "|--------|-------|----------|------------------------|\n",
    "| **Filter** | ‚ö° Fast | Good | ‚ùå No |\n",
    "| **Wrapper** | üêå Slow | Best | ‚úÖ Yes |\n",
    "| **Embedded** | üöÄ Medium | Great | ‚úÖ Yes |\n",
    "\n",
    "Let's apply each method!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection Methods - Comprehensive Examples\n",
    "\n",
    "# Prepare data\n",
    "print(\"=\" * 60)\n",
    "print(\"FEATURE SELECTION METHODS COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "feature_cols = [\n",
    "    \"age\",\n",
    "    \"income\",\n",
    "    \"education_years\",\n",
    "    \"experience_years\",\n",
    "    \"num_dependents\",\n",
    "    \"income_per_dependent\",\n",
    "    \"experience_efficiency\",\n",
    "    \"work_start_age\",\n",
    "]\n",
    "\n",
    "X_full = df[feature_cols].fillna(df[feature_cols].mean())\n",
    "y = df[\"loan_approved\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_full, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"\\nTotal features available: {X_train.shape[1]}\")\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "\n",
    "# METHOD 1: SelectKBest (Filter)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"METHOD 1: SelectKBest (Filter Method)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "selector_kbest = SelectKBest(score_func=f_classif, k=5)\n",
    "X_train_kbest = selector_kbest.fit_transform(X_train, y_train)\n",
    "selected_kbest = X_full.columns[selector_kbest.get_support()].tolist()\n",
    "\n",
    "print(f\"\\nSelected {len(selected_kbest)} features: {selected_kbest}\")\n",
    "\n",
    "# METHOD 2: RFE (Wrapper)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"METHOD 2: RFE (Wrapper Method)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "selector_rfe = RFE(LogisticRegression(random_state=42, max_iter=1000), n_features_to_select=5)\n",
    "X_train_rfe = selector_rfe.fit_transform(X_train, y_train)\n",
    "selected_rfe = X_full.columns[selector_rfe.get_support()].tolist()\n",
    "\n",
    "print(f\"\\nSelected {len(selected_rfe)} features: {selected_rfe}\")\n",
    "\n",
    "# METHOD 3: Random Forest Importance (Embedded)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"METHOD 3: Random Forest Feature Importance (Embedded)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "importances = pd.DataFrame(\n",
    "    {\"Feature\": feature_cols, \"Importance\": rf.feature_importances_}\n",
    ").sort_values(\"Importance\", ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importances:\")\n",
    "display(importances)\n",
    "\n",
    "# Select top 5 features\n",
    "top_5_features = importances.head(5)[\"Feature\"].tolist()\n",
    "print(f\"\\nTop 5 features: {top_5_features}\")\n",
    "\n",
    "# COMPARE PERFORMANCE\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results = []\n",
    "\n",
    "# Baseline: All features\n",
    "lr_all = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr_all.fit(X_train, y_train)\n",
    "results.append((\"All Features (8)\", lr_all.score(X_test, y_test), 8))\n",
    "\n",
    "# SelectKBest\n",
    "lr_kb = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr_kb.fit(X_train_kbest, y_train)\n",
    "results.append((\"SelectKBest (5)\", lr_kb.score(selector_kbest.transform(X_test), y_test), 5))\n",
    "\n",
    "# RFE\n",
    "lr_rfe = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr_rfe.fit(X_train_rfe, y_train)\n",
    "results.append((\"RFE (5)\", lr_rfe.score(selector_rfe.transform(X_test), y_test), 5))\n",
    "\n",
    "# RF top features\n",
    "X_train_rf = X_train[top_5_features]\n",
    "X_test_rf = X_test[top_5_features]\n",
    "lr_rf = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr_rf.fit(X_train_rf, y_train)\n",
    "results.append((\"RF Importance (5)\", lr_rf.score(X_test_rf, y_test), 5))\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=[\"Method\", \"Accuracy\", \"Num Features\"])\n",
    "print(\"\\n\" + results_df.to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "colors = [\"#3498db\", \"#e74c3c\", \"#2ecc71\", \"#f39c12\"]\n",
    "bars = ax.bar(\n",
    "    results_df[\"Method\"],\n",
    "    results_df[\"Accuracy\"],\n",
    "    color=colors,\n",
    "    alpha=0.7,\n",
    "    edgecolor=\"black\",\n",
    "    linewidth=2,\n",
    ")\n",
    "\n",
    "for bar, acc in zip(bars, results_df[\"Accuracy\"]):\n",
    "    height = bar.get_height()\n",
    "    ax.text(\n",
    "        bar.get_x() + bar.get_width() / 2.0,\n",
    "        height,\n",
    "        f\"{acc:.4f}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=11,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "ax.set_ylabel(\"Accuracy\", fontsize=12)\n",
    "ax.set_title(\"Feature Selection Methods - Performance Comparison\", fontsize=14, fontweight=\"bold\")\n",
    "ax.set_ylim(0, 1)\n",
    "ax.axhline(\n",
    "    y=results_df[\"Accuracy\"].iloc[0], color=\"gray\", linestyle=\"--\", alpha=0.5, label=\"Baseline\"\n",
    ")\n",
    "ax.legend()\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "plt.xticks(rotation=15, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Feature selection complete!\")\n",
    "print(f\"üí° Best method: {results_df.loc[results_df['Accuracy'].idxmax(), 'Method']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Importance Analysis\n",
    "\n",
    "Understanding which features contribute most to predictions helps with:\n",
    "- **Model interpretation** - Explain decisions to stakeholders\n",
    "- **Feature selection** - Focus on what matters\n",
    "- **Domain insights** - Learn about the problem\n",
    "- **Debugging** - Identify data quality issues\n",
    "\n",
    "### Methods for Feature Importance\n",
    "\n",
    "1. **Tree-based models** - Built-in feature_importances_\n",
    "2. **Permutation importance** - Shuffle feature and measure impact\n",
    "3. **SHAP values** - Game-theory based explanations (covered in advanced modules)\n",
    "4. **Coefficients** - For linear models\n",
    "\n",
    "Let's visualize feature importance from our Random Forest model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance Visualization\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get feature importances from our trained Random Forest\n",
    "feature_importance_df = pd.DataFrame(\n",
    "    {\"Feature\": feature_cols, \"Importance\": rf.feature_importances_}\n",
    ").sort_values(\n",
    "    \"Importance\", ascending=True\n",
    ")  # Ascending for horizontal bar chart\n",
    "\n",
    "print(\"\\nFeature Importance Rankings:\")\n",
    "for idx, row in feature_importance_df.sort_values(\"Importance\", ascending=False).iterrows():\n",
    "    bar = \"‚ñà\" * int(row[\"Importance\"] * 100)\n",
    "    print(f\"{row['Feature']:30s} {row['Importance']:.4f} {bar}\")\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Horizontal bar chart\n",
    "axes[0, 0].barh(\n",
    "    feature_importance_df[\"Feature\"],\n",
    "    feature_importance_df[\"Importance\"],\n",
    "    color=\"steelblue\",\n",
    "    edgecolor=\"black\",\n",
    "    alpha=0.8,\n",
    ")\n",
    "axes[0, 0].set_xlabel(\"Importance\", fontweight=\"bold\")\n",
    "axes[0, 0].set_title(\"Feature Importance - Horizontal View\", fontweight=\"bold\", fontsize=12)\n",
    "axes[0, 0].grid(axis=\"x\", alpha=0.3)\n",
    "\n",
    "# 2. Vertical bar chart\n",
    "sorted_features = feature_importance_df.sort_values(\"Importance\", ascending=False)\n",
    "colors_gradient = plt.cm.RdYlGn(sorted_features[\"Importance\"] / sorted_features[\"Importance\"].max())\n",
    "axes[0, 1].bar(\n",
    "    range(len(sorted_features)),\n",
    "    sorted_features[\"Importance\"],\n",
    "    color=colors_gradient,\n",
    "    edgecolor=\"black\",\n",
    "    alpha=0.8,\n",
    ")\n",
    "axes[0, 1].set_xticks(range(len(sorted_features)))\n",
    "axes[0, 1].set_xticklabels(sorted_features[\"Feature\"], rotation=45, ha=\"right\")\n",
    "axes[0, 1].set_ylabel(\"Importance\", fontweight=\"bold\")\n",
    "axes[0, 1].set_title(\"Feature Importance - Ranked\", fontweight=\"bold\", fontsize=12)\n",
    "axes[0, 1].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# 3. Pie chart\n",
    "top_6 = feature_importance_df.sort_values(\"Importance\", ascending=False).head(6)\n",
    "other_importance = (\n",
    "    feature_importance_df.sort_values(\"Importance\", ascending=False).iloc[6:][\"Importance\"].sum()\n",
    ")\n",
    "pie_data = list(top_6[\"Importance\"]) + [other_importance]\n",
    "pie_labels = list(top_6[\"Feature\"]) + [\"Other Features\"]\n",
    "colors_pie = plt.cm.Set3(range(len(pie_labels)))\n",
    "\n",
    "axes[1, 0].pie(\n",
    "    pie_data,\n",
    "    labels=pie_labels,\n",
    "    autopct=\"%1.1f%%\",\n",
    "    startangle=90,\n",
    "    colors=colors_pie,\n",
    "    textprops={\"fontsize\": 10, \"fontweight\": \"bold\"},\n",
    ")\n",
    "axes[1, 0].set_title(\"Feature Importance Distribution\", fontweight=\"bold\", fontsize=12)\n",
    "\n",
    "# 4. Cumulative importance\n",
    "cumulative_importance = feature_importance_df.sort_values(\"Importance\", ascending=False)[\n",
    "    \"Importance\"\n",
    "].cumsum()\n",
    "axes[1, 1].plot(\n",
    "    range(1, len(cumulative_importance) + 1),\n",
    "    cumulative_importance,\n",
    "    marker=\"o\",\n",
    "    linewidth=2,\n",
    "    markersize=8,\n",
    "    color=\"darkgreen\",\n",
    ")\n",
    "axes[1, 1].axhline(y=0.8, color=\"red\", linestyle=\"--\", label=\"80% threshold\", linewidth=2)\n",
    "axes[1, 1].axhline(y=0.9, color=\"orange\", linestyle=\"--\", label=\"90% threshold\", linewidth=2)\n",
    "axes[1, 1].set_xlabel(\"Number of Features\", fontweight=\"bold\")\n",
    "axes[1, 1].set_ylabel(\"Cumulative Importance\", fontweight=\"bold\")\n",
    "axes[1, 1].set_title(\"Cumulative Feature Importance\", fontweight=\"bold\", fontsize=12)\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "axes[1, 1].set_xticks(range(1, len(cumulative_importance) + 1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate how many features needed for 80% importance\n",
    "cumsum = feature_importance_df.sort_values(\"Importance\", ascending=False)[\"Importance\"].cumsum()\n",
    "features_for_80 = (cumsum <= 0.80).sum() + 1\n",
    "features_for_90 = (cumsum <= 0.90).sum() + 1\n",
    "\n",
    "print(f\"\\nüí° Insights:\")\n",
    "print(\n",
    "    f\"   ‚Ä¢ Top feature: {feature_importance_df.iloc[-1]['Feature']} ({feature_importance_df.iloc[-1]['Importance']:.4f})\"\n",
    ")\n",
    "print(f\"   ‚Ä¢ {features_for_80} features explain 80% of importance\")\n",
    "print(f\"   ‚Ä¢ {features_for_90} features explain 90% of importance\")\n",
    "print(f\"\\n‚úì Feature importance analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Automated Feature Engineering\n",
    "\n",
    "Manual feature engineering is powerful but time-consuming. Automated tools can help discover features you might miss!\n",
    "\n",
    "### Tools for Automated Feature Engineering\n",
    "\n",
    "1. **Featuretools** - Deep feature synthesis\n",
    "2. **tsfresh** - Time series features\n",
    "3. **AutoFeat** - Linear model-based\n",
    "4. **pandas_profiling** - Data exploration\n",
    "\n",
    "### Simple Automation Example\n",
    "\n",
    "While we won't use external libraries here, we can create simple automation functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automated Feature Engineering - Simple Example\n",
    "\n",
    "\n",
    "def auto_create_features(dataframe, numerical_cols):\n",
    "    \"\"\"\n",
    "    Automatically create common feature transformations.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframe: Input DataFrame\n",
    "    - numerical_cols: List of numerical column names\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with new features\n",
    "    \"\"\"\n",
    "    df_auto = dataframe.copy()\n",
    "\n",
    "    print(\"Generating automated features...\")\n",
    "    features_created = 0\n",
    "\n",
    "    # 1. Pairwise ratios\n",
    "    for i, col1 in enumerate(numerical_cols):\n",
    "        for col2 in numerical_cols[i + 1 :]:\n",
    "            feat_name = f\"{col1}_div_{col2}\"\n",
    "            df_auto[feat_name] = df_auto[col1] / (df_auto[col2] + 1)  # +1 to avoid division by zero\n",
    "            features_created += 1\n",
    "\n",
    "    # 2. Pairwise products\n",
    "    for i, col1 in enumerate(numerical_cols):\n",
    "        for col2 in numerical_cols[i + 1 :]:\n",
    "            feat_name = f\"{col1}_times_{col2}\"\n",
    "            df_auto[feat_name] = df_auto[col1] * df_auto[col2]\n",
    "            features_created += 1\n",
    "\n",
    "    # 3. Squares\n",
    "    for col in numerical_cols:\n",
    "        df_auto[f\"{col}_squared\"] = df_auto[col] ** 2\n",
    "        features_created += 1\n",
    "\n",
    "    # 4. Square roots\n",
    "    for col in numerical_cols:\n",
    "        df_auto[f\"{col}_sqrt\"] = np.sqrt(np.abs(df_auto[col]))\n",
    "        features_created += 1\n",
    "\n",
    "    print(f\"‚úì Created {features_created} new features automatically!\")\n",
    "    return df_auto\n",
    "\n",
    "\n",
    "# Apply automated feature engineering\n",
    "print(\"=\" * 60)\n",
    "print(\"AUTOMATED FEATURE ENGINEERING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "numerical_cols_subset = [\"age\", \"income\", \"education_years\"]\n",
    "print(f\"\\nStarting with {len(numerical_cols_subset)} numerical features\")\n",
    "print(f\"Original shape: {df.shape}\")\n",
    "\n",
    "df_automated = auto_create_features(df, numerical_cols_subset)\n",
    "\n",
    "print(f\"New shape: {df_automated.shape}\")\n",
    "print(f\"Features added: {df_automated.shape[1] - df.shape[1]}\")\n",
    "\n",
    "# Show sample of new features\n",
    "new_cols = [col for col in df_automated.columns if col not in df.columns]\n",
    "print(f\"\\nSample of {min(10, len(new_cols))} automated features:\")\n",
    "for i, col in enumerate(new_cols[:10], 1):\n",
    "    print(f\"  {i}. {col}\")\n",
    "\n",
    "print(\"\\nüí° In practice, use libraries like:\")\n",
    "print(\"   ‚Ä¢ Featuretools for deep feature synthesis\")\n",
    "print(\"   ‚Ä¢ tsfresh for time series\")\n",
    "print(\"   ‚Ä¢ AutoFeat for automatic feature engineering\")\n",
    "\n",
    "print(\"\\n‚úì Automated feature engineering demonstrated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Practical Feature Engineering Pipeline\n",
    "\n",
    "Let's put it all together into a complete, production-ready pipeline!\n",
    "\n",
    "### Best Practices for Feature Engineering Pipelines\n",
    "\n",
    "1. **Make it reproducible** - Same input ‚Üí Same output\n",
    "2. **Use fit/transform pattern** - Prevent data leakage\n",
    "3. **Document everything** - Future you will thank you\n",
    "4. **Version your features** - Track what works\n",
    "5. **Monitor feature distributions** - Detect drift in production\n",
    "\n",
    "### Complete End-to-End Pipeline\n",
    "\n",
    "We'll create a pipeline that:\n",
    "1. Handles missing values\n",
    "2. Creates derived features\n",
    "3. Encodes categorical variables\n",
    "4. Scales numerical features\n",
    "5. Selects best features\n",
    "6. Trains model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Feature Engineering Pipeline\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPLETE FEATURE ENGINEERING PIPELINE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Reload fresh data\n",
    "df_pipeline = pd.read_csv(\"../../data_advanced/feature_engineering.csv\")\n",
    "\n",
    "print(\"\\nPipeline Steps:\")\n",
    "print(\"  1. Create derived features\")\n",
    "print(\"  2. Encode categorical variables\")\n",
    "print(\"  3. Scale numerical features\")\n",
    "print(\"  4. Select best features\")\n",
    "print(\"  5. Train model\")\n",
    "\n",
    "# Step 1: Create derived features\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 1: Feature Creation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "df_pipeline[\"income_per_dependent\"] = df_pipeline[\"income\"] / (df_pipeline[\"num_dependents\"] + 1)\n",
    "df_pipeline[\"experience_efficiency\"] = df_pipeline[\"experience_years\"] / (\n",
    "    df_pipeline[\"education_years\"] + 1\n",
    ")\n",
    "df_pipeline[\"income_education\"] = df_pipeline[\"income\"] * df_pipeline[\"education_years\"]\n",
    "\n",
    "print(\"‚úì Created 3 derived features\")\n",
    "\n",
    "# Step 2: Encode categorical\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 2: Categorical Encoding\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le_city = LabelEncoder()\n",
    "le_job = LabelEncoder()\n",
    "\n",
    "df_pipeline[\"city_encoded\"] = le_city.fit_transform(df_pipeline[\"city\"])\n",
    "df_pipeline[\"job_encoded\"] = le_job.fit_transform(df_pipeline[\"job_category\"])\n",
    "\n",
    "print(\"‚úì Encoded 2 categorical features\")\n",
    "\n",
    "# Step 3: Prepare features\n",
    "numerical_features_final = [\n",
    "    \"age\",\n",
    "    \"income\",\n",
    "    \"education_years\",\n",
    "    \"experience_years\",\n",
    "    \"num_dependents\",\n",
    "    \"income_per_dependent\",\n",
    "    \"experience_efficiency\",\n",
    "    \"income_education\",\n",
    "    \"city_encoded\",\n",
    "    \"job_encoded\",\n",
    "]\n",
    "\n",
    "X = df_pipeline[numerical_features_final]\n",
    "y = df_pipeline[\"loan_approved\"]\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Step 4: Create pipeline\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 3-5: Build sklearn Pipeline\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"feature_selection\", SelectKBest(f_classif, k=7)),\n",
    "        (\"classifier\", LogisticRegression(random_state=42, max_iter=1000)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Pipeline created:\")\n",
    "print(\"  ‚Ä¢ StandardScaler\")\n",
    "print(\"  ‚Ä¢ SelectKBest (k=7)\")\n",
    "print(\"  ‚Ä¢ LogisticRegression\")\n",
    "\n",
    "# Step 5: Train and evaluate\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 6: Train and Evaluate\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Fit pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "train_score = pipeline.score(X_train, y_train)\n",
    "test_score = pipeline.score(X_test, y_test)\n",
    "\n",
    "print(f\"\\nTraining Accuracy: {train_score:.4f}\")\n",
    "print(f\"Testing Accuracy:  {test_score:.4f}\")\n",
    "print(f\"Difference:        {abs(train_score - test_score):.4f}\")\n",
    "\n",
    "if abs(train_score - test_score) < 0.05:\n",
    "    print(\"\\n‚úì Good generalization (difference < 5%)\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Possible overfitting (difference >= 5%)\")\n",
    "\n",
    "# Show selected features\n",
    "selected_mask = pipeline.named_steps[\"feature_selection\"].get_support()\n",
    "selected_features = [f for f, selected in zip(numerical_features_final, selected_mask) if selected]\n",
    "\n",
    "print(f\"\\n Selected {len(selected_features)} features:\")\n",
    "for feat in selected_features:\n",
    "    print(f\"  ‚úì {feat}\")\n",
    "\n",
    "# Visualize pipeline performance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "axes[0].imshow(cm, cmap=\"Blues\", alpha=0.7)\n",
    "axes[0].set_title(\"Confusion Matrix\", fontweight=\"bold\")\n",
    "axes[0].set_xlabel(\"Predicted\")\n",
    "axes[0].set_ylabel(\"Actual\")\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        axes[0].text(j, i, str(cm[i, j]), ha=\"center\", va=\"center\", fontsize=20, fontweight=\"bold\")\n",
    "\n",
    "axes[0].set_xticks([0, 1])\n",
    "axes[0].set_yticks([0, 1])\n",
    "axes[0].set_xticklabels([\"Not Approved\", \"Approved\"])\n",
    "axes[0].set_yticklabels([\"Not Approved\", \"Approved\"])\n",
    "\n",
    "# Train vs Test performance\n",
    "scores = [train_score, test_score]\n",
    "labels = [\"Training\", \"Testing\"]\n",
    "colors = [\"#3498db\", \"#2ecc71\"]\n",
    "\n",
    "bars = axes[1].bar(labels, scores, color=colors, alpha=0.7, edgecolor=\"black\", linewidth=2)\n",
    "axes[1].set_ylabel(\"Accuracy\")\n",
    "axes[1].set_title(\"Training vs Testing Performance\", fontweight=\"bold\")\n",
    "axes[1].set_ylim(0, 1)\n",
    "axes[1].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "for bar, score in zip(bars, scores):\n",
    "    height = bar.get_height()\n",
    "    axes[1].text(\n",
    "        bar.get_x() + bar.get_width() / 2.0,\n",
    "        height,\n",
    "        f\"{score:.4f}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=12,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DETAILED CLASSIFICATION REPORT\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Not Approved\", \"Approved\"]))\n",
    "\n",
    "print(\"\\n‚úì Complete pipeline successfully built and tested!\")\n",
    "print(\"\\nüí° This pipeline can be saved and reused:\")\n",
    "print(\"   import joblib\")\n",
    "print(\"   joblib.dump(pipeline, 'loan_approval_pipeline.pkl')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Hands-On Exercises\n",
    "\n",
    "Practice what you've learned with these challenges!\n",
    "\n",
    "### Exercise 1: Create Custom Features\n",
    "Load the housing dataset and create 5 meaningful derived features.\n",
    "\n",
    "### Exercise 2: Encoding Challenge\n",
    "Apply all 4 encoding methods (Label, One-Hot, Frequency, Target) to a categorical feature and compare results.\n",
    "\n",
    "### Exercise 3: Feature Selection\n",
    "Use SelectKBest, RFE, and Random Forest importance to select features. Which method gives best performance?\n",
    "\n",
    "### Exercise 4: Complete Pipeline\n",
    "Build an end-to-end pipeline for the customer_data.csv dataset including:\n",
    "- Feature creation\n",
    "- Encoding\n",
    "- Scaling\n",
    "- Feature selection\n",
    "- Model training\n",
    "\n",
    "Ready to practice? Complete the exercises below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise Solutions - Try these yourself first!\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EXERCISES - Complete These to Master Feature Engineering!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# EXERCISE 1: Create Custom Features\n",
    "print(\"\\nExercise 1: Create Custom Features\")\n",
    "print(\"-\" * 60)\n",
    "print(\"TODO: Load ../data/housing_prices.csv\")\n",
    "print(\"TODO: Create 5 derived features (ratios, products, etc.)\")\n",
    "print(\"TODO: Visualize the new features\")\n",
    "print(\"\\n# Your code here:\")\n",
    "print()\n",
    "\n",
    "# EXERCISE 2: Encoding Challenge\n",
    "print(\"\\nExercise 2: Apply All Encoding Methods\")\n",
    "print(\"-\" * 60)\n",
    "print(\"TODO: Choose a categorical feature\")\n",
    "print(\"TODO: Apply Label, One-Hot, Frequency, and Target encoding\")\n",
    "print(\"TODO: Train models with each and compare performance\")\n",
    "print(\"\\n# Your code here:\")\n",
    "print()\n",
    "\n",
    "# EXERCISE 3: Feature Selection Comparison\n",
    "print(\"\\nExercise 3: Feature Selection Comparison\")\n",
    "print(\"-\" * 60)\n",
    "print(\"TODO: Apply SelectKBest, RFE, and RF importance\")\n",
    "print(\"TODO: Compare which features each method selects\")\n",
    "print(\"TODO: Evaluate model performance with each\")\n",
    "print(\"\\n# Your code here:\")\n",
    "print()\n",
    "\n",
    "# EXERCISE 4: Complete Pipeline\n",
    "print(\"\\nExercise 4: Build End-to-End Pipeline\")\n",
    "print(\"-\" * 60)\n",
    "print(\"TODO: Load ../data/customer_data.csv\")\n",
    "print(\"TODO: Create a complete sklearn Pipeline with:\")\n",
    "print(\"      - Feature engineering\")\n",
    "print(\"      - Encoding\")\n",
    "print(\"      - Scaling\")\n",
    "print(\"      - Feature selection\")\n",
    "print(\"      - Model training\")\n",
    "print(\"\\n# Your code here:\")\n",
    "print()\n",
    "\n",
    "# BONUS CHALLENGE\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BONUS CHALLENGE\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Create an automated feature engineering function that:\")\n",
    "print(\"  1. Detects feature types automatically\")\n",
    "print(\"  2. Applies appropriate transformations\")\n",
    "print(\"  3. Selects the best features\")\n",
    "print(\"  4. Returns a trained model\")\n",
    "print(\"\\n# Your code here:\")\n",
    "print()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üí° TIPS:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"  ‚Ä¢ Start simple, then add complexity\")\n",
    "print(\"  ‚Ä¢ Always validate on a test set\")\n",
    "print(\"  ‚Ä¢ Document your feature creation logic\")\n",
    "print(\"  ‚Ä¢ Use cross-validation for robust evaluation\")\n",
    "print(\"  ‚Ä¢ Compare against a baseline\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Key Takeaways & Next Steps\n",
    "\n",
    "Congratulations! You've mastered feature engineering - one of the most impactful skills in data science!\n",
    "\n",
    "### What You've Learned\n",
    "\n",
    "#### 1. **Numerical Feature Engineering**\n",
    "- ‚úì Scaling methods (Standard, MinMax, Robust)\n",
    "- ‚úì Transformations (log, sqrt, Box-Cox) for skewed data\n",
    "- ‚úì Binning continuous features\n",
    "- ‚úì Creating derived features (ratios, differences)\n",
    "\n",
    "#### 2. **Categorical Encoding**\n",
    "- ‚úì Label Encoding for ordinal data\n",
    "- ‚úì One-Hot Encoding for nominal data\n",
    "- ‚úì Frequency Encoding for high cardinality\n",
    "- ‚úì Target Encoding with cross-validation\n",
    "- ‚úì When to use each method\n",
    "\n",
    "#### 3. **Datetime Features**\n",
    "- ‚úì Extracting temporal components (year, month, day, etc.)\n",
    "- ‚úì Creating boolean flags (weekend, month_start, etc.)\n",
    "- ‚úì Time-based calculations (tenure, days_since)\n",
    "- ‚úì Cyclical encoding with sin/cos\n",
    "\n",
    "#### 4. **Advanced Techniques**\n",
    "- ‚úì Polynomial features for non-linear relationships\n",
    "- ‚úì Interaction features to capture combined effects\n",
    "- ‚úì Impact on model performance\n",
    "\n",
    "#### 5. **Feature Selection**\n",
    "- ‚úì Filter methods (SelectKBest) - fast, statistical\n",
    "- ‚úì Wrapper methods (RFE) - slow, thorough\n",
    "- ‚úì Embedded methods (RF importance, Lasso) - balanced\n",
    "- ‚úì Comparing methods empirically\n",
    "\n",
    "#### 6. **Feature Importance**\n",
    "- ‚úì Tree-based feature_importances_\n",
    "- ‚úì Coefficient analysis for linear models\n",
    "- ‚úì Cumulative importance analysis\n",
    "- ‚úì Visualization techniques\n",
    "\n",
    "#### 7. **Production Pipelines**\n",
    "- ‚úì sklearn Pipeline for reproducibility\n",
    "- ‚úì fit/transform pattern to prevent leakage\n",
    "- ‚úì End-to-end automation\n",
    "- ‚úì Model serialization\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "> **\"Feature engineering is often more important than the choice of algorithm.\"**\n",
    "\n",
    "- Good features can make a simple model outperform a complex one\n",
    "- Domain knowledge is your superpower\n",
    "- Always validate on held-out test data\n",
    "- Document your feature engineering decisions\n",
    "- Version your features for reproducibility\n",
    "\n",
    "### Common Pitfalls to Avoid\n",
    "\n",
    "1. **Data Leakage** - Never use test data information in training\n",
    "2. **Overfitting** - Too many features can hurt generalization\n",
    "3. **Forgetting to Scale** - Linear models need scaled features\n",
    "4. **Ignoring Domain Knowledge** - Best features come from understanding the problem\n",
    "5. **Not Documenting** - Future you needs to know what you did\n",
    "\n",
    "### When to Use What\n",
    "\n",
    "| Scenario | Recommended Approach |\n",
    "|----------|---------------------|\n",
    "| Linear models | One-hot encoding + scaling + polynomial features |\n",
    "| Tree models | Label/target encoding + feature interactions |\n",
    "| High cardinality | Target/frequency encoding or embeddings |\n",
    "| Skewed distribution | Log/Box-Cox transformation |\n",
    "| Cyclical features | Sin/cos encoding |\n",
    "| Too many features | SelectKBest or RFE |\n",
    "| Need interpretability | L1 regularization or tree importance |\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "Feature engineering is crucial in:\n",
    "- **Finance**: Credit scoring, fraud detection\n",
    "- **Marketing**: Customer segmentation, churn prediction\n",
    "- **Healthcare**: Disease prediction, patient risk scores\n",
    "- **E-commerce**: Recommendation systems, demand forecasting\n",
    "- **Manufacturing**: Predictive maintenance, quality control\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "#### Continue Your Learning\n",
    "1. **Module 13: Model Selection & Hyperparameter Tuning**\n",
    "   - Grid Search, Random Search, Bayesian Optimization\n",
    "   - Cross-validation strategies\n",
    "   - Model comparison frameworks\n",
    "\n",
    "2. **Module 14: Ensemble Methods**\n",
    "   - XGBoost, LightGBM, CatBoost\n",
    "   - Stacking and blending\n",
    "   - Kaggle competition techniques\n",
    "\n",
    "3. **Practice Projects**\n",
    "   - Kaggle competitions (Titanic, House Prices)\n",
    "   - Real-world datasets from your domain\n",
    "   - Build and deploy your own models\n",
    "\n",
    "#### Resources for Deep Dive\n",
    "- **Books**:\n",
    "  - \"Feature Engineering for Machine Learning\" by Alice Zheng\n",
    "  - \"Feature Engineering Handbook\" (online)\n",
    "- **Libraries**:\n",
    "  - Featuretools (automated feature engineering)\n",
    "  - category_encoders (advanced encoding methods)\n",
    "  - SHAP (model interpretation)\n",
    "- **Competitions**:\n",
    "  - Kaggle - Learn from winning solutions\n",
    "  - DrivenData - Social good competitions\n",
    "\n",
    "### Recommended Practice\n",
    "\n",
    "Spend **2-3 hours** on these:\n",
    "1. Complete all 4 exercises above\n",
    "2. Apply feature engineering to your own dataset\n",
    "3. Create a reusable feature engineering template\n",
    "4. Document your feature engineering pipeline\n",
    "\n",
    "### Final Wisdom\n",
    "\n",
    "> \"Data scientists spend 80% of their time on data preparation and feature engineering. Master this, and you've mastered the job.\"\n",
    "\n",
    "Feature engineering is both **art and science**:\n",
    "- **Science**: Statistical methods, algorithms, validation\n",
    "- **Art**: Creativity, domain knowledge, intuition\n",
    "\n",
    "Keep experimenting, keep learning, and most importantly - **have fun with data**!\n",
    "\n",
    "---\n",
    "\n",
    "### Module Complete! üéâ\n",
    "\n",
    "**Total time invested**: ~75 minutes\n",
    "**Skills gained**: Production-ready feature engineering\n",
    "**Confidence level**: Intermediate ‚Üí Advanced\n",
    "\n",
    "**Next Module**: `13_model_selection.ipynb` - Take your models to the next level!\n",
    "\n",
    "---\n",
    "\n",
    "*Built with Claude Code | Module 12: Feature Engineering Mastery*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
