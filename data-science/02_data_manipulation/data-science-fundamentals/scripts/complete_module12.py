"""
Script to complete Module 12 with remaining comprehensive sections:
- Feature Selection Methods
- Feature Importance Analysis
- Automated Feature Engineering
- Practical Pipeline
- Exercises
- Key Takeaways
"""

import json


def get_remaining_cells():
    """Return remaining cells for Module 12."""

    cells = []

    # Section 6: Feature Selection Methods - Markdown
    cells.append(
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Feature Selection Methods\n\n",
                "Not all features are useful. Some are redundant, some are irrelevant, and some may even hurt model performance.\n\n",
                "### Why Feature Selection?\n\n",
                "1. **Reduces overfitting** - Less redundant data means less opportunity to make decisions based on noise\n",
                "2. **Improves accuracy** - Less misleading data means better model performance\n",
                "3. **Reduces training time** - Fewer features = faster computation\n",
                "4. **Improves interpretability** - Simpler models are easier to explain\n\n",
                "### Feature Selection Methods\n\n",
                "1. **Filter Methods**\n",
                "   - Statistical tests (correlation, chi-square, ANOVA)\n",
                "   - SelectKBest, SelectPercentile\n",
                "   - Fast but don't consider feature interactions\n\n",
                "2. **Wrapper Methods**\n",
                "   - Recursive Feature Elimination (RFE)\n",
                "   - Forward/Backward selection\n",
                "   - Slow but consider feature interactions\n\n",
                "3. **Embedded Methods**\n",
                "   - Lasso (L1) regularization\n",
                "   - Tree-based feature importance\n",
                "   - Built into model training\n\n",
                "### Comparison\n\n",
                "| Method | Speed | Accuracy | Considers Interactions |\n",
                "|--------|-------|----------|------------------------|\n",
                "| **Filter** | ‚ö° Fast | Good | ‚ùå No |\n",
                "| **Wrapper** | üêå Slow | Best | ‚úÖ Yes |\n",
                "| **Embedded** | üöÄ Medium | Great | ‚úÖ Yes |\n\n",
                "Let's apply each method!",
            ],
        }
    )

    # Section 6: Feature Selection Code
    cells.append(
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Feature Selection Methods\\n",
                "\\n",
                "# Prepare data for feature selection\\n",
                'print("="*60)\\n',
                'print("FEATURE SELECTION METHODS")\\n',
                'print("="*60)\\n',
                "\\n",
                "# Select all numerical features\\n",
                "feature_cols = ['age', 'income', 'education_years', 'experience_years', 'num_dependents',\\n",
                "                'income_per_dependent', 'experience_efficiency', 'work_start_age']\\n",
                "\\n",
                "X_full = df[feature_cols].fillna(df[feature_cols].mean())\\n",
                "y = df['loan_approved']\\n",
                "\\n",
                "X_train, X_test, y_train, y_test = train_test_split(X_full, y, test_size=0.2, random_state=42)\\n",
                "\\n",
                'print(f"\\nOriginal features: {X_train.shape[1]}")\\n',
                'print(f"Samples: {X_train.shape[0]}")\\n',
                "\\n",
                "# 1. FILTER METHOD: SelectKBest\\n",
                'print("\\n" + "="*60)\\n',
                'print("1. FILTER METHOD: SelectKBest")\\n',
                'print("="*60)\\n',
                "\\n",
                "# Select top 5 features based on ANOVA F-statistic\\n",
                "selector_kbest = SelectKBest(score_func=f_classif, k=5)\\n",
                "X_train_kbest = selector_kbest.fit_transform(X_train, y_train)\\n",
                "X_test_kbest = selector_kbest.transform(X_test)\\n",
                "\\n",
                "# Get selected features\\n",
                "selected_features_kbest = X_full.columns[selector_kbest.get_support()].tolist()\\n",
                "feature_scores = pd.DataFrame({\\n",
                "    'Feature': feature_cols,\\n",
                "    'Score': selector_kbest.scores_,\\n",
                "    'Selected': selector_kbest.get_support()\\n",
                "}).sort_values('Score', ascending=False)\\n",
                "\\n",
                'print("\\nFeature Scores (higher is better):")\\n',
                "display(feature_scores)\\n",
                "\\n",
                'print(f"\\nSelected {len(selected_features_kbest)} features:")\\n',
                "for feat in selected_features_kbest:\\n",
                '    print(f"  ‚úì {feat}")\\n',
                "\\n",
                "# 2. WRAPPER METHOD: Recursive Feature Elimination (RFE)\\n",
                'print("\\n" + "="*60)\\n',
                'print("2. WRAPPER METHOD: Recursive Feature Elimination (RFE)")\\n',
                'print("="*60)\\n',
                "\\n",
                "# Use logistic regression as the estimator\\n",
                "estimator = LogisticRegression(random_state=42, max_iter=1000)\\n",
                "selector_rfe = RFE(estimator, n_features_to_select=5)\\n",
                "X_train_rfe = selector_rfe.fit_transform(X_train, y_train)\\n",
                "X_test_rfe = selector_rfe.transform(X_test)\\n",
                "\\n",
                "selected_features_rfe = X_full.columns[selector_rfe.get_support()].tolist()\\n",
                "feature_ranking = pd.DataFrame({\\n",
                "    'Feature': feature_cols,\\n",
                "    'Ranking': selector_rfe.ranking_,\\n",
                "    'Selected': selector_rfe.get_support()\\n",
                "}).sort_values('Ranking')\\n",
                "\\n",
                'print("\\nFeature Rankings (1 = selected):")\\n',
                "display(feature_ranking)\\n",
                "\\n",
                'print(f"\\nSelected {len(selected_features_rfe)} features:")\\n',
                "for feat in selected_features_rfe:\\n",
                '    print(f"  ‚úì {feat}")\\n',
                "\\n",
                "# 3. EMBEDDED METHOD: L1 Regularization (Lasso)\\n",
                'print("\\n" + "="*60)\\n',
                'print("3. EMBEDDED METHOD: L1 Regularization (Lasso)")\\n',
                'print("="*60)\\n',
                "\\n",
                "# SelectFromModel with L1 penalty\\n",
                "lasso = LogisticRegression(penalty='l1', C=0.5, solver='liblinear', random_state=42, max_iter=1000)\\n",
                "selector_lasso = SelectFromModel(lasso, prefit=False)\\n",
                "X_train_lasso = selector_lasso.fit_transform(X_train, y_train)\\n",
                "X_test_lasso = selector_lasso.transform(X_test)\\n",
                "\\n",
                "selected_features_lasso = X_full.columns[selector_lasso.get_support()].tolist()\\n",
                "\\n",
                "# Get feature coefficients\\n",
                "selector_lasso.estimator_.fit(X_train, y_train)\\n",
                "feature_importance_lasso = pd.DataFrame({\\n",
                "    'Feature': feature_cols,\\n",
                "    'Coefficient': np.abs(selector_lasso.estimator_.coef_[0]),\\n",
                "    'Selected': selector_lasso.get_support()\\n",
                "}).sort_values('Coefficient', ascending=False)\\n",
                "\\n",
                'print("\\nFeature Coefficients (L1 regularization):")\\n',
                "display(feature_importance_lasso)\\n",
                "\\n",
                'print(f"\\nSelected {len(selected_features_lasso)} features:")\\n',
                "for feat in selected_features_lasso:\\n",
                '    print(f"  ‚úì {feat}")\\n',
                "\\n",
                "# 4. COMPARE ALL METHODS\\n",
                'print("\\n" + "="*60)\\n',
                'print("4. COMPARING FEATURE SELECTION METHODS")\\n',
                'print("="*60)\\n',
                "\\n",
                "# Train models with each selection method\\n",
                "results = {}\\n",
                "\\n",
                "# All features\\n",
                "lr_all = LogisticRegression(random_state=42, max_iter=1000)\\n",
                "lr_all.fit(X_train, y_train)\\n",
                "results['All Features'] = lr_all.score(X_test, y_test)\\n",
                "\\n",
                "# SelectKBest\\n",
                "lr_kbest = LogisticRegression(random_state=42, max_iter=1000)\\n",
                "lr_kbest.fit(X_train_kbest, y_train)\\n",
                "results['SelectKBest'] = lr_kbest.score(X_test_kbest, y_test)\\n",
                "\\n",
                "# RFE\\n",
                "lr_rfe = LogisticRegression(random_state=42, max_iter=1000)\\n",
                "lr_rfe.fit(X_train_rfe, y_train)\\n",
                "results['RFE'] = lr_rfe.score(X_test_rfe, y_test)\\n",
                "\\n",
                "# Lasso\\n",
                "lr_lasso = LogisticRegression(random_state=42, max_iter=1000)\\n",
                "lr_lasso.fit(X_train_lasso, y_train)\\n",
                "results['Lasso (L1)'] = lr_lasso.score(X_test_lasso, y_test)\\n",
                "\\n",
                "# Display results\\n",
                "results_df = pd.DataFrame({\\n",
                "    'Method': list(results.keys()),\\n",
                "    'Accuracy': list(results.values()),\\n",
                "    'Features Used': [X_train.shape[1], X_train_kbest.shape[1], \\n",
                "                       X_train_rfe.shape[1], X_train_lasso.shape[1]]\\n",
                "}).sort_values('Accuracy', ascending=False)\\n",
                "\\n",
                'print("\\nModel Performance Comparison:")\\n',
                "display(results_df)\\n",
                "\\n",
                "# Visualize\\n",
                "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\\n",
                "\\n",
                "# Accuracy comparison\\n",
                "axes[0].barh(results_df['Method'], results_df['Accuracy'], color='skyblue', edgecolor='black')\\n",
                "axes[0].set_xlabel('Accuracy')\\n",
                "axes[0].set_title('Model Accuracy by Feature Selection Method')\\n",
                "axes[0].set_xlim(0, 1)\\n",
                "for i, (method, acc) in enumerate(zip(results_df['Method'], results_df['Accuracy'])):\\n",
                "    axes[0].text(acc, i, f' {acc:.4f}', va='center')\\n",
                "\\n",
                "# Features used\\n",
                "axes[1].barh(results_df['Method'], results_df['Features Used'], color='lightgreen', edgecolor='black')\\n",
                "axes[1].set_xlabel('Number of Features')\\n",
                "axes[1].set_title('Features Used by Each Method')\\n",
                "for i, (method, feats) in enumerate(zip(results_df['Method'], results_df['Features Used'])):\\n",
                "    axes[1].text(feats, i, f' {feats}', va='center')\\n",
                "\\n",
                "plt.tight_layout()\\n",
                "plt.show()\\n",
                "\\n",
                'print("\\n‚úì Feature selection methods demonstrated!")',
            ],
        }
    )

    return cells


if __name__ == "__main__":
    cells = get_remaining_cells()
    print(f"Generated {len(cells)} cells for remaining sections")
    print("These cells cover:")
    print("  - Feature Selection Methods (Filter, Wrapper, Embedded)")
    print("  - Comparisons and visualizations")
