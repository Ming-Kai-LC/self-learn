{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Model Development\n",
    "\n",
    "**Purpose**: Develop and evaluate a sentiment analysis model for production deployment\n",
    "\n",
    "**Model**: Logistic Regression with TF-IDF features\n",
    "\n",
    "**Deployment**: AWS SageMaker with MLOps pipeline\n",
    "\n",
    "## Notebook Structure\n",
    "1. Environment Setup\n",
    "2. Data Loading and Exploration\n",
    "3. Text Preprocessing\n",
    "4. Feature Engineering\n",
    "5. Model Training\n",
    "6. Model Evaluation\n",
    "7. Model Saving and Export\n",
    "8. Local Testing\n",
    "9. Preparation for Cloud Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path for imports\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "\n",
    "# Custom modules\n",
    "from preprocess import TextPreprocessor\n",
    "\n",
    "# Configurations\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Exploration\n",
    "\n",
    "For this demo, we'll create synthetic sentiment data. In production, you would load real customer reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic sentiment data for demonstration\n",
    "positive_reviews = [\n",
    "    \"This product is absolutely amazing! Best purchase ever.\",\n",
    "    \"Excellent quality and fast shipping. Highly recommend!\",\n",
    "    \"Love it! Exceeded my expectations in every way.\",\n",
    "    \"Great value for money. Very satisfied with this purchase.\",\n",
    "    \"Outstanding product! Will definitely buy again.\",\n",
    "    \"Fantastic! Exactly what I was looking for.\",\n",
    "    \"Superb quality and excellent customer service.\",\n",
    "    \"Best product in its category. Cannot recommend enough!\",\n",
    "    \"Wonderful experience from start to finish.\",\n",
    "    \"Amazing features and great performance!\",\n",
    "] * 50  # Repeat to create more samples\n",
    "\n",
    "negative_reviews = [\n",
    "    \"Terrible product. Complete waste of money.\",\n",
    "    \"Poor quality. Broke after just one use.\",\n",
    "    \"Very disappointed. Not as described.\",\n",
    "    \"Awful experience. Would not recommend to anyone.\",\n",
    "    \"Worst purchase ever. Total garbage.\",\n",
    "    \"Bad quality and terrible customer service.\",\n",
    "    \"Don't buy this! Save your money.\",\n",
    "    \"Horrible product. Completely useless.\",\n",
    "    \"Disappointing quality. Not worth the price.\",\n",
    "    \"Terrible! Nothing works as advertised.\",\n",
    "] * 50  # Repeat to create more samples\n",
    "\n",
    "# Create DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'text': positive_reviews + negative_reviews,\n",
    "    'label': [1] * len(positive_reviews) + [0] * len(negative_reviews)\n",
    "})\n",
    "\n",
    "# Shuffle data\n",
    "data = data.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(data['label'].value_counts())\n",
    "print(f\"\\nFirst few samples:\")\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize label distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "data['label'].value_counts().plot(kind='bar')\n",
    "plt.title('Sentiment Label Distribution', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Sentiment (0=Negative, 1=Positive)', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Positive reviews: {(data['label'] == 1).sum()} ({(data['label'] == 1).mean():.1%})\")\n",
    "print(f\"Negative reviews: {(data['label'] == 0).sum()} ({(data['label'] == 0).mean():.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Preprocessing\n",
    "\n",
    "Apply our custom preprocessing pipeline to clean and normalize the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessor\n",
    "preprocessor = TextPreprocessor(\n",
    "    lowercase=True,\n",
    "    remove_stopwords=True,\n",
    "    remove_numbers=False\n",
    ")\n",
    "\n",
    "# Example of preprocessing\n",
    "sample_text = data['text'].iloc[0]\n",
    "processed_text = preprocessor.preprocess(sample_text)\n",
    "\n",
    "print(\"Original text:\")\n",
    "print(sample_text)\n",
    "print(\"\\nProcessed text:\")\n",
    "print(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess all texts\n",
    "print(\"Preprocessing texts...\")\n",
    "data['processed_text'] = data['text'].apply(preprocessor.preprocess)\n",
    "\n",
    "# Show comparison\n",
    "comparison_df = data[['text', 'processed_text', 'label']].head(5)\n",
    "print(\"\\nSample of preprocessed data:\")\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "\n",
    "Convert text to TF-IDF features for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "X = data['processed_text'].values\n",
    "y = data['label'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "print(f\"\\nTrain label distribution:\")\n",
    "print(pd.Series(y_train).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    max_df=0.95\n",
    ")\n",
    "\n",
    "# Fit on training data and transform\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"Feature matrix shape (train): {X_train_tfidf.shape}\")\n",
    "print(f\"Feature matrix shape (test): {X_test_tfidf.shape}\")\n",
    "print(f\"\\nNumber of features: {len(vectorizer.get_feature_names_out())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training\n",
    "\n",
    "Train a Logistic Regression model for binary sentiment classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "model = LogisticRegression(\n",
    "    C=1.0,\n",
    "    max_iter=1000,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    "    solver='lbfgs'\n",
    ")\n",
    "\n",
    "print(\"Training model...\")\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_train = model.predict(X_train_tfidf)\n",
    "y_pred_test = model.predict(X_test_tfidf)\n",
    "\n",
    "y_pred_proba_train = model.predict_proba(X_train_tfidf)[:, 1]\n",
    "y_pred_proba_test = model.predict_proba(X_test_tfidf)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "train_f1 = f1_score(y_train, y_pred_train)\n",
    "test_f1 = f1_score(y_test, y_pred_test)\n",
    "\n",
    "train_auc = roc_auc_score(y_train, y_pred_proba_train)\n",
    "test_auc = roc_auc_score(y_test, y_pred_proba_test)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"MODEL PERFORMANCE METRICS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nTRAINING SET:\")\n",
    "print(f\"  Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"  F1 Score: {train_f1:.4f}\")\n",
    "print(f\"  ROC AUC:  {train_auc:.4f}\")\n",
    "\n",
    "print(f\"\\nTEST SET:\")\n",
    "print(f\"  Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"  F1 Score: {test_f1:.4f}\")\n",
    "print(f\"  ROC AUC:  {test_auc:.4f}\")\n",
    "print(\"\\n\" + \"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "print(\"\\nCLASSIFICATION REPORT (Test Set):\")\n",
    "print(classification_report(y_test, y_pred_test, target_names=['Negative', 'Positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Negative', 'Positive'],\n",
    "            yticklabels=['Negative', 'Positive'])\n",
    "plt.title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba_test)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, linewidth=2, label=f'ROC Curve (AUC = {test_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curve', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Saving and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Create models directory\n",
    "model_dir = Path('../models/local')\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "model_path = model_dir / 'model.pkl'\n",
    "joblib.dump(model, model_path)\n",
    "print(f\"Model saved to {model_path}\")\n",
    "\n",
    "# Save vectorizer\n",
    "vectorizer_path = model_dir / 'vectorizer.pkl'\n",
    "joblib.dump(vectorizer, vectorizer_path)\n",
    "print(f\"Vectorizer saved to {vectorizer_path}\")\n",
    "\n",
    "# Save preprocessor\n",
    "preprocessor_path = model_dir / 'preprocessor.pkl'\n",
    "joblib.dump(preprocessor, preprocessor_path)\n",
    "print(f\"Preprocessor saved to {preprocessor_path}\")\n",
    "\n",
    "print(\"\\nAll artifacts saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Local Testing\n",
    "\n",
    "Test the saved model on new examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved artifacts\n",
    "loaded_model = joblib.load(model_path)\n",
    "loaded_vectorizer = joblib.load(vectorizer_path)\n",
    "loaded_preprocessor = joblib.load(preprocessor_path)\n",
    "\n",
    "print(\"Model artifacts loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text):\n",
    "    \"\"\"Predict sentiment for a single text\"\"\"\n",
    "    # Preprocess\n",
    "    processed = loaded_preprocessor.preprocess(text)\n",
    "    \n",
    "    # Vectorize\n",
    "    features = loaded_vectorizer.transform([processed])\n",
    "    \n",
    "    # Predict\n",
    "    prediction = loaded_model.predict(features)[0]\n",
    "    probabilities = loaded_model.predict_proba(features)[0]\n",
    "    \n",
    "    sentiment = \"Positive\" if prediction == 1 else \"Negative\"\n",
    "    confidence = probabilities[prediction]\n",
    "    \n",
    "    return {\n",
    "        'sentiment': sentiment,\n",
    "        'confidence': confidence,\n",
    "        'probabilities': {\n",
    "            'negative': probabilities[0],\n",
    "            'positive': probabilities[1]\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Test on new examples\n",
    "test_examples = [\n",
    "    \"This product is absolutely fantastic! I love it!\",\n",
    "    \"Terrible experience. Would not recommend.\",\n",
    "    \"It's okay, nothing special.\",\n",
    "    \"Best purchase of the year! Highly recommend!\",\n",
    "    \"Complete waste of money. Very disappointed.\"\n",
    "]\n",
    "\n",
    "print(\"\\nTESTING PREDICTIONS ON NEW EXAMPLES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, text in enumerate(test_examples, 1):\n",
    "    result = predict_sentiment(text)\n",
    "    print(f\"\\n{i}. Text: {text}\")\n",
    "    print(f\"   Sentiment: {result['sentiment']}\")\n",
    "    print(f\"   Confidence: {result['confidence']:.2%}\")\n",
    "    print(f\"   Probabilities: Neg={result['probabilities']['negative']:.2%}, Pos={result['probabilities']['positive']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Preparation for Cloud Deployment\n",
    "\n",
    "Next steps for deploying to AWS SageMaker:\n",
    "\n",
    "1. **Upload model artifacts to S3**\n",
    "2. **Build and push Docker container to ECR**\n",
    "3. **Create SageMaker model and endpoint**\n",
    "4. **Set up monitoring and auto-scaling**\n",
    "\n",
    "See the main README.md for detailed deployment instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model metadata for deployment\n",
    "model_metadata = {\n",
    "    'model_name': 'sentiment-classifier',\n",
    "    'version': '1.0',\n",
    "    'train_samples': len(X_train),\n",
    "    'test_samples': len(X_test),\n",
    "    'test_accuracy': test_accuracy,\n",
    "    'test_f1': test_f1,\n",
    "    'test_auc': test_auc,\n",
    "    'features': {\n",
    "        'max_features': 5000,\n",
    "        'ngram_range': '(1, 2)',\n",
    "        'vectorizer': 'TF-IDF'\n",
    "    },\n",
    "    'model_type': 'LogisticRegression',\n",
    "    'preprocessing': {\n",
    "        'lowercase': True,\n",
    "        'remove_stopwords': True,\n",
    "        'remove_numbers': False\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save metadata\n",
    "import json\n",
    "\n",
    "metadata_path = model_dir / 'metadata.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(model_metadata, f, indent=2)\n",
    "\n",
    "print(\"Model metadata:\")\n",
    "print(json.dumps(model_metadata, indent=2))\n",
    "print(f\"\\nMetadata saved to {metadata_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "✅ Data loaded and explored\n",
    "\n",
    "✅ Text preprocessing pipeline implemented\n",
    "\n",
    "✅ TF-IDF features extracted\n",
    "\n",
    "✅ Model trained and evaluated\n",
    "\n",
    "✅ Model artifacts saved\n",
    "\n",
    "✅ Local testing completed\n",
    "\n",
    "✅ Ready for cloud deployment\n",
    "\n",
    "**Next Steps**: Follow the deployment guide in README.md to deploy this model to AWS SageMaker with full MLOps pipeline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
