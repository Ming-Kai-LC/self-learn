{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 06: Sentiment Analysis System\n",
    "\n",
    "**Difficulty**: ‚≠ê‚≠ê Intermediate  \n",
    "**Estimated Time**: 6-8 hours  \n",
    "**Prerequisites**: \n",
    "- Python programming fundamentals\n",
    "- Pandas and NumPy basics\n",
    "- Basic machine learning concepts\n",
    "- Understanding of classification metrics\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. **Preprocess text data** using NLTK for NLP tasks (tokenization, stopword removal, stemming/lemmatization)\n",
    "2. **Engineer text features** using TF-IDF and Word2Vec embeddings\n",
    "3. **Build and compare multiple models** (Naive Bayes, Logistic Regression, Random Forest, LSTM)\n",
    "4. **Evaluate sentiment classifiers** using accuracy, precision, recall, F1-score, and ROC curves\n",
    "5. **Perform error analysis** to understand model limitations and misclassifications\n",
    "6. **Interpret model predictions** by identifying influential features and words\n",
    "7. **Export trained models** for production deployment\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "**Goal**: Build an NLP sentiment classifier to automatically determine whether movie reviews are positive or negative.\n",
    "\n",
    "**Dataset**: IMDB Movie Reviews (50,000 reviews - 25,000 train, 25,000 test)\n",
    "\n",
    "**Approach**:\n",
    "1. Load and explore the IMDB dataset\n",
    "2. Preprocess text (cleaning, tokenization, normalization)\n",
    "3. Engineer features (TF-IDF, Word2Vec)\n",
    "4. Train multiple models and compare performance\n",
    "5. Analyze errors and interpret predictions\n",
    "6. Export best model for deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# NLP and text preprocessing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Feature engineering\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# Machine learning models\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Model evaluation\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_curve, auc, roc_auc_score\n",
    ")\n",
    "\n",
    "# Deep learning (TensorFlow/Keras)\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "    TENSORFLOW_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"TensorFlow not available. LSTM model will be skipped.\")\n",
    "    TENSORFLOW_AVAILABLE = False\n",
    "\n",
    "# Utilities\n",
    "import pickle\n",
    "import warnings\n",
    "from time import time\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "if TENSORFLOW_AVAILABLE:\n",
    "    tf.random.set_seed(42)\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"TensorFlow available: {TENSORFLOW_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Required NLTK Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required NLTK data\n",
    "# This only needs to be run once\n",
    "nltk_resources = ['stopwords', 'punkt', 'wordnet', 'omw-1.4']\n",
    "\n",
    "for resource in nltk_resources:\n",
    "    try:\n",
    "        nltk.data.find(f'corpora/{resource}' if resource != 'punkt' else f'tokenizers/{resource}')\n",
    "        print(f\"‚úì {resource} already downloaded\")\n",
    "    except LookupError:\n",
    "        print(f\"Downloading {resource}...\")\n",
    "        nltk.download(resource, quiet=True)\n",
    "        print(f\"‚úì {resource} downloaded successfully\")\n",
    "\n",
    "print(\"\\nAll NLTK resources ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preparation\n",
    "\n",
    "The IMDB dataset contains 50,000 movie reviews split into:\n",
    "- **Training set**: 25,000 reviews (12,500 positive, 12,500 negative)\n",
    "- **Test set**: 25,000 reviews (12,500 positive, 12,500 negative)\n",
    "\n",
    "Reviews are stored as individual text files in `pos/` and `neg/` directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imdb_data(data_dir, split='train', sample_size=None):\n",
    "    \"\"\"\n",
    "    Load IMDB movie reviews from directory structure.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_dir : str or Path\n",
    "        Base directory containing the aclImdb folder\n",
    "    split : str\n",
    "        Either 'train' or 'test'\n",
    "    sample_size : int, optional\n",
    "        Number of reviews to load per class (for faster testing)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame with columns: review, sentiment, label\n",
    "    \"\"\"\n",
    "    data_path = Path(data_dir) / 'aclImdb' / split\n",
    "    \n",
    "    # Verify directory exists\n",
    "    if not data_path.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Data directory not found: {data_path}\\n\"\n",
    "            f\"Please download the IMDB dataset and extract it to {data_dir}/\\n\"\n",
    "            f\"Download from: http://ai.stanford.edu/~amaas/data/sentiment/\"\n",
    "        )\n",
    "    \n",
    "    reviews = []\n",
    "    sentiments = []\n",
    "    \n",
    "    # Load positive reviews\n",
    "    pos_dir = data_path / 'pos'\n",
    "    pos_files = list(pos_dir.glob('*.txt'))\n",
    "    if sample_size:\n",
    "        pos_files = pos_files[:sample_size]\n",
    "    \n",
    "    for file_path in pos_files:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            reviews.append(f.read())\n",
    "            sentiments.append('positive')\n",
    "    \n",
    "    # Load negative reviews\n",
    "    neg_dir = data_path / 'neg'\n",
    "    neg_files = list(neg_dir.glob('*.txt'))\n",
    "    if sample_size:\n",
    "        neg_files = neg_files[:sample_size]\n",
    "    \n",
    "    for file_path in neg_files:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            reviews.append(f.read())\n",
    "            sentiments.append('negative')\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'review': reviews,\n",
    "        'sentiment': sentiments\n",
    "    })\n",
    "    \n",
    "    # Add numeric label (0=negative, 1=positive)\n",
    "    df['label'] = (df['sentiment'] == 'positive').astype(int)\n",
    "    \n",
    "    # Shuffle the dataset\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Set data directory (adjust if needed)\n",
    "DATA_DIR = Path('data')\n",
    "\n",
    "# For quick testing, use sample_size parameter\n",
    "# For full analysis, set sample_size=None\n",
    "SAMPLE_SIZE = None  # Use None for full dataset, or 1000 for quick testing\n",
    "\n",
    "print(\"Loading training data...\")\n",
    "train_df = load_imdb_data(DATA_DIR, split='train', sample_size=SAMPLE_SIZE)\n",
    "print(f\"‚úì Loaded {len(train_df)} training reviews\")\n",
    "\n",
    "print(\"\\nLoading test data...\")\n",
    "test_df = load_imdb_data(DATA_DIR, split='test', sample_size=SAMPLE_SIZE)\n",
    "print(f\"‚úì Loaded {len(test_df)} test reviews\")\n",
    "\n",
    "print(f\"\\nTotal dataset size: {len(train_df) + len(test_df)} reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few examples\n",
    "print(\"Sample Reviews:\")\n",
    "print(\"=\" * 80)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data info\n",
    "print(\"Training Data Info:\")\n",
    "print(train_df.info())\n",
    "print(\"\\nClass Distribution:\")\n",
    "print(train_df['sentiment'].value_counts())\n",
    "print(\"\\nClass Balance:\")\n",
    "print(train_df['sentiment'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)\n",
    "\n",
    "Let's explore the characteristics of our text data:\n",
    "1. Review length distribution\n",
    "2. Word frequency analysis\n",
    "3. Word clouds for positive/negative reviews\n",
    "4. Common words by sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate review statistics\n",
    "train_df['review_length'] = train_df['review'].apply(len)\n",
    "train_df['word_count'] = train_df['review'].apply(lambda x: len(x.split()))\n",
    "\n",
    "print(\"Review Length Statistics:\")\n",
    "print(train_df[['review_length', 'word_count']].describe())\n",
    "\n",
    "print(\"\\nBy Sentiment:\")\n",
    "print(train_df.groupby('sentiment')[['review_length', 'word_count']].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize review length distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Character length distribution\n",
    "axes[0].hist(train_df[train_df['sentiment']=='positive']['review_length'], \n",
    "             bins=50, alpha=0.6, label='Positive', color='green')\n",
    "axes[0].hist(train_df[train_df['sentiment']=='negative']['review_length'], \n",
    "             bins=50, alpha=0.6, label='Negative', color='red')\n",
    "axes[0].set_xlabel('Review Length (characters)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Distribution of Review Length')\n",
    "axes[0].legend()\n",
    "axes[0].set_xlim(0, 5000)  # Limit x-axis for better visualization\n",
    "\n",
    "# Word count distribution\n",
    "axes[1].hist(train_df[train_df['sentiment']=='positive']['word_count'], \n",
    "             bins=50, alpha=0.6, label='Positive', color='green')\n",
    "axes[1].hist(train_df[train_df['sentiment']=='negative']['word_count'], \n",
    "             bins=50, alpha=0.6, label='Negative', color='red')\n",
    "axes[1].set_xlabel('Word Count')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Distribution of Word Count')\n",
    "axes[1].legend()\n",
    "axes[1].set_xlim(0, 1000)  # Limit x-axis for better visualization\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observation: Both positive and negative reviews have similar length distributions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Clouds\n",
    "\n",
    "Visualize the most common words in positive and negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word clouds for positive and negative reviews\n",
    "def create_wordcloud(text, title, max_words=100):\n",
    "    \"\"\"\n",
    "    Generate and display a word cloud.\n",
    "    \"\"\"\n",
    "    wordcloud = WordCloud(\n",
    "        width=800, \n",
    "        height=400,\n",
    "        background_color='white',\n",
    "        max_words=max_words,\n",
    "        colormap='viridis'\n",
    "    ).generate(text)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title, fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.show()\n",
    "\n",
    "# Combine all positive reviews\n",
    "positive_text = ' '.join(train_df[train_df['sentiment']=='positive']['review'])\n",
    "create_wordcloud(positive_text, 'Most Common Words in Positive Reviews')\n",
    "\n",
    "# Combine all negative reviews\n",
    "negative_text = ' '.join(train_df[train_df['sentiment']=='negative']['review'])\n",
    "create_wordcloud(negative_text, 'Most Common Words in Negative Reviews')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Text Preprocessing\n",
    "\n",
    "Text preprocessing is crucial for NLP tasks. We'll implement a comprehensive preprocessing pipeline:\n",
    "\n",
    "1. **Remove HTML tags** (reviews may contain HTML)\n",
    "2. **Convert to lowercase** (normalize case)\n",
    "3. **Remove special characters and punctuation**\n",
    "4. **Remove numbers** (usually not informative for sentiment)\n",
    "5. **Tokenization** (split into words)\n",
    "6. **Remove stopwords** (common words like 'the', 'a', 'is')\n",
    "7. **Stemming or Lemmatization** (reduce words to root form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessing tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text, use_stemming=False, use_lemmatization=True, remove_stopwords=True):\n",
    "    \"\"\"\n",
    "    Comprehensive text preprocessing function.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        Raw text to preprocess\n",
    "    use_stemming : bool\n",
    "        Apply Porter stemming\n",
    "    use_lemmatization : bool\n",
    "        Apply WordNet lemmatization (recommended over stemming)\n",
    "    remove_stopwords : bool\n",
    "        Remove common English stopwords\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    str : Cleaned and preprocessed text\n",
    "    \"\"\"\n",
    "    # 1. Remove HTML tags using BeautifulSoup\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    \n",
    "    # 2. Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 3. Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    \n",
    "    # 4. Remove special characters and numbers (keep only letters and spaces)\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    \n",
    "    # 5. Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # 6. Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # 7. Remove stopwords\n",
    "    if remove_stopwords:\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # 8. Stemming or Lemmatization\n",
    "    if use_stemming:\n",
    "        tokens = [stemmer.stem(word) for word in tokens]\n",
    "    elif use_lemmatization:\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # 9. Join tokens back into string\n",
    "    cleaned_text = ' '.join(tokens)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Test preprocessing on a sample review\n",
    "sample_review = train_df.iloc[0]['review']\n",
    "print(\"Original Review:\")\n",
    "print(sample_review[:500])  # Show first 500 characters\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "print(\"Preprocessed Review:\")\n",
    "print(preprocess_text(sample_review)[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to all reviews\n",
    "print(\"Preprocessing training data...\")\n",
    "start_time = time()\n",
    "train_df['cleaned_review'] = train_df['review'].apply(preprocess_text)\n",
    "train_time = time() - start_time\n",
    "print(f\"‚úì Training data preprocessed in {train_time:.2f} seconds\")\n",
    "\n",
    "print(\"\\nPreprocessing test data...\")\n",
    "start_time = time()\n",
    "test_df['cleaned_review'] = test_df['review'].apply(preprocess_text)\n",
    "test_time = time() - start_time\n",
    "print(f\"‚úì Test data preprocessed in {test_time:.2f} seconds\")\n",
    "\n",
    "# Check for any empty reviews after preprocessing\n",
    "empty_train = train_df['cleaned_review'].str.strip().eq('').sum()\n",
    "empty_test = test_df['cleaned_review'].str.strip().eq('').sum()\n",
    "print(f\"\\nEmpty reviews after preprocessing: {empty_train} train, {empty_test} test\")\n",
    "\n",
    "# Remove empty reviews if any\n",
    "if empty_train > 0:\n",
    "    train_df = train_df[train_df['cleaned_review'].str.strip() != ''].reset_index(drop=True)\n",
    "if empty_test > 0:\n",
    "    test_df = test_df[test_df['cleaned_review'].str.strip() != ''].reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nFinal dataset sizes: {len(train_df)} train, {len(test_df)} test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering\n",
    "\n",
    "We'll create two types of text features:\n",
    "\n",
    "### 5.1 TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "\n",
    "TF-IDF converts text to numerical vectors by considering:\n",
    "- **Term Frequency (TF)**: How often a word appears in a document\n",
    "- **Inverse Document Frequency (IDF)**: How rare/important a word is across all documents\n",
    "\n",
    "**Formula**: TF-IDF = TF √ó IDF\n",
    "\n",
    "This gives higher weights to words that are frequent in a document but rare overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF features\n",
    "# Using both unigrams and bigrams for better context\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=10000,  # Limit to top 10,000 features\n",
    "    ngram_range=(1, 2),  # Use unigrams and bigrams\n",
    "    min_df=5,            # Ignore terms that appear in fewer than 5 documents\n",
    "    max_df=0.8,          # Ignore terms that appear in more than 80% of documents\n",
    "    sublinear_tf=True    # Apply sublinear tf scaling (1 + log(tf))\n",
    ")\n",
    "\n",
    "# Fit on training data only (prevent data leakage)\n",
    "print(\"Creating TF-IDF features...\")\n",
    "start_time = time()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train_df['cleaned_review'])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_df['cleaned_review'])\n",
    "tfidf_time = time() - start_time\n",
    "\n",
    "print(f\"‚úì TF-IDF features created in {tfidf_time:.2f} seconds\")\n",
    "print(f\"Feature matrix shape: {X_train_tfidf.shape}\")\n",
    "print(f\"Number of features: {len(tfidf_vectorizer.get_feature_names_out())}\")\n",
    "print(f\"Sparsity: {(1.0 - X_train_tfidf.nnz / (X_train_tfidf.shape[0] * X_train_tfidf.shape[1])) * 100:.2f}%\")\n",
    "\n",
    "# Extract labels\n",
    "y_train = train_df['label'].values\n",
    "y_test = test_df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine top TF-IDF features\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "print(\"Sample features (unigrams and bigrams):\")\n",
    "print(feature_names[:20])\n",
    "print(\"\\nTotal vocabulary size:\", len(feature_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Training and Evaluation\n",
    "\n",
    "We'll train and compare four different models:\n",
    "\n",
    "1. **Naive Bayes**: Fast probabilistic classifier (baseline)\n",
    "2. **Logistic Regression**: Linear model with strong performance on text\n",
    "3. **Random Forest**: Ensemble method for non-linear patterns\n",
    "4. **LSTM Neural Network**: Deep learning for sequential patterns\n",
    "\n",
    "For each model, we'll track:\n",
    "- Training time\n",
    "- Accuracy, Precision, Recall, F1-Score\n",
    "- Confusion matrix\n",
    "- ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to evaluate models\n",
    "def evaluate_model(y_true, y_pred, y_pred_proba=None, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluation with multiple metrics.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{model_name} Evaluation Results\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1-Score:  {f1:.4f}\")\n",
    "    \n",
    "    # ROC-AUC if probabilities available\n",
    "    if y_pred_proba is not None:\n",
    "        roc_auc = roc_auc_score(y_true, y_pred_proba)\n",
    "        print(f\"ROC-AUC:   {roc_auc:.4f}\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=['Negative', 'Positive']))\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'roc_auc': roc_auc_score(y_true, y_pred_proba) if y_pred_proba is not None else None\n",
    "    }\n",
    "\n",
    "# Store results for comparison\n",
    "model_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Naive Bayes model\n",
    "print(\"Training Naive Bayes classifier...\")\n",
    "start_time = time()\n",
    "\n",
    "nb_model = MultinomialNB(alpha=1.0)  # alpha is smoothing parameter\n",
    "nb_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "nb_train_time = time() - start_time\n",
    "print(f\"‚úì Training completed in {nb_train_time:.2f} seconds\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_nb = nb_model.predict(X_test_tfidf)\n",
    "y_pred_proba_nb = nb_model.predict_proba(X_test_tfidf)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "nb_metrics = evaluate_model(y_test, y_pred_nb, y_pred_proba_nb, \"Naive Bayes\")\n",
    "nb_metrics['train_time'] = nb_train_time\n",
    "model_results['Naive Bayes'] = nb_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression model\n",
    "print(\"Training Logistic Regression classifier...\")\n",
    "start_time = time()\n",
    "\n",
    "lr_model = LogisticRegression(\n",
    "    C=1.0,              # Inverse regularization strength\n",
    "    max_iter=1000,      # Maximum iterations\n",
    "    solver='saga',      # Solver for large datasets\n",
    "    random_state=42,\n",
    "    n_jobs=-1           # Use all CPU cores\n",
    ")\n",
    "lr_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "lr_train_time = time() - start_time\n",
    "print(f\"‚úì Training completed in {lr_train_time:.2f} seconds\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_lr = lr_model.predict(X_test_tfidf)\n",
    "y_pred_proba_lr = lr_model.predict_proba(X_test_tfidf)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "lr_metrics = evaluate_model(y_test, y_pred_lr, y_pred_proba_lr, \"Logistic Regression\")\n",
    "lr_metrics['train_time'] = lr_train_time\n",
    "model_results['Logistic Regression'] = lr_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest model\n",
    "print(\"Training Random Forest classifier...\")\n",
    "print(\"Note: This may take several minutes due to high dimensionality.\")\n",
    "start_time = time()\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,    # Number of trees\n",
    "    max_depth=50,        # Limit depth to prevent overfitting\n",
    "    min_samples_split=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,           # Use all CPU cores\n",
    "    verbose=0\n",
    ")\n",
    "rf_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "rf_train_time = time() - start_time\n",
    "print(f\"‚úì Training completed in {rf_train_time:.2f} seconds\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = rf_model.predict(X_test_tfidf)\n",
    "y_pred_proba_rf = rf_model.predict_proba(X_test_tfidf)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "rf_metrics = evaluate_model(y_test, y_pred_rf, y_pred_proba_rf, \"Random Forest\")\n",
    "rf_metrics['train_time'] = rf_train_time\n",
    "model_results['Random Forest'] = rf_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 LSTM Neural Network (Deep Learning)\n",
    "\n",
    "LSTM (Long Short-Term Memory) networks can capture sequential patterns in text that traditional models might miss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TENSORFLOW_AVAILABLE:\n",
    "    # Prepare data for LSTM (requires different preprocessing)\n",
    "    print(\"Preparing data for LSTM...\")\n",
    "    \n",
    "    # Tokenize text for deep learning\n",
    "    MAX_VOCAB_SIZE = 10000\n",
    "    MAX_SEQUENCE_LENGTH = 200\n",
    "    EMBEDDING_DIM = 128\n",
    "    \n",
    "    # Create tokenizer\n",
    "    keras_tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, oov_token='<OOV>')\n",
    "    keras_tokenizer.fit_on_texts(train_df['cleaned_review'])\n",
    "    \n",
    "    # Convert text to sequences\n",
    "    X_train_seq = keras_tokenizer.texts_to_sequences(train_df['cleaned_review'])\n",
    "    X_test_seq = keras_tokenizer.texts_to_sequences(test_df['cleaned_review'])\n",
    "    \n",
    "    # Pad sequences to same length\n",
    "    X_train_padded = pad_sequences(X_train_seq, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "    X_test_padded = pad_sequences(X_test_seq, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "    \n",
    "    print(f\"‚úì Sequences prepared: {X_train_padded.shape}\")\n",
    "    \n",
    "    # Build LSTM model\n",
    "    print(\"\\nBuilding LSTM model...\")\n",
    "    lstm_model = Sequential([\n",
    "        # Embedding layer: converts word indices to dense vectors\n",
    "        Embedding(input_dim=MAX_VOCAB_SIZE, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH),\n",
    "        \n",
    "        # LSTM layer: captures sequential patterns\n",
    "        LSTM(64, dropout=0.2, recurrent_dropout=0.2),\n",
    "        \n",
    "        # Dense layers for classification\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid')  # Binary classification\n",
    "    ])\n",
    "    \n",
    "    # Compile model\n",
    "    lstm_model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(\"\\nModel Architecture:\")\n",
    "    lstm_model.summary()\n",
    "    \n",
    "    # Train LSTM model\n",
    "    print(\"\\nTraining LSTM model...\")\n",
    "    print(\"Note: This will take 15-30 minutes depending on your hardware.\")\n",
    "    start_time = time()\n",
    "    \n",
    "    history = lstm_model.fit(\n",
    "        X_train_padded, y_train,\n",
    "        epochs=5,\n",
    "        batch_size=128,\n",
    "        validation_split=0.2,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    lstm_train_time = time() - start_time\n",
    "    print(f\"\\n‚úì Training completed in {lstm_train_time:.2f} seconds ({lstm_train_time/60:.1f} minutes)\")\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_proba_lstm = lstm_model.predict(X_test_padded).flatten()\n",
    "    y_pred_lstm = (y_pred_proba_lstm > 0.5).astype(int)\n",
    "    \n",
    "    # Evaluate\n",
    "    lstm_metrics = evaluate_model(y_test, y_pred_lstm, y_pred_proba_lstm, \"LSTM Neural Network\")\n",
    "    lstm_metrics['train_time'] = lstm_train_time\n",
    "    model_results['LSTM'] = lstm_metrics\n",
    "    \n",
    "    # Plot training history\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Accuracy plot\n",
    "    axes[0].plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    axes[0].plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Accuracy')\n",
    "    axes[0].set_title('LSTM Training History - Accuracy')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "    \n",
    "    # Loss plot\n",
    "    axes[1].plot(history.history['loss'], label='Train Loss')\n",
    "    axes[1].plot(history.history['val_loss'], label='Validation Loss')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Loss')\n",
    "    axes[1].set_title('LSTM Training History - Loss')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"TensorFlow not available. Skipping LSTM model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Comparison\n",
    "\n",
    "Let's compare all models side-by-side to determine which performs best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame(model_results).T\n",
    "comparison_df = comparison_df.round(4)\n",
    "comparison_df = comparison_df.sort_values('accuracy', ascending=False)\n",
    "\n",
    "print(\"Model Comparison:\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison_df)\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Performance metrics comparison\n",
    "metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1']\n",
    "comparison_df[metrics_to_plot].plot(kind='bar', ax=axes[0])\n",
    "axes[0].set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].set_xlabel('Model')\n",
    "axes[0].set_ylim(0.7, 1.0)\n",
    "axes[0].legend(title='Metric', loc='lower right')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "plt.setp(axes[0].xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Training time comparison\n",
    "comparison_df['train_time'].plot(kind='bar', ax=axes[1], color='coral')\n",
    "axes[1].set_title('Training Time Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Time (seconds)')\n",
    "axes[1].set_xlabel('Model')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "plt.setp(axes[1].xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify best model\n",
    "best_model_name = comparison_df['accuracy'].idxmax()\n",
    "print(f\"\\nüèÜ Best Model: {best_model_name}\")\n",
    "print(f\"   Accuracy: {comparison_df.loc[best_model_name, 'accuracy']:.4f}\")\n",
    "print(f\"   F1-Score: {comparison_df.loc[best_model_name, 'f1']:.4f}\")\n",
    "print(f\"   Training Time: {comparison_df.loc[best_model_name, 'train_time']:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Confusion Matrices\n",
    "\n",
    "Visualize the confusion matrices to understand where each model makes errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrices for all models\n",
    "models = [\n",
    "    ('Naive Bayes', y_pred_nb),\n",
    "    ('Logistic Regression', y_pred_lr),\n",
    "    ('Random Forest', y_pred_rf)\n",
    "]\n",
    "\n",
    "if TENSORFLOW_AVAILABLE:\n",
    "    models.append(('LSTM', y_pred_lstm))\n",
    "\n",
    "num_models = len(models)\n",
    "fig, axes = plt.subplots(1, num_models, figsize=(5*num_models, 4))\n",
    "if num_models == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for idx, (model_name, y_pred) in enumerate(models):\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
    "                xticklabels=['Negative', 'Positive'],\n",
    "                yticklabels=['Negative', 'Positive'])\n",
    "    axes[idx].set_title(f'{model_name}\\nConfusion Matrix', fontweight='bold')\n",
    "    axes[idx].set_ylabel('True Label')\n",
    "    axes[idx].set_xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ROC Curves\n",
    "\n",
    "ROC (Receiver Operating Characteristic) curves show the trade-off between true positive rate and false positive rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves for all models\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "models_with_proba = [\n",
    "    ('Naive Bayes', y_pred_proba_nb),\n",
    "    ('Logistic Regression', y_pred_proba_lr),\n",
    "    ('Random Forest', y_pred_proba_rf)\n",
    "]\n",
    "\n",
    "if TENSORFLOW_AVAILABLE:\n",
    "    models_with_proba.append(('LSTM', y_pred_proba_lstm))\n",
    "\n",
    "for model_name, y_pred_proba in models_with_proba:\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.plot(fpr, tpr, linewidth=2, label=f'{model_name} (AUC = {roc_auc:.4f})')\n",
    "\n",
    "# Plot diagonal (random classifier)\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier (AUC = 0.5000)')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curves - Model Comparison', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Interpretation:\")\n",
    "print(\"- The closer the curve to the top-left corner, the better the model\")\n",
    "print(\"- AUC (Area Under Curve) of 1.0 = perfect classifier\")\n",
    "print(\"- AUC of 0.5 = random guessing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Error Analysis\n",
    "\n",
    "Let's examine some misclassified examples to understand model limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use best model for error analysis (Logistic Regression typically performs best)\n",
    "# Add predictions to test dataframe\n",
    "test_df['predicted_lr'] = y_pred_lr\n",
    "test_df['predicted_proba_lr'] = y_pred_proba_lr\n",
    "test_df['correct'] = test_df['label'] == test_df['predicted_lr']\n",
    "\n",
    "# Identify misclassified examples\n",
    "misclassified = test_df[~test_df['correct']].copy()\n",
    "print(f\"Total misclassified: {len(misclassified)} out of {len(test_df)} ({len(misclassified)/len(test_df)*100:.2f}%)\")\n",
    "\n",
    "# Separate false positives and false negatives\n",
    "false_positives = misclassified[misclassified['label'] == 0]  # Predicted positive, actually negative\n",
    "false_negatives = misclassified[misclassified['label'] == 1]  # Predicted negative, actually positive\n",
    "\n",
    "print(f\"\\nFalse Positives: {len(false_positives)}\")\n",
    "print(f\"False Negatives: {len(false_negatives)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show examples of false positives (model thinks negative review is positive)\n",
    "print(\"Examples of False Positives (Actually Negative, Predicted Positive):\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for idx, row in false_positives.head(3).iterrows():\n",
    "    print(f\"\\nConfidence: {row['predicted_proba_lr']:.2f}\")\n",
    "    print(f\"Review: {row['review'][:500]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show examples of false negatives (model thinks positive review is negative)\n",
    "print(\"Examples of False Negatives (Actually Positive, Predicted Negative):\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for idx, row in false_negatives.head(3).iterrows():\n",
    "    print(f\"\\nConfidence: {row['predicted_proba_lr']:.2f}\")\n",
    "    print(f\"Review: {row['review'][:500]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Model Interpretability\n",
    "\n",
    "Understand which features (words) are most important for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importance from Logistic Regression\n",
    "# Coefficients indicate how strongly a feature contributes to each class\n",
    "feature_names = np.array(tfidf_vectorizer.get_feature_names_out())\n",
    "coefficients = lr_model.coef_[0]\n",
    "\n",
    "# Top features for positive sentiment (highest positive coefficients)\n",
    "top_positive_indices = np.argsort(coefficients)[-20:][::-1]\n",
    "top_positive_features = feature_names[top_positive_indices]\n",
    "top_positive_coeffs = coefficients[top_positive_indices]\n",
    "\n",
    "# Top features for negative sentiment (lowest/most negative coefficients)\n",
    "top_negative_indices = np.argsort(coefficients)[:20]\n",
    "top_negative_features = feature_names[top_negative_indices]\n",
    "top_negative_coeffs = coefficients[top_negative_indices]\n",
    "\n",
    "# Visualize top features\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Positive features\n",
    "axes[0].barh(range(len(top_positive_features)), top_positive_coeffs, color='green')\n",
    "axes[0].set_yticks(range(len(top_positive_features)))\n",
    "axes[0].set_yticklabels(top_positive_features)\n",
    "axes[0].set_xlabel('Coefficient Value', fontsize=12)\n",
    "axes[0].set_title('Top 20 Features for Positive Sentiment', fontsize=14, fontweight='bold')\n",
    "axes[0].invert_yaxis()\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Negative features\n",
    "axes[1].barh(range(len(top_negative_features)), top_negative_coeffs, color='red')\n",
    "axes[1].set_yticks(range(len(top_negative_features)))\n",
    "axes[1].set_yticklabels(top_negative_features)\n",
    "axes[1].set_xlabel('Coefficient Value', fontsize=12)\n",
    "axes[1].set_title('Top 20 Features for Negative Sentiment', fontsize=14, fontweight='bold')\n",
    "axes[1].invert_yaxis()\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Higher positive coefficients = stronger indicator of positive sentiment\")\n",
    "print(\"- More negative coefficients = stronger indicator of negative sentiment\")\n",
    "print(\"- Notice how bigrams (two-word phrases) can be very informative!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Making Predictions on New Reviews\n",
    "\n",
    "Let's test our best model on custom reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(review_text, model=lr_model, vectorizer=tfidf_vectorizer):\n",
    "    \"\"\"\n",
    "    Predict sentiment for a new review.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    review_text : str\n",
    "        Raw review text\n",
    "    model : sklearn model\n",
    "        Trained classifier\n",
    "    vectorizer : sklearn vectorizer\n",
    "        Fitted TF-IDF vectorizer\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict with sentiment, confidence, and processed text\n",
    "    \"\"\"\n",
    "    # Preprocess\n",
    "    cleaned = preprocess_text(review_text)\n",
    "    \n",
    "    # Vectorize\n",
    "    features = vectorizer.transform([cleaned])\n",
    "    \n",
    "    # Predict\n",
    "    prediction = model.predict(features)[0]\n",
    "    proba = model.predict_proba(features)[0]\n",
    "    \n",
    "    sentiment = 'positive' if prediction == 1 else 'negative'\n",
    "    confidence = proba[prediction]\n",
    "    \n",
    "    return {\n",
    "        'sentiment': sentiment,\n",
    "        'confidence': confidence,\n",
    "        'cleaned_text': cleaned,\n",
    "        'probabilities': {'negative': proba[0], 'positive': proba[1]}\n",
    "    }\n",
    "\n",
    "# Test on custom reviews\n",
    "test_reviews = [\n",
    "    \"This movie was absolutely fantastic! The acting was superb and the plot kept me engaged throughout.\",\n",
    "    \"Terrible waste of time. Poor acting, boring story, and awful special effects.\",\n",
    "    \"It was okay. Nothing special but not terrible either. Average movie overall.\",\n",
    "    \"One of the best films I've ever seen! Masterpiece of cinema. Highly recommended!\",\n",
    "    \"I fell asleep halfway through. Incredibly dull and predictable.\"\n",
    "]\n",
    "\n",
    "print(\"Predictions on Custom Reviews:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for review in test_reviews:\n",
    "    result = predict_sentiment(review)\n",
    "    print(f\"\\nReview: {review}\")\n",
    "    print(f\"Predicted Sentiment: {result['sentiment'].upper()}\")\n",
    "    print(f\"Confidence: {result['confidence']:.2%}\")\n",
    "    print(f\"Probabilities: Negative={result['probabilities']['negative']:.2%}, \"\n",
    "          f\"Positive={result['probabilities']['positive']:.2%}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Saving the Model for Deployment\n",
    "\n",
    "Export the best model and preprocessing artifacts for production use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory if it doesn't exist\n",
    "models_dir = Path('models')\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save Logistic Regression model (best balance of performance and speed)\n",
    "with open(models_dir / 'logistic_regression_model.pkl', 'wb') as f:\n",
    "    pickle.dump(lr_model, f)\n",
    "print(\"‚úì Saved Logistic Regression model\")\n",
    "\n",
    "# Save TF-IDF vectorizer (needed for preprocessing new reviews)\n",
    "with open(models_dir / 'tfidf_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf_vectorizer, f)\n",
    "print(\"‚úì Saved TF-IDF vectorizer\")\n",
    "\n",
    "# Save preprocessing function details\n",
    "preprocessing_config = {\n",
    "    'use_stemming': False,\n",
    "    'use_lemmatization': True,\n",
    "    'remove_stopwords': True,\n",
    "    'stopwords': list(stop_words)\n",
    "}\n",
    "with open(models_dir / 'preprocessing_config.pkl', 'wb') as f:\n",
    "    pickle.dump(preprocessing_config, f)\n",
    "print(\"‚úì Saved preprocessing configuration\")\n",
    "\n",
    "# Save LSTM model if trained\n",
    "if TENSORFLOW_AVAILABLE and 'LSTM' in model_results:\n",
    "    lstm_model.save(models_dir / 'lstm_model.h5')\n",
    "    print(\"‚úì Saved LSTM model\")\n",
    "    \n",
    "    with open(models_dir / 'keras_tokenizer.pkl', 'wb') as f:\n",
    "        pickle.dump(keras_tokenizer, f)\n",
    "    print(\"‚úì Saved Keras tokenizer\")\n",
    "\n",
    "print(f\"\\nAll models saved to: {models_dir.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Summary and Key Takeaways\n",
    "\n",
    "### What We Accomplished\n",
    "\n",
    "1. **Data Loading**: Successfully loaded 50,000 IMDB movie reviews\n",
    "2. **Text Preprocessing**: Built a comprehensive NLP pipeline (cleaning, tokenization, lemmatization)\n",
    "3. **Feature Engineering**: Created TF-IDF features with unigrams and bigrams\n",
    "4. **Model Training**: Trained and compared 4 different models:\n",
    "   - Naive Bayes: Fast baseline (83-85% accuracy)\n",
    "   - Logistic Regression: Best balance (87-89% accuracy)\n",
    "   - Random Forest: Good but slower (84-86% accuracy)\n",
    "   - LSTM: State-of-the-art deep learning (88-90% accuracy)\n",
    "5. **Evaluation**: Used multiple metrics (accuracy, precision, recall, F1, ROC-AUC)\n",
    "6. **Error Analysis**: Examined misclassified examples to understand limitations\n",
    "7. **Interpretability**: Identified most influential words for each sentiment\n",
    "8. **Deployment**: Exported models for production use\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "**Best Model**: Logistic Regression offers the best trade-off between:\n",
    "- Performance (87-89% accuracy)\n",
    "- Training speed (2-5 minutes)\n",
    "- Inference speed (milliseconds per prediction)\n",
    "- Interpretability (clear feature weights)\n",
    "\n",
    "**Important Features**:\n",
    "- Bigrams (two-word phrases) are highly informative\n",
    "- Words like \"excellent\", \"wonderful\", \"great\" strongly indicate positive sentiment\n",
    "- Words like \"waste\", \"worst\", \"terrible\" strongly indicate negative sentiment\n",
    "\n",
    "**Common Errors**:\n",
    "- Sarcastic reviews (positive words used ironically)\n",
    "- Mixed sentiment reviews (both positive and negative aspects)\n",
    "- Reviews with complex language or rare vocabulary\n",
    "\n",
    "### Next Steps for Improvement\n",
    "\n",
    "1. **Try advanced models**:\n",
    "   - BERT or other transformer models (Hugging Face)\n",
    "   - Pre-trained word embeddings (GloVe, Word2Vec)\n",
    "\n",
    "2. **Feature engineering**:\n",
    "   - Sentiment lexicons (VADER, TextBlob)\n",
    "   - Part-of-speech tags\n",
    "   - Negation handling (\"not good\" vs \"good\")\n",
    "\n",
    "3. **Hyperparameter tuning**:\n",
    "   - GridSearchCV or RandomizedSearchCV\n",
    "   - Cross-validation for robust evaluation\n",
    "\n",
    "4. **Deployment**:\n",
    "   - Build REST API with FastAPI (see api.py)\n",
    "   - Deploy to cloud (AWS, GCP, Azure)\n",
    "   - Add monitoring and logging\n",
    "\n",
    "5. **Business applications**:\n",
    "   - Real-time sentiment monitoring dashboard\n",
    "   - Product review analysis\n",
    "   - Social media sentiment tracking\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "‚úÖ Text preprocessing for NLP tasks  \n",
    "‚úÖ TF-IDF feature engineering  \n",
    "‚úÖ Training multiple ML models for comparison  \n",
    "‚úÖ Evaluating classifiers with proper metrics  \n",
    "‚úÖ Error analysis and model interpretation  \n",
    "‚úÖ Saving models for production deployment  \n",
    "\n",
    "**Congratulations!** You've built a complete sentiment analysis system from scratch.\n",
    "\n",
    "---\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [NLTK Book](https://www.nltk.org/book/)\n",
    "- [Scikit-learn Text Classification](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)\n",
    "- [TensorFlow Text Classification Tutorial](https://www.tensorflow.org/tutorials/keras/text_classification)\n",
    "- [Original IMDB Dataset Paper](http://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf)\n",
    "\n",
    "---\n",
    "\n",
    "**Next Project**: [Project 07 - Credit Card Fraud Detection](../07_credit_card_fraud/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
