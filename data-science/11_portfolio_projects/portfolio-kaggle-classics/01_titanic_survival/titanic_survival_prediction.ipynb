{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 01: Titanic Survival Prediction\n",
    "\n",
    "**Difficulty**: ‚≠ê Beginner\n",
    "\n",
    "**Estimated Time**: 20-25 hours\n",
    "\n",
    "**Project Type**: Binary Classification\n",
    "\n",
    "**Dataset**: Titanic from Kaggle\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this project, you will be able to:\n",
    "1. Perform comprehensive exploratory data analysis (EDA)\n",
    "2. Handle missing data using multiple strategies\n",
    "3. Engineer features from existing variables\n",
    "4. Compare multiple classification algorithms\n",
    "5. Evaluate models using appropriate metrics\n",
    "6. Create a professional portfolio project\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "The sinking of the Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew.\n",
    "\n",
    "**Goal**: Build a predictive model that answers the question: \"What sorts of people were more likely to survive?\" using passenger data (name, age, gender, socio-economic class, etc.).\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Machine Learning Fundamentals (Module 05)\n",
    "- Data Manipulation with Pandas (Module 02)\n",
    "- Data Visualization (Module 03)\n",
    "- Feature Engineering basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# ML Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    ")\n",
    "\n",
    "# Utilities\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure visualization defaults\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Inspect Data\n",
    "\n",
    "For this project, we'll use the Titanic dataset. You can download it from:\n",
    "- **Kaggle**: https://www.kaggle.com/c/titanic/data\n",
    "- **Seaborn**: Built-in dataset\n",
    "\n",
    "We'll use the seaborn built-in dataset for easy access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Titanic dataset from seaborn\n",
    "titanic_data = sns.load_dataset('titanic')\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Dataset shape: {titanic_data.shape}\")\n",
    "print(f\"Number of rows: {len(titanic_data)}\")\n",
    "print(f\"Number of columns: {len(titanic_data.columns)}\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Display first few rows\n",
    "titanic_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display column information\n",
    "titanic_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "titanic_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Dictionary\n",
    "\n",
    "| Variable | Definition | Key |\n",
    "|----------|------------|-----|\n",
    "| survived | Survival | 0 = No, 1 = Yes |\n",
    "| pclass | Ticket class | 1 = 1st, 2 = 2nd, 3 = 3rd |\n",
    "| sex | Sex | male/female |\n",
    "| age | Age in years | |\n",
    "| sibsp | # of siblings / spouses aboard | |\n",
    "| parch | # of parents / children aboard | |\n",
    "| fare | Passenger fare | |\n",
    "| embarked | Port of Embarkation | C = Cherbourg, Q = Queenstown, S = Southampton |\n",
    "| class | Same as pclass but categorical | |\n",
    "| who | man, woman, or child | |\n",
    "| deck | Deck number | |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)\n",
    "\n",
    "Let's explore the data to understand patterns and relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Missing Values Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = titanic_data.isnull().sum()\n",
    "missing_percentage = (missing_values / len(titanic_data)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_values,\n",
    "    'Percentage': missing_percentage\n",
    "})\n",
    "\n",
    "# Show only columns with missing values\n",
    "missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing values\n",
    "plt.figure(figsize=(12, 6))\n",
    "missing_cols = missing_df[missing_df['Missing Count'] > 0].sort_values('Percentage', ascending=False)\n",
    "\n",
    "plt.barh(missing_cols.index, missing_cols['Percentage'], color='coral')\n",
    "plt.xlabel('Percentage of Missing Values')\n",
    "plt.title('Missing Values by Feature')\n",
    "plt.xlim(0, 100)\n",
    "\n",
    "# Add percentage labels\n",
    "for i, v in enumerate(missing_cols['Percentage']):\n",
    "    plt.text(v + 1, i, f'{v:.1f}%', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Key Observations:\")\n",
    "print(\"- Deck has 77.2% missing values (may need to drop)\")\n",
    "print(\"- Age has 19.9% missing values (will impute)\")\n",
    "print(\"- Embarked has minimal missing values (will impute)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Survival Rate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall survival rate\n",
    "survival_rate = titanic_data['survived'].mean()\n",
    "print(f\"Overall Survival Rate: {survival_rate:.2%}\")\n",
    "print(f\"Survived: {titanic_data['survived'].sum()} passengers\")\n",
    "print(f\"Died: {len(titanic_data) - titanic_data['survived'].sum()} passengers\")\n",
    "\n",
    "# Visualize survival distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Pie chart\n",
    "survival_counts = titanic_data['survived'].value_counts()\n",
    "axes[0].pie(survival_counts, labels=['Died', 'Survived'], autopct='%1.1f%%',\n",
    "            colors=['#FF6B6B', '#4ECDC4'], startangle=90)\n",
    "axes[0].set_title('Survival Distribution')\n",
    "\n",
    "# Bar chart\n",
    "sns.countplot(data=titanic_data, x='survived', palette=['#FF6B6B', '#4ECDC4'], ax=axes[1])\n",
    "axes[1].set_xlabel('Survived')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Survival Count')\n",
    "axes[1].set_xticklabels(['Died (0)', 'Survived (1)'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Survival by Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Survival rate by gender\n",
    "gender_survival = titanic_data.groupby('sex')['survived'].agg(['sum', 'count', 'mean'])\n",
    "gender_survival.columns = ['Survived', 'Total', 'Survival Rate']\n",
    "print(\"Survival by Gender:\")\n",
    "print(gender_survival)\n",
    "print(f\"\\nFemales were {gender_survival.loc['female', 'Survival Rate'] / gender_survival.loc['male', 'Survival Rate']:.1f}x more likely to survive\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Count plot\n",
    "sns.countplot(data=titanic_data, x='sex', hue='survived', palette=['#FF6B6B', '#4ECDC4'], ax=axes[0])\n",
    "axes[0].set_title('Survival Count by Gender')\n",
    "axes[0].set_xlabel('Gender')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].legend(['Died', 'Survived'])\n",
    "\n",
    "# Survival rate\n",
    "gender_survival['Survival Rate'].plot(kind='bar', color=['#FF6B6B', '#4ECDC4'], ax=axes[1])\n",
    "axes[1].set_title('Survival Rate by Gender')\n",
    "axes[1].set_xlabel('Gender')\n",
    "axes[1].set_ylabel('Survival Rate')\n",
    "axes[1].set_xticklabels(['Female', 'Male'], rotation=0)\n",
    "axes[1].set_ylim(0, 1)\n",
    "\n",
    "# Add percentage labels\n",
    "for i, v in enumerate(gender_survival['Survival Rate']):\n",
    "    axes[1].text(i, v + 0.02, f'{v:.1%}', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Survival by Passenger Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Survival rate by class\n",
    "class_survival = titanic_data.groupby('pclass')['survived'].agg(['sum', 'count', 'mean'])\n",
    "class_survival.columns = ['Survived', 'Total', 'Survival Rate']\n",
    "print(\"Survival by Passenger Class:\")\n",
    "print(class_survival)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Count plot\n",
    "sns.countplot(data=titanic_data, x='pclass', hue='survived', palette=['#FF6B6B', '#4ECDC4'], ax=axes[0])\n",
    "axes[0].set_title('Survival Count by Passenger Class')\n",
    "axes[0].set_xlabel('Passenger Class')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].legend(['Died', 'Survived'])\n",
    "\n",
    "# Survival rate\n",
    "class_survival['Survival Rate'].plot(kind='bar', color=['#4ECDC4', '#95E1D3', '#FF6B6B'], ax=axes[1])\n",
    "axes[1].set_title('Survival Rate by Passenger Class')\n",
    "axes[1].set_xlabel('Passenger Class')\n",
    "axes[1].set_ylabel('Survival Rate')\n",
    "axes[1].set_xticklabels(['1st Class', '2nd Class', '3rd Class'], rotation=0)\n",
    "axes[1].set_ylim(0, 1)\n",
    "\n",
    "# Add percentage labels\n",
    "for i, v in enumerate(class_survival['Survival Rate']):\n",
    "    axes[1].text(i, v + 0.02, f'{v:.1%}', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Key Insight: Higher class passengers had much better survival rates!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Survival by Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age distribution by survival\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "titanic_data[titanic_data['survived'] == 0]['age'].hist(bins=30, alpha=0.7, label='Died',\n",
    "                                                         color='#FF6B6B', ax=axes[0])\n",
    "titanic_data[titanic_data['survived'] == 1]['age'].hist(bins=30, alpha=0.7, label='Survived',\n",
    "                                                         color='#4ECDC4', ax=axes[0])\n",
    "axes[0].set_xlabel('Age')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Age Distribution by Survival')\n",
    "axes[0].legend()\n",
    "\n",
    "# Box plot\n",
    "sns.boxplot(data=titanic_data, x='survived', y='age', palette=['#FF6B6B', '#4ECDC4'], ax=axes[1])\n",
    "axes[1].set_xlabel('Survived')\n",
    "axes[1].set_ylabel('Age')\n",
    "axes[1].set_title('Age Distribution by Survival')\n",
    "axes[1].set_xticklabels(['Died', 'Survived'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Age statistics\n",
    "print(\"Age Statistics by Survival:\")\n",
    "print(titanic_data.groupby('survived')['age'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Combined Analysis: Gender + Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Survival rate by gender and class\n",
    "gender_class_survival = titanic_data.groupby(['sex', 'pclass'])['survived'].mean().unstack()\n",
    "print(\"Survival Rate by Gender and Class:\")\n",
    "print(gender_class_survival)\n",
    "\n",
    "# Visualize\n",
    "gender_class_survival.plot(kind='bar', figsize=(10, 6), color=['#4ECDC4', '#95E1D3', '#FF6B6B'])\n",
    "plt.title('Survival Rate by Gender and Passenger Class')\n",
    "plt.xlabel('Gender')\n",
    "plt.ylabel('Survival Rate')\n",
    "plt.legend(['1st Class', '2nd Class', '3rd Class'])\n",
    "plt.xticks(rotation=0)\n",
    "plt.ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Key Insight: 1st class females had >95% survival rate, while 3rd class males had <15%!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numeric columns for correlation\n",
    "numeric_cols = ['survived', 'pclass', 'age', 'sibsp', 'parch', 'fare']\n",
    "correlation_matrix = titanic_data[numeric_cols].corr()\n",
    "\n",
    "# Plot correlation heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm',\n",
    "            center=0, square=True, linewidths=1)\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show correlations with survival\n",
    "print(\"\\nCorrelations with Survival:\")\n",
    "print(correlation_matrix['survived'].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing\n",
    "\n",
    "Now we'll prepare the data for machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Feature Selection and Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for preprocessing\n",
    "df = titanic_data.copy()\n",
    "\n",
    "# Drop columns with too many missing values or not useful\n",
    "columns_to_drop = ['deck', 'embark_town', 'alive', 'class', 'who', 'adult_male', 'alone']\n",
    "df = df.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "print(f\"Remaining columns: {df.columns.tolist()}\")\n",
    "print(f\"Dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering: Create new features\n",
    "\n",
    "# 1. Family Size\n",
    "df['family_size'] = df['sibsp'] + df['parch'] + 1\n",
    "\n",
    "# 2. Is Alone\n",
    "df['is_alone'] = (df['family_size'] == 1).astype(int)\n",
    "\n",
    "# 3. Fare per person\n",
    "df['fare_per_person'] = df['fare'] / df['family_size']\n",
    "\n",
    "# 4. Age categories\n",
    "df['age_category'] = pd.cut(df['age'], bins=[0, 12, 18, 35, 60, 100],\n",
    "                            labels=['Child', 'Teenager', 'Adult', 'Middle-aged', 'Senior'])\n",
    "\n",
    "print(\"‚úÖ New features created:\")\n",
    "print(\"- family_size: Total family members aboard\")\n",
    "print(\"- is_alone: Whether passenger traveled alone\")\n",
    "print(\"- fare_per_person: Fare divided by family size\")\n",
    "print(\"- age_category: Age grouped into categories\")\n",
    "\n",
    "# Display sample\n",
    "df[['survived', 'family_size', 'is_alone', 'fare_per_person', 'age_category']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current missing values\n",
    "print(\"Missing values before imputation:\")\n",
    "print(df.isnull().sum()[df.isnull().sum() > 0])\n",
    "\n",
    "# 1. Fill Age with median by sex and pclass (more accurate than overall median)\n",
    "df['age'] = df.groupby(['sex', 'pclass'])['age'].transform(\n",
    "    lambda x: x.fillna(x.median())\n",
    ")\n",
    "\n",
    "# 2. Fill Embarked with mode (most common port)\n",
    "df['embarked'] = df['embarked'].fillna(df['embarked'].mode()[0])\n",
    "\n",
    "# 3. Fill fare_per_person (created from fare, so same missing values)\n",
    "df['fare_per_person'] = df['fare_per_person'].fillna(df['fare_per_person'].median())\n",
    "\n",
    "# 4. Age category will be filled after age imputation\n",
    "df['age_category'] = pd.cut(df['age'], bins=[0, 12, 18, 35, 60, 100],\n",
    "                            labels=['Child', 'Teenager', 'Adult', 'Middle-aged', 'Senior'])\n",
    "\n",
    "print(\"\\nMissing values after imputation:\")\n",
    "print(df.isnull().sum()[df.isnull().sum() > 0])\n",
    "print(\"\\n‚úÖ All missing values handled!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Encode Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "\n",
    "# 1. Sex: male=1, female=0\n",
    "df['sex_encoded'] = df['sex'].map({'male': 1, 'female': 0})\n",
    "\n",
    "# 2. Embarked: one-hot encoding\n",
    "embarked_dummies = pd.get_dummies(df['embarked'], prefix='embarked')\n",
    "df = pd.concat([df, embarked_dummies], axis=1)\n",
    "\n",
    "# 3. Age category: ordinal encoding\n",
    "age_cat_mapping = {'Child': 0, 'Teenager': 1, 'Adult': 2, 'Middle-aged': 3, 'Senior': 4}\n",
    "df['age_cat_encoded'] = df['age_category'].map(age_cat_mapping)\n",
    "\n",
    "print(\"‚úÖ Categorical variables encoded\")\n",
    "print(f\"Dataset shape after encoding: {df.shape}\")\n",
    "\n",
    "# Display sample\n",
    "df[['sex', 'sex_encoded', 'embarked', 'embarked_C', 'embarked_Q', 'embarked_S']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Select Final Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for modeling\n",
    "feature_columns = [\n",
    "    'pclass', 'sex_encoded', 'age', 'sibsp', 'parch', 'fare',\n",
    "    'family_size', 'is_alone', 'fare_per_person',\n",
    "    'embarked_C', 'embarked_Q', 'embarked_S', 'age_cat_encoded'\n",
    "]\n",
    "\n",
    "X = df[feature_columns]\n",
    "y = df['survived']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nFeatures used: {feature_columns}\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(y.value_counts())\n",
    "print(f\"Survival rate: {y.mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train-Test Split and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets (80/20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} ({X_train.shape[0]/len(X):.1%})\")\n",
    "print(f\"Test set size: {X_test.shape[0]} ({X_test.shape[0]/len(X):.1%})\")\n",
    "print(f\"\\nTraining set survival rate: {y_train.mean():.2%}\")\n",
    "print(f\"Test set survival rate: {y_test.mean():.2%}\")\n",
    "\n",
    "# Scale features (important for some algorithms like SVM, KNN)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\n‚úÖ Data split and scaled successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Training and Evaluation\n",
    "\n",
    "We'll train and compare multiple classification algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Train Multiple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    'SVM': SVC(random_state=42, probability=True),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "results = {}\n",
    "\n",
    "print(\"Training models...\\n\")\n",
    "for name, model in models.items():\n",
    "    # Use scaled data for models that benefit from it\n",
    "    if name in ['Logistic Regression', 'SVM', 'K-Nearest Neighbors']:\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    results[name] = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'ROC AUC': roc_auc,\n",
    "        'model': model,\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úÖ {name}: Accuracy={accuracy:.4f}, F1={f1:.4f}, ROC AUC={roc_auc:.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ All models trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Compare Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "results_df = pd.DataFrame({\n",
    "    name: {metric: values[metric] for metric in ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC']}\n",
    "    for name, values in results.items()\n",
    "}).T\n",
    "\n",
    "print(\"Model Performance Comparison:\")\n",
    "print(results_df.sort_values('Accuracy', ascending=False))\n",
    "\n",
    "# Find best model\n",
    "best_model_name = results_df['Accuracy'].idxmax()\n",
    "print(f\"\\nüèÜ Best Model: {best_model_name} with {results_df.loc[best_model_name, 'Accuracy']:.2%} accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "colors = ['#4ECDC4', '#FF6B6B', '#95E1D3', '#FFD93D']\n",
    "\n",
    "for idx, (metric, color) in enumerate(zip(metrics, colors)):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    sorted_results = results_df.sort_values(metric, ascending=True)\n",
    "    sorted_results[metric].plot(kind='barh', ax=ax, color=color)\n",
    "    ax.set_xlabel(metric)\n",
    "    ax.set_title(f'Model Comparison: {metric}')\n",
    "    ax.set_xlim(0, 1)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(sorted_results[metric]):\n",
    "        ax.text(v + 0.01, i, f'{v:.3f}', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 ROC Curve Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves for all models\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for name, values in results.items():\n",
    "    fpr, tpr, _ = roc_curve(y_test, values['probabilities'])\n",
    "    plt.plot(fpr, tpr, label=f\"{name} (AUC = {values['ROC AUC']:.3f})\")\n",
    "\n",
    "# Plot diagonal line (random classifier)\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier (AUC = 0.500)')\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves - Model Comparison')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Confusion Matrix - Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best model predictions\n",
    "best_predictions = results[best_model_name]['predictions']\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_test, best_predictions)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', square=True,\n",
    "            xticklabels=['Died', 'Survived'],\n",
    "            yticklabels=['Died', 'Survived'])\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.title(f'Confusion Matrix - {best_model_name}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed classification report\n",
    "print(f\"\\nDetailed Classification Report - {best_model_name}:\")\n",
    "print(classification_report(y_test, best_predictions, target_names=['Died', 'Survived']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Feature Importance (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Random Forest model\n",
    "rf_model = results['Random Forest']['model']\n",
    "\n",
    "# Get feature importances\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': feature_columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importance (Random Forest):\")\n",
    "print(feature_importance)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(feature_importance['Feature'], feature_importance['Importance'], color='#4ECDC4')\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importance - Random Forest')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Key Insights:\")\n",
    "top_3 = feature_importance.head(3)['Feature'].tolist()\n",
    "print(f\"Top 3 most important features: {', '.join(top_3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Optimization (Hyperparameter Tuning)\n",
    "\n",
    "Let's tune the best model to improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for Random Forest (one of the best performers)\n",
    "print(\"Performing hyperparameter tuning for Random Forest...\\n\")\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [5, 10, 15, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\n‚úÖ Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_tuned = grid_search.predict(X_test)\n",
    "tuned_accuracy = accuracy_score(y_test, y_pred_tuned)\n",
    "\n",
    "print(f\"\\nTest set accuracy (before tuning): {results['Random Forest']['Accuracy']:.4f}\")\n",
    "print(f\"Test set accuracy (after tuning): {tuned_accuracy:.4f}\")\n",
    "print(f\"Improvement: {(tuned_accuracy - results['Random Forest']['Accuracy']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cross-Validation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation on all models\n",
    "print(\"Performing 5-fold cross-validation on all models...\\n\")\n",
    "\n",
    "cv_results = {}\n",
    "\n",
    "for name, model_info in results.items():\n",
    "    model = model_info['model']\n",
    "    \n",
    "    # Use scaled data for models that benefit from it\n",
    "    if name in ['Logistic Regression', 'SVM', 'K-Nearest Neighbors']:\n",
    "        scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
    "    else:\n",
    "        scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    \n",
    "    cv_results[name] = {\n",
    "        'Mean': scores.mean(),\n",
    "        'Std': scores.std(),\n",
    "        'Min': scores.min(),\n",
    "        'Max': scores.max()\n",
    "    }\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Mean Accuracy: {scores.mean():.4f} (+/- {scores.std() * 2:.4f})\")\n",
    "    print(f\"  Range: [{scores.min():.4f}, {scores.max():.4f}]\\n\")\n",
    "\n",
    "# Create visualization\n",
    "cv_df = pd.DataFrame(cv_results).T\n",
    "cv_df = cv_df.sort_values('Mean', ascending=True)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(cv_df.index, cv_df['Mean'], xerr=cv_df['Std'], color='#4ECDC4', alpha=0.7)\n",
    "plt.xlabel('Mean Cross-Validation Accuracy')\n",
    "plt.title('5-Fold Cross-Validation Results')\n",
    "plt.xlim(0, 1)\n",
    "\n",
    "# Add value labels\n",
    "for i, (idx, row) in enumerate(cv_df.iterrows()):\n",
    "    plt.text(row['Mean'] + 0.01, i, f\"{row['Mean']:.3f}\", va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Key Insights and Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Findings\n",
    "\n",
    "**1. Survival Patterns:**\n",
    "- Overall survival rate was 38.4%\n",
    "- Women had 74.2% survival rate vs 18.9% for men (\"Women and children first\")\n",
    "- 1st class passengers had 63.0% survival rate vs 24.2% for 3rd class\n",
    "- Children (age < 12) had higher survival rates than adults\n",
    "\n",
    "**2. Most Important Features:**\n",
    "- Gender (sex_encoded) - Most predictive feature\n",
    "- Passenger class (pclass) - Strong indicator of survival\n",
    "- Fare - Correlates with class and survival\n",
    "- Age - Younger passengers more likely to survive\n",
    "- Family size - Being alone or in very large families reduced survival\n",
    "\n",
    "**3. Model Performance:**\n",
    "- Best model achieved ~80-85% accuracy\n",
    "- Random Forest and Gradient Boosting performed best\n",
    "- Simple models (Logistic Regression) performed surprisingly well\n",
    "- Cross-validation showed consistent performance across folds\n",
    "\n",
    "**4. Business Insights:**\n",
    "- Socioeconomic status strongly influenced survival (1st class priority in lifeboats)\n",
    "- Gender was the strongest predictor (\"Women and children first\" protocol)\n",
    "- Location on ship (deck) would have been valuable but had too much missing data\n",
    "- Family connections affected survival (traveling alone was disadvantageous)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Next Steps and Improvements\n",
    "\n",
    "**Potential Improvements:**\n",
    "1. **Feature Engineering:**\n",
    "   - Extract titles from names (Mr., Mrs., Master, etc.)\n",
    "   - Create cabin deck features from cabin numbers\n",
    "   - Interaction features (e.g., sex * class)\n",
    "\n",
    "2. **Advanced Modeling:**\n",
    "   - Try ensemble methods (stacking, blending)\n",
    "   - Use XGBoost or LightGBM\n",
    "   - Neural networks for comparison\n",
    "\n",
    "3. **Model Deployment:**\n",
    "   - Create a web app with Streamlit\n",
    "   - Deploy as REST API with FastAPI\n",
    "   - Containerize with Docker\n",
    "\n",
    "4. **Further Analysis:**\n",
    "   - Analyze prediction errors (false positives/negatives)\n",
    "   - SHAP values for model interpretability\n",
    "   - Cost-sensitive learning (different costs for different errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Try these exercises to deepen your understanding:\n",
    "\n",
    "### Exercise 1: Title Extraction\n",
    "Extract titles (Mr., Mrs., Miss, etc.) from passenger names and use them as a feature. Do titles improve model performance?\n",
    "\n",
    "### Exercise 2: Ensemble Methods\n",
    "Create a voting classifier that combines the top 3 performing models. Does it improve accuracy?\n",
    "\n",
    "### Exercise 3: Cost-Sensitive Learning\n",
    "In a real scenario, false negatives (predicting death when survived) might be worse than false positives. Implement class weights to penalize false negatives more heavily.\n",
    "\n",
    "### Exercise 4: SHAP Analysis\n",
    "Use SHAP (SHapley Additive exPlanations) to explain individual predictions. Why did the model predict survival/death for specific passengers?\n",
    "\n",
    "### Exercise 5: Deployment\n",
    "Create a simple Streamlit app that takes passenger information as input and predicts survival probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Checklist\n",
    "\n",
    "‚úÖ **Completed:**\n",
    "- [x] Data loading and initial exploration\n",
    "- [x] Comprehensive EDA with visualizations\n",
    "- [x] Missing value handling\n",
    "- [x] Feature engineering\n",
    "- [x] Data preprocessing and encoding\n",
    "- [x] Train-test split\n",
    "- [x] Multiple model training and comparison\n",
    "- [x] Model evaluation with multiple metrics\n",
    "- [x] Hyperparameter tuning\n",
    "- [x] Cross-validation analysis\n",
    "- [x] Feature importance analysis\n",
    "- [x] Clear insights and conclusions\n",
    "\n",
    "üìã **For Portfolio:**\n",
    "- [ ] Create professional README.md\n",
    "- [ ] Add requirements.txt\n",
    "- [ ] Clean and organize code\n",
    "- [ ] Add comments and documentation\n",
    "- [ ] Create presentation slides\n",
    "- [ ] Deploy as web app (optional)\n",
    "- [ ] Write blog post about project (optional)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
