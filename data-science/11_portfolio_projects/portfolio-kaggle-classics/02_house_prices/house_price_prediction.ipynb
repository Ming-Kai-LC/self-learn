{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 02: House Price Prediction\n",
    "\n",
    "**Difficulty**: ‚≠ê Beginner\n",
    "\n",
    "**Estimated Time**: 25-30 hours\n",
    "\n",
    "**Project Type**: Regression\n",
    "\n",
    "**Dataset**: California Housing Dataset\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this project, you will be able to:\n",
    "1. Perform regression analysis on real-world data\n",
    "2. Handle multivariate feature relationships\n",
    "3. Apply feature selection and regularization techniques\n",
    "4. Compare multiple regression algorithms\n",
    "5. Evaluate regression models using appropriate metrics\n",
    "6. Create predictive models for continuous targets\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "**Goal**: Predict median house prices in California districts based on features like location, demographics, and housing characteristics.\n",
    "\n",
    "This project demonstrates:\n",
    "- Regression modeling (vs classification)\n",
    "- Feature engineering for continuous targets\n",
    "- Handling geographic and demographic data\n",
    "- Regularization techniques (Ridge, Lasso, ElasticNet)\n",
    "- Ensemble methods for regression\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Machine Learning Fundamentals (Module 05)\n",
    "- Linear Regression concepts\n",
    "- Feature Engineering\n",
    "- Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "\n",
    "# Regression Models\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, mean_absolute_error, r2_score,\n",
    "    mean_absolute_percentage_error\n",
    ")\n",
    "\n",
    "# Utilities\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure visualization defaults\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load California Housing dataset\n",
    "housing_data = fetch_california_housing(as_frame=True)\n",
    "\n",
    "# Create DataFrame\n",
    "df = housing_data.frame\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Number of rows: {len(df)}\")\n",
    "print(f\"Number of columns: {len(df.columns)}\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Display first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dataset description\n",
    "print(\"Dataset Description:\")\n",
    "print(housing_data.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display column information\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Dictionary\n",
    "\n",
    "| Variable | Definition | Unit |\n",
    "|----------|------------|------|\n",
    "| MedInc | Median income in block group | tens of thousands of dollars |\n",
    "| HouseAge | Median house age in block group | years |\n",
    "| AveRooms | Average number of rooms per household | rooms |\n",
    "| AveBedrms | Average number of bedrooms per household | bedrooms |\n",
    "| Population | Block group population | people |\n",
    "| AveOccup | Average number of household members | people |\n",
    "| Latitude | Latitude | degrees |\n",
    "| Longitude | Longitude | degrees |\n",
    "| **MedHouseVal** | **Median house value (TARGET)** | **hundreds of thousands of dollars** |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Missing Values Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing values by column:\")\n",
    "print(missing_values)\n",
    "print(f\"\\nTotal missing values: {missing_values.sum()}\")\n",
    "\n",
    "if missing_values.sum() == 0:\n",
    "    print(\"‚úÖ No missing values in this dataset!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Target Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze target variable (MedHouseVal)\n",
    "target = df['MedHouseVal']\n",
    "\n",
    "print(\"House Price Statistics:\")\n",
    "print(f\"Mean: ${target.mean():.2f}00k = ${target.mean() * 100:.2f}k\")\n",
    "print(f\"Median: ${target.median():.2f}00k = ${target.median() * 100:.2f}k\")\n",
    "print(f\"Std Dev: ${target.std():.2f}00k\")\n",
    "print(f\"Min: ${target.min():.2f}00k = ${target.min() * 100:.2f}k\")\n",
    "print(f\"Max: ${target.max():.2f}00k = ${target.max() * 100:.2f}k\")\n",
    "\n",
    "# Visualize target distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(target, bins=50, color='#4ECDC4', edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(target.mean(), color='red', linestyle='--', label=f'Mean: ${target.mean():.2f}00k')\n",
    "axes[0].axvline(target.median(), color='green', linestyle='--', label=f'Median: ${target.median():.2f}00k')\n",
    "axes[0].set_xlabel('Median House Value ($100k)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Distribution of House Prices')\n",
    "axes[0].legend()\n",
    "\n",
    "# Box plot\n",
    "axes[1].boxplot(target, vert=True)\n",
    "axes[1].set_ylabel('Median House Value ($100k)')\n",
    "axes[1].set_title('House Price Box Plot')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Observation: Distribution is right-skewed with a cap at $500k (5.0 in dataset)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Geographic Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot house locations colored by price\n",
    "plt.figure(figsize=(14, 10))\n",
    "scatter = plt.scatter(df['Longitude'], df['Latitude'],\n",
    "                     c=df['MedHouseVal'], cmap='viridis',\n",
    "                     alpha=0.4, s=10)\n",
    "plt.colorbar(scatter, label='Median House Value ($100k)')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('California Housing Prices by Location')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Key Insight: Coastal areas (especially Bay Area and LA) have highest prices!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Feature Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distributions of all features\n",
    "fig, axes = plt.subplots(3, 3, figsize=(16, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(df.columns):\n",
    "    axes[idx].hist(df[col], bins=50, color='#4ECDC4', edgecolor='black', alpha=0.7)\n",
    "    axes[idx].set_title(col)\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation matrix\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm',\n",
    "            center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Feature Correlation Matrix', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show correlations with target\n",
    "print(\"\\nCorrelations with House Price (MedHouseVal):\")\n",
    "target_corr = correlation_matrix['MedHouseVal'].sort_values(ascending=False)\n",
    "print(target_corr)\n",
    "\n",
    "print(\"\\nüìä Key Insights:\")\n",
    "print(f\"- Strongest positive correlation: {target_corr.index[1]} ({target_corr.iloc[1]:.3f})\")\n",
    "print(f\"- Strongest negative correlation: {target_corr.index[-1]} ({target_corr.iloc[-1]:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Relationship: Income vs Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot: Median Income vs House Price\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(df['MedInc'], df['MedHouseVal'], alpha=0.3, s=10, c='#4ECDC4')\n",
    "plt.xlabel('Median Income ($10k)')\n",
    "plt.ylabel('Median House Value ($100k)')\n",
    "plt.title('House Price vs Median Income')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# Add trend line\n",
    "z = np.polyfit(df['MedInc'], df['MedHouseVal'], 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(df['MedInc'].sort_values(), p(df['MedInc'].sort_values()),\n",
    "         \"r--\", label=f'Trend: y={z[0]:.2f}x+{z[1]:.2f}')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Correlation: {df['MedInc'].corr(df['MedHouseVal']):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots for outlier detection\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(df.columns[:-1]):  # Exclude target\n",
    "    axes[idx].boxplot(df[col])\n",
    "    axes[idx].set_title(col)\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "axes[-1].axis('off')  # Hide last empty subplot\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Observation: Several features have outliers, especially AveRooms, AveBedrms, and AveOccup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for feature engineering\n",
    "df_engineered = df.copy()\n",
    "\n",
    "# 1. Rooms per Bedroom (indicator of apartment type)\n",
    "df_engineered['RoomsPerBedroom'] = df['AveRooms'] / (df['AveBedrms'] + 1e-5)  # Add small value to avoid division by zero\n",
    "\n",
    "# 2. Population Density (people per household)\n",
    "df_engineered['PopulationDensity'] = df['Population'] / (df['AveOccup'] + 1e-5)\n",
    "\n",
    "# 3. Bedroom Ratio (proportion of bedrooms to total rooms)\n",
    "df_engineered['BedroomRatio'] = df['AveBedrms'] / (df['AveRooms'] + 1e-5)\n",
    "\n",
    "# 4. Log transformations for skewed features\n",
    "df_engineered['Log_MedInc'] = np.log1p(df['MedInc'])\n",
    "df_engineered['Log_AveRooms'] = np.log1p(df['AveRooms'])\n",
    "df_engineered['Log_Population'] = np.log1p(df['Population'])\n",
    "\n",
    "# 5. Geographic features (distance from city center approximations)\n",
    "# Approximate San Francisco coordinates\n",
    "sf_lat, sf_lon = 37.7749, -122.4194\n",
    "df_engineered['DistanceFromSF'] = np.sqrt(\n",
    "    (df['Latitude'] - sf_lat)**2 + (df['Longitude'] - sf_lon)**2\n",
    ")\n",
    "\n",
    "# Approximate Los Angeles coordinates\n",
    "la_lat, la_lon = 34.0522, -118.2437\n",
    "df_engineered['DistanceFromLA'] = np.sqrt(\n",
    "    (df['Latitude'] - la_lat)**2 + (df['Longitude'] - la_lon)**2\n",
    ")\n",
    "\n",
    "print(\"‚úÖ New features created:\")\n",
    "new_features = ['RoomsPerBedroom', 'PopulationDensity', 'BedroomRatio',\n",
    "                'Log_MedInc', 'Log_AveRooms', 'Log_Population',\n",
    "                'DistanceFromSF', 'DistanceFromLA']\n",
    "for feat in new_features:\n",
    "    print(f\"  - {feat}\")\n",
    "\n",
    "print(f\"\\nDataset shape after feature engineering: {df_engineered.shape}\")\n",
    "\n",
    "# Display sample\n",
    "df_engineered[new_features].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for modeling\n",
    "# We'll compare original features vs engineered features\n",
    "\n",
    "# Original features\n",
    "original_features = ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms',\n",
    "                    'Population', 'AveOccup', 'Latitude', 'Longitude']\n",
    "\n",
    "# All features (original + engineered)\n",
    "all_features = original_features + new_features\n",
    "\n",
    "# Prepare X and y\n",
    "X_original = df[original_features]\n",
    "X_engineered = df_engineered[all_features]\n",
    "y = df['MedHouseVal']\n",
    "\n",
    "print(f\"Original features: {len(original_features)}\")\n",
    "print(f\"Engineered features: {len(all_features)}\")\n",
    "print(f\"Target shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data (80/20 split)\n",
    "X_train_orig, X_test_orig, y_train, y_test = train_test_split(\n",
    "    X_original, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X_train_eng, X_test_eng, _, _ = train_test_split(\n",
    "    X_engineered, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train_orig.shape[0]} ({X_train_orig.shape[0]/len(X_original):.1%})\")\n",
    "print(f\"Test set size: {X_test_orig.shape[0]} ({X_test_orig.shape[0]/len(X_original):.1%})\")\n",
    "print(f\"\\nTarget statistics:\")\n",
    "print(f\"Training mean: ${y_train.mean():.2f}00k\")\n",
    "print(f\"Test mean: ${y_test.mean():.2f}00k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "scaler_orig = StandardScaler()\n",
    "X_train_orig_scaled = scaler_orig.fit_transform(X_train_orig)\n",
    "X_test_orig_scaled = scaler_orig.transform(X_test_orig)\n",
    "\n",
    "scaler_eng = StandardScaler()\n",
    "X_train_eng_scaled = scaler_eng.fit_transform(X_train_eng)\n",
    "X_test_eng_scaled = scaler_eng.transform(X_test_eng)\n",
    "\n",
    "print(\"‚úÖ Features scaled successfully!\")\n",
    "print(f\"\\nScaled training data shape: {X_train_orig_scaled.shape}\")\n",
    "print(f\"Scaled test data shape: {X_test_orig_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Baseline: Simple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train baseline model\n",
    "baseline_model = LinearRegression()\n",
    "baseline_model.fit(X_train_orig, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_baseline = baseline_model.predict(X_test_orig)\n",
    "\n",
    "# Evaluate\n",
    "rmse_baseline = np.sqrt(mean_squared_error(y_test, y_pred_baseline))\n",
    "mae_baseline = mean_absolute_error(y_test, y_pred_baseline)\n",
    "r2_baseline = r2_score(y_test, y_pred_baseline)\n",
    "\n",
    "print(\"Baseline Linear Regression Results:\")\n",
    "print(f\"RMSE: ${rmse_baseline:.4f}00k = ${rmse_baseline * 100:.2f}k\")\n",
    "print(f\"MAE: ${mae_baseline:.4f}00k = ${mae_baseline * 100:.2f}k\")\n",
    "print(f\"R¬≤ Score: {r2_baseline:.4f}\")\n",
    "\n",
    "# Visualize predictions\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred_baseline, alpha=0.3, s=10, c='#4ECDC4')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],\n",
    "         'r--', lw=2, label='Perfect Prediction')\n",
    "plt.xlabel('Actual Price ($100k)')\n",
    "plt.ylabel('Predicted Price ($100k)')\n",
    "plt.title(f'Baseline: Actual vs Predicted (R¬≤ = {r2_baseline:.3f})')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Train Multiple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define regression models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(random_state=42),\n",
    "    'Lasso Regression': Lasso(random_state=42, max_iter=10000),\n",
    "    'ElasticNet': ElasticNet(random_state=42, max_iter=10000),\n",
    "    'Decision Tree': DecisionTreeRegressor(random_state=42),\n",
    "    'Random Forest': RandomForestRegressor(random_state=42, n_estimators=100),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(random_state=42),\n",
    "    'SVR': SVR(),\n",
    "    'KNN': KNeighborsRegressor()\n",
    "}\n",
    "\n",
    "# Train and evaluate each model on ORIGINAL features\n",
    "results_original = {}\n",
    "\n",
    "print(\"Training models on ORIGINAL features...\\n\")\n",
    "for name, model in models.items():\n",
    "    # Use scaled data for models that benefit from it\n",
    "    if name in ['Ridge Regression', 'Lasso Regression', 'ElasticNet', 'SVR', 'KNN']:\n",
    "        model.fit(X_train_orig_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_orig_scaled)\n",
    "    else:\n",
    "        model.fit(X_train_orig, y_train)\n",
    "        y_pred = model.predict(X_test_orig)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred) * 100\n",
    "    \n",
    "    results_original[name] = {\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R¬≤ Score': r2,\n",
    "        'MAPE (%)': mape,\n",
    "        'model': model,\n",
    "        'predictions': y_pred\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úÖ {name}: RMSE={rmse:.4f}, R¬≤={r2:.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ All models trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Compare Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "results_df = pd.DataFrame({\n",
    "    name: {metric: values[metric] for metric in ['RMSE', 'MAE', 'R¬≤ Score', 'MAPE (%)']}\n",
    "    for name, values in results_original.items()\n",
    "}).T\n",
    "\n",
    "# Sort by R¬≤ Score\n",
    "results_df_sorted = results_df.sort_values('R¬≤ Score', ascending=False)\n",
    "\n",
    "print(\"Model Performance Comparison (Original Features):\")\n",
    "print(results_df_sorted)\n",
    "\n",
    "# Find best model\n",
    "best_model_name = results_df['R¬≤ Score'].idxmax()\n",
    "print(f\"\\nüèÜ Best Model: {best_model_name} with R¬≤ = {results_df.loc[best_model_name, 'R¬≤ Score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "metrics = ['RMSE', 'MAE', 'R¬≤ Score', 'MAPE (%)']\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#95E1D3', '#FFD93D']\n",
    "\n",
    "for idx, (metric, color) in enumerate(zip(metrics, colors)):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    if metric == 'R¬≤ Score':\n",
    "        sorted_results = results_df.sort_values(metric, ascending=True)\n",
    "    else:\n",
    "        sorted_results = results_df.sort_values(metric, ascending=False)  # Lower is better\n",
    "    \n",
    "    sorted_results[metric].plot(kind='barh', ax=ax, color=color)\n",
    "    ax.set_xlabel(metric)\n",
    "    ax.set_title(f'Model Comparison: {metric}')\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(sorted_results[metric]):\n",
    "        ax.text(v + 0.01, i, f'{v:.3f}', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Prediction Visualization - Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best model predictions\n",
    "best_predictions = results_original[best_model_name]['predictions']\n",
    "best_r2 = results_original[best_model_name]['R¬≤ Score']\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Scatter plot: Actual vs Predicted\n",
    "axes[0].scatter(y_test, best_predictions, alpha=0.3, s=10, c='#4ECDC4')\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],\n",
    "             'r--', lw=2, label='Perfect Prediction')\n",
    "axes[0].set_xlabel('Actual Price ($100k)')\n",
    "axes[0].set_ylabel('Predicted Price ($100k)')\n",
    "axes[0].set_title(f'{best_model_name}: Actual vs Predicted (R¬≤ = {best_r2:.3f})')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Residuals plot\n",
    "residuals = y_test - best_predictions\n",
    "axes[1].scatter(best_predictions, residuals, alpha=0.3, s=10, c='#FF6B6B')\n",
    "axes[1].axhline(y=0, color='black', linestyle='--', lw=2)\n",
    "axes[1].set_xlabel('Predicted Price ($100k)')\n",
    "axes[1].set_ylabel('Residuals')\n",
    "axes[1].set_title(f'{best_model_name}: Residual Plot')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Residual Statistics:\")\n",
    "print(f\"Mean: {residuals.mean():.4f}\")\n",
    "print(f\"Std Dev: {residuals.std():.4f}\")\n",
    "print(f\"Min: {residuals.min():.4f}\")\n",
    "print(f\"Max: {residuals.max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Feature Importance (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Random Forest model\n",
    "rf_model = results_original['Random Forest']['model']\n",
    "\n",
    "# Get feature importances\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': original_features,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importance (Random Forest):\")\n",
    "print(feature_importance)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance['Feature'], feature_importance['Importance'], color='#4ECDC4')\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importance - Random Forest')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Key Insights:\")\n",
    "top_3 = feature_importance.head(3)['Feature'].tolist()\n",
    "print(f\"Top 3 most important features: {', '.join(top_3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for Gradient Boosting (one of the best performers)\n",
    "print(\"Performing hyperparameter tuning for Gradient Boosting...\\n\")\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    GradientBoostingRegressor(random_state=42),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_orig, y_train)\n",
    "\n",
    "print(f\"\\n‚úÖ Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation R¬≤ score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_tuned = grid_search.predict(X_test_orig)\n",
    "rmse_tuned = np.sqrt(mean_squared_error(y_test, y_pred_tuned))\n",
    "r2_tuned = r2_score(y_test, y_pred_tuned)\n",
    "\n",
    "print(f\"\\nTest set R¬≤ (before tuning): {results_original['Gradient Boosting']['R¬≤ Score']:.4f}\")\n",
    "print(f\"Test set R¬≤ (after tuning): {r2_tuned:.4f}\")\n",
    "print(f\"Test set RMSE (after tuning): ${rmse_tuned:.4f}00k = ${rmse_tuned * 100:.2f}k\")\n",
    "print(f\"Improvement: {(r2_tuned - results_original['Gradient Boosting']['R¬≤ Score']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cross-Validation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation on top models\n",
    "print(\"Performing 5-fold cross-validation...\\n\")\n",
    "\n",
    "cv_results = {}\n",
    "top_models = ['Random Forest', 'Gradient Boosting', 'Ridge Regression']\n",
    "\n",
    "for name in top_models:\n",
    "    model = results_original[name]['model']\n",
    "    \n",
    "    # Use scaled data for Ridge\n",
    "    if name == 'Ridge Regression':\n",
    "        scores = cross_val_score(model, X_train_orig_scaled, y_train,\n",
    "                                cv=5, scoring='r2')\n",
    "    else:\n",
    "        scores = cross_val_score(model, X_train_orig, y_train,\n",
    "                                cv=5, scoring='r2')\n",
    "    \n",
    "    cv_results[name] = {\n",
    "        'Mean': scores.mean(),\n",
    "        'Std': scores.std(),\n",
    "        'Min': scores.min(),\n",
    "        'Max': scores.max()\n",
    "    }\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Mean R¬≤: {scores.mean():.4f} (+/- {scores.std() * 2:.4f})\")\n",
    "    print(f\"  Range: [{scores.min():.4f}, {scores.max():.4f}]\\n\")\n",
    "\n",
    "# Visualize CV results\n",
    "cv_df = pd.DataFrame(cv_results).T\n",
    "cv_df = cv_df.sort_values('Mean', ascending=True)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(cv_df.index, cv_df['Mean'], xerr=cv_df['Std'], color='#4ECDC4', alpha=0.7)\n",
    "plt.xlabel('Mean Cross-Validation R¬≤ Score')\n",
    "plt.title('5-Fold Cross-Validation Results')\n",
    "\n",
    "# Add value labels\n",
    "for i, (idx, row) in enumerate(cv_df.iterrows()):\n",
    "    plt.text(row['Mean'] + 0.01, i, f\"{row['Mean']:.3f}\", va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Key Insights and Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Findings\n",
    "\n",
    "**1. Price Patterns:**\n",
    "- Average house price: ~$200k (in 1990s dollars)\n",
    "- Strong geographic influence (coastal areas more expensive)\n",
    "- Median income is the strongest predictor (correlation: 0.69)\n",
    "- Location (Latitude/Longitude) also highly important\n",
    "\n",
    "**2. Most Important Features:**\n",
    "- Median Income (MedInc) - Strongest predictor\n",
    "- Location (Latitude, Longitude) - Geographic premium\n",
    "- House Age - Moderate importance\n",
    "- Average Rooms/Bedrooms - Housing quality indicators\n",
    "\n",
    "**3. Model Performance:**\n",
    "- Best models: Random Forest and Gradient Boosting\n",
    "- Achieved R¬≤ scores of ~0.80-0.82\n",
    "- Prediction error (RMSE): ~$50-55k\n",
    "- Linear models (Ridge/Lasso) performed reasonably well (R¬≤ ~0.60)\n",
    "- Complex models significantly outperformed simple linear regression\n",
    "\n",
    "**4. Business Insights:**\n",
    "- Income is the primary driver of house prices (socioeconomic factor)\n",
    "- Location matters significantly (coastal premium)\n",
    "- Older houses don't necessarily have lower prices (vintage value)\n",
    "- Average occupancy has minimal impact on price\n",
    "\n",
    "**5. Model Limitations:**\n",
    "- Capped target at $500k may affect predictions for luxury homes\n",
    "- Block-level data averages out individual house characteristics\n",
    "- No information about house condition, school quality, crime rates\n",
    "- Data is from 1990s (prices adjusted for modern use)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Next Steps and Improvements\n",
    "\n",
    "**Potential Improvements:**\n",
    "1. **Feature Engineering:**\n",
    "   - Test engineered features (RoomsPerBedroom, DistanceFromSF, etc.)\n",
    "   - Create interaction features (Income √ó Location)\n",
    "   - Polynomial features for non-linear relationships\n",
    "\n",
    "2. **Advanced Modeling:**\n",
    "   - XGBoost and LightGBM\n",
    "   - Neural networks\n",
    "   - Ensemble methods (stacking)\n",
    "\n",
    "3. **Error Analysis:**\n",
    "   - Analyze predictions with large errors\n",
    "   - Identify patterns in misclassifications\n",
    "   - Address outliers and capped values\n",
    "\n",
    "4. **Model Deployment:**\n",
    "   - Create Streamlit app for price predictions\n",
    "   - Deploy as REST API with FastAPI\n",
    "   - Dockerize for production\n",
    "\n",
    "5. **Further Analysis:**\n",
    "   - Geographic clustering analysis\n",
    "   - Time series analysis if temporal data available\n",
    "   - Compare with real estate APIs (Zillow, Redfin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Try these exercises to deepen your understanding:\n",
    "\n",
    "### Exercise 1: Test Engineered Features\n",
    "Train the models using the engineered features (X_engineered). Do they improve R¬≤ scores?\n",
    "\n",
    "### Exercise 2: Polynomial Features\n",
    "Use `PolynomialFeatures` to create quadratic features. How does this affect linear regression performance?\n",
    "\n",
    "### Exercise 3: Learning Curves\n",
    "Plot learning curves to diagnose bias vs variance. Are the models overfitting or underfitting?\n",
    "\n",
    "### Exercise 4: Feature Selection\n",
    "Use Lasso's coefficients to identify and remove unimportant features. Does this simplify the model without hurting performance?\n",
    "\n",
    "### Exercise 5: Deployment\n",
    "Create a Streamlit app that takes house characteristics as input and predicts price with the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Checklist\n",
    "\n",
    "‚úÖ **Completed:**\n",
    "- [x] Data loading and exploration\n",
    "- [x] Comprehensive EDA with visualizations\n",
    "- [x] Geographic analysis\n",
    "- [x] Correlation analysis\n",
    "- [x] Feature engineering\n",
    "- [x] Train-test split\n",
    "- [x] Feature scaling\n",
    "- [x] Multiple regression models (9 algorithms)\n",
    "- [x] Model comparison with multiple metrics\n",
    "- [x] Hyperparameter tuning\n",
    "- [x] Cross-validation analysis\n",
    "- [x] Feature importance analysis\n",
    "- [x] Residual analysis\n",
    "- [x] Clear insights and conclusions\n",
    "\n",
    "üìã **For Portfolio:**\n",
    "- [ ] Create professional README.md\n",
    "- [ ] Add requirements.txt\n",
    "- [ ] Test engineered features\n",
    "- [ ] Create presentation slides\n",
    "- [ ] Deploy as web app (optional)\n",
    "- [ ] Write blog post (optional)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
