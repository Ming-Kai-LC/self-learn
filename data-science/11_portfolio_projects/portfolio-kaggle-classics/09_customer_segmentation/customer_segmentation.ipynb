{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 09: Customer Segmentation\n",
    "\n",
    "**Difficulty**: ⭐⭐ Intermediate\n",
    "**Estimated Time**: 4-5 hours\n",
    "**Prerequisites**: \n",
    "- Basic understanding of unsupervised learning\n",
    "- Pandas for data manipulation\n",
    "- Familiarity with scikit-learn\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Perform RFM (Recency, Frequency, Monetary) analysis on customer transaction data\n",
    "2. Apply and compare multiple clustering algorithms (K-Means, DBSCAN, Hierarchical, GMM)\n",
    "3. Determine optimal number of clusters using elbow method and silhouette analysis\n",
    "4. Visualize high-dimensional customer segments using PCA and t-SNE\n",
    "5. Profile customer segments and derive actionable business insights\n",
    "6. Develop targeted marketing strategies for different customer groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "First, let's import all necessary libraries and configure our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning - Clustering\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Machine Learning - Preprocessing and Metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.metrics import silhouette_samples\n",
    "\n",
    "# Dimensionality Reduction\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Hierarchical clustering visualization\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Date and time handling\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure visualization defaults\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Dataset\n",
    "\n",
    "We'll use the Online Retail dataset which contains transactions from a UK-based online retailer.\n",
    "\n",
    "**Note**: Download the dataset from [Kaggle](https://www.kaggle.com/datasets/vijayuv/onlineretail) and place it in the `data/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# Using relative path so it works on any computer\n",
    "data_path = 'data/online_retail.csv'\n",
    "\n",
    "try:\n",
    "    retail_data = pd.read_csv(data_path, encoding='ISO-8859-1')\n",
    "    print(f\"Dataset loaded successfully!\")\n",
    "    print(f\"Shape: {retail_data.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {data_path}\")\n",
    "    print(\"Please download the dataset from Kaggle and place it in the data/ directory\")\n",
    "    # Create sample data for demonstration\n",
    "    print(\"\\nCreating sample data for demonstration...\")\n",
    "    np.random.seed(42)\n",
    "    n_transactions = 10000\n",
    "    n_customers = 500\n",
    "    \n",
    "    retail_data = pd.DataFrame({\n",
    "        'InvoiceNo': [f'INV{i:06d}' for i in range(n_transactions)],\n",
    "        'StockCode': np.random.choice(['PROD' + str(i) for i in range(100)], n_transactions),\n",
    "        'Description': np.random.choice(['Product ' + str(i) for i in range(100)], n_transactions),\n",
    "        'Quantity': np.random.randint(1, 50, n_transactions),\n",
    "        'InvoiceDate': pd.date_range(start='2010-01-01', periods=n_transactions, freq='H'),\n",
    "        'UnitPrice': np.random.uniform(1, 100, n_transactions),\n",
    "        'CustomerID': np.random.choice(range(10000, 10000 + n_customers), n_transactions),\n",
    "        'Country': np.random.choice(['United Kingdom', 'France', 'Germany', 'Spain'], n_transactions)\n",
    "    })\n",
    "    print(f\"Sample data created with shape: {retail_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "retail_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset information\n",
    "print(\"Dataset Information:\")\n",
    "print(\"=\" * 50)\n",
    "retail_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"\\nNumerical columns statistics:\")\n",
    "retail_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values per column:\")\n",
    "missing_values = retail_data.isnull().sum()\n",
    "missing_percentage = (missing_values / len(retail_data)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_values,\n",
    "    'Percentage': missing_percentage\n",
    "})\n",
    "\n",
    "print(missing_df[missing_df['Missing Count'] > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Data Exploration\n",
    "\n",
    "1. How many unique customers are in the dataset?\n",
    "2. What is the date range of transactions?\n",
    "3. Which country has the most transactions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: Use unique(), value_counts(), and date operations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning and Preprocessing\n",
    "\n",
    "Before performing RFM analysis, we need to clean the data:\n",
    "- Remove rows with missing CustomerID\n",
    "- Remove cancelled transactions (InvoiceNo starting with 'C')\n",
    "- Remove negative quantities and prices\n",
    "- Convert InvoiceDate to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for cleaning\n",
    "clean_data = retail_data.copy()\n",
    "\n",
    "print(f\"Original data shape: {clean_data.shape}\")\n",
    "\n",
    "# Remove missing CustomerID\n",
    "clean_data = clean_data.dropna(subset=['CustomerID'])\n",
    "print(f\"After removing missing CustomerID: {clean_data.shape}\")\n",
    "\n",
    "# Convert CustomerID to integer\n",
    "clean_data['CustomerID'] = clean_data['CustomerID'].astype(int)\n",
    "\n",
    "# Convert InvoiceDate to datetime\n",
    "clean_data['InvoiceDate'] = pd.to_datetime(clean_data['InvoiceDate'])\n",
    "\n",
    "# Remove cancelled transactions (InvoiceNo starting with 'C')\n",
    "clean_data = clean_data[~clean_data['InvoiceNo'].astype(str).str.startswith('C')]\n",
    "print(f\"After removing cancellations: {clean_data.shape}\")\n",
    "\n",
    "# Remove negative quantities and prices\n",
    "clean_data = clean_data[(clean_data['Quantity'] > 0) & (clean_data['UnitPrice'] > 0)]\n",
    "print(f\"After removing negative values: {clean_data.shape}\")\n",
    "\n",
    "# Calculate total amount for each transaction\n",
    "clean_data['TotalAmount'] = clean_data['Quantity'] * clean_data['UnitPrice']\n",
    "\n",
    "# Remove extreme outliers (transactions > 99.9th percentile)\n",
    "threshold = clean_data['TotalAmount'].quantile(0.999)\n",
    "clean_data = clean_data[clean_data['TotalAmount'] <= threshold]\n",
    "print(f\"After removing extreme outliers: {clean_data.shape}\")\n",
    "\n",
    "print(f\"\\nFinal cleaned data shape: {clean_data.shape}\")\n",
    "print(f\"Data retained: {(len(clean_data) / len(retail_data)) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify cleaned data\n",
    "print(\"Cleaned data summary:\")\n",
    "print(f\"Number of unique customers: {clean_data['CustomerID'].nunique()}\")\n",
    "print(f\"Number of transactions: {len(clean_data)}\")\n",
    "print(f\"Date range: {clean_data['InvoiceDate'].min()} to {clean_data['InvoiceDate'].max()}\")\n",
    "print(f\"Total revenue: £{clean_data['TotalAmount'].sum():,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RFM Analysis\n",
    "\n",
    "**RFM Analysis** segments customers based on:\n",
    "- **Recency (R)**: Days since last purchase\n",
    "- **Frequency (F)**: Total number of purchases\n",
    "- **Monetary (M)**: Total amount spent\n",
    "\n",
    "Lower recency is better (more recent), while higher frequency and monetary values are better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define reference date (day after last transaction)\n",
    "reference_date = clean_data['InvoiceDate'].max() + timedelta(days=1)\n",
    "print(f\"Reference date for recency calculation: {reference_date}\")\n",
    "\n",
    "# Calculate RFM metrics for each customer\n",
    "rfm_data = clean_data.groupby('CustomerID').agg({\n",
    "    'InvoiceDate': lambda x: (reference_date - x.max()).days,  # Recency\n",
    "    'InvoiceNo': 'nunique',  # Frequency (number of unique invoices)\n",
    "    'TotalAmount': 'sum'  # Monetary\n",
    "}).reset_index()\n",
    "\n",
    "# Rename columns\n",
    "rfm_data.columns = ['CustomerID', 'Recency', 'Frequency', 'Monetary']\n",
    "\n",
    "print(f\"\\nRFM data created for {len(rfm_data)} customers\")\n",
    "print(\"\\nFirst 5 customers:\")\n",
    "rfm_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFM statistics\n",
    "print(\"RFM Statistics:\")\n",
    "print(\"=\" * 50)\n",
    "rfm_data[['Recency', 'Frequency', 'Monetary']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize RFM distributions\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Recency distribution\n",
    "axes[0].hist(rfm_data['Recency'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('Recency (days)')\n",
    "axes[0].set_ylabel('Number of Customers')\n",
    "axes[0].set_title('Recency Distribution')\n",
    "axes[0].axvline(rfm_data['Recency'].median(), color='red', linestyle='--', \n",
    "                label=f\"Median: {rfm_data['Recency'].median():.0f} days\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Frequency distribution (log scale for better visualization)\n",
    "axes[1].hist(rfm_data['Frequency'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1].set_xlabel('Frequency (number of orders)')\n",
    "axes[1].set_ylabel('Number of Customers')\n",
    "axes[1].set_title('Frequency Distribution')\n",
    "axes[1].axvline(rfm_data['Frequency'].median(), color='red', linestyle='--',\n",
    "                label=f\"Median: {rfm_data['Frequency'].median():.0f} orders\")\n",
    "axes[1].legend()\n",
    "\n",
    "# Monetary distribution (log scale)\n",
    "axes[2].hist(np.log1p(rfm_data['Monetary']), bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[2].set_xlabel('Monetary (log scale)')\n",
    "axes[2].set_ylabel('Number of Customers')\n",
    "axes[2].set_title('Monetary Distribution (Log Scale)')\n",
    "axes[2].axvline(np.log1p(rfm_data['Monetary'].median()), color='red', linestyle='--',\n",
    "                label=f\"Median: £{rfm_data['Monetary'].median():.0f}\")\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Note: Monetary is shown on log scale due to high skewness\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: RFM Analysis\n",
    "\n",
    "1. What percentage of customers made only one purchase?\n",
    "2. Calculate the correlation between Frequency and Monetary values\n",
    "3. Identify the top 10 customers by Monetary value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering and Scaling\n",
    "\n",
    "Before clustering, we need to:\n",
    "1. Handle skewness using log transformation\n",
    "2. Standardize features to have mean=0 and std=1\n",
    "\n",
    "This ensures all features contribute equally to distance calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log transformation to reduce skewness\n",
    "# Adding 1 to handle zero values (log1p)\n",
    "rfm_log = rfm_data.copy()\n",
    "rfm_log['Recency_log'] = np.log1p(rfm_log['Recency'])\n",
    "rfm_log['Frequency_log'] = np.log1p(rfm_log['Frequency'])\n",
    "rfm_log['Monetary_log'] = np.log1p(rfm_log['Monetary'])\n",
    "\n",
    "# Select features for clustering\n",
    "features_for_clustering = ['Recency_log', 'Frequency_log', 'Monetary_log']\n",
    "X = rfm_log[features_for_clustering].values\n",
    "\n",
    "# Standardize features (mean=0, std=1)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(f\"Features shape: {X_scaled.shape}\")\n",
    "print(f\"\\nScaled features statistics:\")\n",
    "print(f\"Mean: {X_scaled.mean(axis=0)}\")\n",
    "print(f\"Std: {X_scaled.std(axis=0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize before and after transformation\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "# Original features\n",
    "for idx, col in enumerate(['Recency', 'Frequency', 'Monetary']):\n",
    "    axes[0, idx].hist(rfm_data[col], bins=50, edgecolor='black', alpha=0.7)\n",
    "    axes[0, idx].set_title(f'{col} (Original)')\n",
    "    axes[0, idx].set_ylabel('Count')\n",
    "\n",
    "# Scaled features\n",
    "for idx, col in enumerate(features_for_clustering):\n",
    "    axes[1, idx].hist(X_scaled[:, idx], bins=50, edgecolor='black', alpha=0.7)\n",
    "    axes[1, idx].set_title(f'{col} (Scaled)')\n",
    "    axes[1, idx].set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Transformation reduces skewness and standardizes scale\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Determining Optimal Number of Clusters\n",
    "\n",
    "We'll use two methods:\n",
    "1. **Elbow Method**: Plot WCSS (Within-Cluster Sum of Squares) vs. K\n",
    "2. **Silhouette Analysis**: Measure cluster cohesion and separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow Method - Calculate WCSS for different values of K\n",
    "wcss_values = []\n",
    "silhouette_scores = []\n",
    "K_range = range(2, 11)\n",
    "\n",
    "print(\"Calculating metrics for different K values...\")\n",
    "for k in K_range:\n",
    "    # Fit K-Means\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_scaled)\n",
    "    \n",
    "    # Calculate WCSS (inertia)\n",
    "    wcss_values.append(kmeans.inertia_)\n",
    "    \n",
    "    # Calculate silhouette score\n",
    "    silhouette_avg = silhouette_score(X_scaled, kmeans.labels_)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "    \n",
    "    print(f\"K={k}: WCSS={kmeans.inertia_:.2f}, Silhouette={silhouette_avg:.3f}\")\n",
    "\n",
    "print(\"\\nCalculations complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Elbow Curve and Silhouette Scores\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Elbow curve\n",
    "axes[0].plot(K_range, wcss_values, marker='o', linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('Number of Clusters (K)')\n",
    "axes[0].set_ylabel('WCSS (Within-Cluster Sum of Squares)')\n",
    "axes[0].set_title('Elbow Method for Optimal K')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_xticks(K_range)\n",
    "\n",
    "# Silhouette scores\n",
    "axes[1].plot(K_range, silhouette_scores, marker='s', linewidth=2, markersize=8, color='green')\n",
    "axes[1].set_xlabel('Number of Clusters (K)')\n",
    "axes[1].set_ylabel('Silhouette Score')\n",
    "axes[1].set_title('Silhouette Score for Different K')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_xticks(K_range)\n",
    "\n",
    "# Highlight optimal K (highest silhouette score)\n",
    "optimal_k = K_range[np.argmax(silhouette_scores)]\n",
    "axes[1].axvline(optimal_k, color='red', linestyle='--', \n",
    "                label=f'Optimal K={optimal_k}')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBased on silhouette score, optimal K = {optimal_k}\")\n",
    "print(f\"Highest silhouette score: {max(silhouette_scores):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Optimal Clusters\n",
    "\n",
    "1. Looking at the elbow curve, where does the \"elbow\" appear to be?\n",
    "2. Calculate the Davies-Bouldin Index for K=4, 5, 6 (lower is better)\n",
    "3. Based on business constraints, would you choose 4, 5, or 6 segments? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: Use davies_bouldin_score from sklearn.metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. K-Means Clustering\n",
    "\n",
    "We'll apply K-Means with our optimal K value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set optimal K (you can adjust based on analysis)\n",
    "optimal_k = 5\n",
    "\n",
    "# Fit K-Means model\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "kmeans_labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Add cluster labels to RFM data\n",
    "rfm_data['KMeans_Cluster'] = kmeans_labels\n",
    "\n",
    "# Calculate clustering metrics\n",
    "silhouette_avg = silhouette_score(X_scaled, kmeans_labels)\n",
    "davies_bouldin = davies_bouldin_score(X_scaled, kmeans_labels)\n",
    "calinski_harabasz = calinski_harabasz_score(X_scaled, kmeans_labels)\n",
    "\n",
    "print(f\"K-Means Clustering with K={optimal_k}\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Silhouette Score: {silhouette_avg:.3f} (higher is better, range: -1 to 1)\")\n",
    "print(f\"Davies-Bouldin Index: {davies_bouldin:.3f} (lower is better)\")\n",
    "print(f\"Calinski-Harabasz Index: {calinski_harabasz:.3f} (higher is better)\")\n",
    "print(\"\\nCluster sizes:\")\n",
    "print(rfm_data['KMeans_Cluster'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cluster distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "cluster_counts = rfm_data['KMeans_Cluster'].value_counts().sort_index()\n",
    "bars = plt.bar(cluster_counts.index, cluster_counts.values, edgecolor='black', alpha=0.7)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{int(height)}\\n({height/len(rfm_data)*100:.1f}%)',\n",
    "             ha='center', va='bottom')\n",
    "\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Number of Customers')\n",
    "plt.title('K-Means Cluster Distribution')\n",
    "plt.xticks(cluster_counts.index)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. DBSCAN Clustering\n",
    "\n",
    "DBSCAN (Density-Based Spatial Clustering) can find arbitrarily shaped clusters and identify outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN requires eps and min_samples parameters\n",
    "# eps: Maximum distance between two samples to be considered neighbors\n",
    "# min_samples: Minimum number of samples in a neighborhood\n",
    "\n",
    "# Try different eps values to find optimal clustering\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "dbscan_labels = dbscan.fit_predict(X_scaled)\n",
    "\n",
    "# Add cluster labels to RFM data\n",
    "rfm_data['DBSCAN_Cluster'] = dbscan_labels\n",
    "\n",
    "# Count clusters (excluding outliers labeled as -1)\n",
    "n_clusters = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\n",
    "n_outliers = list(dbscan_labels).count(-1)\n",
    "\n",
    "print(f\"DBSCAN Clustering Results (eps=0.5, min_samples=5)\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Number of clusters: {n_clusters}\")\n",
    "print(f\"Number of outliers: {n_outliers} ({n_outliers/len(dbscan_labels)*100:.1f}%)\")\n",
    "\n",
    "# Calculate silhouette score (excluding outliers)\n",
    "if n_clusters > 1:\n",
    "    mask = dbscan_labels != -1\n",
    "    if sum(mask) > n_clusters:  # Need enough points for silhouette\n",
    "        silhouette_dbscan = silhouette_score(X_scaled[mask], dbscan_labels[mask])\n",
    "        print(f\"Silhouette Score (excluding outliers): {silhouette_dbscan:.3f}\")\n",
    "\n",
    "print(\"\\nCluster sizes:\")\n",
    "print(rfm_data['DBSCAN_Cluster'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Hierarchical Clustering\n",
    "\n",
    "Hierarchical clustering creates a tree-like structure (dendrogram) showing cluster relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dendrogram (using subset for visualization)\n",
    "# Using all data can be too dense to visualize\n",
    "sample_size = min(1000, len(X_scaled))\n",
    "sample_indices = np.random.choice(len(X_scaled), sample_size, replace=False)\n",
    "X_sample = X_scaled[sample_indices]\n",
    "\n",
    "# Calculate linkage matrix\n",
    "linkage_matrix = linkage(X_sample, method='ward')\n",
    "\n",
    "# Plot dendrogram\n",
    "plt.figure(figsize=(15, 7))\n",
    "dendrogram(linkage_matrix, truncate_mode='lastp', p=30, \n",
    "           leaf_font_size=10, show_contracted=True)\n",
    "plt.xlabel('Sample Index or (Cluster Size)')\n",
    "plt.ylabel('Distance')\n",
    "plt.title(f'Hierarchical Clustering Dendrogram (sample of {sample_size} customers)')\n",
    "plt.axhline(y=10, color='red', linestyle='--', label='Potential cut height')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Dendrogram shows hierarchical cluster structure\")\n",
    "print(\"Red line suggests a potential cut height for cluster formation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Agglomerative Clustering with optimal K\n",
    "hierarchical = AgglomerativeClustering(n_clusters=optimal_k, linkage='ward')\n",
    "hierarchical_labels = hierarchical.fit_predict(X_scaled)\n",
    "\n",
    "# Add cluster labels to RFM data\n",
    "rfm_data['Hierarchical_Cluster'] = hierarchical_labels\n",
    "\n",
    "# Calculate metrics\n",
    "silhouette_hier = silhouette_score(X_scaled, hierarchical_labels)\n",
    "davies_bouldin_hier = davies_bouldin_score(X_scaled, hierarchical_labels)\n",
    "\n",
    "print(f\"Hierarchical Clustering with K={optimal_k}\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Silhouette Score: {silhouette_hier:.3f}\")\n",
    "print(f\"Davies-Bouldin Index: {davies_bouldin_hier:.3f}\")\n",
    "print(\"\\nCluster sizes:\")\n",
    "print(rfm_data['Hierarchical_Cluster'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Gaussian Mixture Model (GMM)\n",
    "\n",
    "GMM provides probabilistic (soft) clustering where each customer has a probability of belonging to each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Gaussian Mixture Model\n",
    "gmm = GaussianMixture(n_components=optimal_k, random_state=42, n_init=10)\n",
    "gmm.fit(X_scaled)\n",
    "gmm_labels = gmm.predict(X_scaled)\n",
    "\n",
    "# Get probability of belonging to each cluster\n",
    "gmm_proba = gmm.predict_proba(X_scaled)\n",
    "\n",
    "# Add cluster labels to RFM data\n",
    "rfm_data['GMM_Cluster'] = gmm_labels\n",
    "\n",
    "# Calculate metrics\n",
    "silhouette_gmm = silhouette_score(X_scaled, gmm_labels)\n",
    "davies_bouldin_gmm = davies_bouldin_score(X_scaled, gmm_labels)\n",
    "\n",
    "print(f\"Gaussian Mixture Model with K={optimal_k}\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Silhouette Score: {silhouette_gmm:.3f}\")\n",
    "print(f\"Davies-Bouldin Index: {davies_bouldin_gmm:.3f}\")\n",
    "print(f\"BIC Score: {gmm.bic(X_scaled):.2f} (lower is better)\")\n",
    "print(f\"AIC Score: {gmm.aic(X_scaled):.2f} (lower is better)\")\n",
    "print(\"\\nCluster sizes:\")\n",
    "print(rfm_data['GMM_Cluster'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine cluster assignment confidence\n",
    "max_probabilities = gmm_proba.max(axis=1)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(max_probabilities, bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Maximum Cluster Probability')\n",
    "plt.ylabel('Number of Customers')\n",
    "plt.title('GMM Cluster Assignment Confidence')\n",
    "plt.axvline(max_probabilities.mean(), color='red', linestyle='--',\n",
    "            label=f'Mean: {max_probabilities.mean():.3f}')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average cluster assignment confidence: {max_probabilities.mean():.3f}\")\n",
    "print(f\"Customers with >90% confidence: {(max_probabilities > 0.9).sum()} \"\n",
    "      f\"({(max_probabilities > 0.9).sum()/len(max_probabilities)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Comparing Clustering Methods\n",
    "\n",
    "1. Create a comparison table of silhouette scores for all methods\n",
    "2. Which method produces the most balanced cluster sizes?\n",
    "3. For which business scenarios would you prefer GMM over K-Means?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Dimensionality Reduction for Visualization\n",
    "\n",
    "We'll use PCA and t-SNE to visualize our 3D clusters in 2D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA - Linear dimensionality reduction\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "print(\"PCA Results:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "print(f\"Total variance explained: {pca.explained_variance_ratio_.sum():.2%}\")\n",
    "print(f\"\\nPCA components shape: {X_pca.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE - Non-linear dimensionality reduction\n",
    "# Note: t-SNE can be slow for large datasets\n",
    "print(\"Running t-SNE (this may take a minute for large datasets)...\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
    "X_tsne = tsne.fit_transform(X_scaled)\n",
    "print(f\"t-SNE components shape: {X_tsne.shape}\")\n",
    "print(\"t-SNE complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize K-Means clusters using PCA and t-SNE\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# PCA visualization\n",
    "scatter1 = axes[0].scatter(X_pca[:, 0], X_pca[:, 1], \n",
    "                          c=rfm_data['KMeans_Cluster'], \n",
    "                          cmap='viridis', alpha=0.6, s=50)\n",
    "axes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "axes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "axes[0].set_title('K-Means Clusters (PCA Projection)')\n",
    "plt.colorbar(scatter1, ax=axes[0], label='Cluster')\n",
    "\n",
    "# t-SNE visualization\n",
    "scatter2 = axes[1].scatter(X_tsne[:, 0], X_tsne[:, 1], \n",
    "                          c=rfm_data['KMeans_Cluster'], \n",
    "                          cmap='viridis', alpha=0.6, s=50)\n",
    "axes[1].set_xlabel('t-SNE Component 1')\n",
    "axes[1].set_ylabel('t-SNE Component 2')\n",
    "axes[1].set_title('K-Means Clusters (t-SNE Projection)')\n",
    "plt.colorbar(scatter2, ax=axes[1], label='Cluster')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"PCA preserves global structure, t-SNE preserves local structure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Cluster Profiling and Analysis\n",
    "\n",
    "Now let's analyze what characterizes each cluster and give them business-meaningful names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cluster profiles using K-Means results\n",
    "cluster_profile = rfm_data.groupby('KMeans_Cluster').agg({\n",
    "    'Recency': ['mean', 'median'],\n",
    "    'Frequency': ['mean', 'median'],\n",
    "    'Monetary': ['mean', 'median', 'sum'],\n",
    "    'CustomerID': 'count'\n",
    "}).round(2)\n",
    "\n",
    "# Flatten column names\n",
    "cluster_profile.columns = ['_'.join(col).strip() for col in cluster_profile.columns.values]\n",
    "cluster_profile = cluster_profile.rename(columns={'CustomerID_count': 'Customer_Count'})\n",
    "\n",
    "# Add percentage of total customers\n",
    "cluster_profile['Percentage'] = (cluster_profile['Customer_Count'] / \n",
    "                                 cluster_profile['Customer_Count'].sum() * 100).round(1)\n",
    "\n",
    "# Add revenue percentage\n",
    "cluster_profile['Revenue_Percentage'] = (cluster_profile['Monetary_sum'] / \n",
    "                                         cluster_profile['Monetary_sum'].sum() * 100).round(1)\n",
    "\n",
    "print(\"Cluster Profile Summary:\")\n",
    "print(\"=\" * 100)\n",
    "cluster_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cluster characteristics\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "\n",
    "clusters = sorted(rfm_data['KMeans_Cluster'].unique())\n",
    "\n",
    "# Recency by cluster\n",
    "recency_means = [rfm_data[rfm_data['KMeans_Cluster']==c]['Recency'].mean() for c in clusters]\n",
    "axes[0, 0].bar(clusters, recency_means, edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Cluster')\n",
    "axes[0, 0].set_ylabel('Average Recency (days)')\n",
    "axes[0, 0].set_title('Average Recency by Cluster')\n",
    "axes[0, 0].set_xticks(clusters)\n",
    "\n",
    "# Frequency by cluster\n",
    "frequency_means = [rfm_data[rfm_data['KMeans_Cluster']==c]['Frequency'].mean() for c in clusters]\n",
    "axes[0, 1].bar(clusters, frequency_means, edgecolor='black', alpha=0.7, color='orange')\n",
    "axes[0, 1].set_xlabel('Cluster')\n",
    "axes[0, 1].set_ylabel('Average Frequency (orders)')\n",
    "axes[0, 1].set_title('Average Frequency by Cluster')\n",
    "axes[0, 1].set_xticks(clusters)\n",
    "\n",
    "# Monetary by cluster\n",
    "monetary_means = [rfm_data[rfm_data['KMeans_Cluster']==c]['Monetary'].mean() for c in clusters]\n",
    "axes[0, 2].bar(clusters, monetary_means, edgecolor='black', alpha=0.7, color='green')\n",
    "axes[0, 2].set_xlabel('Cluster')\n",
    "axes[0, 2].set_ylabel('Average Monetary (£)')\n",
    "axes[0, 2].set_title('Average Monetary Value by Cluster')\n",
    "axes[0, 2].set_xticks(clusters)\n",
    "\n",
    "# Customer count by cluster\n",
    "customer_counts = rfm_data['KMeans_Cluster'].value_counts().sort_index()\n",
    "axes[1, 0].bar(customer_counts.index, customer_counts.values, edgecolor='black', alpha=0.7, color='red')\n",
    "axes[1, 0].set_xlabel('Cluster')\n",
    "axes[1, 0].set_ylabel('Number of Customers')\n",
    "axes[1, 0].set_title('Customer Count by Cluster')\n",
    "axes[1, 0].set_xticks(clusters)\n",
    "\n",
    "# Revenue contribution by cluster\n",
    "revenue_by_cluster = rfm_data.groupby('KMeans_Cluster')['Monetary'].sum().sort_index()\n",
    "axes[1, 1].bar(revenue_by_cluster.index, revenue_by_cluster.values, \n",
    "               edgecolor='black', alpha=0.7, color='purple')\n",
    "axes[1, 1].set_xlabel('Cluster')\n",
    "axes[1, 1].set_ylabel('Total Revenue (£)')\n",
    "axes[1, 1].set_title('Total Revenue by Cluster')\n",
    "axes[1, 1].set_xticks(clusters)\n",
    "\n",
    "# Revenue percentage pie chart\n",
    "axes[1, 2].pie(revenue_by_cluster.values, labels=[f'Cluster {i}' for i in clusters],\n",
    "               autopct='%1.1f%%', startangle=90)\n",
    "axes[1, 2].set_title('Revenue Distribution Across Clusters')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign business-meaningful names to clusters\n",
    "# Based on RFM characteristics, we'll name each cluster\n",
    "\n",
    "# Calculate normalized scores for easier interpretation\n",
    "# Lower recency is better, so we invert it\n",
    "cluster_summary = rfm_data.groupby('KMeans_Cluster').agg({\n",
    "    'Recency': 'mean',\n",
    "    'Frequency': 'mean',\n",
    "    'Monetary': 'mean',\n",
    "    'CustomerID': 'count'\n",
    "})\n",
    "\n",
    "# Create scoring system (higher is better)\n",
    "cluster_summary['Recency_Score'] = 1 / (cluster_summary['Recency'] + 1)  # Inverse for recency\n",
    "cluster_summary['Frequency_Score'] = cluster_summary['Frequency']\n",
    "cluster_summary['Monetary_Score'] = cluster_summary['Monetary']\n",
    "\n",
    "# Normalize scores to 0-1 range\n",
    "for col in ['Recency_Score', 'Frequency_Score', 'Monetary_Score']:\n",
    "    min_val = cluster_summary[col].min()\n",
    "    max_val = cluster_summary[col].max()\n",
    "    cluster_summary[f'{col}_Norm'] = (cluster_summary[col] - min_val) / (max_val - min_val)\n",
    "\n",
    "print(\"Normalized Cluster Scores (0=Worst, 1=Best):\")\n",
    "print(\"=\" * 70)\n",
    "print(cluster_summary[['Recency_Score_Norm', 'Frequency_Score_Norm', 'Monetary_Score_Norm']])\n",
    "\n",
    "# Define cluster names based on characteristics\n",
    "# You may need to adjust these based on your actual data\n",
    "cluster_names = {}\n",
    "for cluster_id in cluster_summary.index:\n",
    "    r_score = cluster_summary.loc[cluster_id, 'Recency_Score_Norm']\n",
    "    f_score = cluster_summary.loc[cluster_id, 'Frequency_Score_Norm']\n",
    "    m_score = cluster_summary.loc[cluster_id, 'Monetary_Score_Norm']\n",
    "    \n",
    "    # Classification logic\n",
    "    if r_score > 0.7 and f_score > 0.7 and m_score > 0.7:\n",
    "        cluster_names[cluster_id] = \"Champions\"\n",
    "    elif r_score > 0.6 and f_score > 0.5:\n",
    "        cluster_names[cluster_id] = \"Loyal Customers\"\n",
    "    elif m_score > 0.7:\n",
    "        cluster_names[cluster_id] = \"Big Spenders\"\n",
    "    elif r_score < 0.3 and f_score < 0.3:\n",
    "        cluster_names[cluster_id] = \"Lost Customers\"\n",
    "    elif r_score < 0.4:\n",
    "        cluster_names[cluster_id] = \"At Risk\"\n",
    "    else:\n",
    "        cluster_names[cluster_id] = \"Potential Loyalists\"\n",
    "\n",
    "# Apply names to dataframe\n",
    "rfm_data['Segment_Name'] = rfm_data['KMeans_Cluster'].map(cluster_names)\n",
    "\n",
    "print(\"\\nCluster Names:\")\n",
    "for cluster_id, name in cluster_names.items():\n",
    "    print(f\"Cluster {cluster_id}: {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Business Recommendations by Segment\n",
    "\n",
    "Based on our cluster analysis, let's develop targeted strategies for each customer segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive segment profiles\n",
    "segment_profiles = rfm_data.groupby('Segment_Name').agg({\n",
    "    'CustomerID': 'count',\n",
    "    'Recency': ['mean', 'median'],\n",
    "    'Frequency': ['mean', 'median'],\n",
    "    'Monetary': ['mean', 'median', 'sum']\n",
    "}).round(2)\n",
    "\n",
    "segment_profiles.columns = ['_'.join(col).strip() for col in segment_profiles.columns.values]\n",
    "segment_profiles = segment_profiles.rename(columns={'CustomerID_count': 'Customer_Count'})\n",
    "\n",
    "# Calculate percentages\n",
    "segment_profiles['Pct_Customers'] = (\n",
    "    segment_profiles['Customer_Count'] / segment_profiles['Customer_Count'].sum() * 100\n",
    ").round(1)\n",
    "\n",
    "segment_profiles['Pct_Revenue'] = (\n",
    "    segment_profiles['Monetary_sum'] / segment_profiles['Monetary_sum'].sum() * 100\n",
    ").round(1)\n",
    "\n",
    "# Calculate average customer lifetime value per segment\n",
    "segment_profiles['Avg_CLV'] = segment_profiles['Monetary_mean']\n",
    "\n",
    "print(\"Detailed Segment Profiles:\")\n",
    "print(\"=\" * 100)\n",
    "segment_profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define marketing strategies for each segment\n",
    "marketing_strategies = {\n",
    "    \"Champions\": {\n",
    "        \"Priority\": \"HIGH\",\n",
    "        \"Objective\": \"Retention and Advocacy\",\n",
    "        \"Strategies\": [\n",
    "            \"Exclusive VIP rewards and early access to new products\",\n",
    "            \"Personalized thank-you messages and recognition\",\n",
    "            \"Referral program incentives\",\n",
    "            \"Request reviews and testimonials\",\n",
    "            \"Upsell premium products and services\"\n",
    "        ],\n",
    "        \"Budget_Allocation\": \"25-30%\",\n",
    "        \"Expected_ROI\": \"High\"\n",
    "    },\n",
    "    \"Loyal Customers\": {\n",
    "        \"Priority\": \"HIGH\",\n",
    "        \"Objective\": \"Increase spending and frequency\",\n",
    "        \"Strategies\": [\n",
    "            \"Loyalty points program\",\n",
    "            \"Cross-sell complementary products\",\n",
    "            \"Exclusive member-only discounts\",\n",
    "            \"Educational content about products\",\n",
    "            \"Subscription or bundle offers\"\n",
    "        ],\n",
    "        \"Budget_Allocation\": \"20-25%\",\n",
    "        \"Expected_ROI\": \"High\"\n",
    "    },\n",
    "    \"Big Spenders\": {\n",
    "        \"Priority\": \"HIGH\",\n",
    "        \"Objective\": \"Increase frequency of purchases\",\n",
    "        \"Strategies\": [\n",
    "            \"Personalized product recommendations\",\n",
    "            \"Time-limited premium offers\",\n",
    "            \"Concierge-style customer service\",\n",
    "            \"Bundle deals on high-value items\",\n",
    "            \"Free shipping and premium delivery options\"\n",
    "        ],\n",
    "        \"Budget_Allocation\": \"15-20%\",\n",
    "        \"Expected_ROI\": \"Medium-High\"\n",
    "    },\n",
    "    \"Potential Loyalists\": {\n",
    "        \"Priority\": \"MEDIUM\",\n",
    "        \"Objective\": \"Convert to loyal customers\",\n",
    "        \"Strategies\": [\n",
    "            \"Onboarding email series\",\n",
    "            \"First-purchase discount for next order\",\n",
    "            \"Product education and tutorials\",\n",
    "            \"Engagement campaigns (contests, surveys)\",\n",
    "            \"Introduce loyalty program benefits\"\n",
    "        ],\n",
    "        \"Budget_Allocation\": \"15-20%\",\n",
    "        \"Expected_ROI\": \"Medium\"\n",
    "    },\n",
    "    \"At Risk\": {\n",
    "        \"Priority\": \"MEDIUM-HIGH\",\n",
    "        \"Objective\": \"Re-engagement and retention\",\n",
    "        \"Strategies\": [\n",
    "            \"Win-back email campaigns\",\n",
    "            \"Special 'we miss you' discounts (15-20%)\",\n",
    "            \"Survey to understand why they left\",\n",
    "            \"Highlight new products or improvements\",\n",
    "            \"Limited-time reactivation offers\"\n",
    "        ],\n",
    "        \"Budget_Allocation\": \"10-15%\",\n",
    "        \"Expected_ROI\": \"Medium\"\n",
    "    },\n",
    "    \"Lost Customers\": {\n",
    "        \"Priority\": \"LOW\",\n",
    "        \"Objective\": \"Cost-effective reactivation\",\n",
    "        \"Strategies\": [\n",
    "            \"Automated low-cost email campaigns\",\n",
    "            \"Aggressive discounts (25-30%) if high historical value\",\n",
    "            \"Survey for feedback and improvements\",\n",
    "            \"Remarketing ads with special offers\",\n",
    "            \"Consider removing from active lists if no response\"\n",
    "        ],\n",
    "        \"Budget_Allocation\": \"5-10%\",\n",
    "        \"Expected_ROI\": \"Low-Medium\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Display strategies\n",
    "for segment, strategy in marketing_strategies.items():\n",
    "    if segment in segment_profiles.index:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"SEGMENT: {segment}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Priority: {strategy['Priority']}\")\n",
    "        print(f\"Objective: {strategy['Objective']}\")\n",
    "        print(f\"Budget Allocation: {strategy['Budget_Allocation']} of marketing budget\")\n",
    "        print(f\"Expected ROI: {strategy['Expected_ROI']}\")\n",
    "        print(f\"\\nCustomers: {segment_profiles.loc[segment, 'Customer_Count']:.0f} \"\n",
    "              f\"({segment_profiles.loc[segment, 'Pct_Customers']:.1f}% of total)\")\n",
    "        print(f\"Revenue: £{segment_profiles.loc[segment, 'Monetary_sum']:,.2f} \"\n",
    "              f\"({segment_profiles.loc[segment, 'Pct_Revenue']:.1f}% of total)\")\n",
    "        print(f\"\\nRecommended Strategies:\")\n",
    "        for i, strat in enumerate(strategy['Strategies'], 1):\n",
    "            print(f\"  {i}. {strat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Marketing Strategy Development\n",
    "\n",
    "1. Calculate the total marketing budget allocation percentage (should sum to ~100%)\n",
    "2. If you have a $100,000 marketing budget, how much would you allocate to each segment?\n",
    "3. Which segment offers the best balance of size and revenue potential?\n",
    "4. Design a specific email campaign for the \"At Risk\" segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Key Insights and Action Items\n",
    "\n",
    "Let's summarize the most important findings and next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate executive summary\n",
    "print(\"CUSTOMER SEGMENTATION - EXECUTIVE SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nAnalysis Date: {datetime.now().strftime('%Y-%m-%d')}\")\n",
    "print(f\"Total Customers Analyzed: {len(rfm_data):,}\")\n",
    "print(f\"Total Revenue: £{rfm_data['Monetary'].sum():,.2f}\")\n",
    "print(f\"Average Customer Value: £{rfm_data['Monetary'].mean():,.2f}\")\n",
    "print(f\"\\nNumber of Segments Identified: {optimal_k}\")\n",
    "print(f\"Clustering Method: K-Means\")\n",
    "print(f\"Silhouette Score: {silhouette_avg:.3f}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SEGMENT BREAKDOWN\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "for segment in segment_profiles.index:\n",
    "    count = segment_profiles.loc[segment, 'Customer_Count']\n",
    "    pct = segment_profiles.loc[segment, 'Pct_Customers']\n",
    "    revenue = segment_profiles.loc[segment, 'Monetary_sum']\n",
    "    rev_pct = segment_profiles.loc[segment, 'Pct_Revenue']\n",
    "    avg_value = segment_profiles.loc[segment, 'Monetary_mean']\n",
    "    \n",
    "    print(f\"{segment}:\")\n",
    "    print(f\"  - Customers: {count:.0f} ({pct:.1f}%)\")\n",
    "    print(f\"  - Revenue: £{revenue:,.2f} ({rev_pct:.1f}%)\")\n",
    "    print(f\"  - Avg Customer Value: £{avg_value:,.2f}\")\n",
    "    print()\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(\"TOP 3 PRIORITY ACTIONS\")\n",
    "print(f\"{'='*80}\")\n",
    "print(\"1. PROTECT HIGH-VALUE SEGMENTS\")\n",
    "print(\"   → Launch VIP program for Champions and Big Spenders\")\n",
    "print(\"   → Implement personalized communication strategy\")\n",
    "print(\"\\n2. REDUCE CHURN IN AT-RISK SEGMENT\")\n",
    "print(\"   → Immediate win-back campaign with special offers\")\n",
    "print(\"   → Conduct customer satisfaction surveys\")\n",
    "print(\"\\n3. GROW POTENTIAL LOYALISTS\")\n",
    "print(\"   → Onboarding program to increase engagement\")\n",
    "print(\"   → Introduce to loyalty rewards program\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"RECOMMENDED NEXT STEPS\")\n",
    "print(f\"{'='*80}\")\n",
    "print(\"□ Present findings to marketing team\")\n",
    "print(\"□ Develop segment-specific email templates\")\n",
    "print(\"□ Set up automated customer journey triggers based on segments\")\n",
    "print(\"□ Create dashboard to monitor segment migration\")\n",
    "print(\"□ A/B test different strategies within each segment\")\n",
    "print(\"□ Re-run segmentation quarterly to track changes\")\n",
    "print(\"□ Integrate segments into CRM system\")\n",
    "print(\"□ Train customer service team on segment characteristics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Summary and Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "In this comprehensive customer segmentation project, we:\n",
    "\n",
    "1. **Performed RFM Analysis**: Calculated Recency, Frequency, and Monetary metrics for each customer\n",
    "2. **Applied Multiple Clustering Algorithms**: \n",
    "   - K-Means for efficient partitioning\n",
    "   - DBSCAN for density-based clustering\n",
    "   - Hierarchical clustering to understand segment relationships\n",
    "   - GMM for probabilistic clustering\n",
    "3. **Determined Optimal K**: Used elbow method and silhouette analysis\n",
    "4. **Visualized Clusters**: Applied PCA and t-SNE for 2D visualization\n",
    "5. **Created Business Segments**: Translated clusters into actionable customer segments\n",
    "6. **Developed Marketing Strategies**: Designed targeted campaigns for each segment\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- **RFM Analysis**: A proven framework for customer value assessment\n",
    "- **Feature Scaling**: Critical for distance-based algorithms\n",
    "- **Cluster Validation**: Multiple metrics provide different perspectives on quality\n",
    "- **Dimensionality Reduction**: PCA and t-SNE serve different visualization purposes\n",
    "- **Business Translation**: Technical clusters must map to actionable segments\n",
    "\n",
    "### Practical Applications\n",
    "\n",
    "Customer segmentation enables:\n",
    "- **Personalized Marketing**: Tailor messages to segment characteristics\n",
    "- **Resource Optimization**: Focus budget on high-value/high-potential segments\n",
    "- **Churn Prevention**: Identify and act on at-risk customers\n",
    "- **Product Development**: Understand needs of different customer types\n",
    "- **Pricing Strategy**: Segment-based pricing and promotions\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "To extend this analysis:\n",
    "1. **Predictive Modeling**: Build classifier to assign new customers to segments\n",
    "2. **Temporal Analysis**: Track how customers move between segments over time\n",
    "3. **Advanced Features**: Include product categories, channel preferences, geography\n",
    "4. **A/B Testing**: Measure effectiveness of segment-specific strategies\n",
    "5. **Customer Lifetime Value**: Predict CLV for each segment\n",
    "6. **Cohort Analysis**: Combine segmentation with cohort tracking\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- [RFM Analysis Guide](https://www.putler.com/rfm-analysis/)\n",
    "- [Scikit-learn Clustering](https://scikit-learn.org/stable/modules/clustering.html)\n",
    "- [Customer Segmentation Best Practices](https://www.optimove.com/resources/learning-center/customer-segmentation)\n",
    "- [Marketing Analytics with Python](https://www.datacamp.com/courses/marketing-analytics-with-python)\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You've completed a comprehensive customer segmentation project. You now have the skills to segment customers, interpret clusters, and develop data-driven marketing strategies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
