{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 07: Literature Review Methodologies\n",
    "\n",
    "**Difficulty**: ‚≠ê‚≠ê (Intermediate)\n",
    "\n",
    "**Estimated Time**: 90 minutes\n",
    "\n",
    "**Prerequisites**: Module 06: Research Question Formulation\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Conduct systematic reviews following PRISMA 2020 guidelines\n",
    "2. Perform scoping reviews using JBI methodology\n",
    "3. Write narrative reviews with SANRA scale evaluation\n",
    "4. Understand meta-analysis basics and when to apply them\n",
    "5. Choose appropriate review type for specific research questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's import the libraries we'll use in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard data science libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import FancyBboxPatch, FancyArrowPatch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration for better visualizations\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úì Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Literature Review Types\n",
    "\n",
    "### The Landscape of Literature Reviews\n",
    "\n",
    "Literature reviews are systematic approaches to summarizing, synthesizing, and critically evaluating existing research on a topic. However, not all reviews are created equal.\n",
    "\n",
    "**Key principle**: The type of review you conduct should match your research question and the state of the evidence base.\n",
    "\n",
    "### Three Main Review Types\n",
    "\n",
    "| Feature | Systematic Review | Scoping Review | Narrative Review |\n",
    "|---------|------------------|-----------------|------------------|\n",
    "| **Purpose** | Answer specific question with high evidence | Map evidence landscape and identify gaps | Explore broad/emerging topics |\n",
    "| **Scope** | Narrow, focused research question (PICO) | Broad exploratory question (PCC) | Flexible, author-determined |\n",
    "| **Rigor** | Highest - standardized, transparent | High - structured methodology | Lower - interpretive approach |\n",
    "| **Time** | 1-3 years | 6-12 months | 2-6 months |\n",
    "| **Search** | Comprehensive (multiple databases) | Structured 3-step strategy | Ad-hoc or expert selection |\n",
    "| **Analysis** | Meta-analysis possible | Thematic synthesis | Integrative synthesis |\n",
    "| **Team** | Large team required | 2-3 researchers | Often single author |\n",
    "| **Registration** | PROSPERO required | Recommended | Not required |\n",
    "\n",
    "### When to Choose Each Type\n",
    "\n",
    "**Choose SYSTEMATIC REVIEW when:**\n",
    "- You have a narrow, well-defined research question\n",
    "- You need the highest level of evidence\n",
    "- Enough high-quality studies exist on the topic\n",
    "- You want to inform policy or clinical practice\n",
    "- You need to quantitatively combine results (meta-analysis)\n",
    "\n",
    "**Choose SCOPING REVIEW when:**\n",
    "- Your research question is broad or exploratory\n",
    "- You want to map what evidence exists\n",
    "- You need to identify evidence gaps and research priorities\n",
    "- The evidence base is heterogeneous or immature\n",
    "- You're planning a future systematic review\n",
    "\n",
    "**Choose NARRATIVE REVIEW when:**\n",
    "- You're exploring an emerging or highly specialized topic\n",
    "- Limited primary research exists\n",
    "- You need critical interpretation and expert judgment\n",
    "- You're synthesizing theory and empirical evidence\n",
    "- You want to provide context and historical perspective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Systematic Reviews: PRISMA 2020\n",
    "\n",
    "### What is PRISMA?\n",
    "\n",
    "**PRISMA** = Preferred Reporting Items for Systematic Reviews and Meta-Analyses\n",
    "\n",
    "The 2020 update (PRISMA 2020) provides:\n",
    "- 27-item checklist for reporting\n",
    "- 8 tables and figures for presenting results\n",
    "- Guidance for transparency and reproducibility\n",
    "- Extensions for specific review types\n",
    "\n",
    "### PICO Framework: Structuring Your Review Question\n",
    "\n",
    "PICO is used to structure systematic review questions:\n",
    "\n",
    "- **P (Population)**: Who is the study about?\n",
    "  - Example: \"Patients with type 2 diabetes aged 40-70\"\n",
    "  \n",
    "- **I (Intervention)**: What is being done?\n",
    "  - Example: \"Long-acting insulin therapy\"\n",
    "  \n",
    "- **C (Comparison)**: What is it compared to?\n",
    "  - Example: \"Standard short-acting insulin regimen\"\n",
    "  \n",
    "- **O (Outcome)**: What is measured/evaluated?\n",
    "  - Example: \"Hemoglobin A1c levels, hypoglycemic episodes, quality of life\"\n",
    "\n",
    "**Complete PICO Question**: \"In patients with type 2 diabetes aged 40-70, how does long-acting insulin therapy compared to standard short-acting insulin regimen affect hemoglobin A1c levels, hypoglycemic episodes, and quality of life?\"\n",
    "\n",
    "### PROSPERO Registration\n",
    "\n",
    "Before starting your systematic review:\n",
    "1. Register your protocol in PROSPERO (CRD at University of York)\n",
    "2. Document your methods and analysis plan BEFORE analyzing data\n",
    "3. Prevents \"research question drift\" (changing questions based on results)\n",
    "4. Ensures transparency and reduces bias\n",
    "\n",
    "### Risk of Bias Assessment\n",
    "\n",
    "After finding studies, assess quality using standardized tools:\n",
    "\n",
    "**For Randomized Controlled Trials: RoB 2**\n",
    "- Bias from randomization process\n",
    "- Bias due to deviations from intended interventions\n",
    "- Bias due to missing outcome data\n",
    "- Bias in measurement of outcome\n",
    "- Bias in selection of reported results\n",
    "\n",
    "**For Non-Randomized Studies: ROBINS-I**\n",
    "- Includes assessment of confounding and selection bias\n",
    "\n",
    "### Certainty of Evidence: GRADE\n",
    "\n",
    "Rate evidence quality across four domains:\n",
    "\n",
    "1. **High**: Further research very unlikely to change confidence in effect\n",
    "2. **Moderate**: Further research likely to impact confidence in effect\n",
    "3. **Low**: Further research very likely to impact confidence\n",
    "4. **Very Low**: Estimate may be substantially different from true effect\n",
    "\n",
    "### The PRISMA Checklist (Key Items)\n",
    "\n",
    "**Protocol and Registration** (Items 1-2)\n",
    "- Protocol registered in PROSPERO\n",
    "- Deviations from protocol documented\n",
    "\n",
    "**Methods** (Items 4-8)\n",
    "- Objectives clearly stated\n",
    "- Eligibility criteria defined\n",
    "- Search strategy comprehensive\n",
    "- Study selection process documented\n",
    "- Data extraction standardized\n",
    "\n",
    "**Results** (Items 13-20)\n",
    "- PRISMA flow diagram showing selection process\n",
    "- Study characteristics reported\n",
    "- Risk of bias assessment for each study\n",
    "- Meta-analysis results (if performed)\n",
    "- Assessment of publication bias\n",
    "\n",
    "**Discussion** (Items 21-27)\n",
    "- Key findings summarized\n",
    "- Limitations discussed\n",
    "- Implications for practice and future research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Scoping Reviews: JBI Methodology\n",
    "\n",
    "### What is a Scoping Review?\n",
    "\n",
    "A scoping review is a rigorous and transparent method for mapping evidence in a topic area, identifying:\n",
    "- What research has been done\n",
    "- What evidence exists\n",
    "- Where gaps exist\n",
    "- Where future research is needed\n",
    "\n",
    "**JBI Framework** (Joanna Briggs Institute) provides structured methodology:\n",
    "\n",
    "### PCC Framework: Structuring Scoping Review Questions\n",
    "\n",
    "Similar to PICO but broader:\n",
    "\n",
    "- **P (Population)**: Who is the review about?\n",
    "  - Example: \"Healthcare professionals in low-income settings\"\n",
    "  \n",
    "- **C (Concept)**: What is the main idea?\n",
    "  - Example: \"Digital health adoption and implementation barriers\"\n",
    "  \n",
    "- **C (Context)**: In what setting/situation?\n",
    "  - Example: \"Primary care clinics in Sub-Saharan Africa\"\n",
    "\n",
    "**Complete PCC Question**: \"What evidence exists regarding digital health adoption barriers among healthcare professionals in primary care clinics in Sub-Saharan Africa?\"\n",
    "\n",
    "### JBI's Five-Stage Methodology\n",
    "\n",
    "**Stage 1: Define the research question**\n",
    "- Use PCC framework\n",
    "- Ensure clarity and scope appropriateness\n",
    "- Consider what answer will inform\n",
    "\n",
    "**Stage 2: Develop a search strategy**\n",
    "- 3-step search process:\n",
    "  1. Initial search (limited sources) to identify keywords/phrases\n",
    "  2. Second search (multiple databases) with refined terms\n",
    "  3. Citation searching in selected articles\n",
    "\n",
    "**Stage 3: Study selection and data extraction**\n",
    "- Clearly defined inclusion/exclusion criteria\n",
    "- Usually single reviewer (vs systematic reviews' dual review)\n",
    "- Standardized data extraction tool\n",
    "- Does NOT assess methodological quality\n",
    "\n",
    "**Stage 4: Chart the data**\n",
    "- Present characteristics of included studies\n",
    "- Extract meaningful data to answer research question\n",
    "- Create evidence mapping tables\n",
    "\n",
    "**Stage 5: Collate, summarize, and report**\n",
    "- Thematic analysis of findings\n",
    "- Descriptive summaries\n",
    "- Identify gaps and implications\n",
    "- No meta-analysis (unlike systematic reviews)\n",
    "\n",
    "### Scoping vs. Systematic Reviews\n",
    "\n",
    "Key differences:\n",
    "\n",
    "| Aspect | Scoping | Systematic |\n",
    "|--------|---------|------------|\n",
    "| Research question | Broader | Narrow, focused |\n",
    "| Quality assessment | No | Yes (Risk of Bias) |\n",
    "| Meta-analysis | No | Possible |\n",
    "| Inclusion criteria | Flexible | Strict |\n",
    "| Reviewer team | 1-2 | 2+ (dual screening) |\n",
    "| Timeline | 6-12 months | 1-3 years |\n",
    "| Goal | Map evidence landscape | Answer specific question |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Narrative Reviews: SANRA Scale\n",
    "\n",
    "### What is a Narrative Review?\n",
    "\n",
    "A narrative review uses:\n",
    "- **Interpretive synthesis** rather than systematic synthesis\n",
    "- **Expert judgment** to integrate findings\n",
    "- **Critical evaluation** of evidence quality\n",
    "- **Flexible approach** to literature selection\n",
    "\n",
    "**Best for:**\n",
    "- Emerging topics with limited evidence\n",
    "- Highly specialized domains\n",
    "- Theoretical synthesis and framework development\n",
    "- Historical perspectives and context-setting\n",
    "\n",
    "### SANRA Scale: Evaluating Narrative Review Quality\n",
    "\n",
    "The **SANRA scale** (6 dimensions) helps assess narrative review quality:\n",
    "\n",
    "**S - Search Strategy** (0-2 points)\n",
    "- 0: Not mentioned or unclear\n",
    "- 1: Only limited search strategy (one database, no time limit)\n",
    "- 2: Comprehensively described search in multiple databases, defined period\n",
    "\n",
    "**A - Author Expertise** (0-2 points)\n",
    "- 0: Not apparent\n",
    "- 1: Some evidence of expertise\n",
    "- 2: Clear evidence of topic expertise and research record\n",
    "\n",
    "**N - Number of References** (0-2 points)\n",
    "- 0: <25 references\n",
    "- 1: 25-49 references\n",
    "- 2: 50+ references\n",
    "\n",
    "**R - Recency of References** (0-2 points)\n",
    "- 0: <50% published in last 10 years\n",
    "- 1: 50-80% published in last 10 years\n",
    "- 2: >80% published in last 10 years\n",
    "\n",
    "**A - Assessment of Evidence** (0-2 points)\n",
    "- 0: Not mentioned\n",
    "- 1: Limited discussion of evidence quality\n",
    "- 2: Explicit discussion of evidence quality, critical appraisal\n",
    "\n",
    "**Second A - Author Independence** (0-2 points)\n",
    "- 0: Financial conflicts of interest not addressed\n",
    "- 1: Conflicts of interest mentioned\n",
    "- 2: Clear statement of independence and transparency\n",
    "\n",
    "**Total Score Range: 0-12**\n",
    "- Higher scores indicate higher quality reviews\n",
    "\n",
    "### Narrative Review Best Practices\n",
    "\n",
    "Even though less rigorous than systematic reviews, narrative reviews should:\n",
    "\n",
    "1. **Justify the narrative approach**\n",
    "   - Explain why systematic review wasn't appropriate\n",
    "   - Document the decision-making process\n",
    "\n",
    "2. **Be transparent about search strategy**\n",
    "   - Document which databases searched\n",
    "   - Report search terms used\n",
    "   - Acknowledge limitations in search approach\n",
    "\n",
    "3. **Critically appraise evidence**\n",
    "   - Don't treat all sources as equally valid\n",
    "   - Distinguish between empirical studies, opinion pieces, theory\n",
    "   - Use explicit criteria for evaluating quality\n",
    "\n",
    "4. **Acknowledge limitations and biases**\n",
    "   - Author selection bias\n",
    "   - Publication bias\n",
    "   - Author expertise/conflicts\n",
    "   - Emerging vs. established topics\n",
    "\n",
    "5. **Synthesize rather than summarize**\n",
    "   - Integrate findings into coherent narrative\n",
    "   - Identify themes and patterns\n",
    "   - Show relationships between studies\n",
    "   - Develop conceptual frameworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Practical Implementation: Literature Search Simulation\n",
    "\n",
    "Let's simulate conducting a literature search and managing the results through different review methodologies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a literature search across multiple databases\n",
    "\n",
    "# Create a simulated database of studies\n",
    "# In real research, these would come from PubMed, Web of Science, etc.\n",
    "\n",
    "studies_data = {\n",
    "    'Study ID': [f'S{i:03d}' for i in range(1, 51)],\n",
    "    'Title': [\n",
    "        'Effectiveness of cognitive behavioral therapy in depression',\n",
    "        'Meta-analysis of antidepressant efficacy',\n",
    "        'Mindfulness-based interventions for anxiety',\n",
    "        'Psychotherapy outcomes in community settings',\n",
    "        'Comparison of antidepressants and psychotherapy',\n",
    "        'Long-term effects of cognitive therapy',\n",
    "        'Digital mental health interventions',\n",
    "        'Depression screening in primary care',\n",
    "        'Suicide prevention programs review',\n",
    "        'Pharmacogenomics in psychiatric treatment',\n",
    "        'Exercise as adjunct to depression treatment',\n",
    "        'Childhood depression and family therapy',\n",
    "        'Antidepressant side effects systematic review',\n",
    "        'Depression in geriatric populations',\n",
    "        'Transcranial magnetic stimulation for depression',\n",
    "    ] + ['Sample study title'] * 35,\n",
    "    'Year': np.random.randint(2010, 2024, 50),\n",
    "    'Study Type': np.random.choice(\n",
    "        ['RCT', 'Cohort', 'Case-control', 'Cross-sectional', 'Qualitative', 'Opinion'],\n",
    "        50\n",
    "    ),\n",
    "    'Sample Size': np.random.randint(10, 5000, 50),\n",
    "    'Relevant to PICO': np.random.choice([True, False], 50, p=[0.6, 0.4]),\n",
    "    'Quality Score': np.random.uniform(0, 1, 50)\n",
    "}\n",
    "\n",
    "studies_df = pd.DataFrame(studies_data)\n",
    "\n",
    "print(\"LITERATURE SEARCH RESULTS\")\n",
    "print(f\"Total studies found: {len(studies_df)}\")\n",
    "print(f\"\\nFirst 10 studies:\")\n",
    "print(studies_df.head(10).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the search results for different review types\n",
    "\n",
    "print(\"\\nSTUDY CHARACTERISTICS\")\n",
    "print(f\"\\nStudy Type Distribution:\")\n",
    "print(studies_df['Study Type'].value_counts())\n",
    "\n",
    "print(f\"\\nYearly Publication Distribution:\")\n",
    "print(studies_df['Year'].value_counts().sort_index())\n",
    "\n",
    "print(f\"\\nRelevance to PICO Question:\")\n",
    "relevant_count = studies_df['Relevant to PICO'].sum()\n",
    "print(f\"Relevant studies: {relevant_count} ({100*relevant_count/len(studies_df):.1f}%)\")\n",
    "print(f\"Potentially relevant: {len(studies_df) - relevant_count} ({100*(len(studies_df)-relevant_count)/len(studies_df):.1f}%)\")\n",
    "\n",
    "print(f\"\\nStudy Quality Distribution:\")\n",
    "quality_categories = pd.cut(\n",
    "    studies_df['Quality Score'],\n",
    "    bins=[0, 0.33, 0.67, 1.0],\n",
    "    labels=['Low', 'Moderate', 'High']\n",
    ")\n",
    "print(quality_categories.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRISMA FLOW DIAGRAM\n",
    "# This shows the study selection process for systematic reviews\n",
    "\n",
    "# Define numbers for the flow diagram\n",
    "initial_search = 847  # Results from all databases\n",
    "after_dedup = 650     # After removing duplicates\n",
    "screened_titles = 650 # Title/abstract screening\n",
    "excluded_irrelevant = 580  # Not meeting inclusion criteria\n",
    "full_text_reviewed = 70    # Retrieved for full-text\n",
    "excluded_full_text = 15    # Excluded after full-text review\n",
    "final_included = 55        # Included in meta-analysis\n",
    "\n",
    "# Create PRISMA flow diagram\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.axis('off')\n",
    "\n",
    "# Define box function\n",
    "def add_box(ax, x, y, width, height, text, color='lightblue'):\n",
    "    box = FancyBboxPatch(\n",
    "        (x - width/2, y - height/2), width, height,\n",
    "        boxstyle=\"round,pad=0.1\", \n",
    "        edgecolor='black', \n",
    "        facecolor=color,\n",
    "        linewidth=2\n",
    "    )\n",
    "    ax.add_patch(box)\n",
    "    ax.text(x, y, text, ha='center', va='center', fontsize=10, fontweight='bold', wrap=True)\n",
    "\n",
    "# Define arrow function\n",
    "def add_arrow(ax, x1, y1, x2, y2):\n",
    "    arrow = FancyArrowPatch(\n",
    "        (x1, y1), (x2, y2),\n",
    "        arrowstyle='->', mutation_scale=20, \n",
    "        linewidth=2, color='black'\n",
    "    )\n",
    "    ax.add_patch(arrow)\n",
    "\n",
    "# Build PRISMA flow diagram\n",
    "# Identification phase\n",
    "add_box(ax, 5, 9.5, 3, 0.6, f'Records identified\\n(n={initial_search})', 'lightblue')\n",
    "add_arrow(ax, 5, 9.2, 5, 8.6)\n",
    "\n",
    "add_box(ax, 5, 8.2, 3, 0.6, f'After removing duplicates\\n(n={after_dedup})', 'lightblue')\n",
    "add_arrow(ax, 5, 7.9, 5, 7.2)\n",
    "\n",
    "# Screening phase\n",
    "add_box(ax, 5, 6.8, 3, 0.6, f'Records screened\\n(n={screened_titles})', 'lightgreen')\n",
    "add_box(ax, 1.5, 6.8, 2, 0.6, f'Records excluded\\n(n={excluded_irrelevant})', 'lightyellow')\n",
    "add_arrow(ax, 3.5, 6.8, 2.5, 6.8)\n",
    "add_arrow(ax, 5, 6.5, 5, 5.8)\n",
    "\n",
    "# Full-text assessment phase\n",
    "add_box(ax, 5, 5.4, 3, 0.6, f'Full-text reviewed\\n(n={full_text_reviewed})', 'lightcoral')\n",
    "add_box(ax, 8.5, 5.4, 2, 0.6, f'Excluded with reasons\\n(n={excluded_full_text})', 'lightyellow')\n",
    "add_arrow(ax, 6.5, 5.4, 7.5, 5.4)\n",
    "add_arrow(ax, 5, 5.1, 5, 4.4)\n",
    "\n",
    "# Inclusion phase\n",
    "add_box(ax, 5, 4, 3, 0.6, f'Studies included in review\\n(n={final_included})', 'lightgreen')\n",
    "\n",
    "# Add phase labels\n",
    "ax.text(0.2, 9, 'IDENTIFICATION', fontsize=11, fontweight='bold', style='italic')\n",
    "ax.text(0.2, 7, 'SCREENING', fontsize=11, fontweight='bold', style='italic')\n",
    "ax.text(0.2, 5.5, 'FULL-TEXT\\nASSESSMENT', fontsize=11, fontweight='bold', style='italic')\n",
    "ax.text(0.2, 4, 'INCLUSION', fontsize=11, fontweight='bold', style='italic')\n",
    "\n",
    "plt.title('PRISMA 2020 Flow Diagram: Study Selection Process', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate and display filter rates\n",
    "print(f\"\\nPRISMA FLOW DIAGRAM STATISTICS\")\n",
    "print(f\"=\"*50)\n",
    "print(f\"Initial database search: {initial_search} records\")\n",
    "print(f\"After deduplication: {after_dedup} ({100*after_dedup/initial_search:.1f}% retained)\")\n",
    "print(f\"After title/abstract screening: {screened_titles - excluded_irrelevant} ({100*(screened_titles-excluded_irrelevant)/screened_titles:.1f}% retained)\")\n",
    "print(f\"After full-text review: {final_included} ({100*final_included/(screened_titles-excluded_irrelevant):.1f}% retained)\")\n",
    "print(f\"\\nFinal inclusion rate: {100*final_included/initial_search:.1f}% of initial records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVIDENCE HIERARCHY VISUALIZATION\n",
    "# Shows the hierarchy of evidence quality\n",
    "\n",
    "evidence_types = [\n",
    "    ('Meta-analysis of RCTs', 8, 'darkgreen'),\n",
    "    ('Individual RCTs', 7, 'green'),\n",
    "    ('Cohort Studies', 5, 'yellow'),\n",
    "    ('Case-Control Studies', 4, 'orange'),\n",
    "    ('Cross-Sectional Studies', 3, 'darkorange'),\n",
    "    ('Case Reports/Series', 2, 'red'),\n",
    "    ('Expert Opinion', 1, 'darkred')\n",
    "]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "names = [e[0] for e in evidence_types]\n",
    "levels = [e[1] for e in evidence_types]\n",
    "colors = [e[2] for e in evidence_types]\n",
    "\n",
    "# Create pyramid using barh\n",
    "y_pos = np.arange(len(names))\n",
    "bars = ax.barh(y_pos, levels, color=colors, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "# Customize\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(names, fontsize=11)\n",
    "ax.set_xlabel('Level of Evidence', fontsize=12, fontweight='bold')\n",
    "ax.set_xlim(0, 9)\n",
    "ax.invert_yaxis()  # Strongest at top\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, level) in enumerate(zip(bars, levels)):\n",
    "    ax.text(level + 0.1, bar.get_y() + bar.get_height()/2, \n",
    "            f'Level {level}', va='center', fontweight='bold')\n",
    "\n",
    "ax.set_title('Evidence Hierarchy: Quality Levels Used in Systematic Reviews', \n",
    "             fontsize=13, fontweight='bold', pad=15)\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nEVIDENCE HIERARCHY EXPLANATION\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\nüîù Level 8 (Strongest):\")\n",
    "print(\"   Meta-analysis of randomized controlled trials\")\n",
    "print(\"\\nüü¢ Levels 6-7 (Strong):\")\n",
    "print(\"   Individual randomized controlled trials\")\n",
    "print(\"\\nüü° Levels 3-5 (Moderate):\")\n",
    "print(\"   Observational studies (cohort, case-control, cross-sectional)\")\n",
    "print(\"\\nüî¥ Levels 1-2 (Weak):\")\n",
    "print(\"   Case reports and expert opinion\")\n",
    "print(\"\\nNote: Evidence quality depends on study design AND execution\")\n",
    "print(\"A well-conducted cohort study > a poorly-done RCT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Risk of Bias Assessment Tool\n",
    "\n",
    "For systematic reviews, we assess the quality of each study using standardized tools. Let's create a simple bias assessment tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a risk of bias assessment tool for RCTs\n",
    "\n",
    "class RobAssessmentTool:\n",
    "    \"\"\"\n",
    "    Simplified Risk of Bias (RoB 2) assessment for RCTs.\n",
    "    Evaluates five domains of bias.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, study_id, study_name):\n",
    "        self.study_id = study_id\n",
    "        self.study_name = study_name\n",
    "        self.domains = [\n",
    "            'Randomization Process',\n",
    "            'Deviations from Intervention',\n",
    "            'Missing Outcome Data',\n",
    "            'Outcome Measurement',\n",
    "            'Selective Reporting'\n",
    "        ]\n",
    "        self.assessments = {}\n",
    "    \n",
    "    def assess_domain(self, domain, risk_level, notes=''):\n",
    "        \"\"\"\n",
    "        Assess risk of bias for a domain.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        domain : str\n",
    "            The domain to assess\n",
    "        risk_level : str\n",
    "            'Low', 'Some concerns', or 'High'\n",
    "        notes : str\n",
    "            Explanation for the assessment\n",
    "        \"\"\"\n",
    "        if domain not in self.domains:\n",
    "            raise ValueError(f\"Domain must be one of {self.domains}\")\n",
    "        \n",
    "        if risk_level not in ['Low', 'Some concerns', 'High']:\n",
    "            raise ValueError(\"Risk level must be 'Low', 'Some concerns', or 'High'\")\n",
    "        \n",
    "        self.assessments[domain] = {\n",
    "            'risk': risk_level,\n",
    "            'notes': notes\n",
    "        }\n",
    "    \n",
    "    def overall_risk(self):\n",
    "        \"\"\"\n",
    "        Determine overall risk of bias.\n",
    "        High in any domain ‚Üí High overall\n",
    "        \"\"\"\n",
    "        if not self.assessments:\n",
    "            return None\n",
    "        \n",
    "        risks = [a['risk'] for a in self.assessments.values()]\n",
    "        \n",
    "        if 'High' in risks:\n",
    "            return 'High'\n",
    "        elif 'Some concerns' in risks:\n",
    "            return 'Some concerns'\n",
    "        else:\n",
    "            return 'Low'\n",
    "    \n",
    "    def report(self):\n",
    "        \"\"\"\n",
    "        Generate a bias assessment report.\n",
    "        \"\"\"\n",
    "        print(f\"\\nRISK OF BIAS ASSESSMENT REPORT\")\n",
    "        print(f\"Study: {self.study_id} - {self.study_name}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for domain, assessment in self.assessments.items():\n",
    "            risk = assessment['risk']\n",
    "            notes = assessment['notes']\n",
    "            symbol = 'üü¢' if risk == 'Low' else 'üü°' if risk == 'Some concerns' else 'üî¥'\n",
    "            \n",
    "            print(f\"\\n{symbol} {domain}: {risk}\")\n",
    "            if notes:\n",
    "                print(f\"   Notes: {notes}\")\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        overall = self.overall_risk()\n",
    "        symbol = 'üü¢' if overall == 'Low' else 'üü°' if overall == 'Some concerns' else 'üî¥'\n",
    "        print(f\"Overall Risk of Bias: {symbol} {overall}\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "# Example: Assess a study\n",
    "study_assessment = RobAssessmentTool('S001', 'RCT of Cognitive Therapy vs Placebo')\n",
    "\n",
    "study_assessment.assess_domain(\n",
    "    'Randomization Process',\n",
    "    'Low',\n",
    "    'Computer-generated random allocation sequence, adequate concealment reported'\n",
    ")\n",
    "\n",
    "study_assessment.assess_domain(\n",
    "    'Deviations from Intervention',\n",
    "    'Low',\n",
    "    'Blinding of participants and therapists, intention-to-treat analysis'\n",
    ")\n",
    "\n",
    "study_assessment.assess_domain(\n",
    "    'Missing Outcome Data',\n",
    "    'Some concerns',\n",
    "    '12% dropout rate, reasons for missing data not clearly reported'\n",
    ")\n",
    "\n",
    "study_assessment.assess_domain(\n",
    "    'Outcome Measurement',\n",
    "    'Low',\n",
    "    'Pre-validated outcome measures, standardized administration'\n",
    ")\n",
    "\n",
    "study_assessment.assess_domain(\n",
    "    'Selective Reporting',\n",
    "    'Low',\n",
    "    'Protocol registered in ClinicalTrials.gov, all pre-specified outcomes reported'\n",
    ")\n",
    "\n",
    "study_assessment.report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize risk of bias across multiple studies\n",
    "\n",
    "# Create assessments for multiple studies\n",
    "study_assessments = {\n",
    "    'S001': {\n",
    "        'Randomization': 'Low',\n",
    "        'Deviations': 'Low',\n",
    "        'Missing Data': 'Some concerns',\n",
    "        'Outcome Measurement': 'Low',\n",
    "        'Selective Reporting': 'Low'\n",
    "    },\n",
    "    'S002': {\n",
    "        'Randomization': 'Some concerns',\n",
    "        'Deviations': 'Some concerns',\n",
    "        'Missing Data': 'High',\n",
    "        'Outcome Measurement': 'Some concerns',\n",
    "        'Selective Reporting': 'Low'\n",
    "    },\n",
    "    'S003': {\n",
    "        'Randomization': 'Low',\n",
    "        'Deviations': 'Low',\n",
    "        'Missing Data': 'Low',\n",
    "        'Outcome Measurement': 'Low',\n",
    "        'Selective Reporting': 'Some concerns'\n",
    "    },\n",
    "    'S004': {\n",
    "        'Randomization': 'High',\n",
    "        'Deviations': 'Some concerns',\n",
    "        'Missing Data': 'Some concerns',\n",
    "        'Outcome Measurement': 'Some concerns',\n",
    "        'Selective Reporting': 'High'\n",
    "    },\n",
    "    'S005': {\n",
    "        'Randomization': 'Low',\n",
    "        'Deviations': 'Low',\n",
    "        'Missing Data': 'Some concerns',\n",
    "        'Outcome Measurement': 'Low',\n",
    "        'Selective Reporting': 'Low'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Convert to a format suitable for visualization\n",
    "domains = list(list(study_assessments.values())[0].keys())\n",
    "\n",
    "# Create risk level mapping\n",
    "risk_map = {'Low': 3, 'Some concerns': 2, 'High': 1}\n",
    "color_map = {'Low': 'green', 'Some concerns': 'yellow', 'High': 'red'}\n",
    "\n",
    "# Create figure\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Create the visualization\n",
    "studies = list(study_assessments.keys())\n",
    "x = np.arange(len(domains))\n",
    "width = 0.15\n",
    "\n",
    "for i, study in enumerate(studies):\n",
    "    risks = [risk_map[study_assessments[study][domain]] for domain in domains]\n",
    "    colors = [color_map[study_assessments[study][domain]] for domain in domains]\n",
    "    \n",
    "    ax.bar(x + i*width, risks, width, label=study, color=colors, edgecolor='black')\n",
    "\n",
    "ax.set_xlabel('Risk of Bias Domains', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Risk Level', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Risk of Bias Assessment Across Included Studies', fontsize=13, fontweight='bold')\n",
    "ax.set_xticks(x + width*2)\n",
    "ax.set_xticklabels(domains, rotation=45, ha='right')\n",
    "ax.set_yticks([1, 2, 3])\n",
    "ax.set_yticklabels(['High', 'Some concerns', 'Low'])\n",
    "ax.legend(loc='upper right', fontsize=10)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nRISK OF BIAS SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for domain in domains:\n",
    "    risks = [study_assessments[study][domain] for study in studies]\n",
    "    low_count = risks.count('Low')\n",
    "    some_count = risks.count('Some concerns')\n",
    "    high_count = risks.count('High')\n",
    "    \n",
    "    print(f\"\\n{domain}:\")\n",
    "    print(f\"  Low: {low_count}/5 ({100*low_count/5:.0f}%)\")\n",
    "    print(f\"  Some concerns: {some_count}/5 ({100*some_count/5:.0f}%)\")\n",
    "    print(f\"  High: {high_count}/5 ({100*high_count/5:.0f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Formulating a PICO Question\n",
    "\n",
    "You're planning a systematic review on digital health interventions for managing chronic pain. Develop a well-structured PICO question:\n",
    "\n",
    "**Your Task:**\n",
    "1. Define the **Population**: Who is this research about? Consider age, condition, setting\n",
    "2. Define the **Intervention**: What specifically are you examining?\n",
    "3. Define the **Comparison**: What is it being compared to?\n",
    "4. Define the **Outcomes**: What will you measure? (Primary and secondary)\n",
    "5. Write the complete PICO question as a single sentence\n",
    "\n",
    "**Consider:**\n",
    "- Should the population be adults only or include children?\n",
    "- What types of digital health (apps, wearables, telemedicine)?\n",
    "- Compare to standard treatment, placebo, or no intervention?\n",
    "- Pain reduction? Functional improvement? Quality of life?\n",
    "\n",
    "*Write your PICO question in the code cell below:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: PICO Question Formulation\n",
    "\n",
    "# Define each component:\n",
    "population = \"???\"\n",
    "intervention = \"???\"\n",
    "comparison = \"???\"\n",
    "outcomes = \"???\"\n",
    "\n",
    "# Complete PICO question:\n",
    "pico_question = f\"In {population}, how does {intervention} compared to {comparison} affect {outcomes}?\"\n",
    "\n",
    "print(\"YOUR PICO QUESTION:\")\n",
    "print(\"=\"*60)\n",
    "print(pico_question)\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Evaluation checklist\n",
    "print(\"\\nPICO QUESTION CHECKLIST:\")\n",
    "print(\"‚úì Population is specific (age, condition, setting): [ ]\")\n",
    "print(\"‚úì Intervention is clearly defined: [ ]\")\n",
    "print(\"‚úì Comparison is explicitly stated: [ ]\")\n",
    "print(\"‚úì Outcomes are measurable and specific: [ ]\")\n",
    "print(\"‚úì Question is answerable with research evidence: [ ]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Creating a PRISMA Flowchart\n",
    "\n",
    "Based on a hypothetical literature search, your team screened studies and made inclusion decisions. Complete the PRISMA flow diagram numbers:\n",
    "\n",
    "**Your search results:**\n",
    "- PubMed: 245 records\n",
    "- Web of Science: 312 records\n",
    "- Cochrane: 89 records\n",
    "- Google Scholar: 156 records (first 100 unique results)\n",
    "- Manual search: 12 records\n",
    "\n",
    "**After deduplication:** 642 unique records\n",
    "\n",
    "**Title/Abstract Screening Results:**\n",
    "- Clearly irrelevant: 580 excluded\n",
    "- Potentially relevant: 62\n",
    "\n",
    "**Full-Text Review:**\n",
    "- Excluded (language): 3\n",
    "- Excluded (not RCT): 12\n",
    "- Excluded (missing outcomes): 8\n",
    "- Excluded (outcomes outside scope): 5\n",
    "- Included in review: 34\n",
    "\n",
    "*Calculate the missing numbers and explain the filtering at each stage:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: PRISMA Flow Diagram Completion\n",
    "\n",
    "# Calculate total initial records\n",
    "pubmed = 245\n",
    "wos = 312\n",
    "cochrane = 89\n",
    "scholar = 100  # Only unique first 100\n",
    "manual = 12\n",
    "\n",
    "total_initial = pubmed + wos + cochrane + scholar + manual\n",
    "print(f\"Total initial records: {total_initial}\")\n",
    "print(f\"After deduplication: 642\")\n",
    "\n",
    "# Title/abstract screening\n",
    "excluded_titles = 580\n",
    "potentially_relevant = 62\n",
    "\n",
    "# Full-text review\n",
    "excluded_language = 3\n",
    "excluded_not_rct = 12\n",
    "excluded_missing = 8\n",
    "excluded_outside_scope = 5\n",
    "included_final = 34\n",
    "\n",
    "# Calculate total excluded in full-text\n",
    "total_excluded_full_text = excluded_language + excluded_not_rct + excluded_missing + excluded_outside_scope\n",
    "\n",
    "print(f\"\\nFull-text reviewed: {total_excluded_full_text + included_final}\")\n",
    "print(f\"Excluded in full-text review: {total_excluded_full_text}\")\n",
    "print(f\"Final included studies: {included_final}\")\n",
    "\n",
    "# Calculate retention rates\n",
    "retention_dedup = (642 / total_initial) * 100\n",
    "retention_title = (potentially_relevant / 642) * 100\n",
    "retention_fulltext = (included_final / potentially_relevant) * 100\n",
    "retention_overall = (included_final / total_initial) * 100\n",
    "\n",
    "print(f\"\\nRETENTION RATES:\")\n",
    "print(f\"Deduplication: {retention_dedup:.1f}% retained\")\n",
    "print(f\"Title/abstract: {retention_title:.1f}% retained\")\n",
    "print(f\"Full-text: {retention_fulltext:.1f}% retained\")\n",
    "print(f\"Overall: {retention_overall:.1f}% of initial records included\")\n",
    "\n",
    "print(f\"\\nEXCLUSION REASONS (Full-text stage):\")\n",
    "print(f\"Language issues: {excluded_language} studies\")\n",
    "print(f\"Not RCT design: {excluded_not_rct} studies\")\n",
    "print(f\"Missing key outcomes: {excluded_missing} studies\")\n",
    "print(f\"Outcomes outside scope: {excluded_outside_scope} studies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Choosing the Appropriate Review Type\n",
    "\n",
    "For each research question below, decide whether a **Systematic Review**, **Scoping Review**, or **Narrative Review** is most appropriate. Justify your choice.\n",
    "\n",
    "**Scenario A**: \"What is the effectiveness of cognitive behavioral therapy for treating social anxiety disorder in adolescents aged 12-18?\"\n",
    "\n",
    "**Scenario B**: \"What are the various approaches to implementing artificial intelligence in healthcare settings, and what barriers and facilitators exist across different countries?\"\n",
    "\n",
    "**Scenario C**: \"How have perspectives on work-life balance changed among millennial professionals in the past 20 years?\"\n",
    "\n",
    "*Analyze each scenario in the code cell below:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Review Type Selection\n",
    "\n",
    "scenarios = {\n",
    "    'A': {\n",
    "        'question': 'Effectiveness of CBT for social anxiety in adolescents',\n",
    "        'recommendation': '???',  # Systematic, Scoping, or Narrative\n",
    "        'justification': '???'  # Your reasoning\n",
    "    },\n",
    "    'B': {\n",
    "        'question': 'AI implementation approaches and barriers in healthcare',\n",
    "        'recommendation': '???',\n",
    "        'justification': '???'\n",
    "    },\n",
    "    'C': {\n",
    "        'question': 'Evolution of work-life balance perspectives in millennials',\n",
    "        'recommendation': '???',\n",
    "        'justification': '???'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Analysis framework\n",
    "print(\"REVIEW TYPE SELECTION FRAMEWORK\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nConsider these factors:\")\n",
    "print(\"‚úì Scope: Narrow and specific ‚Üí Systematic\")\n",
    "print(\"        Broad and exploratory ‚Üí Scoping\")\n",
    "print(\"        Emerging or specialized ‚Üí Narrative\")\n",
    "print(\"\\n‚úì Evidence quality: Well-established ‚Üí Systematic\")\n",
    "print(\"                   Heterogeneous ‚Üí Scoping\")\n",
    "print(\"                   Limited ‚Üí Narrative\")\n",
    "print(\"\\n‚úì Timeline: Urgent decision-making ‚Üí Narrative (faster)\")\n",
    "print(\"            Comprehensive evidence ‚Üí Systematic (slower)\")\n",
    "print(\"            Map the landscape ‚Üí Scoping (medium)\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Your analysis:\n",
    "print(\"\\nYOUR ANALYSIS:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for scenario, details in scenarios.items():\n",
    "    print(f\"\\nScenario {scenario}: {details['question']}\")\n",
    "    print(f\"Your recommendation: {details['recommendation']}\")\n",
    "    print(f\"Justification: {details['justification']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison table for review types\n",
    "print(\"\\nDECISION TABLE: Which Review Type is Right?\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "comparison_data = {\n",
    "    'Characteristic': [\n",
    "        'Research question specificity',\n",
    "        'Evidence base maturity',\n",
    "        'Team size needed',\n",
    "        'Time required',\n",
    "        'Methodological rigor',\n",
    "        'Meta-analysis possible',\n",
    "        'Best for identifying gaps',\n",
    "        'Best for synthesis quality'\n",
    "    ],\n",
    "    'Systematic': [\n",
    "        'Very specific (PICO)',\n",
    "        'Mature with good evidence',\n",
    "        'Large (usually 4+)',\n",
    "        '1-3 years',\n",
    "        'Highest',\n",
    "        'Yes',\n",
    "        'No - too narrow',\n",
    "        'Excellent'\n",
    "    ],\n",
    "    'Scoping': [\n",
    "        'Broad exploratory (PCC)',\n",
    "        'Heterogeneous, immature',\n",
    "        'Medium (2-3)',\n",
    "        '6-12 months',\n",
    "        'High',\n",
    "        'No',\n",
    "        'Yes - key strength',\n",
    "        'Good'\n",
    "    ],\n",
    "    'Narrative': [\n",
    "        'Flexible, author-defined',\n",
    "        'Emerging, specialist topics',\n",
    "        'Small (1-2)',\n",
    "        '2-6 months',\n",
    "        'Lower (interpretive)',\n",
    "        'No',\n",
    "        'No',\n",
    "        'Fair'\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Meta-Analysis Basics\n",
    "\n",
    "### What is Meta-Analysis?\n",
    "\n",
    "**Meta-analysis** is the statistical technique of combining results from multiple independent studies to produce an overall estimate of effect.\n",
    "\n",
    "### When to Perform Meta-Analysis\n",
    "\n",
    "Meta-analysis is appropriate when:\n",
    "1. **Multiple studies** exist on the same topic (usually ‚â•3)\n",
    "2. **Similar populations** and interventions across studies\n",
    "3. **Comparable outcomes** measured in similar ways\n",
    "4. **Similar methodological quality** (or quality differences assessed)\n",
    "5. **Lack of high heterogeneity** (studies don't conflict dramatically)\n",
    "\n",
    "### When NOT to Do Meta-Analysis\n",
    "\n",
    "‚ùå Too few studies (fewer than 3-5)\n",
    "‚ùå Highly different populations or interventions\n",
    "‚ùå Outcomes measured differently across studies\n",
    "‚ùå Extreme quality differences\n",
    "‚ùå High heterogeneity indicating conflicting results\n",
    "‚ùå Only one study of adequate quality\n",
    "\n",
    "### Key Meta-Analysis Concepts\n",
    "\n",
    "**Effect Size**\n",
    "- Standardized measure of intervention effectiveness\n",
    "- Common metrics: Odds Ratio (OR), Relative Risk (RR), Mean Difference (MD)\n",
    "- Allows comparison across studies with different measurements\n",
    "\n",
    "**Heterogeneity (I¬≤)**\n",
    "- Measures variability between studies beyond what's expected by chance\n",
    "- I¬≤ = 0%: No heterogeneity (results consistent)\n",
    "- I¬≤ = 25-50%: Low heterogeneity\n",
    "- I¬≤ = 50-75%: Moderate heterogeneity\n",
    "- I¬≤ = 75-100%: High heterogeneity (conflicting results)\n",
    "\n",
    "**Fixed vs Random Effects Models**\n",
    "- **Fixed effects**: Assumes all variation is due to sampling error\n",
    "  - Use when heterogeneity is low (I¬≤ < 30%)\n",
    "  - Gives narrower confidence intervals\n",
    "\n",
    "- **Random effects**: Accounts for between-study variation\n",
    "  - Use when heterogeneity is present\n",
    "  - More conservative (wider confidence intervals)\n",
    "  - Recommended for most healthcare reviews\n",
    "\n",
    "**Publication Bias**\n",
    "- Studies with positive results more likely to be published\n",
    "- Meta-analysis may overestimate true effect\n",
    "- Assessed using funnel plots and statistical tests\n",
    "\n",
    "### Forest Plot Interpretation\n",
    "\n",
    "A **forest plot** is the standard visualization for meta-analysis:\n",
    "- Each horizontal line = one study's effect size with confidence interval\n",
    "- Diamond at bottom = overall pooled effect\n",
    "- Vertical line at 1.0 = null effect (no difference)\n",
    "- If confidence interval crosses 1.0 ‚Üí Not statistically significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate meta-analysis forest plot\n",
    "\n",
    "# Simulated study data (Odds Ratios with confidence intervals)\n",
    "studies = [\n",
    "    {'name': 'Smith et al. (2018)', 'or': 0.75, 'ci_lower': 0.58, 'ci_upper': 0.97},\n",
    "    {'name': 'Johnson et al. (2019)', 'or': 0.82, 'ci_lower': 0.65, 'ci_upper': 1.04},\n",
    "    {'name': 'Lee et al. (2020)', 'or': 0.69, 'ci_lower': 0.52, 'ci_upper': 0.92},\n",
    "    {'name': 'Chen et al. (2021)', 'or': 0.88, 'ci_lower': 0.71, 'ci_upper': 1.09},\n",
    "    {'name': 'Davis et al. (2022)', 'or': 0.73, 'ci_lower': 0.57, 'ci_upper': 0.94},\n",
    "]\n",
    "\n",
    "# Pooled effect (simulated)\n",
    "pooled_or = 0.77\n",
    "pooled_ci_lower = 0.68\n",
    "pooled_ci_upper = 0.88\n",
    "\n",
    "# Create forest plot\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "# Plot each study\n",
    "y_positions = range(len(studies), 0, -1)\n",
    "\n",
    "for i, (study, y_pos) in enumerate(zip(studies, y_positions)):\n",
    "    # Draw confidence interval as horizontal line\n",
    "    ax.plot([study['ci_lower'], study['ci_upper']], [y_pos, y_pos], \n",
    "            'k-', linewidth=2, zorder=2)\n",
    "    \n",
    "    # Draw effect point as diamond\n",
    "    size = 100\n",
    "    ax.scatter(study['or'], y_pos, s=size, marker='D', \n",
    "               color='blue', edgecolors='black', linewidth=1.5, zorder=3)\n",
    "\n",
    "# Plot pooled effect\n",
    "y_pool = 0.5\n",
    "ax.plot([pooled_ci_lower, pooled_ci_upper], [y_pool, y_pool], \n",
    "        'darkred', linewidth=4, zorder=2)\n",
    "ax.scatter(pooled_or, y_pool, s=300, marker='D', \n",
    "          color='red', edgecolors='black', linewidth=2, zorder=3)\n",
    "\n",
    "# Add vertical line at OR=1 (null effect)\n",
    "ax.axvline(x=1.0, color='gray', linestyle='--', linewidth=2, alpha=0.7, label='Null effect (OR=1.0)')\n",
    "\n",
    "# Labels and formatting\n",
    "study_labels = [study['name'] for study in studies] + ['POOLED EFFECT']\n",
    "ax.set_yticks(list(y_positions) + [y_pool])\n",
    "ax.set_yticklabels(study_labels, fontsize=11)\n",
    "ax.set_xlabel('Odds Ratio (95% CI)', fontsize=12, fontweight='bold')\n",
    "ax.set_xlim(0.4, 1.3)\n",
    "ax.set_title('Meta-Analysis: Effectiveness of Intervention\\n(Forest Plot)', \n",
    "            fontsize=13, fontweight='bold', pad=15)\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "# Add value labels\n",
    "for study, y_pos in zip(studies, y_positions):\n",
    "    label_text = f\"OR={study['or']:.2f} (95% CI: {study['ci_lower']:.2f}-{study['ci_upper']:.2f})\"\n",
    "    ax.text(1.15, y_pos, label_text, fontsize=9, va='center')\n",
    "\n",
    "# Add pooled label\n",
    "label_text = f\"OR={pooled_or:.2f} (95% CI: {pooled_ci_lower:.2f}-{pooled_ci_upper:.2f})\"\n",
    "ax.text(1.15, y_pool, label_text, fontsize=10, fontweight='bold', va='center', color='red')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Interpretation\n",
    "print(\"\\nMETA-ANALYSIS INTERPRETATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nPooled Odds Ratio: {pooled_or:.2f}\")\n",
    "print(f\"95% Confidence Interval: {pooled_ci_lower:.2f} - {pooled_ci_upper:.2f}\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"‚úì The intervention reduces the odds of the outcome by {(1-pooled_or)*100:.0f}%\")\n",
    "print(f\"‚úì The 95% CI does NOT cross 1.0 ‚Üí Result is statistically significant\")\n",
    "print(f\"‚úì All individual studies show similar effect direction\")\n",
    "print(f\"‚úì This suggests robust evidence for the intervention's effectiveness\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "‚úÖ **Three main review types** serve different purposes:\n",
    "- Systematic reviews: Answer specific questions with highest rigor\n",
    "- Scoping reviews: Map evidence landscape and identify gaps\n",
    "- Narrative reviews: Explore emerging topics with critical interpretation\n",
    "\n",
    "‚úÖ **PRISMA 2020** provides 27-item checklist for rigorous systematic review reporting\n",
    "\n",
    "‚úÖ **PICO framework** structures systematic review questions for maximum specificity\n",
    "\n",
    "‚úÖ **PROSPERO registration** ensures transparency and prevents research drift\n",
    "\n",
    "‚úÖ **JBI methodology** offers structured approach to scoping reviews with PCC framework\n",
    "\n",
    "‚úÖ **Risk of Bias assessment** (RoB 2, ROBINS-I) evaluates study quality systematically\n",
    "\n",
    "‚úÖ **GRADE certainty** categorizes evidence quality across four levels\n",
    "\n",
    "‚úÖ **SANRA scale** helps evaluate narrative review quality across six dimensions\n",
    "\n",
    "‚úÖ **Meta-analysis** combines study results statistically when appropriate\n",
    "\n",
    "‚úÖ **Evidence hierarchy** ranks study designs by methodological rigor\n",
    "\n",
    "‚úÖ **Choosing the right review type** depends on research question scope and evidence maturity\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In **Module 08: Data Extraction and Synthesis**, you'll learn:\n",
    "- How to standardize and extract data from diverse studies\n",
    "- Thematic analysis and narrative synthesis techniques\n",
    "- Quality assessment frameworks\n",
    "- Presenting results visually and in text\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- **PRISMA 2020 Statement**: https://www.prisma-statement.org/\n",
    "- **JBI Scoping Reviews**: https://jbi.global/\n",
    "- **PROSPERO**: https://www.crd.york.ac.uk/prospero/\n",
    "- **Cochrane Handbook**: https://training.cochrane.org/handbook\n",
    "- **SANRA Scale Paper**: Barr√≥n-Cede√±o et al. (2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Assessment\n",
    "\n",
    "Before moving to Module 08, ensure you can:\n",
    "\n",
    "- [ ] Distinguish between systematic, scoping, and narrative reviews\n",
    "- [ ] Formulate a well-structured PICO question\n",
    "- [ ] Explain the 27 key items of PRISMA 2020\n",
    "- [ ] Describe the JBI 5-stage scoping review methodology\n",
    "- [ ] Assess risk of bias using RoB 2 framework\n",
    "- [ ] Interpret GRADE certainty ratings\n",
    "- [ ] Evaluate narrative review quality using SANRA scale\n",
    "- [ ] Explain when meta-analysis is and isn't appropriate\n",
    "- [ ] Interpret a forest plot from meta-analysis\n",
    "- [ ] Choose appropriate review type for a given research question\n",
    "\n",
    "If you can confidently check all boxes, you're ready for Module 08! üéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}