{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 06: Reproducibility Crisis and Documentation Standards\n",
    "\n",
    "**Difficulty**: ‚≠ê‚≠ê (Intermediate)\n",
    "\n",
    "**Estimated Time**: 75 minutes\n",
    "\n",
    "**Prerequisites**: Module 05: Statistical Validation and Hypothesis Testing\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Understand the reproducibility crisis in ML-based science\n",
    "2. Apply the NeurIPS reproducibility checklist\n",
    "3. Implement comprehensive documentation standards\n",
    "4. Use version control effectively for research\n",
    "5. Create reproducible computational environments\n",
    "6. Design data preprocessing pipelines that prevent data leakage\n",
    "7. Document data lineage and transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's import required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "print('‚úì Libraries imported successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Reproducibility Crisis in ML-Based Science\n",
    "\n",
    "### Understanding the Scale\n",
    "\n",
    "The reproducibility crisis is a systemic problem in modern science.\n",
    "\n",
    "**Key Statistics:**\n",
    "\n",
    "- **Princeton Study**: 41 papers from 30 fields had reproducibility failures\n",
    "- **Cascading Impact**: These 41 papers affected 648 subsequent papers\n",
    "- **Nature 2016 Survey**: 70% of researchers couldn't reproduce others' work\n",
    "- **Self-Reproducibility**: Over 50% couldn't reproduce their own work\n",
    "- **Root Cause**: Data leakage is the most pervasive cause\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "Irreproducible research has serious consequences:\n",
    "1. Wasted resources building on flawed findings\n",
    "2. Scientific community moves in wrong directions\n",
    "3. The scientific method breaks down\n",
    "4. Loss of public trust in science\n",
    "5. Real-world harm in applied domains (medicine, autonomous systems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "categories = ['Flawed Papers', 'Papers Affected']\n",
    "values = [41, 648]\n",
    "colors = ['#e74c3c', '#c0392b']\n",
    "\n",
    "axes[0, 0].bar(categories, values, color=colors, edgecolor='black', linewidth=2)\n",
    "axes[0, 0].set_ylabel('Number of Papers', fontsize=11, fontweight='bold')\n",
    "axes[0, 0].set_title('Princeton Study: Cascading Effects', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_ylim(0, 700)\n",
    "\n",
    "for i, val in enumerate(values):\n",
    "    axes[0, 0].text(i, val + 20, str(val), ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "axes[0, 1].bar([1, 2], [30, 50], color=['#e74c3c', '#e74c3c'], edgecolor='black', linewidth=2)\n",
    "axes[0, 1].set_xticks([1, 2])\n",
    "axes[0, 1].set_xticklabels(['Reproduce Others', 'Reproduce Own'])\n",
    "axes[0, 1].set_ylabel('Success Rate (%)', fontsize=11, fontweight='bold')\n",
    "axes[0, 1].set_title('Nature 2016: Success Rates', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_ylim(0, 100)\n",
    "\n",
    "causes = ['Data Leakage', 'Poor Docs', 'Missing Params', 'No Seed', 'Env Variation']\n",
    "freqs = [45, 28, 18, 12, 10]\n",
    "\n",
    "axes[1, 0].barh(causes, freqs, color=['#e74c3c', '#e67e22', '#f39c12', '#f1c40f', '#2ecc71'], edgecolor='black')\n",
    "axes[1, 0].set_xlabel('Frequency (%)', fontsize=11, fontweight='bold')\n",
    "axes[1, 0].set_title('Common Causes of Irreproducibility', fontsize=12, fontweight='bold')\n",
    "\n",
    "years = [2019, 2020, 2021, 2022, 2023, 2024]\n",
    "adoption = [15, 32, 48, 65, 78, 88]\n",
    "\n",
    "axes[1, 1].plot(years, adoption, marker='o', linewidth=3, markersize=10, color='#3498db')\n",
    "axes[1, 1].set_xlabel('Year', fontsize=11, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Papers with Checklist (%)', fontsize=11, fontweight='bold')\n",
    "axes[1, 1].set_title('NeurIPS Checklist Adoption', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_ylim(0, 100)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Reproducibility crisis declining with better standards!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. The NeurIPS Reproducibility Checklist\n\n### The Seven Core Requirements\n\nThe NeurIPS reproducibility checklist is the gold standard for ensuring research can be replicated. Let's examine each requirement in detail with practical examples.\n\n**1. Claims Accuracy** - All claims must match actual findings\n**2. Limitations Documentation** - Clearly acknowledge assumptions and limitations\n**3. Experimental Reproducibility** - Sufficient detail for others to replicate\n**4. Open Access** - Make data and code publicly available\n**5. Experimental Settings** - Report all hyperparameters and hardware specs\n**6. Statistical Significance** - Results with confidence intervals and error bars\n**7. Compute Resources** - Report training time, memory, GPU hours\n\n### Applying the NeurIPS Checklist\n\nLet's see how to apply each requirement systematically."
  },
  {
   "cell_type": "code",
   "source": "# NeurIPS Reproducibility Checklist Implementation\nclass ReproducibilityChecklist:\n    \"\"\"\n    Comprehensive checklist for ensuring research reproducibility.\n    Based on NeurIPS 2020+ standards.\n    \"\"\"\n    def __init__(self, paper_title):\n        self.paper_title = paper_title\n        self.checklist = {\n            '1. Claims Accuracy': {\n                'status': False,\n                'requirements': [\n                    'All claims backed by evidence in results',\n                    'No overgeneralization beyond tested scenarios',\n                    'Clearly state what IS and IS NOT claimed',\n                    'Match abstract/intro claims to conclusion'\n                ],\n                'example_good': 'Our model achieves 92.3% accuracy on CIFAR-10 test set',\n                'example_bad': 'Our model works well on most image tasks'\n            },\n            '2. Limitations': {\n                'status': False,\n                'requirements': [\n                    'Acknowledge dataset limitations',\n                    'Describe computational constraints',\n                    'Identify assumptions made',\n                    'Discuss where method may fail'\n                ],\n                'example_good': 'Limited to images <1024x1024, assumes RGB format',\n                'example_bad': 'No major limitations identified'\n            },\n            '3. Experimental Detail': {\n                'status': False,\n                'requirements': [\n                    'Report all hyperparameters',\n                    'Specify random seeds used',\n                    'Document train/val/test splits',\n                    'Describe preprocessing steps'\n                ],\n                'example_good': 'Learning rate=0.001, batch_size=32, seed=42, 80/10/10 split',\n                'example_bad': 'Standard hyperparameters used'\n            },\n            '4. Open Access': {\n                'status': False,\n                'requirements': [\n                    'Code available on GitHub/GitLab',\n                    'Data publicly accessible or described',\n                    'Pretrained models shared',\n                    'Clear instructions for reproduction'\n                ],\n                'example_good': 'github.com/user/repo with README and requirements.txt',\n                'example_bad': 'Code available upon request'\n            },\n            '5. Experimental Settings': {\n                'status': False,\n                'requirements': [\n                    'Hardware specifications (GPU model, RAM)',\n                    'Software versions (Python, PyTorch, etc.)',\n                    'Operating system',\n                    'Number of runs/seeds reported'\n                ],\n                'example_good': 'NVIDIA RTX 3090, PyTorch 1.12, Python 3.9, Ubuntu 20.04, 5 seeds',\n                'example_bad': 'Ran on GPU with PyTorch'\n            },\n            '6. Statistical Significance': {\n                'status': False,\n                'requirements': [\n                    'Report mean AND standard deviation',\n                    'Confidence intervals (95% CI)',\n                    'Error bars in visualizations',\n                    'Multiple runs (min 3-5 seeds)'\n                ],\n                'example_good': 'Accuracy: 92.3% ¬± 0.8% (95% CI: 91.5-93.1, n=5)',\n                'example_bad': 'Accuracy: 92.3%'\n            },\n            '7. Compute Resources': {\n                'status': False,\n                'requirements': [\n                    'Total training time',\n                    'GPU hours consumed',\n                    'Memory requirements',\n                    'Cost estimate if using cloud'\n                ],\n                'example_good': '48 hours on 4x RTX 3090 (192 GPU-hours), ~$300 on AWS',\n                'example_bad': 'Training took a few days'\n            }\n        }\n    \n    def mark_complete(self, item_number):\n        \"\"\"Mark a checklist item as complete.\"\"\"\n        key = [k for k in self.checklist.keys() if k.startswith(str(item_number))][0]\n        self.checklist[key]['status'] = True\n    \n    def get_completion_rate(self):\n        \"\"\"Calculate overall completion percentage.\"\"\"\n        total = len(self.checklist)\n        completed = sum(1 for item in self.checklist.values() if item['status'])\n        return (completed / total) * 100\n    \n    def generate_report(self):\n        \"\"\"Generate a comprehensive checklist report.\"\"\"\n        print(f\"Reproducibility Checklist: {self.paper_title}\")\n        print(\"=\" * 80)\n        print(f\"Overall Completion: {self.get_completion_rate():.0f}%\\n\")\n        \n        for item_name, details in self.checklist.items():\n            status_symbol = \"‚úì\" if details['status'] else \"‚úó\"\n            print(f\"{status_symbol} {item_name}\")\n            print(f\"   Requirements:\")\n            for req in details['requirements']:\n                print(f\"     ‚Ä¢ {req}\")\n            print(f\"   ‚úì Good: {details['example_good']}\")\n            print(f\"   ‚úó Bad:  {details['example_bad']}\\n\")\n        \n        if self.get_completion_rate() < 100:\n            print(\"‚ö†Ô∏è  WARNING: Checklist incomplete. Paper not ready for submission.\")\n        else:\n            print(\"‚úì All requirements met. Paper ready for submission!\")\n\n# Example usage\npaper = ReproducibilityChecklist(\"Deep Learning for Time Series Forecasting\")\n\n# Mark some items as complete\npaper.mark_complete(1)  # Claims accuracy\npaper.mark_complete(2)  # Limitations\npaper.mark_complete(5)  # Experimental settings\n\n# Generate report\npaper.generate_report()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Research Documentation Standards\n",
    "\n",
    "### The Core Principle\n",
    "\n",
    "**'If methods cannot be reproduced from documentation alone, the documentation is insufficient.'**\n",
    "\n",
    "### What to Document\n",
    "\n",
    "**Electronic Lab Notebooks:**\n",
    "- Date and time of work\n",
    "- Detailed methods for colleague replication\n",
    "- Equipment settings and procedures\n",
    "- Deviations from protocol\n",
    "- Negative results\n",
    "- Links to raw data with versions\n",
    "\n",
    "**Data Dictionary:**\n",
    "- Short and long variable names\n",
    "- Format and units\n",
    "- Allowable values\n",
    "- Complete definitions\n",
    "\n",
    "**Data Lineage:**\n",
    "- Source of origin\n",
    "- All transformations in order\n",
    "- Processing pipeline steps\n",
    "- Version information\n",
    "- Software dependencies"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Electronic Lab Notebook Entry Template\nclass LabNotebook:\n    \"\"\"Template for documenting research experiments.\"\"\"\n    \n    @staticmethod\n    def create_entry(experiment_name, researcher, date=None):\n        \"\"\"Generate a structured lab notebook entry.\"\"\"\n        if date is None:\n            date = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        \n        template = f\"\"\"\n{'='*80}\nELECTRONIC LAB NOTEBOOK ENTRY\n{'='*80}\n\nExperiment: {experiment_name}\nResearcher: {researcher}\nDate/Time: {date}\n\n{'='*80}\n1. OBJECTIVE\n{'='*80}\nWhat question are you trying to answer?\nWhat is the specific goal of this experiment?\n\nExample: \"Test whether adding attention mechanism improves model accuracy by >5%\"\n\n{'='*80}\n2. HYPOTHESIS\n{'='*80}\nWhat do you expect to happen and why?\n\nExample: \"Attention will improve accuracy because it allows the model to focus on \nrelevant features, particularly for long sequences where context matters.\"\n\n{'='*80}\n3. METHODS\n{'='*80}\nDetailed procedure that allows exact replication:\n\na) Data:\n   - Dataset: [name, version, source, size]\n   - Split: [train/val/test proportions, random seed]\n   - Preprocessing: [all transformations applied]\n\nb) Model Architecture:\n   - Type: [model family]\n   - Layers: [detailed architecture]\n   - Parameters: [total count]\n\nc) Training Configuration:\n   - Hyperparameters: [learning rate, batch size, epochs, optimizer]\n   - Hardware: [GPU model, RAM, CPU]\n   - Software: [Python version, library versions]\n   - Random Seeds: [all seeds used]\n\nd) Evaluation Metrics:\n   - Primary: [main metric to judge success]\n   - Secondary: [additional metrics for context]\n\n{'='*80}\n4. RESULTS\n{'='*80}\na) Quantitative Results:\n   - Report mean, std dev, 95% CI\n   - Include all runs (don't cherry-pick!)\n   - Negative results are valuable!\n\nb) Observations:\n   - Training curves (convergence, overfitting?)\n   - Unexpected behaviors\n   - Error analysis\n\nc) Visualizations:\n   - Include figures with captions\n   - Reference saved plot files\n\n{'='*80}\n5. DEVIATIONS FROM PROTOCOL\n{'='*80}\nDocument ANY changes from planned procedure:\n- Why the change was made\n- What was changed\n- Impact on results\n\nExample: \"Reduced batch size from 64 to 32 due to OOM error. \nMay affect training stability.\"\n\n{'='*80}\n6. DATA LINEAGE\n{'='*80}\nRaw Data: data/raw/dataset_v1.csv (SHA256: abc123...)\n  ‚Üì [clean_missing_values.py v1.2]\nProcessed: data/processed/cleaned_v1.csv (SHA256: def456...)\n  ‚Üì [feature_engineering.py v2.0]\nFeatures: data/features/features_v1.csv (SHA256: ghi789...)\n  ‚Üì [train_model.py v3.1]\nModel: models/attention_model_run5.pkl (SHA256: jkl012...)\n\n{'='*80}\n7. CONCLUSIONS\n{'='*80}\n- Was hypothesis supported?\n- What are the key takeaways?\n- What should be done next?\n- Any concerns or limitations?\n\n{'='*80}\n8. NEXT STEPS\n{'='*80}\nBased on these results:\n1. [action item 1]\n2. [action item 2]\n3. [action item 3]\n\n{'='*80}\n9. FILES AND ARTIFACTS\n{'='*80}\nCode: experiments/exp_2024_01_15/train.py (commit: 1a2b3c4)\nNotebook: notebooks/analysis_attention.ipynb\nModel: models/attention_v1_seed42.pkl\nLogs: logs/training_20240115.log\nFigures: figures/attention_analysis_20240115/\n\n{'='*80}\n10. SIGNATURE\n{'='*80}\nResearcher: {researcher}\nReviewed by: [name of reviewer, if applicable]\nDate: {date}\n{'='*80}\n\"\"\"\n        return template\n\n# Example: Create a lab notebook entry\nentry = LabNotebook.create_entry(\n    experiment_name=\"Attention Mechanism Evaluation - Run 5\",\n    researcher=\"Dr. Jane Smith\"\n)\n\nprint(entry)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Electronic Lab Notebook Template\n\nAn electronic lab notebook (ELN) is essential for tracking research progress and ensuring reproducibility. Here's a comprehensive template:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = pd.DataFrame({\n",
    "    'Short Name': ['cust_id', 'age', 'tenure', 'churn', 'charges'],\n",
    "    'Long Name': ['Customer ID', 'Age', 'Months with Company', 'Churn Status', 'Monthly Charges'],\n",
    "    'Format': ['Integer 6 digits', 'Integer 18-80', 'Integer 0-72', 'Binary 0/1', 'Decimal 2 places'],\n",
    "    'Units': ['ID', 'Years', 'Months', 'Binary', 'USD/month'],\n",
    "    'Definition': ['Unique customer ID', 'Age at extraction', 'Months with account', 'Stopped service (1=yes)', 'Monthly charges billed']\n",
    "})\n",
    "\n",
    "print('DATA DICTIONARY EXAMPLE')\n",
    "print('='*100)\n",
    "print(data_dict.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Docker for Complete Environment Reproducibility\n\nprint(\"=\"*80)\nprint(\"OPTION 3: DOCKER CONTAINERS\")\nprint(\"=\"*80)\nprint(\"\\nDocker guarantees identical environments across:\")\nprint(\"  ‚Ä¢ Different operating systems (Linux, Mac, Windows)\")\nprint(\"  ‚Ä¢ Different hardware configurations\")\nprint(\"  ‚Ä¢ Different points in time (frozen dependencies)\")\nprint()\n\ndockerfile_example = \"\"\"# Dockerfile for Research Project\n# This creates a complete reproducible environment\n\nFROM python:3.9.13-slim-buster\n\n# Document maintainer\nLABEL maintainer=\"researcher@university.edu\"\nLABEL description=\"Reproducible environment for Customer Churn Prediction research\"\nLABEL version=\"1.0\"\n\n# Set working directory\nWORKDIR /app\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y \\\\\n    gcc \\\\\n    g++ \\\\\n    && rm -rf /var/lib/apt/lists/*\n\n# Copy requirements first (for Docker layer caching)\nCOPY requirements.txt .\n\n# Install Python dependencies with exact versions\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy project files\nCOPY . .\n\n# Set environment variables\nENV PYTHONUNBUFFERED=1\nENV MPLBACKEND=Agg\n\n# Document how to run\nCMD [\"python\", \"train_model.py\"]\n\n# Usage instructions:\n# Build: docker build -t churn-prediction:v1.0 .\n# Run:   docker run -v $(pwd)/data:/app/data churn-prediction:v1.0\n# Shell: docker run -it churn-prediction:v1.0 /bin/bash\n\"\"\"\n\nprint(\"=\"*80)\nprint(\"DOCKERFILE EXAMPLE\")\nprint(\"=\"*80)\nprint(dockerfile_example)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"DOCKER COMPOSE FOR COMPLEX PROJECTS\")\nprint(\"=\"*80)\n\ndocker_compose_example = \"\"\"# docker-compose.yml\n# For projects with multiple services (database, web app, etc.)\n\nversion: '3.8'\n\nservices:\n  research:\n    build: .\n    volumes:\n      - ./data:/app/data\n      - ./results:/app/results\n    environment:\n      - PYTHONUNBUFFERED=1\n      - RANDOM_SEED=42\n    command: python train_model.py\n\n  jupyter:\n    build: .\n    ports:\n      - \"8888:8888\"\n    volumes:\n      - ./notebooks:/app/notebooks\n      - ./data:/app/data\n    command: jupyter notebook --ip=0.0.0.0 --allow-root --no-browser\n\n# Usage:\n# Start all services: docker-compose up\n# Stop: docker-compose down\n# Run specific service: docker-compose run research python experiment.py\n\"\"\"\n\nprint(docker_compose_example)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"ENVIRONMENT COMPARISON\")\nprint(\"=\"*80)\n\ncomparison = pd.DataFrame({\n    'Method': ['venv + requirements.txt', 'conda + environment.yml', 'Docker + Dockerfile'],\n    'Setup Time': ['Fast (seconds)', 'Medium (minutes)', 'Slow (minutes)'],\n    'Reproducibility': ['Good', 'Excellent', 'Perfect'],\n    'OS Independence': ['No', 'Partial', 'Yes'],\n    'Learning Curve': ['Easy', 'Medium', 'Hard'],\n    'Best For': ['Simple Python', 'Scientific computing', 'Production/Publishing']\n})\n\nprint(comparison.to_string(index=False))\n\nprint(\"\\n‚úì Recommendation: Start with venv, use conda for complex dependencies,\")\nprint(\"  use Docker for published research or production deployment\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Option 3: Docker for Complete Reproducibility\n\n**Best for**: Maximum reproducibility across different operating systems and hardware\n\nDocker creates a complete containerized environment including OS, system libraries, and all dependencies.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Environment Management Best Practices\n\nprint(\"=\"*80)\nprint(\"OPTION 1: VIRTUAL ENVIRONMENTS (venv)\")\nprint(\"=\"*80)\n\nvenv_commands = \"\"\"\n# Step 1: Create a virtual environment\npython3 -m venv research_env\n\n# Step 2: Activate it\n# On Linux/Mac:\nsource research_env/bin/activate\n# On Windows:\nresearch_env\\\\Scripts\\\\activate\n\n# Step 3: Install packages\npip install numpy pandas scikit-learn matplotlib\n\n# Step 4: Freeze exact versions\npip freeze > requirements.txt\n\n# Step 5: Share requirements.txt with your code\n# Others can recreate your environment with:\npip install -r requirements.txt\n\"\"\"\n\nprint(venv_commands)\nprint(\"\\n\" + \"=\"*80)\nprint(\"REQUIREMENTS.TXT BEST PRACTICES\")\nprint(\"=\"*80)\n\n# Example requirements.txt with proper formatting\nrequirements_example = \"\"\"# requirements.txt for Customer Churn Prediction\n# Generated: 2024-01-15\n# Python version: 3.9.13\n\n# Core Data Science\nnumpy==1.24.3\npandas==2.0.3\nscipy==1.11.1\n\n# Machine Learning\nscikit-learn==1.3.0\nxgboost==1.7.6\n\n# Visualization\nmatplotlib==3.7.2\nseaborn==0.12.2\n\n# Jupyter\njupyter==1.0.0\nipykernel==6.25.0\n\n# Utilities\ntqdm==4.65.0\npython-dotenv==1.0.0\n\n# DO NOT use >= or ~ (unpinned versions)\n# DO pin exact versions (==) for reproducibility\n# DO document Python version separately\n# DO regenerate when adding new dependencies\n\"\"\"\n\nprint(requirements_example)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"OPTION 2: CONDA ENVIRONMENTS\")\nprint(\"=\"*80)\n\nconda_commands = \"\"\"\n# Step 1: Create conda environment with specific Python version\nconda create -n research_env python=3.9\n\n# Step 2: Activate it\nconda activate research_env\n\n# Step 3: Install packages\nconda install numpy pandas scikit-learn matplotlib\n\n# Step 4: Export complete environment\nconda env export > environment.yml\n\n# Step 5: Share environment.yml\n# Others can recreate with:\nconda env create -f environment.yml\n\"\"\"\n\nprint(conda_commands)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"ENVIRONMENT.YML EXAMPLE\")\nprint(\"=\"*80)\n\nenvironment_yml = \"\"\"name: research_env\nchannels:\n  - conda-forge\n  - defaults\ndependencies:\n  - python=3.9.13\n  - numpy=1.24.3\n  - pandas=2.0.3\n  - scikit-learn=1.3.0\n  - matplotlib=3.7.2\n  - pip=23.2.1\n  - pip:\n    - xgboost==1.7.6\n    - seaborn==0.12.2\n\"\"\"\n\nprint(environment_yml)\n\nprint(\"\\n‚úì Choose venv for simple projects, conda for complex scientific computing\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Creating Reproducible Computational Environments\n\nOne of the top causes of irreproducibility is environment variation. Different Python versions, library versions, or operating systems can produce different results.\n\n**The Solution**: Document and isolate your computational environment.\n\n### Option 1: Virtual Environments + requirements.txt\n\n**Best for**: Simple Python projects without complex dependencies",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Code Reproducibility and Environment Management\n",
    "\n",
    "### The Cardinal Rule\n",
    "\n",
    "**'Fit preprocessing transformations ONLY on training data. Never use test set statistics.'**\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "Using test set information in preprocessing is DATA LEAKAGE‚Äîone of the top causes of irreproducibility.\n",
    "\n",
    "### Common Data Leakage Sources\n",
    "\n",
    "1. Preprocessing before train-test split\n",
    "2. Using future information (time series)\n",
    "3. Including proxy variables\n",
    "4. Improper group handling\n",
    "5. Feature selection on full dataset\n",
    "\n",
    "### The Seven-Step Data Preprocessing Workflow\n",
    "\n",
    "1. Data Acquisition\n",
    "2. Library Import\n",
    "3. Data Loading & Inspection\n",
    "4. Missing Value Handling (fit training only)\n",
    "5. Categorical Encoding (fit training only)\n",
    "6. Feature Scaling (fit training only)\n",
    "7. Data Splitting (do this FIRST!)\n",
    "\n",
    "**Critical Order**: Split data FIRST (step 7), then apply steps 4-6 ONLY to training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(42)\n",
    "X_raw = np.random.randn(100, 3) * 10 + np.array([100, 50, 20])\n",
    "\n",
    "print('INCORRECT APPROACH (DATA LEAKAGE):')\n",
    "print('='*60)\n",
    "scaler_wrong = StandardScaler()\n",
    "X_scaled = scaler_wrong.fit_transform(X_raw)\n",
    "X_train, X_test = train_test_split(X_scaled, test_size=0.2, random_state=42)\n",
    "print('Problem: Scaler fit on ALL data (includes test set)')\n",
    "print('Result: Test statistics influenced the scaler!\\n')\n",
    "\n",
    "print('CORRECT APPROACH (NO LEAKAGE):')\n",
    "print('='*60)\n",
    "X_train, X_test = train_test_split(X_raw, test_size=0.2, random_state=42)\n",
    "scaler_correct = StandardScaler()\n",
    "X_train_scaled = scaler_correct.fit_transform(X_train)\n",
    "X_test_scaled = scaler_correct.transform(X_test)\n",
    "print('Solution: Scaler fit ONLY on training data')\n",
    "print('Result: Test data never influences the scaler!')"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Exercise 4: Create reproducibility package templates\n\nprint(\"=\"*80)\nprint(\"EXERCISE 4: REPRODUCIBILITY PACKAGE\")\nprint(\"=\"*80)\n\n# Task 1: NeurIPS Checklist\nprint(\"\\n1. NeurIPS CHECKLIST\")\nprint(\"-\" * 80)\nprint(\"Use the ReproducibilityChecklist class to assess your project:\")\nprint()\nprint(\"house_price_project = ReproducibilityChecklist('House Price Prediction')\")\nprint(\"# Mark completed items (1-7)\")\nprint(\"house_price_project.mark_complete(1)  # Claims accuracy\")\nprint(\"# ... mark others as appropriate\")\nprint(\"house_price_project.generate_report()\")\n\n# Task 2: README template\nprint(\"\\n2. README.MD TEMPLATE\")\nprint(\"-\" * 80)\n\nreadme_template = \"\"\"# House Price Prediction\n\nReproducible machine learning project predicting house prices using Random Forest.\n\n## Quick Start\n\n```bash\n# Clone repository\ngit clone https://github.com/username/house-price-prediction.git\ncd house-price-prediction\n\n# Create environment\npython3 -m venv venv\nsource venv/bin/activate  # On Windows: venv\\\\Scripts\\\\activate\n\n# Install dependencies\npip install -r requirements.txt\n\n# Run experiment (reproduces published results)\npython train_model.py --seed 42\n\n# Expected output: MAE = $23,450 ¬± $1,200\n```\n\n## Results Summary\n\n- **Model**: Random Forest Regression\n- **Performance**: MAE = $23,450 ¬± $1,200 (95% CI: $21,050-$25,850)\n- **Runs**: n=5 with seeds [42, 123, 456, 789, 1011]\n- **Training Time**: ~2.3 hours on Intel i7, 16GB RAM\n\n## Repository Structure\n\n```\nhouse-price-prediction/\n‚îú‚îÄ‚îÄ data/\n‚îÇ   ‚îú‚îÄ‚îÄ raw/                 # Original data (not in git)\n‚îÇ   ‚îú‚îÄ‚îÄ processed/           # Cleaned data (not in git)\n‚îÇ   ‚îî‚îÄ‚îÄ sample/              # Small sample for testing (<10MB)\n‚îú‚îÄ‚îÄ notebooks/               # Analysis notebooks\n‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îú‚îÄ‚îÄ preprocessing.py     # Data cleaning pipeline\n‚îÇ   ‚îú‚îÄ‚îÄ features.py          # Feature engineering\n‚îÇ   ‚îú‚îÄ‚îÄ model.py             # Random Forest implementation\n‚îÇ   ‚îî‚îÄ‚îÄ evaluate.py          # Evaluation metrics\n‚îú‚îÄ‚îÄ models/                  # Saved models (not in git)\n‚îú‚îÄ‚îÄ results/                 # Output files\n‚îú‚îÄ‚îÄ tests/                   # Unit tests\n‚îú‚îÄ‚îÄ train_model.py           # Main training script\n‚îú‚îÄ‚îÄ requirements.txt         # Python dependencies\n‚îú‚îÄ‚îÄ .gitignore\n‚îî‚îÄ‚îÄ README.md               # This file\n```\n\n## Environment\n\n- Python 3.9.13\n- See `requirements.txt` for all dependencies\n- Tested on Ubuntu 20.04, macOS 12.0, Windows 11\n\n## Data\n\nDataset: 10,000 houses with 15 features\n- Source: [provide source]\n- License: [provide license]\n- Download: [provide link or instructions]\n- Place in `data/raw/houses.csv`\n\n## Reproducing Results\n\nTo exactly reproduce the published results:\n\n1. Use Python 3.9.13\n2. Install exact dependency versions from requirements.txt\n3. Run with seed=42 (default)\n4. Results may vary slightly (<1%) due to floating point precision\n\n## Citation\n\nIf you use this code, please cite:\n\n```\n@article{smith2024house,\n  title={Reproducible House Price Prediction},\n  author={Smith, Jane},\n  journal={Journal of ML Reproducibility},\n  year={2024}\n}\n```\n\n## License\n\nMIT License - see LICENSE file\n\"\"\"\n\nprint(readme_template)\n\n# Task 3: requirements.txt\nprint(\"\\n3. REQUIREMENTS.TXT\")\nprint(\"-\" * 80)\n\nrequirements = \"\"\"# requirements.txt\n# Python 3.9.13\n# Generated: 2024-01-15\n\n# Core\nnumpy==1.24.3\npandas==2.0.3\nscipy==1.11.1\n\n# Machine Learning\nscikit-learn==1.3.0\n\n# Utilities\njoblib==1.3.1\ntqdm==4.65.0\n\n# Testing\npytest==7.4.0\n\"\"\"\nprint(requirements)\n\n# Task 4: .gitignore\nprint(\"\\n4. .GITIGNORE\")\nprint(\"-\" * 80)\n\ngitignore = \"\"\"# Python\n__pycache__/\n*.py[cod]\n*$py.class\n*.so\n.Python\nvenv/\nenv/\nENV/\n\n# Jupyter\n.ipynb_checkpoints\n*_tested.ipynb\n\n# Data (ignore large files)\ndata/raw/\ndata/processed/\n*.csv\n*.parquet\n*.h5\n\n# Models\nmodels/*.pkl\nmodels/*.h5\nmodels/*.pt\n\n# Results\nresults/\nlogs/\n\n# IDE\n.vscode/\n.idea/\n*.swp\n\n# OS\n.DS_Store\nThumbs.db\n\n# Keep sample data\n!data/sample/*.csv\n\"\"\"\nprint(gitignore)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"‚úì Complete all 5 components for a reproducible research package\")\nprint(\"=\"*80)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 9. Exercise 4: Create a Complete Reproducibility Package\n\nYou've completed a machine learning project predicting house prices. Create a complete reproducibility package that includes:\n\n1. **NeurIPS Checklist** - Mark which items are complete\n2. **README.md** - Instructions for reproducing your results\n3. **requirements.txt** - All dependencies with exact versions\n4. **.gitignore** - What files to exclude from version control\n5. **Lab Notebook Entry** - Document your final experiment\n\n**Project Details:**\n- Model: Random Forest regression\n- Dataset: 10,000 houses, 15 features\n- Results: MAE = $23,450 ¬± $1,200 (95% CI: $21,050-$25,850, n=5 seeds)\n- Training time: 2.3 hours on Intel i7, 16GB RAM\n- Python 3.9, scikit-learn 1.3.0, pandas 2.0.3\n- Splits: 70% train, 15% val, 15% test (seed=42)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Version Control for Reproducible Research\n",
    "\n",
    "### Why Git Matters\n",
    "\n",
    "- Creates audit trails of changes\n",
    "- Enables rollback to working versions\n",
    "- Documents decisions through commit messages\n",
    "- Facilitates collaboration\n",
    "- Enables reproducibility at specific commits\n",
    "\n",
    "### Repository Best Practices\n",
    "\n",
    "**Track (‚úì):**\n",
    "- Notebooks without outputs\n",
    "- Scripts and source code\n",
    "- Sample data <10MB\n",
    "- README and documentation\n",
    "- Requirements.txt and environment.yml\n",
    "\n",
    "**Ignore (‚ùå):**\n",
    "- Notebook outputs\n",
    "- Large datasets >10MB\n",
    "- Virtual environments\n",
    "- Cache files\n",
    "- Credentials and secrets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Exercise 1: Create a Reproducibility Checklist\n",
    "\n",
    "You're submitting a paper 'Deep Learning for Time Series' to NeurIPS.\n",
    "\n",
    "**Current Status:**\n",
    "- ‚úì Code on GitHub\n",
    "- ‚úì Random seeds set\n",
    "- ‚úó No confidence intervals (only mean: 94.3%)\n",
    "- ‚úó Training time not documented (48 hours)\n",
    "- ‚úì Limitations described\n",
    "- ‚úó No setup instructions on GitHub\n",
    "\n",
    "**Task**: Analyze which reproducibility items are complete and what needs fixing before publication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('EXERCISE 1: REPRODUCIBILITY CHECKLIST')\n",
    "print('='*70)\n",
    "print('\\nReview the paper status above.')\n",
    "print('\\nQuestions to answer:')\n",
    "print('1. Which items are clearly complete?')\n",
    "print('2. What is preventing publication readiness?')\n",
    "print('3. What priority order for fixes?')\n",
    "print('4. Which checklist items need most work?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Exercise 2: Write a Data Dictionary\n",
    "\n",
    "Create a data dictionary for house price prediction with variables:\n",
    "- sqft: Square footage\n",
    "- beds: Number of bedrooms\n",
    "- price: Sale price\n",
    "- zip: Postal code\n",
    "- year: Year built\n",
    "- cond: Condition rating\n",
    "\n",
    "Include short names, long names, format, units, and definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('EXERCISE 2: DATA DICTIONARY')\n",
    "print('='*70)\n",
    "print('\\nCreate a DataFrame with columns:')\n",
    "print('- Short Name, Long Name, Format, Units, Definition')\n",
    "print('\\nFor variables: sqft, beds, price, zip, year, cond')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Exercise 3: Identify Data Leakage\n",
    "\n",
    "Analyze three scenarios:\n",
    "\n",
    "**Scenario A**: Impute missing values on full dataset, then split\n",
    "\n",
    "**Scenario B**: Split first, then select features using training only\n",
    "\n",
    "**Scenario C**: Use information from AFTER the prediction time\n",
    "\n",
    "For each: (1) Has leakage? (2) Why? (3) How to fix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('EXERCISE 3: DATA LEAKAGE ANALYSIS')\n",
    "print('='*70)\n",
    "print('\\nAnalyze each scenario for data leakage:')\n",
    "print('\\nScenario A: Preprocess all, then split')\n",
    "print('  - Has leakage? YES/NO')\n",
    "print('  - Why?')\n",
    "print('  - Fix?')\n",
    "print('\\nScenario B: Split, then feature select on training')\n",
    "print('  - Has leakage? YES/NO')\n",
    "print('  - Why?')\n",
    "print('  - Fix?')\n",
    "print('\\nScenario C: Use future information')\n",
    "print('  - Has leakage? YES/NO')\n",
    "print('  - Why?')\n",
    "print('  - Fix?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "‚úÖ **Reproducibility Crisis**: 70% of researchers can't reproduce others' work; data leakage is primary cause\n",
    "\n",
    "‚úÖ **NeurIPS Checklist**: Seven requirements (claims, limitations, experimental details, open access, settings, statistics, compute)\n",
    "\n",
    "‚úÖ **Documentation**: Electronic lab notebooks, data dictionaries, READMEs, data lineage\n",
    "\n",
    "‚úÖ **Environment**: Specify Python/library versions, set random seeds, document dependencies\n",
    "\n",
    "‚úÖ **Data Leakage Prevention**: Cardinal rule‚Äîfit preprocessing ONLY on training data\n",
    "\n",
    "‚úÖ **Version Control**: Use Git for audit trails, track code/data/docs, ignore outputs/secrets\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "**Module 07: Literature Review Methodologies** covers:\n",
    "- Systematic reviews (PRISMA 2020)\n",
    "- Scoping reviews (JBI methodology)\n",
    "- Meta-analysis techniques\n",
    "- Risk of bias assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Assessment\n",
    "\n",
    "Before Module 07, ensure you can:\n",
    "\n",
    "- [ ] Explain the reproducibility crisis and causes\n",
    "- [ ] Apply the NeurIPS seven-item checklist\n",
    "- [ ] Create comprehensive data dictionaries\n",
    "- [ ] Document data lineage and transformations\n",
    "- [ ] Set up reproducible environments\n",
    "- [ ] Prevent data leakage in preprocessing\n",
    "- [ ] Use version control for research\n",
    "- [ ] Write clear README files\n",
    "- [ ] Report results with uncertainty measures\n",
    "\n",
    "If all boxes checked, you're ready for Module 07! üéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}