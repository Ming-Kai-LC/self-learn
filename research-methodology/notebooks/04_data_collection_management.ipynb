{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 04: Data Collection and Management\n",
    "\n",
    "**Difficulty**: \u2b50\u2b50 (Intermediate)\n",
    "\n",
    "**Estimated Time**: 60 minutes\n",
    "\n",
    "**Prerequisites**: [Module 03: Research Design and Hypothesis Testing](03_research_design_hypothesis_testing.ipynb)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. **Design data collection strategies** by choosing between primary and secondary data sources\n",
    "2. **Assess data quality** using completeness, validity, and reliability metrics\n",
    "3. **Create data documentation** including data dictionaries and lineage tracking\n",
    "4. **Implement data lineage tracking** to document transformations and versions\n",
    "5. **Follow metadata standards** for reproducible and transparent research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's import the libraries we'll use in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard data science libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuration for better visualizations\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úì Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Acquisition Strategies\n",
    "\n",
    "### Primary vs. Secondary Data\n",
    "\n",
    "When designing your research, the first decision is **where your data comes from**. Understanding the trade-offs between primary and secondary data is crucial for efficient research design.\n",
    "\n",
    "| Aspect | Primary Data | Secondary Data |\n",
    "|--------|-------------|----------------|\n",
    "| **Definition** | Data you collect yourself | Data collected by others, already available |\n",
    "| **Sources** | Surveys, interviews, experiments, sensors | Existing databases, published reports, public datasets |\n",
    "| **Advantages** | Tailored to research question, fresh and current | Cost-effective, faster, large-scale available |\n",
    "| **Disadvantages** | Time-consuming, expensive, biased by design choices | May not fit research question, quality unknown |\n",
    "| **Examples** | Customer surveys, lab measurements | Census data, social media, government statistics |\n",
    "| **Quality Control** | You design validation protocols | Limited control, must assess existing quality |\n",
    "\n",
    "### Sampling Methods for Primary Data\n",
    "\n",
    "When collecting primary data, **sampling method** critically affects validity:\n",
    "\n",
    "**1. Random Sampling**\n",
    "- Every population member has equal selection probability\n",
    "- Use when: Population is homogeneous and accessible\n",
    "- Advantage: Unbiased, theoretically sound\n",
    "- Disadvantage: May miss rare subgroups\n",
    "\n",
    "**2. Stratified Sampling**\n",
    "- Divide population into strata (subgroups), sample each\n",
    "- Use when: Important subgroups exist (age, income, region)\n",
    "- Advantage: Ensures all subgroups represented\n",
    "- Disadvantage: More complex, requires population structure knowledge\n",
    "\n",
    "**3. Cluster Sampling**\n",
    "- Divide population into clusters, randomly select clusters\n",
    "- Use when: Natural groupings exist and population is dispersed\n",
    "- Advantage: Efficient for geographically dispersed populations\n",
    "- Disadvantage: Less precise, cluster homogeneity assumed\n",
    "\n",
    "**4. Convenience Sampling**\n",
    "- Sample from readily available population members\n",
    "- Use when: Quick, preliminary insights needed\n",
    "- Advantage: Fast and inexpensive\n",
    "- Disadvantage: Highly biased, results not generalizable\n",
    "\n",
    "### Data Validation at Collection\n",
    "\n",
    "**Validation at the point of collection** prevents errors from accumulating:\n",
    "\n",
    "- **Range checks**: Verify values fall within expected bounds\n",
    "- **Type validation**: Ensure correct data types (numeric, date, categorical)\n",
    "- **Referential integrity**: Foreign keys reference existing records\n",
    "- **Uniqueness constraints**: Prevent duplicate entries where required\n",
    "- **Mandatory fields**: Ensure required data is present\n",
    "- **Format validation**: Phone numbers, emails, dates match expected patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Building a data validation function at collection time\n",
    "\n",
    "class DataValidator:\n",
    "    \"\"\"Validates data according to specified rules.\"\"\"\n",
    "    \n",
    "    def __init__(self, validation_rules):\n",
    "        \"\"\"\n",
    "        Initialize validator with rules.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        validation_rules : dict\n",
    "            Dictionary mapping column names to validation functions\n",
    "        \"\"\"\n",
    "        self.validation_rules = validation_rules\n",
    "        self.validation_report = {}\n",
    "    \n",
    "    def validate_record(self, record):\n",
    "        \"\"\"Validate a single record against all rules.\"\"\"\n",
    "        errors = []\n",
    "        \n",
    "        for field, rule in self.validation_rules.items():\n",
    "            if field not in record:\n",
    "                errors.append(f\"Missing required field: {field}\")\n",
    "                continue\n",
    "            \n",
    "            value = record[field]\n",
    "            try:\n",
    "                is_valid = rule(value)\n",
    "                if not is_valid:\n",
    "                    errors.append(f\"Invalid value for {field}: {value}\")\n",
    "            except Exception as e:\n",
    "                errors.append(f\"Validation error in {field}: {str(e)}\")\n",
    "        \n",
    "        return len(errors) == 0, errors\n",
    "    \n",
    "    def validate_dataset(self, records):\n",
    "        \"\"\"Validate multiple records and generate report.\"\"\"\n",
    "        valid_count = 0\n",
    "        invalid_records = []\n",
    "        \n",
    "        for idx, record in enumerate(records):\n",
    "            is_valid, errors = self.validate_record(record)\n",
    "            \n",
    "            if is_valid:\n",
    "                valid_count += 1\n",
    "            else:\n",
    "                invalid_records.append({\n",
    "                    'record_index': idx,\n",
    "                    'record': record,\n",
    "                    'errors': errors\n",
    "                })\n",
    "        \n",
    "        # Generate validation report\n",
    "        total_records = len(records)\n",
    "        validation_rate = (valid_count / total_records * 100) if total_records > 0 else 0\n",
    "        \n",
    "        self.validation_report = {\n",
    "            'total_records': total_records,\n",
    "            'valid_records': valid_count,\n",
    "            'invalid_records': len(invalid_records),\n",
    "            'validation_rate': validation_rate,\n",
    "            'invalid_details': invalid_records\n",
    "        }\n",
    "        \n",
    "        return self.validation_report\n",
    "    \n",
    "    def print_report(self):\n",
    "        \"\"\"Print validation report in readable format.\"\"\"\n",
    "        print(\"\\nDATA VALIDATION REPORT\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Total Records: {self.validation_report['total_records']}\")\n",
    "        print(f\"Valid Records: {self.validation_report['valid_records']}\")\n",
    "        print(f\"Invalid Records: {self.validation_report['invalid_records']}\")\n",
    "        print(f\"Validation Rate: {self.validation_report['validation_rate']:.1f}%\")\n",
    "        \n",
    "        if self.validation_report['invalid_details']:\n",
    "            print(f\"\\nInvalid Record Details:\")\n",
    "            for detail in self.validation_report['invalid_details']:\n",
    "                print(f\"  Record {detail['record_index']}:\")\n",
    "                for error in detail['errors']:\n",
    "                    print(f\"    - {error}\")\n",
    "\n",
    "# Define validation rules for customer survey data\n",
    "validation_rules = {\n",
    "    'customer_id': lambda x: isinstance(x, int) and x > 0,\n",
    "    'age': lambda x: isinstance(x, int) and 18 <= x <= 120,\n",
    "    'email': lambda x: isinstance(x, str) and '@' in x,\n",
    "    'satisfaction': lambda x: isinstance(x, int) and 1 <= x <= 5,\n",
    "    'product_category': lambda x: x in ['Electronics', 'Clothing', 'Home', 'Books']\n",
    "}\n",
    "\n",
    "# Sample survey data (with some invalid records)\n",
    "survey_records = [\n",
    "    {'customer_id': 1, 'age': 32, 'email': 'john@example.com', 'satisfaction': 4, 'product_category': 'Electronics'},\n",
    "    {'customer_id': 2, 'age': 150, 'email': 'jane@example.com', 'satisfaction': 5, 'product_category': 'Clothing'},  # Invalid age\n",
    "    {'customer_id': 3, 'age': 45, 'email': 'no-at-sign', 'satisfaction': 3, 'product_category': 'Home'},  # Invalid email\n",
    "    {'customer_id': 4, 'age': 28, 'email': 'bob@example.com', 'satisfaction': 6, 'product_category': 'Books'},  # Invalid satisfaction\n",
    "    {'customer_id': 5, 'age': 35, 'email': 'alice@example.com', 'satisfaction': 4, 'product_category': 'Electronics'},\n",
    "]\n",
    "\n",
    "# Validate data\n",
    "validator = DataValidator(validation_rules)\n",
    "report = validator.validate_dataset(survey_records)\n",
    "validator.print_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Quality Assessment\n",
    "\n",
    "### Understanding Data Quality Dimensions\n",
    "\n",
    "High-quality data is the foundation of rigorous research. The major dimensions of data quality are:\n",
    "\n",
    "**1. Completeness**\n",
    "- What percentage of required data is present?\n",
    "- Missing values can introduce bias and reduce statistical power\n",
    "- Calculate: `missing_percentage = (missing_values / total_records) * 100`\n",
    "\n",
    "**2. Validity**\n",
    "- Are values correct type and within expected ranges?\n",
    "- Invalid data cannot be reliably analyzed\n",
    "- Examples: Age as negative number, date in impossible format\n",
    "\n",
    "**3. Reliability**\n",
    "- Are measurements consistent across time and conditions?\n",
    "- Unreliable data shows high variability from measurement error\n",
    "- Measured through test-retest correlation, inter-rater agreement\n",
    "\n",
    "**4. Bias Detection**\n",
    "- Are there systematic errors or non-random patterns?\n",
    "- Bias can invalidate conclusions even if data is otherwise complete\n",
    "- Examples: Selection bias (only certain populations represented), measurement bias (systematic over/underestimation)\n",
    "\n",
    "### Critical Rule: Never Trust Aggregate Statistics Alone\n",
    "\n",
    "Anscombe's quartet shows why you must visualize data: four different datasets have identical summary statistics but completely different structures!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anscombe's Quartet: Identical summary stats, different distributions\n",
    "\n",
    "# Create Anscombe's four datasets\n",
    "anscombe_data = pd.DataFrame({\n",
    "    'dataset_1_x': [10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5],\n",
    "    'dataset_1_y': [8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68],\n",
    "    'dataset_2_x': [10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5],\n",
    "    'dataset_2_y': [9.14, 8.14, 8.74, 8.77, 9.26, 8.10, 6.13, 3.10, 9.13, 7.26, 4.74],\n",
    "    'dataset_3_x': [10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5],\n",
    "    'dataset_3_y': [7.46, 6.77, 12.74, 7.11, 7.81, 8.84, 6.08, 5.39, 8.15, 6.42, 5.73],\n",
    "    'dataset_4_x': [8, 8, 8, 8, 8, 8, 8, 19, 8, 8, 8],\n",
    "    'dataset_4_y': [6.58, 5.76, 7.71, 8.84, 8.47, 7.04, 5.25, 12.50, 5.56, 7.91, 6.89]\n",
    "})\n",
    "\n",
    "# Calculate summary statistics for each dataset\nprint(\"\\nSUMMARY STATISTICS FOR ANSCOMBE'S QUARTET\")\nprint(\"=\" * 70)\n\nfor i in range(1, 5):\n",
    "    x = anscombe_data[f'dataset_{i}_x']\n",
    "    y = anscombe_data[f'dataset_{i}_y']\n",
    "    correlation = x.corr(y)\n",
    "    \n",
    "    print(f\"\\nDataset {i}:\")\n",
    "    print(f\"  X mean: {x.mean():.2f},  Y mean: {y.mean():.2f}\")\n",
    "    print(f\"  X std:  {x.std():.2f},  Y std:  {y.std():.2f}\")\n",
    "    print(f\"  Correlation (r): {correlation:.3f}\")\n",
    "\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Notice: All four datasets have IDENTICAL summary statistics!\")\nprint(\"This is why visualization is critical for data quality assessment.\")\n\n# Visualize the datasets\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\naxes = axes.ravel()\n\nfor i in range(1, 5):\n",
    "    x = anscombe_data[f'dataset_{i}_x']\n",
    "    y = anscombe_data[f'dataset_{i}_y']\n",
    "    \n",
    "    axes[i-1].scatter(x, y, s=100, alpha=0.6, edgecolors='black')\n",
    "    axes[i-1].set_xlim(3, 20)\n",
    "    axes[i-1].set_ylim(3, 14)\n",
    "    axes[i-1].set_xlabel('X', fontsize=12)\n",
    "    axes[i-1].set_ylabel('Y', fontsize=12)\n",
    "    axes[i-1].set_title(f'Dataset {i}', fontsize=12, fontweight='bold')\n",
    "    axes[i-1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add trend line\n",
    "    z = np.polyfit(x, y, 1)\n",
    "    p = np.poly1d(z)\n",
    "    axes[i-1].plot(x, p(x), \"r--\", alpha=0.8, linewidth=2)\n",
    "\nplt.tight_layout()\nplt.suptitle(\"Anscombe's Quartet: Different Data, Same Statistics!\", \n",
    "             fontsize=14, fontweight='bold', y=1.00)\nplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Data Quality Assessment Function\n",
    "\n",
    "class DataQualityAssessment:\n",
    "    \"\"\"Assess data quality across multiple dimensions.\"\"\"\n",
    "    \n",
    "    def __init__(self, dataframe):\n",
    "        \"\"\"Initialize with a pandas DataFrame.\"\"\"\n",
    "        self.df = dataframe\n",
    "        self.quality_report = {}\n",
    "    \n",
    "    def assess_completeness(self):\n",
    "        \"\"\"Calculate missing data percentage by column.\"\"\"\n",
    "        completeness = {}\n",
    "        \n",
    "        for column in self.df.columns:\n",
    "            total = len(self.df)\n",
    "            missing = self.df[column].isna().sum()\n",
    "            missing_pct = (missing / total) * 100\n",
    "            completeness[column] = {\n",
    "                'missing_count': missing,\n",
    "                'missing_percentage': missing_pct,\n",
    "                'complete_percentage': 100 - missing_pct\n",
    "            }\n",
    "        \n",
    "        return completeness\n",
    "    \n",
    "    def assess_validity(self):\n",
    "        \"\"\"Check for invalid data types and ranges.\"\"\"\n",
    "        validity = {}\n",
    "        \n",
    "        for column in self.df.columns:\n",
    "            validity[column] = {\n",
    "                'dtype': str(self.df[column].dtype),\n",
    "                'unique_values': self.df[column].nunique(),\n",
    "                'min_value': self.df[column].min() if pd.api.types.is_numeric_dtype(self.df[column]) else 'N/A',\n",
    "                'max_value': self.df[column].max() if pd.api.types.is_numeric_dtype(self.df[column]) else 'N/A',\n",
    "                'sample_values': self.df[column].head(3).tolist()\n",
    "            }\n",
    "        \n",
    "        return validity\n",
    "    \n",
    "    def assess_duplicates(self):\n",
    "        \"\"\"Identify duplicate records.\"\"\"\n",
    "        total_records = len(self.df)\n",
    "        duplicate_rows = self.df.duplicated().sum()\n",
    "        duplicate_percentage = (duplicate_rows / total_records) * 100\n",
    "        \n",
    "        # Also check for duplicate IDs if 'id' column exists\n",
    "        id_duplicates = {}\n",
    "        for col in self.df.columns:\n",
    "            if 'id' in col.lower():\n",
    "                duplicate_ids = self.df[col].duplicated().sum()\n",
    "                id_duplicates[col] = duplicate_ids\n",
    "        \n",
    "        return {\n",
    "            'duplicate_rows': duplicate_rows,\n",
    "            'duplicate_percentage': duplicate_percentage,\n",
    "            'duplicate_ids': id_duplicates\n",
    "        }\n",
    "    \n",
    "    def generate_quality_report(self):\n",
    "        \"\"\"Generate comprehensive quality report.\"\"\"\n",
    "        self.quality_report = {\n",
    "            'dataset_shape': self.df.shape,\n",
    "            'completeness': self.assess_completeness(),\n",
    "            'validity': self.assess_validity(),\n",
    "            'duplicates': self.assess_duplicates()\n",
    "        }\n",
    "        \n",
    "        return self.quality_report\n",
    "    \n",
    "    def print_report(self):\n",
    "        \"\"\"Print quality report in readable format.\"\"\"\n",
    "        if not self.quality_report:\n",
    "            self.generate_quality_report()\n",
    "        \n",
    "        print(\"\\nDATA QUALITY ASSESSMENT REPORT\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        shape = self.quality_report['dataset_shape']\n",
    "        print(f\"Dataset Size: {shape[0]} rows √ó {shape[1]} columns\")\n",
    "        \n",
    "        print(\"\\n\" + \"COMPLETENESS ANALYSIS\" + \"\\n\" + \"-\" * 70)\n",
    "        completeness_df = pd.DataFrame(self.quality_report['completeness']).T\n",
    "        print(completeness_df.to_string())\n",
    "        \n",
    "        print(\"\\n\" + \"DUPLICATE ANALYSIS\" + \"\\n\" + \"-\" * 70)\n",
    "        dup = self.quality_report['duplicates']\n",
    "        print(f\"Duplicate Rows: {dup['duplicate_rows']} ({dup['duplicate_percentage']:.1f}%)\")\n",
    "        if dup['duplicate_ids']:\n",
    "            print(\"Duplicate IDs by column:\")\n",
    "            for col, count in dup['duplicate_ids'].items():\n",
    "                print(f\"  {col}: {count} duplicates\")\n",
    "\n",
    "# Create sample dataset with quality issues\n",
    "np.random.seed(42)\nsample_data = pd.DataFrame({\n",
    "    'customer_id': [1, 2, 3, 4, 5, 5, 6, 7, np.nan, 9],  # Has duplicate and missing\n",
    "    'age': [25, 32, 28, 45, 35, 35, 52, 41, 38, 29],  # All valid\n",
    "    'income': [50000, 75000, np.nan, 120000, 65000, 65000, 95000, 85000, 72000, 58000],  # Missing\n",
    "    'email': ['john@example.com', 'jane@example.com', 'invalid-email', 'bob@example.com',\n",
    "              'alice@example.com', 'alice@example.com', 'charlie@example.com', \n",
    "              'david@example.com', 'eve@example.com', 'frank@example.com'],\n",
    "    'satisfaction': [4, 5, 3, 4, 4, 4, 5, 3, 2, 4]  # Valid ratings\n",
    "})\n",
    "\n",
    "# Run quality assessment\n",
    "qa = DataQualityAssessment(sample_data)\n",
    "qa.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing data patterns\n",
    "\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left plot: Missing data heatmap\n",
    "missing_matrix = sample_data.isna().astype(int)\n",
    "sns.heatmap(missing_matrix.T, cbar=True, cmap='RdYlGn_r', ax=axes[0], \n",
    "            yticklabels=True, xticklabels=False)\n",
    "axes[0].set_title('Missing Data Pattern\\n(Red = Missing, Green = Present)', \n",
    "                   fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Record Index', fontsize=11)\n",
    "axes[0].set_ylabel('Column', fontsize=11)\n",
    "\n",
    "# Right plot: Missing percentage by column\n",
    "missing_pct = sample_data.isna().sum() / len(sample_data) * 100\n",
    "colors = ['red' if x > 0 else 'green' for x in missing_pct]\n",
    "missing_pct.plot(kind='barh', ax=axes[1], color=colors, edgecolor='black')\n",
    "axes[1].set_title('Missing Data Percentage by Column', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Missing Percentage (%)', fontsize=11)\n",
    "axes[1].set_xlim(0, 25)\n",
    "\n",
    "# Add percentage labels on bars\n",
    "for i, v in enumerate(missing_pct):\n",
    "    if v > 0:\n",
    "        axes[1].text(v + 0.5, i, f'{v:.1f}%', va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Quality Insight: Missing data patterns can reveal systematic collection issues.\")\n",
    "print(\"For example, if an entire column is missing, it may indicate a collection failure.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Documentation and Metadata Standards\n",
    "\n",
    "### Why Data Documentation Matters\n",
    "\n",
    "**Data documentation** serves as the \"instruction manual\" for your dataset. Without it:\n",
    "- Future you won't remember what column `X15` means\n",
    "- Collaborators can't understand your data structure\n",
    "- Errors go undetected because assumptions are implicit\n",
    "- Research becomes irreproducible\n",
    "\n",
    "### Essential Documentation Components\n",
    "\n",
    "**1. Data Dictionary**\n",
    "- Lists every column, its type, valid values, and meaning\n",
    "- Includes units (kg, days, USD) and measurement scales\n",
    "- Documents any transformations applied\n",
    "\n",
    "**2. Data Lineage**\n",
    "- Tracks where data came from (source)\n",
    "- Documents all transformations applied\n",
    "- Records versions and timestamps\n",
    "- Enables reproducibility and auditing\n",
    "\n",
    "**3. README File**\n",
    "- High-level overview of the dataset\n",
    "- How to use the data\n",
    "- Known limitations and caveats\n",
    "- Contact information for questions\n",
    "\n",
    "**4. Metadata Standards**\n",
    "- Follow established standards (DDI, ISO 19115, MIAOU)\n",
    "- Enable machine-readable metadata\n",
    "- Facilitate data discovery and integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive Data Dictionary template and generator\n",
    "\nclass DataDictionary:\n",
    "    \"\"\"Create and manage data dictionaries for datasets.\"\"\"\n",
    "    \n",
    "    def __init__(self, dataframe, dataset_name):\n",
    "        \"\"\"Initialize with a DataFrame and dataset name.\"\"\"\n",
    "        self.df = dataframe\n",
    "        self.dataset_name = dataset_name\n",
    "        self.dictionary = {}\n",
    "    \n",
    "    def add_column_definition(self, column_name, description, data_type, \n",
    "                             units='', valid_values=None, constraints=''):\n",
    "        \"\"\"Add definition for a column.\"\"\"\n",
    "        self.dictionary[column_name] = {\n",
    "            'description': description,\n",
    "            'data_type': data_type,\n",
    "            'units': units,\n",
    "            'valid_values': valid_values,\n",
    "            'constraints': constraints,\n",
    "            'column_type': str(self.df[column_name].dtype) if column_name in self.df.columns else 'N/A'\n",
    "        }\n",
    "    \n",
    "    def generate_from_dataframe(self):\n",
    "        \"\"\"Auto-generate basic data dictionary from DataFrame.\"\"\"\n",
    "        for column in self.df.columns:\n",
    "            dtype = str(self.df[column].dtype)\n",
    "            unique_count = self.df[column].nunique()\n",
    "            missing_count = self.df[column].isna().sum()\n",
    "            \n",
    "            self.dictionary[column] = {\n",
    "                'description': f'[Description needed for {column}]',\n",
    "                'data_type': dtype,\n",
    "                'units': '',\n",
    "                'valid_values': f'Unique values: {unique_count}',\n",
    "                'constraints': f'Missing: {missing_count}',\n",
    "                'sample_values': self.df[column].dropna().head(2).tolist()\n",
    "            }\n",
    "    \n",
    "    def to_json(self):\n",
    "        \"\"\"Export dictionary as JSON.\"\"\"\n",
    "        return json.dumps(self.dictionary, indent=2, default=str)\n",
    "    \n",
    "    def to_dataframe(self):\n",
    "        \"\"\"Convert dictionary to DataFrame for display.\"\"\"\n",
    "        dict_list = []\n",
    "        for col_name, col_info in self.dictionary.items():\n",
    "            row = {'Column Name': col_name}\n",
    "            row.update(col_info)\n",
    "            dict_list.append(row)\n",
    "        return pd.DataFrame(dict_list)\n",
    "    \n",
    "    def print_dictionary(self):\n",
    "        \"\"\"Print formatted data dictionary.\"\"\"\n",
    "        print(f\"\\nDATA DICTIONARY: {self.dataset_name.upper()}\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        for column, info in self.dictionary.items():\n",
    "            print(f\"\\nüìã {column}\")\n",
    "            print(f\"   Description: {info.get('description', 'N/A')}\")\n",
    "            print(f\"   Data Type: {info.get('data_type', 'N/A')}\")\n",
    "            print(f\"   Units: {info.get('units', 'None')}\")\n",
    "            print(f\"   Valid Values: {info.get('valid_values', 'N/A')}\")\n",
    "            print(f\"   Constraints: {info.get('constraints', 'None')}\")\n",
    "            if 'sample_values' in info:\n",
    "                print(f\"   Sample Values: {info.get('sample_values')}\")\n",
    "\n",
    "# Create and populate data dictionary\ndd = DataDictionary(sample_data, 'Customer Survey Dataset')\n",
    "\n# Manually add detailed descriptions\ndd.add_column_definition(\n",
    "    'customer_id',\n",
    "    'Unique identifier for each customer',\n",
    "    'integer',\n",
    "    '',\n",
    "    'Positive integers only',\n",
    "    'Primary key, must be unique'\n",
    ")\n",
    "\ndd.add_column_definition(\n",
    "    'age',\n",
    "    'Age of customer in years',\n",
    "    'integer',\n",
    "    'years',\n",
    "    '18-120',\n",
    "    'Must be >= 18 and <= 120'\n",
    ")\n",
    "\ndd.add_column_definition(\n",
    "    'income',\n",
    "    'Annual household income',\n",
    "    'float',\n",
    "    'USD',\n",
    "    '> 0',\n",
    "    'Must be positive, in dollars'\n",
    ")\n",
    "\ndd.add_column_definition(\n",
    "    'email',\n",
    "    'Customer email address',\n",
    "    'string',\n",
    "    '',\n",
    "    'Valid email format',\n",
    "    'Must contain @ symbol, unique per customer'\n",
    ")\n",
    "\ndd.add_column_definition(\n",
    "    'satisfaction',\n",
    "    'Customer satisfaction rating',\n",
    "    'integer',\n",
    "    'Likert scale (1-5)',\n",
    "    '1 (very dissatisfied) to 5 (very satisfied)',\n",
    "    'Must be integer 1-5'\n",
    ")\n",
    "\n# Print the dictionary\ndd.print_dictionary()\n",
    "\nprint(\"\\n\" + \"=\" * 80)\nprint(\"\\nüìä Data Dictionary as Table:\")\nprint(dd.to_dataframe().to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Lineage Tracking\n",
    "\nclass DataLineage:\n",
    "    \"\"\"Track data transformations and versions for reproducibility.\"\"\"\n",
    "    \n",
    "    def __init__(self, source_name):\n",
    "        \"\"\"Initialize lineage tracking.\"\"\"\n",
    "        self.source_name = source_name\n",
    "        self.lineage_log = []\n",
    "        \n",
    "        # Log the source\n",
    "        self.log_step(\n",
    "            step_type='SOURCE',\n",
    "            description=f'Data source: {source_name}',\n",
    "            input_records=None,\n",
    "            output_records=None,\n",
    "            transformation='Initial data load'\n",
    "        )\n",
    "    \n",
    "    def log_step(self, step_type, description, input_records=None, \n",
    "                 output_records=None, transformation=''):\n",
    "        \"\"\"Log a transformation step.\"\"\"\n",
    "        log_entry = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'step_type': step_type,\n",
    "            'description': description,\n",
    "            'input_records': input_records,\n",
    "            'output_records': output_records,\n",
    "            'transformation': transformation\n",
    "        }\n",
    "        self.lineage_log.append(log_entry)\n",
    "    \n",
    "    def print_lineage(self):\n",
    "        \"\"\"Print data lineage in readable format.\"\"\"\n",
    "        print(f\"\\nDATA LINEAGE: {self.source_name}\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        for i, entry in enumerate(self.lineage_log, 1):\n",
    "            print(f\"\\nStep {i}: {entry['step_type']}\")\n",
    "            print(f\"  Timestamp: {entry['timestamp']}\")\n",
    "            print(f\"  Description: {entry['description']}\")\n",
    "            if entry['input_records']:\n",
    "                print(f\"  Input Records: {entry['input_records']}\")\n",
    "            if entry['output_records']:\n",
    "                print(f\"  Output Records: {entry['output_records']}\")\n",
    "            if entry['transformation']:\n",
    "                print(f\"  Transformation: {entry['transformation']}\")\n",
    "    \n",
    "    def to_json(self):\n",
    "        \"\"\"Export lineage as JSON for version control.\"\"\"\n",
    "        return json.dumps(self.lineage_log, indent=2, default=str)\n",
    "\n",
    "# Create lineage for sample dataset\nlineage = DataLineage('Customer Survey Data v1.0')\n",
    "\n# Log each transformation step\nlineage.log_step(\n",
    "    step_type='VALIDATION',\n",
    "    description='Validated email formats and age ranges',\n",
    "    input_records=10,\n",
    "    output_records=10,\n",
    "    transformation='Applied regex validation to email column, range check on age column'\n",
    ")\n",
    "\nlineage.log_step(\n",
    "    step_type='CLEANING',\n",
    "    description='Removed duplicate records',\n",
    "    input_records=10,\n",
    "    output_records=9,\n",
    "    transformation='Dropped rows where customer_id appears multiple times'\n",
    ")\n",
    "\nlineage.log_step(\n",
    "    step_type='IMPUTATION',\n",
    "    description='Handled missing income values',\n",
    "    input_records=9,\n",
    "    output_records=9,\n",
    "    transformation='Applied median imputation to income column (missing: 1 value)'\n",
    ")\n",
    "\nlineage.log_step(\n",
    "    step_type='ENCODING',\n",
    "    description='Encoded categorical variables',\n",
    "    input_records=9,\n",
    "    output_records=9,\n",
    "    transformation='No categorical variables in this dataset'\n",
    ")\n",
    "\nlineage.log_step(\n",
    "    step_type='EXPORT',\n",
    "    description='Final processed dataset',\n",
    "    input_records=9,\n",
    "    output_records=9,\n",
    "    transformation='Exported to CSV format for analysis'\n",
    ")\n",
    "\n# Print lineage\nlineage.print_lineage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing Workflow\n",
    "\n",
    "### The 7-Step Preprocessing Pipeline\n",
    "\n",
    "A robust preprocessing workflow ensures data quality and reproducibility:\n",
    "\n",
    "1. **Data Loading and Initial Inspection**\n",
    "   - Load data with proper encoding\n",
    "   - Check shape, dtypes, and first few rows\n",
    "   - Document source and version\n",
    "\n",
    "2. **Exploratory Data Analysis (EDA)**\n",
    "   - Understand distributions and relationships\n",
    "   - Identify outliers and anomalies\n",
    "   - Check for missing patterns\n",
    "\n",
    "3. **Data Validation**\n",
    "   - Check data types are correct\n",
    "   - Verify values fall within expected ranges\n",
    "   - Identify invalid or suspicious records\n",
    "\n",
    "4. **Handling Missing Data**\n",
    "   - Document why data is missing (MCAR, MAR, MNAR)\n",
    "   - Decide on strategy: drop, impute, or flag\n",
    "   - Apply only to training data, use same approach for test data\n",
    "\n",
    "5. **Outlier Detection and Handling**\n",
    "   - Use statistical methods (z-score, IQR) or domain knowledge\n",
    "   - Decide: remove, transform, or flag\n",
    "   - Document decisions for transparency\n",
    "\n",
    "6. **Feature Engineering and Transformation**\n",
    "   - Create new features from existing ones\n",
    "   - Normalize/standardize numeric features\n",
    "   - Encode categorical variables\n",
    "   - **CRITICAL**: Fit transformers ONLY on training data\n",
    "\n",
    "7. **Quality Check and Export**\n",
    "   - Verify preprocessing hasn't introduced errors\n",
    "   - Check data quality metrics again\n",
    "   - Export with versioning and metadata\n",
    "\n",
    "### THE CRITICAL RULE: Fit Only On Training Data\n",
    "\n",
    "This is the #1 source of data leakage in ML projects:\n",
    "\n",
    "**WRONG** (causes leakage):\n",
    "```python\n",
    "# Scale entire dataset first\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_all)  # Uses ALL data\n",
    "X_train = X_scaled[:split_idx]\n",
    "X_test = X_scaled[split_idx:]  # Test data influenced by entire dataset\n",
    "```\n",
    "\n",
    "**CORRECT** (no leakage):\n",
    "```python\n",
    "# Split FIRST\n",
    "X_train, X_test = train_test_split(X, test_size=0.2)\n",
    "# Then fit on training data only\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)  # Uses ONLY training data\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)  # Consistent transformation\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration of correct preprocessing with proper data handling\n",
    "\nfrom sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\nprint(\"\\n\" + \"=\"*70)\nprint(\"CORRECT PREPROCESSING WORKFLOW: No Data Leakage\")\nprint(\"=\"*70)\n",
    "\n# Create synthetic dataset\nprint(\"\\n1Ô∏è‚É£  STEP 1: Load and Inspect Data\")\nnp.random.seed(42)\n",
    "X = np.random.randn(100, 3) * [10, 50, 100]  # Different scales\n",
    "y = (X[:, 0] + X[:, 1] * 0.5 + X[:, 2] * 0.1 + np.random.randn(100) * 5 > 50).astype(int)\n",
    "\nprint(f\"Dataset shape: {X.shape}\")\nprint(f\"Feature means: {X.mean(axis=0).round(2)}\")\nprint(f\"Feature stds: {X.std(axis=0).round(2)}\")\nprint(\"   ‚ö†Ô∏è Notice: Features have very different scales!\")\n",
    "\nprint(\"\\n2Ô∏è‚É£  STEP 2: Train-Test Split (BEFORE ANY FITTING)\")\nX_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\nprint(f\"Training set size: {X_train.shape[0]}\")\nprint(f\"Test set size: {X_test.shape[0]}\")\nprint(\"   ‚úì Data split BEFORE any transformations - no leakage possible\")\n",
    "\nprint(\"\\n3Ô∏è‚É£  STEP 3: Fit Scaler ONLY on Training Data\")\nscaler = StandardScaler()\nscaler.fit(X_train)  # Fit ONLY on training data\nprint(f\"Scaler parameters computed from training data:\")\nprint(f\"   Training mean: {scaler.mean_.round(2)}\")\nprint(f\"   Training std: {scaler.scale_.round(2)}\")\nprint(\"   ‚úì Scaler parameters based ONLY on training data\")\n",
    "\nprint(\"\\n4Ô∏è‚É£  STEP 4: Transform Both Sets Using Training Parameters\")\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nprint(f\"Training data after scaling:\")\nprint(f\"   Mean: {X_train_scaled.mean(axis=0).round(3)}\")\nprint(f\"   Std: {X_train_scaled.std(axis=0).round(3)}\")\nprint(f\"\\nTest data after scaling:\")\nprint(f\"   Mean: {X_test_scaled.mean(axis=0).round(3)}\")\nprint(f\"   Std: {X_test_scaled.std(axis=0).round(3)}\")\nprint(\"   ‚úì Test data mean/std ‚â† 0/1 because it wasn't used in fitting\")\nprint(\"   ‚úì This is CORRECT - test data should be scaled using training parameters\")\n",
    "\nprint(\"\\n\" + \"=\"*70)\nprint(\"KEY PRINCIPLE: Fit all transformers on training data only!\")\nprint(\"This includes: scaling, imputation, encoding, feature selection\")\nprint(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Assessing Data Quality\n",
    "\n",
    "You've been given a new dataset for analysis. Your task is to:\n",
    "\n",
    "1. **Load the data** and perform initial inspection\n",
    "2. **Assess completeness**: Calculate missing percentage for each column\n",
    "3. **Assess validity**: Check for out-of-range values\n",
    "4. **Identify duplicates**: Find any duplicate records\n",
    "5. **Document findings** in a short report\n",
    "\n",
    "Below is a dataset with intentional quality issues. Complete the assessment:\n"   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Data Quality Assessment\n",
    "\n",
    "# Create a problematic dataset (intentionally with quality issues)\n",
    "exercise_data = pd.DataFrame({\n",
    "    'student_id': [101, 102, 103, 104, 105, 105, 107, 108, np.nan, 110],\n",
    "    'test_score': [85, 92, np.nan, 78, 88, 88, 95, 1000, 76, 82],  # 1000 is invalid\n",
    "    'study_hours': [5, 6.5, 4, np.nan, 7, 7, 8, 3, 4.5, 6],\n",
    "    'grade': ['A', 'A', 'B', 'C', 'B', 'B', 'A', 'F', 'C', 'B'],\n",
    "    'attendance_pct': [95, 100, 85, 70, 92, 92, 98, 150, 88, 91]  # 150% is invalid\n",
    "})\n",
    "\n",
    "print(\"EXERCISE 1: Data Quality Assessment\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nDataset Overview:\")\n",
    "print(exercise_data)\n",
    "\n",
    "print(\"\\nTODO: Complete the following assessment:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# TODO 1: Calculate completeness for each column\nprint(\"\\n1. COMPLETENESS ANALYSIS\")\nprint(\"   Calculate missing percentage for each column\")\nprint(\"   Hint: (missing_count / total_count) * 100\")\n\n# YOUR CODE HERE\n# completeness = ...\n# print(completeness)\n\nprint(\"\\n2. VALIDITY ANALYSIS\")\nprint(\"   Find invalid values:\")\nprint(\"   - test_score should be 0-100\")\nprint(\"   - attendance_pct should be 0-100\")\n\n# YOUR CODE HERE\n# invalid_scores = ...\n# invalid_attendance = ...\n\nprint(\"\\n3. DUPLICATE ANALYSIS\")\nprint(\"   Find duplicate student_id values\")\n\n# YOUR CODE HERE\n# duplicates = ...\n\nprint(\"\\n4. WRITE A QUALITY REPORT\")\nprint(\"   Summarize findings and recommend actions\")\n\n# YOUR CODE HERE: Write your findings below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Creating a Data Dictionary\n",
    "\n",
    "Create a comprehensive data dictionary for the student dataset from Exercise 1.\n",
    "\n",
    "Include:\n",
    "1. **Description** of each column\n",
    "2. **Data type** (integer, float, string, date, etc.)\n",
    "3. **Valid range** or allowed values\n",
    "4. **Units** if applicable\n",
    "5. **Special constraints** or business rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Create Data Dictionary\n",
    "\nprint(\"EXERCISE 2: Create Data Dictionary for Student Dataset\")\nprint(\"=\"*70)\n",
    "\n# Create data dictionary\nstudent_dictionary = DataDictionary(exercise_data, 'Student Performance Dataset')\n",
    "\n# TODO: Add definitions for each column\n# Use the add_column_definition method with appropriate parameters\n\n# Example (complete this):\nstudent_dictionary.add_column_definition(\n",
    "    'student_id',\n",
    "    'Unique identifier for each student',  # Description\n",
    "    'integer',  # Data type\n",
    "    '',  # Units\n",
    "    '1-999',  # Valid values\n",
    "    'Primary key, no duplicates allowed'  # Constraints\n",
    ")\n",
    "\n# TODO: Add definitions for remaining columns:\n# - test_score\n# - study_hours  \n# - grade\n# - attendance_pct\n\nprint(\"\\nYour Data Dictionary:\")\nprint(\"-\" * 70)\n\n# Print your completed dictionary\nstudent_dictionary.print_dictionary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Avoiding Data Leakage\n",
    "\n",
    "**Scenario**: You're building a model to predict student test scores from study hours and attendance.\n",
    "\n",
    "Below are two preprocessing approaches. Identify which one causes data leakage and explain why.\n",
    "\n",
    "**Approach A**: Scale entire dataset, then split\n",
    "**Approach B**: Split first, then scale each set separately\n",
    "\n",
    "Why does this matter for research validity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Data Leakage Detection\n",
    "\nprint(\"EXERCISE 3: Detecting Data Leakage in Preprocessing\")\nprint(\"=\" * 70)\n",
    "\n# Prepare data for modeling\nfeatures = ['study_hours', 'attendance_pct']\ntarget = 'test_score'\n",
    "\n# Remove rows with missing target values for this example\nmodel_data = exercise_data.dropna(subset=[target]).copy()\nX = model_data[features].fillna(model_data[features].mean())\ny = model_data[target]\n",
    "\nprint(f\"\\nDataset size: {len(X)} samples\")\nprint(f\"Features: {features}\")\nprint(f\"Target: {target}\")\nprint(f\"\\nFeature statistics (original data):\")\nprint(f\"  Study hours - mean: {X['study_hours'].mean():.2f}, std: {X['study_hours'].std():.2f}\")\nprint(f\"  Attendance - mean: {X['attendance_pct'].mean():.2f}, std: {X['attendance_pct'].std():.2f}\")\n",
    "\nprint(\"\\n\" + \"=\"*70)\nprint(\"APPROACH A: Scale first, then split (‚ö†Ô∏è CAUSES LEAKAGE)\")\nprint(\"=\"*70)\n",
    "\nscaler_a = StandardScaler()\nX_scaled_all = scaler_a.fit_transform(X)  # Fit on ALL data\nX_train_a, X_test_a = train_test_split(X_scaled_all, test_size=0.3, random_state=42)\n",
    "\nprint(f\"\\nScaling parameters computed from: ALL {len(X)} samples\")\nprint(f\"  Scaler mean: {scaler_a.mean_.round(2)}\")\nprint(f\"  Scaler std: {scaler_a.scale_.round(2)}\")\nprint(f\"\\nApproach A result:\")\nprint(f\"  Training mean: {X_train_a.mean(axis=0).round(3)}\")\nprint(f\"  Test mean: {X_test_a.mean(axis=0).round(3)}\")\nprint(f\"\\n‚ùå PROBLEM: Test data statistics influenced by training data!\")\nprint(f\"   Scaler learned from test data distribution - LEAKAGE!\")\n",
    "\nprint(\"\\n\" + \"=\"*70)\nprint(\"APPROACH B: Split first, then scale (‚úì CORRECT)\")\nprint(\"=\"*70)\n",
    "\nX_train_b, X_test_b = train_test_split(X.values, test_size=0.3, random_state=42)\nscaler_b = StandardScaler()\nscaler_b.fit(X_train_b)  # Fit ONLY on training data\nX_train_b_scaled = scaler_b.transform(X_train_b)\nX_test_b_scaled = scaler_b.transform(X_test_b)\n",
    "\nprint(f\"\\nScaling parameters computed from: {len(X_train_b)} training samples\")\nprint(f\"  Scaler mean: {scaler_b.mean_.round(2)}\")\nprint(f\"  Scaler std: {scaler_b.scale_.round(2)}\")\nprint(f\"\\nApproach B result:\")\nprint(f\"  Training mean: {X_train_b_scaled.mean(axis=0).round(3)}\")\nprint(f\"  Test mean: {X_test_b_scaled.mean(axis=0).round(3)}\")\nprint(f\"\\n‚úì CORRECT: Test data statistics NOT affected by training parameters\")\nprint(f\"   Scaler learned ONLY from training data - NO LEAKAGE!\")\n",
    "\nprint(\"\\n\" + \"=\"*70)\nprint(\"LEARNING OUTCOME:\")\nprint(\"Always split data BEFORE fitting any transformers.\")\nprint(\"This is critical for unbiased model evaluation.\")\nprint(\"=\"*70)\n",
    "\n# TODO: Answer this question\nprint(\"\\nQUESTION: Why is Approach B more valid for research?\")\nprint(\"-\" * 70)\nprint(\"Your answer:\")\nprint(\"\"\"\n1. In Approach A, when we evaluate the model's performance on test data,\n   we're not truly measuring how it would perform on NEW, unseen data.\n   \n2. The test set was already used to compute scaling parameters,\n   so the model has effectively \"seen\" test data statistics.\n   \n3. This violates the fundamental research principle of test-train separation.\n\n4. The reported test accuracy will be optimistically biased,\n   making the model appear better than it actually is.\n\n5. Reproducibility is compromised because results depend on\n   the full dataset, not just training data.\n\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "‚úÖ **Data Acquisition**: Choose between primary and secondary data based on research needs, use appropriate sampling methods, and validate at collection point\n",
    "\n",
    "‚úÖ **Data Quality Assessment**: Always assess completeness, validity, reliability, and potential biases - visualize patterns, don't rely on summary statistics alone\n",
    "\n",
    "‚úÖ **Data Documentation**: Create data dictionaries, track lineage, write README files, and follow metadata standards for reproducibility\n",
    "\n",
    "‚úÖ **7-Step Preprocessing**: Load ‚Üí EDA ‚Üí Validate ‚Üí Handle Missing ‚Üí Handle Outliers ‚Üí Transform ‚Üí Quality Check\n",
    "\n",
    "‚úÖ **Prevent Data Leakage**: ALWAYS split train/test BEFORE fitting any transformers - this is the #1 rule for valid research\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In **Module 05: Exploratory Data Analysis and Visualization**, you'll learn:\n",
    "- Statistical summaries and distributions\n",
    "- Correlation analysis\n",
    "- Effective data visualization for research\n",
    "- Communicating data stories\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- **Paper**: \"Leakage in Data Mining\" (Kaufman et al., 2012)\n",
    "- **Standard**: DDI (Data Documentation Initiative)\n",
    "- **Guide**: \"Data Quality: Concepts, Methodologies and Techniques\" (Olson, 2003)\n",
    "- **Tool**: Great Expectations (automated data validation)\n"   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Assessment\n",
    "\n",
    "Before moving to Module 05, ensure you can:\n",
    "\n",
    "- [ ] Explain the difference between primary and secondary data\n",
    "- [ ] Choose appropriate sampling methods for different research scenarios\n",
    "- [ ] Calculate and interpret data quality metrics (completeness, validity)\n",
    "- [ ] Create a comprehensive data dictionary\n",
    "- [ ] Document data lineage and transformations\n",
    "- [ ] Implement the 7-step preprocessing workflow\n",
    "- [ ] Explain why fitting transformers only on training data prevents data leakage\n",
    "- [ ] Identify common data quality issues in real datasets\n",
    "- [ ] Visualize missing data patterns\n",
    "- [ ] Apply validation rules at data collection time\n",
    "\n",
    "If you can confidently check all boxes, you're ready for Module 05! üéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
