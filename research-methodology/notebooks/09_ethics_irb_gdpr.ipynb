{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 09: Ethics, IRB, and GDPR Compliance\n",
    "\n",
    "**Difficulty**: \u2b50\u2b50\u2b50 (Advanced)\n",
    "\n",
    "**Estimated Time**: 90 minutes\n",
    "\n",
    "**Prerequisites**: Module 08 - Research Design and Validity\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Apply five core ethical principles (fairness, transparency, accountability, privacy, beneficence) to research design\n",
    "2. Determine when Institutional Review Board (IRB) review is required and navigate exemption categories\n",
    "3. Ensure GDPR compliance for data processing involving European data subjects\n",
    "4. Detect algorithmic bias using fairness metrics and implement mitigation strategies\n",
    "5. Implement privacy by design principles in research and ML systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let\u2019s import the libraries we\u2019ll use in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard data science libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from enum import Enum\n",
    "from typing import Dict, List, Tuple\n",
    "import hashlib\n",
    "import json\n",
    "\n",
    "# Configuration for better visualizations\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"âœ“ Libraries imported successfully!\")\n",
    "print(f\"âœ“ Notebook created: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Five Core Ethical Principles in Research\n",
    "\n",
    "Modern research ethics rests on five foundational principles that guide how we conduct research responsibly. These principles emerged from centuries of ethical thought, from the Hippocratic Oath to the Nuremberg Code and contemporary frameworks like the EU Ethics Guidelines.\n",
    "\n",
    "### Principle 1: Fairness\n",
    "\n",
    "**Definition**: Avoid bias and discrimination; ensure benefits and burdens are distributed equitably across all population groups.\n",
    "\n",
    "**In Practice**:\n",
    "- Regularly audit models for disparate impact across demographic groups\n",
    "- Consider multiple definitions of fairness (they often conflict)\n",
    "- Include diverse representation in data collection and testing\n",
    "- Acknowledge that perfect fairness is impossible; choose fairness metrics intentionally\n",
    "\n",
    "**Example**: A hiring algorithm must be tested for equal performance across gender and race. If it has 95% accuracy for men but 78% for women, it violates fairness even if overall accuracy is high.\n",
    "\n",
    "### Principle 2: Transparency\n",
    "\n",
    "**Definition**: Make research methods and decision-making processes explainable and understandable to stakeholders.\n",
    "\n",
    "**In Practice**:\n",
    "- Document model decisions with LIME, SHAP, or similar explanations\n",
    "- Clearly state limitations and assumptions\n",
    "- Provide plain-language summaries alongside technical details\n",
    "- Enable external auditing and review\n",
    "\n",
    "**Example**: A loan approval system should explain to applicants WHY their application was rejected, not just that it was.\n",
    "\n",
    "### Principle 3: Accountability\n",
    "\n",
    "**Definition**: Clearly assign responsibility for research decisions and impacts; conduct and document impact assessments.\n",
    "\n",
    "**In Practice**:\n",
    "- Designate clear roles: who decides, who implements, who verifies?\n",
    "- Conduct Data Protection Impact Assessments (DPIA)\n",
    "- Maintain audit trails of all decisions and changes\n",
    "- Plan for remediation if harms are discovered\n",
    "\n",
    "**Example**: AI/ML teams must document who tested for bias, who signed off on deployment, and how they will respond if bias is discovered post-deployment.\n",
    "\n",
    "### Principle 4: Privacy\n",
    "\n",
    "**Definition**: Protect personal data through encryption, access controls, and purpose limitation.\n",
    "\n",
    "**In Practice**:\n",
    "- Encrypt data at rest and in transit\n",
    "- Implement role-based access controls\n",
    "- De-identify data when possible\n",
    "- Limit data use to stated purposes\n",
    "\n",
    "**Example**: A medical research study collects health data for cardiovascular research. Using the same data to study genetic predispositions without explicit new consent violates privacy principles (purpose limitation).\n",
    "\n",
    "### Principle 5: Beneficence\n",
    "\n",
    "**Definition**: Maximize benefits while minimizing potential harms to research participants.\n",
    "\n",
    "**In Practice**:\n",
    "- Conduct risk-benefit analyses before research begins\n",
    "- Design studies to answer important questions\n",
    "- Implement safeguards to protect vulnerable populations\n",
    "- Share results transparently\n",
    "\n",
    "**Example**: A machine learning model deployed in healthcare must reduce diagnostic errors more than it creates new mistakes. The benefits must justify any harms from occasional errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an ethical principles framework class\n\nclass EthicalPrinciple:\n    \"\"\"Represents an ethical principle with assessment criteria.\"\"\"\n    \n    def __init__(self, name: str, definition: str, key_questions: List[str]):\n        self.name = name\n        self.definition = definition\n        self.key_questions = key_questions\n    \n    def assess(self, research_plan: Dict[str, str]) -> Dict[str, any]:\n        \"\"\"Assess a research plan against this principle.\"\"\"\n        return {\n            'principle': self.name,\n            'status': 'needs_review',\n            'assessment_items': self.key_questions\n        }\n\n# Define the five ethical principles\nprinciples = {\n    'Fairness': EthicalPrinciple(\n        name='Fairness',\n        definition='Avoid bias and ensure equitable distribution of benefits/burdens',\n        key_questions=[\n            'Have we tested for disparate impact across demographic groups?',\n            'Do protected groups have equal model performance?',\n            'Have we considered trade-offs between different fairness metrics?',\n            'Is representation diverse in training data?'\n        ]\n    ),\n    'Transparency': EthicalPrinciple(\n        name='Transparency',\n        definition='Make research methods and decisions explainable',\n        key_questions=[\n            'Can we explain individual model predictions?',\n            'Are limitations clearly documented?',\n            'Can external parties audit our methods?',\n            'Do stakeholders understand the approach in plain language?'\n        ]\n    ),\n    'Accountability': EthicalPrinciple(\n        name='Accountability',\n        definition='Clearly assign responsibility for decisions and impacts',\n        key_questions=[\n            'Who is responsible for this research?',\n            'Do we have audit trails of all key decisions?',\n            'Have we conducted impact assessments?',\n            'What is our plan for remediation if harms occur?'\n        ]\n    ),\n    'Privacy': EthicalPrinciple(\n        name='Privacy',\n        definition='Protect personal data through technical and organizational measures',\n        key_questions=[\n            'Is data encrypted at rest and in transit?',\n            'Are access controls role-based and minimal?',\n            'Have we de-identified data where possible?',\n            'Is data use limited to stated purposes?'\n        ]\n    ),\n    'Beneficence': EthicalPrinciple(\n        name='Beneficence',\n        definition='Maximize benefits while minimizing harms',\n        key_questions=[\n            'Do benefits clearly outweigh risks?',\n            'Have we protected vulnerable populations?',\n            'Is the research question important enough to justify any risks?',\n            'Will we share results transparently?'\n        ]\n    )\n}\n\n# Display the framework\nprint(\"\\n\" + \"=\"*70)\nprint(\"FIVE CORE ETHICAL PRINCIPLES FRAMEWORK\")\nprint(\"=\"*70)\n\nfor principle_name, principle in principles.items():\n    print(f\"\\n{principle_name.upper()}\")\n    print(\"-\" * 70)\n    print(f\"Definition: {principle.definition}\")\n    print(f\"\\nKey Assessment Questions:\")\n    for i, question in enumerate(principle.key_questions, 1):\n        print(f\"  {i}. {question}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Institutional Review Board (IRB) Requirements\n",
    "\n",
    "An Institutional Review Board (IRB) is a committee that reviews and approves research involving human subjects. Understanding when IRB review is required is crucial for research compliance.\n",
    "\n",
    "### What Requires IRB Review?\n",
    "\n",
    "**Research** that involves:\n",
    "1. **Human subjects**: Any living person\n",
    "2. **Interventions or interactions**: Deliberate manipulation, observation, or engagement\n",
    "3. **Private identifiable information**: PII that could identify individuals\n",
    "4. **Generalizable knowledge**: Results intended to be shared or published\n",
    "\n",
    "**Example requiring review**: Survey of 100 customers about product preferences (involves subjects, interaction, and intent to publish)\n",
    "\n",
    "**Example NOT requiring review**: Analysis of pre-existing anonymized dataset with no human contact\n",
    "\n",
    "### Exemption Categories (45 CFR 46.101(b))\n",
    "\n",
    "Some research can be **exempt** from full IRB review if it meets specific criteria:\n",
    "\n",
    "| Category | Description | Typical Examples |\n",
    "|----------|-------------|------------------|\n",
    "| (1) | Educational settings; normal educational practices | Classroom surveys, teaching experiments |\n",
    "| (2) | Educational tests, surveys, interviews (if anonymous) | Anonymous customer satisfaction surveys |\n",
    "| (3) | Existing data analysis (secondary data) | Analysis of published datasets |\n",
    "| (4) | Quality improvement projects | Internal process improvements |\n",
    "| (5) | Public benefit/service programs | Evaluation of government programs |\n",
    "| (6) | Taste/consumer acceptance studies | Food/beverage preference studies |\n",
    "| (7) | Certain foreign research | International studies with local ethics approval |\n",
    "\n",
    "### Levels of Review\n",
    "\n",
    "**Exempt**: No IRB review required (but must be carefully evaluated)\n",
    "- Low risk to subjects\n",
    "- No sensitive data\n",
    "- Minimal burden on subjects\n",
    "\n",
    "**Expedited**: Streamlined review (45 days typical)\n",
    "- Minimal risk projects\n",
    "- Amendments to approved research\n",
    "- Continuing review\n",
    "\n",
    "**Full Board**: Complete committee review (typically 60+ days)\n",
    "- More than minimal risk\n",
    "- Vulnerable populations (children, prisoners, pregnant women)\n",
    "- Sensitive topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IRB Review Determination Tool\n\nclass IRBReviewDetermination:\n    \"\"\"Determines whether research requires IRB review and what level.\"\"\"\n    \n    def __init__(self):\n        self.criteria = {\n            'human_subjects': {\n                'weight': 2,\n                'question': 'Does research involve living human subjects?',\n                'required': True\n            },\n            'intervention_interaction': {\n                'weight': 2,\n                'question': 'Does it involve intervention, interaction, or manipulation?',\n                'required_with': ['human_subjects']\n            },\n            'identifiable_data': {\n                'weight': 2,\n                'question': 'Does it use private identifiable information?',\n                'required_with': ['human_subjects']\n            },\n            'generalizable_knowledge': {\n                'weight': 1,\n                'question': 'Is knowledge intended to be generalizable/published?',\n                'required_with': ['human_subjects']\n            },\n            'minimal_risk': {\n                'weight': 2,\n                'question': 'Is risk to subjects no greater than everyday life?',\n                'increases_exemption': True\n            },\n            'anonymous': {\n                'weight': 2,\n                'question': 'Are all data completely anonymous?',\n                'increases_exemption': True\n            }\n        }\n    \n    def evaluate_research(self, research_info: Dict[str, bool]) -> Dict:\n        \"\"\"Evaluate a research project for IRB requirements.\"\"\"\n        \n        # Check if IRB review needed\n        needs_irb = (\n            research_info.get('human_subjects', False) and\n            (\n                research_info.get('intervention_interaction', False) or\n                research_info.get('identifiable_data', False)\n            )\n        )\n        \n        # Determine level if needed\n        review_level = 'Not Required'\n        if needs_irb:\n            # Check for exemptions\n            if (research_info.get('minimal_risk', True) and \n                research_info.get('anonymous', True)):\n                review_level = 'Exempt (Category 3 - Secondary Data)'\n            elif research_info.get('minimal_risk', False):\n                review_level = 'Expedited Review'\n            else:\n                review_level = 'Full Board Review'\n        \n        return {\n            'requires_irb': needs_irb,\n            'review_level': review_level,\n            'rationale': self._generate_rationale(research_info, needs_irb)\n        }\n    \n    def _generate_rationale(self, info: Dict, needs_irb: bool) -> str:\n        \"\"\"Generate explanation for determination.\"\"\"\n        if needs_irb:\n            if 'Exempt' in self._get_level(info):\n                return 'Secondary analysis of de-identified data qualifies for exemption'\n            elif 'Expedited' in self._get_level(info):\n                return 'Low-risk research with minimal burden qualifies for expedited review'\n            else:\n                return 'Higher risk or vulnerable populations require full board review'\n        return 'No human subject research involved; IRB review not required'\n    \n    def _get_level(self, info: Dict) -> str:\n        if (info.get('minimal_risk', True) and info.get('anonymous', True)):\n            return 'Exempt'\n        elif info.get('minimal_risk', False):\n            return 'Expedited'\n        return 'Full Board'\n\n# Initialize the tool\nirb_tool = IRBReviewDetermination()\n\n# Example: Evaluate different research scenarios\nscenarios = {\n    'Scenario A: Customer Survey': {\n        'human_subjects': True,\n        'intervention_interaction': True,  # Asking questions\n        'identifiable_data': False,  # Anonymous survey\n        'generalizable_knowledge': True,\n        'minimal_risk': True,\n        'anonymous': True\n    },\n    'Scenario B: Medical Trial': {\n        'human_subjects': True,\n        'intervention_interaction': True,\n        'identifiable_data': True,\n        'generalizable_knowledge': True,\n        'minimal_risk': False,  # Medical intervention\n        'anonymous': False\n    },\n    'Scenario C: Secondary Data Analysis': {\n        'human_subjects': False,  # Only analyzing historical data\n        'intervention_interaction': False,\n        'identifiable_data': False,  # De-identified dataset\n        'generalizable_knowledge': True,\n        'minimal_risk': True,\n        'anonymous': True\n    }\n}\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"IRB REVIEW DETERMINATION RESULTS\")\nprint(\"=\"*70)\n\nfor scenario_name, scenario_data in scenarios.items():\n    result = irb_tool.evaluate_research(scenario_data)\n    print(f\"\\n{scenario_name}\")\n    print(\"-\" * 70)\n    print(f\"Requires IRB Review: {result['requires_irb']}\")\n    print(f\"Review Level: {result['review_level']}\")\n    print(f\"Rationale: {result['rationale']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. GDPR Essentials for Data Protection\n",
    "\n",
    "The General Data Protection Regulation (GDPR) is EU law governing how personal data of EU residents must be handled. It applies globally to any organization processing data about European data subjects.\n",
    "\n",
    "### When GDPR Applies\n",
    "\n",
    "GDPR applies if:\n",
    "- You collect data from **anyone in the EEA** (EU + Iceland, Liechtenstein, Norway)\n",
    "- Data subject's location: NOT the location of your business\n",
    "- Data is processed **anywhere** (cloud, servers in any country)\n",
    "\n",
    "**Example**: A US company with no EU offices collecting data on German customers MUST comply with GDPR\n",
    "\n",
    "### Key GDPR Principles (Article 5)\n",
    "\n",
    "1. **Lawfulness, Fairness, Transparency**: Data processing must be legal, fair, and transparent\n",
    "2. **Purpose Limitation**: Data collected for one purpose cannot be reused for another without new consent\n",
    "3. **Data Minimization**: Collect only what's necessary\n",
    "4. **Accuracy**: Keep data accurate and up-to-date\n",
    "5. **Storage Limitation**: Don't keep data longer than needed\n",
    "6. **Integrity & Confidentiality**: Protect data security\n",
    "7. **Accountability**: Demonstrate compliance\n",
    "\n",
    "### Valid Consent (Critical Requirement)\n",
    "\n",
    "Consent under GDPR must be:\n",
    "\n",
    "- **Freely Given**: No coercion; users can refuse\n",
    "- **Specific**: For clearly defined purposes\n",
    "- **Informed**: Users know what data and how it's used\n",
    "- **Unambiguous**: Explicit opt-in (NOT pre-checked boxes)\n",
    "- **Distinguishable**: Clear language separate from other terms\n",
    "- **Revocable**: Users can withdraw anytime\n",
    "\n",
    "**NOT Valid**:\n",
    "- Pre-checked consent boxes âŒ\n",
    "- \"Continue using our service\" = consent âŒ\n",
    "- Bundled with other terms âŒ\n",
    "- Vague descriptions of use âŒ\n",
    "\n",
    "### Special Category Data\n",
    "\n",
    "Data revealing:\n",
    "- Race or ethnicity\n",
    "- Political opinions\n",
    "- Religious/philosophical beliefs\n",
    "- Trade union membership\n",
    "- Genetic data\n",
    "- Biometric data (for identification)\n",
    "- Health data\n",
    "- Sex life or sexual orientation\n",
    "\n",
    "**Rule**: These require **explicit consent** or narrow exceptions (medical, vital interests, etc.)\n",
    "\n",
    "### Privacy by Design\n",
    "\n",
    "Build privacy into systems from the start, not as an afterthought:\n",
    "\n",
    "1. **Data Minimization**: Collect only necessary data\n",
    "2. **Purpose Limitation**: Define and document use cases upfront\n",
    "3. **Access Control**: Least privilege principle\n",
    "4. **Encryption**: Data at rest and in transit\n",
    "5. **De-identification**: Remove/hash PII when possible\n",
    "6. **Retention Policies**: Delete data when no longer needed\n",
    "7. **Audit Logging**: Track who accessed what and when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GDPR Compliance Implementation\n\nclass ConsentValidator:\n    \"\"\"Validates whether consent meets GDPR requirements.\"\"\"\n    \n    def __init__(self):\n        self.requirements = [\n            'freely_given',\n            'specific',\n            'informed',\n            'unambiguous',\n            'distinguishable',\n            'revocable'\n        ]\n    \n    def validate_consent_text(self, consent_statement: str) -> Dict[str, any]:\n        \"\"\"Analyze consent text for GDPR compliance.\"\"\"\n        checks = {\n            'freely_given': 'can refuse' in consent_statement.lower() or \n                            'optional' in consent_statement.lower(),\n            'specific': 'purpose' in consent_statement.lower() or \n                       'for' in consent_statement.lower(),\n            'informed': 'data' in consent_statement.lower() or \n                       'information' in consent_statement.lower(),\n            'unambiguous': 'explicit' in consent_statement.lower() or \n                          'I agree' in consent_statement or\n                          'opt-in' in consent_statement.lower(),\n            'distinguishable': len(consent_statement) > 50,  # Separate substance\n            'revocable': 'withdraw' in consent_statement.lower() or \n                        'revoke' in consent_statement.lower()\n        }\n        \n        compliance_score = sum(checks.values()) / len(checks)\n        \n        return {\n            'individual_checks': checks,\n            'compliance_score': f\"{compliance_score*100:.0f}%\",\n            'is_compliant': compliance_score >= 0.83,  # Need 5/6 minimum\n            'feedback': self._generate_feedback(checks)\n        }\n    \n    def _generate_feedback(self, checks: Dict[str, bool]) -> List[str]:\n        \"\"\"Generate improvement suggestions.\"\"\"\n        feedback = []\n        if not checks['freely_given']:\n            feedback.append(\"Add language indicating consent is optional\")\n        if not checks['specific']:\n            feedback.append(\"Clearly specify the purpose of data collection\")\n        if not checks['informed']:\n            feedback.append(\"Explain what data is collected and how it's used\")\n        if not checks['unambiguous']:\n            feedback.append(\"Use explicit opt-in language (e.g., 'I agree to...')\")\n        if not checks['distinguishable']:\n            feedback.append(\"Separate consent from other terms and conditions\")\n        if not checks['revocable']:\n            feedback.append(\"Include instructions for withdrawing consent\")\n        return feedback if feedback else [\"âœ“ Excellent - Meets GDPR requirements\"]\n\n\nclass PrivacyByDesignFramework:\n    \"\"\"Implements privacy by design principles.\"\"\"\n    \n    def __init__(self):\n        self.principles = {\n            'data_minimization': 'Collect only necessary data',\n            'purpose_limitation': 'Define use cases explicitly',\n            'access_control': 'Implement least privilege',\n            'encryption': 'Encrypt data at rest and transit',\n            'de_identification': 'Remove PII when possible',\n            'retention_policy': 'Delete data when no longer needed',\n            'audit_logging': 'Track all data access'\n        }\n    \n    def create_privacy_policy(self, data_fields: List[str], \n                             purposes: List[str],\n                             retention_days: int) -> Dict:\n        \"\"\"Generate privacy policy elements.\"\"\"\n        \n        return {\n            'collected_data': data_fields,\n            'purposes': purposes,\n            'retention_period': f\"{retention_days} days\",\n            'deletion_policy': f\"Data deleted after {retention_days} days of inactivity\",\n            'access_controls': \"Role-based access control implemented\",\n            'encryption': \"AES-256 encryption for sensitive fields\",\n            'audit_trail': \"All access logged and monitored\"\n        }\n\n# Test the validators\nprint(\"\\n\" + \"=\"*70)\nprint(\"GDPR COMPLIANCE CHECKING\")\nprint(\"=\"*70)\n\nvalidator = ConsentValidator()\n\n# Example 1: Good consent\ngood_consent = \"\"\"\nCONSENT FOR DATA COLLECTION\n\nI understand and agree that my email address will be used to send you \nweekly newsletter updates about product features and promotions. I can \nwithdraw this consent at any time by clicking the unsubscribe link in \nthe email or contacting support@company.com. This consent is optional \nand will not affect my access to core services.\n\"\"\"\n\nprint(\"\\nExample 1: GOOD CONSENT TEXT\")\nprint(\"-\" * 70)\nresult_good = validator.validate_consent_text(good_consent)\nprint(f\"Compliance Score: {result_good['compliance_score']}\")\nprint(f\"Is GDPR Compliant: {result_good['is_compliant']}\")\nprint(f\"\\nChecks:\")\nfor check, passed in result_good['individual_checks'].items():\n    status = \"âœ“\" if passed else \"âœ—\"\n    print(f\"  {status} {check.replace('_', ' ').title()}\")\nif result_good['feedback']:\n    print(f\"\\nFeedback: {result_good['feedback'][0]}\")\n\n# Example 2: Bad consent\nbad_consent = \"By using this website, you agree to our terms.\"\n\nprint(\"\\n\\nExample 2: POOR CONSENT TEXT\")\nprint(\"-\" * 70)\nresult_bad = validator.validate_consent_text(bad_consent)\nprint(f\"Compliance Score: {result_bad['compliance_score']}\")\nprint(f\"Is GDPR Compliant: {result_bad['is_compliant']}\")\nprint(f\"\\nChecks:\")\nfor check, passed in result_bad['individual_checks'].items():\n    status = \"âœ“\" if passed else \"âœ—\"\n    print(f\"  {status} {check.replace('_', ' ').title()}\")\nprint(f\"\\nRequired Improvements:\")\nfor i, feedback in enumerate(result_bad['feedback'], 1):\n    print(f\"  {i}. {feedback}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Algorithmic Bias Detection and Mitigation\n",
    "\n",
    "Bias in machine learning systems can perpetuate discrimination and violate ethical principles. Detecting and mitigating bias requires systematic approaches across three stages:\n",
    "\n",
    "### Three Bias Mitigation Stages\n",
    "\n",
    "#### Pre-processing (Before Model Training)\n",
    "- **Goal**: Clean biased training data\n",
    "- **Techniques**:\n",
    "  - Oversampling underrepresented groups\n",
    "  - Reweighting training samples\n",
    "  - Removing biased features (redlining attributes)\n",
    "  - Improving data representation\n",
    "\n",
    "#### In-processing (During Model Training)\n",
    "- **Goal**: Train model with fairness constraints\n",
    "- **Techniques**:\n",
    "  - Add fairness constraints to loss function\n",
    "  - Use fair learning algorithms\n",
    "  - Adversarial debiasing\n",
    "  - Calibrated predictions\n",
    "\n",
    "#### Post-processing (After Predictions)\n",
    "- **Goal**: Adjust predictions to meet fairness criteria\n",
    "- **Techniques**:\n",
    "  - Threshold optimization per group\n",
    "  - Equalized odds adjustment\n",
    "  - Output post-processing\n",
    "\n",
    "### Fairness Metrics\n",
    "\n",
    "Different metrics capture different aspects of fairness:\n",
    "\n",
    "| Metric | Definition | When to Use |\n",
    "|--------|-----------|-------------|\n",
    "| **Demographic Parity** | P(pred=1&#124;A=0) = P(pred=1&#124;A=1) | Hiring, lending (selection rates equal) |\n",
    "| **Equalized Odds** | False positive and false negative rates equal across groups | Credit approval (minimize disparate impact) |\n",
    "| **Predictive Parity** | P(Y=1&#124;pred=1,A=0) = P(Y=1&#124;pred=1,A=1) | Medical diagnosis (precision equal) |\n",
    "| **Disparate Impact Ratio** | Selection rate minority / selection rate majority â‰¥ 0.8 | Legal standard (80% rule) |\n",
    "\n",
    "**Key Insight**: These metrics often conflict. You must choose which fairness definition matters for your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fairness Metrics Implementation\n\nclass FairnessMetricsCalculator:\n    \"\"\"Calculates fairness metrics for ML models.\"\"\"\n    \n    @staticmethod\n    def demographic_parity(y_pred: np.ndarray, protected_attr: np.ndarray) -> Dict:\n        \"\"\"\n        Calculate demographic parity (selection rates equal across groups).\n        Ideal: ratio close to 1.0 (equal selection rates)\n        \"\"\"\n        group_0_selection = (y_pred[protected_attr == 0] == 1).mean()\n        group_1_selection = (y_pred[protected_attr == 1] == 1).mean()\n        \n        parity_ratio = group_0_selection / group_1_selection if group_1_selection > 0 else 0\n        \n        return {\n            'metric': 'Demographic Parity',\n            'group_0_selection_rate': f\"{group_0_selection*100:.1f}%\",\n            'group_1_selection_rate': f\"{group_1_selection*100:.1f}%\",\n            'parity_ratio': f\"{parity_ratio:.3f}\",\n            'is_fair': 0.8 <= parity_ratio <= 1.25,  # 80% rule threshold\n            'interpretation': f\"Minority selected at {parity_ratio*100:.0f}% of majority rate\"\n        }\n    \n    @staticmethod\n    def equalized_odds(y_true: np.ndarray, y_pred: np.ndarray, \n                      protected_attr: np.ndarray) -> Dict:\n        \"\"\"\n        Calculate equalized odds (FPR and FNR equal across groups).\n        Ideal: both rates equal between groups\n        \"\"\"\n        # True positive rates\n        tpr_group_0 = (y_pred[protected_attr == 0][y_true[protected_attr == 0] == 1] == 1).mean()\n        tpr_group_1 = (y_pred[protected_attr == 1][y_true[protected_attr == 1] == 1] == 1).mean()\n        \n        # False positive rates  \n        fpr_group_0 = (y_pred[protected_attr == 0][y_true[protected_attr == 0] == 0] == 1).mean()\n        fpr_group_1 = (y_pred[protected_attr == 1][y_true[protected_attr == 1] == 0] == 1).mean()\n        \n        return {\n            'metric': 'Equalized Odds',\n            'tpr_difference': f\"{abs(tpr_group_0 - tpr_group_1)*100:.1f}%\",\n            'fpr_difference': f\"{abs(fpr_group_0 - fpr_group_1)*100:.1f}%\",\n            'tpr_group_0': f\"{tpr_group_0*100:.1f}%\",\n            'tpr_group_1': f\"{tpr_group_1*100:.1f}%\",\n            'fpr_group_0': f\"{fpr_group_0*100:.1f}%\",\n            'fpr_group_1': f\"{fpr_group_1*100:.1f}%\"\n        }\n\n# Demonstrate with simulated data\nprint(\"\\n\" + \"=\"*70)\nprint(\"BIAS DETECTION: FAIRNESS METRICS\")\nprint(\"=\"*70)\n\nnp.random.seed(42)\n\n# Create simulated hiring data\nn_samples = 1000\nprotected_attr = np.random.binomial(1, 0.3, n_samples)  # 30% minority group\n\n# Biased model: worse performance for protected group\ny_true = np.random.binomial(1, 0.5, n_samples)\ny_pred = np.zeros(n_samples)\n\n# Group 0 (majority): 70% accuracy\ny_pred[protected_attr == 0] = (np.random.random(sum(protected_attr == 0)) < 0.7).astype(int)\n\n# Group 1 (minority): 55% accuracy (biased!)\ny_pred[protected_attr == 1] = (np.random.random(sum(protected_attr == 1)) < 0.55).astype(int)\n\ncalculator = FairnessMetricsCalculator()\n\n# Calculate metrics\ndem_parity = calculator.demographic_parity(y_pred, protected_attr)\neq_odds = calculator.equalized_odds(y_true, y_pred, protected_attr)\n\nprint(\"\\nDEMOGRAPHIC PARITY (Selection Rates)\")\nprint(\"-\" * 70)\nfor key, value in dem_parity.items():\n    if key not in ['metric']:\n        print(f\"{key.replace('_', ' ').title()}: {value}\")\n\nprint(f\"\\nâš ï¸ BIAS DETECTED: {dem_parity['group_0_selection_rate']} vs {dem_parity['group_1_selection_rate']}\")\nprint(f\"   Minority group selected at {(float(dem_parity['parity_ratio'])*100):.0f}% of majority rate\")\n\nprint(\"\\n\\nEQUALIZED ODDS (Error Rates)\")\nprint(\"-\" * 70)\nfor key, value in eq_odds.items():\n    if key not in ['metric']:\n        print(f\"{key.replace('_', ' ').title()}: {value}\")\n\nprint(f\"\\nâš ï¸ TPR Difference: {eq_odds['tpr_difference']} (should be <5%)\")\nprint(f\"   FPR Difference: {eq_odds['fpr_difference']} (should be <5%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize bias across groups\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# Plot 1: Selection rates\ngroups = ['Majority Group\\n(Protected=0)', 'Minority Group\\n(Protected=1)']\nselection_rates = [\n    float(dem_parity['group_0_selection_rate'].rstrip('%')) / 100,\n    float(dem_parity['group_1_selection_rate'].rstrip('%')) / 100\n]\ncolors = ['#2ecc71', '#e74c3c']\naxes[0, 0].bar(groups, selection_rates, color=colors, alpha=0.7, edgecolor='black')\naxes[0, 0].axhline(y=0.5, color='blue', linestyle='--', linewidth=2, label='Equal (Fair)')\naxes[0, 0].set_ylabel('Selection Rate', fontsize=11, fontweight='bold')\naxes[0, 0].set_title('Demographic Parity: Selection Rates by Group', fontsize=12, fontweight='bold')\naxes[0, 0].set_ylim([0, 1])\naxes[0, 0].legend()\nfor i, rate in enumerate(selection_rates):\n    axes[0, 0].text(i, rate + 0.03, f'{rate*100:.1f}%', ha='center', fontweight='bold')\n\n# Plot 2: True positive rates (recall)\ntpr_group_0 = float(eq_odds['tpr_group_0'].rstrip('%')) / 100\ntpr_group_1 = float(eq_odds['tpr_group_1'].rstrip('%')) / 100\naxes[0, 1].bar(groups, [tpr_group_0, tpr_group_1], color=colors, alpha=0.7, edgecolor='black')\naxes[0, 1].axhline(y=0.5, color='blue', linestyle='--', linewidth=2, label='Equal (Fair)')\naxes[0, 1].set_ylabel('True Positive Rate', fontsize=11, fontweight='bold')\naxes[0, 1].set_title('Equalized Odds: True Positive Rates', fontsize=12, fontweight='bold')\naxes[0, 1].set_ylim([0, 1])\naxes[0, 1].legend()\nfor i, rate in enumerate([tpr_group_0, tpr_group_1]):\n    axes[0, 1].text(i, rate + 0.03, f'{rate*100:.1f}%', ha='center', fontweight='bold')\n\n# Plot 3: False positive rates (false alarm rate)\nfpr_group_0 = float(eq_odds['fpr_group_0'].rstrip('%')) / 100\nfpr_group_1 = float(eq_odds['fpr_group_1'].rstrip('%')) / 100\naxes[1, 0].bar(groups, [fpr_group_0, fpr_group_1], color=colors, alpha=0.7, edgecolor='black')\naxes[1, 0].axhline(y=0.0, color='blue', linestyle='--', linewidth=2, label='Equal (Fair)')\naxes[1, 0].set_ylabel('False Positive Rate', fontsize=11, fontweight='bold')\naxes[1, 0].set_title('Equalized Odds: False Positive Rates', fontsize=12, fontweight='bold')\naxes[1, 0].set_ylim([0, 0.3])\naxes[1, 0].legend()\nfor i, rate in enumerate([fpr_group_0, fpr_group_1]):\n    axes[1, 0].text(i, rate + 0.01, f'{rate*100:.1f}%', ha='center', fontweight='bold')\n\n# Plot 4: Fairness trade-offs (conceptual)\nparity_ratio = float(dem_parity['parity_ratio'])\nfairness_metrics = ['Demographic\\nParity', 'Equalized\\nOdds', 'Predictive\\nParity']\nfairness_scores = [parity_ratio if parity_ratio <= 1 else 2 - parity_ratio,  # Normalize\n                  0.5,  # Simulated\n                  0.6]  # Simulated\n\naxes[1, 1].barh(fairness_metrics, fairness_scores, color=['#e74c3c', '#f39c12', '#e67e22'], alpha=0.7, edgecolor='black')\naxes[1, 1].axvline(x=0.8, color='green', linestyle='--', linewidth=2, label='Acceptable (>0.8)')\naxes[1, 1].set_xlabel('Fairness Score', fontsize=11, fontweight='bold')\naxes[1, 1].set_title('Fairness Metric Trade-offs', fontsize=12, fontweight='bold')\naxes[1, 1].set_xlim([0, 1])\naxes[1, 1].legend()\nfor i, score in enumerate(fairness_scores):\n    axes[1, 1].text(score + 0.02, i, f'{score:.2f}', va='center', fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nðŸ“Š KEY INSIGHT: Fairness metrics often conflict!\")\nprint(\"   Different definitions can't all be satisfied simultaneously.\")\nprint(\"   You must choose which fairness definition is most important for your use case.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Ethical Analysis of a Research Project\n",
    "\n",
    "You are designing a machine learning system to predict employee turnover risk. The model will analyze employee data (age, salary, performance ratings, email sentiment) to identify who is likely to leave.\n",
    "\n",
    "**Task**: Analyze this project against the five ethical principles.\n",
    "\n",
    "For each principle, answer:\n",
    "1. What are the potential ethical concerns?\n",
    "2. What mitigation strategies would you implement?\n",
    "3. What metrics would you monitor?\n",
    "\n",
    "Use the framework below to structure your response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Ethical Analysis\n\nanalysis_template = {\n    'Fairness': {\n        'concerns': \"[Your answer]\",\n        'mitigations': \"[Your answer]\",\n        'metrics': \"[Your answer]\"\n    },\n    'Transparency': {\n        'concerns': \"[Your answer]\",\n        'mitigations': \"[Your answer]\",\n        'metrics': \"[Your answer]\"\n    },\n    'Accountability': {\n        'concerns': \"[Your answer]\",\n        'mitigations': \"[Your answer]\",\n        'metrics': \"[Your answer]\"\n    },\n    'Privacy': {\n        'concerns': \"[Your answer]\",\n        'mitigations': \"[Your answer]\",\n        'metrics': \"[Your answer]\"\n    },\n    'Beneficence': {\n        'concerns': \"[Your answer]\",\n        'mitigations': \"[Your answer]\",\n        'metrics': \"[Your answer]\"\n    }\n}\n\n# Example for Fairness principle (you should complete the rest)\nexample_analysis = {\n    'principle': 'Fairness',\n    'concerns': 'Age bias: older employees might be flagged as flight risks based on retirement patterns',\n    'mitigations': 'Audit model for performance disparities across age groups; use equalized odds metric',\n    'monitoring': 'Track true positive rates by age group monthly'\n}\n\nprint(\"\\nEXERCISE 1: ETHICAL ANALYSIS - TURNOVER PREDICTION MODEL\")\nprint(\"=\"*70)\nprint(\"\\nExample Analysis for Fairness:\")\nfor key, value in example_analysis.items():\n    print(f\"{key.title()}: {value}\")\n    \nprint(\"\\n\\nNow complete your analysis for all five principles...\")\nprint(\"Save your responses in the analysis_template dictionary above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: IRB Determination\n",
    "\n",
    "Classify each research scenario and specify whether IRB review is needed and at what level.\n",
    "\n",
    "**Scenario 1**: A software company anonymously surveys 500 current users about product satisfaction. Responses are collected via online form, no personal information is collected, and results will be used internally only.\n",
    "\n",
    "**Scenario 2**: A university study asks pregnant women about medication use during pregnancy. The researcher will link survey responses to medical records to compare outcomes. This involves gathering new data (surveys) and accessing sensitive medical information.\n",
    "\n",
    "**Scenario 3**: A data science team analyzes a public dataset of de-identified patient outcomes from 2015-2020 to test whether a new algorithm can predict hospital readmission rates. The dataset was previously published in a journal and contains no identifiable information.\n",
    "\n",
    "For each scenario, determine: **Does it require IRB? If yes, what level?** Justify your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: IRB Determination\n",
    "\nscenario_evaluations = {\n    'Scenario 1': {\n        'requires_irb': \"???\",\n        'review_level': \"???\",\n        'justification': \"[Your explanation]\"\n    },\n    'Scenario 2': {\n        'requires_irb': \"???\",\n        'review_level': \"???\",\n        'justification': \"[Your explanation]\"\n    },\n    'Scenario 3': {\n        'requires_irb': \"???\",\n        'review_level': \"???\",\n        'justification': \"[Your explanation]\"\n    }\n}\n\nprint(\"\\nEXERCISE 2: IRB DETERMINATION\")\nprint(\"=\"*70)\nprint(\"\\nComplete the scenario_evaluations dictionary with your answers.\")\nprint(\"\\nConsider these factors:\")\nprint(\"  - Does it involve human subjects?\")\nprint(\"  - Does it have interaction/intervention with subjects?\")\nprint(\"  - Are personal identifiable data used?\")\nprint(\"  - Is there minimal risk to subjects?\")\nprint(\"  - Are data truly de-identified/anonymous?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: GDPR Compliance Check\n",
    "\n",
    "Your company is deploying a new recommendation engine in Germany. It processes:  \n- User email addresses\n- Purchase history\n- Location data\n- Browsing behavior\n\nYou need to prepare a compliance checklist. Use the privacy by design framework to create a data protection strategy.\n",
    "\n",
    "**Requirements**:\n",
    "1. Write GDPR-compliant consent language\n",
    "2. Specify data minimization strategy\n",
    "3. Outline retention policies\n",
    "4. Define access controls\n",
    "5. Document purpose limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: GDPR Compliance Strategy\n",
    "\ngdpr_compliance_plan = {\n    'consent_language': \"\"\"[Write your GDPR-compliant consent text here]\n    \nMust include:\n    - Freely given option (can refuse)\n    - Specific purpose\n    - Clear data types\n    - How data will be used\n    - Right to withdraw\n    - Opt-in (not pre-checked)\n    \"\"\",\n    \n    'data_minimization': {\n        'necessary_fields': \"[List only essential data]\",\n        'optional_fields': \"[List nice-to-have data to remove]\",\n        'justification': \"[Why each necessary field is needed]\"\n    },\n    \n    'retention_policy': {\n        'purchase_history': \"[Days/months to retain]\",\n        'browsing_data': \"[Days/months to retain]\",\n        'email': \"[Days/months to retain]\",\n        'deletion_triggers': \"[When to automatically delete]\"\n    },\n    \n    'access_controls': {\n        'data_scientist_access': \"[What can they see?]\",\n        'product_manager_access': \"[What can they see?]\",\n        'external_vendors': \"[Any third party access?]\",\n        'audit_logging': \"[How to track access?]\"\n    },\n    \n    'purpose_limitation': {\n        'primary_purpose': \"[Recommendation engine only]\",\n        'prohibited_uses': \"[List what data cannot be used for]\",\n        'new_uses': \"[How to handle requests for new uses?]\"\n    }\n}\n\nprint(\"\\nEXERCISE 3: GDPR COMPLIANCE STRATEGY\")\nprint(\"=\"*70)\nprint(\"\\nComplete the gdpr_compliance_plan dictionary with your detailed strategy.\")\nprint(\"\\nRemember GDPR principles:\")\nprint(\"  âœ“ Data minimization: Collect only what you need\")\nprint(\"  âœ“ Purpose limitation: Use data only for stated purposes\")\nprint(\"  âœ“ Storage limitation: Delete when no longer needed\")\nprint(\"  âœ“ Transparency: Clear communication with users\")\nprint(\"  âœ“ Accountability: Document everything\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Bias Detection and Mitigation\n",
    "\n",
    "You build a model to recommend criminal defendants for early release. Testing reveals:\n",
    "- Overall accuracy: 82%\n",
    "- Accuracy for white defendants: 87%\n",
    "- Accuracy for Black defendants: 71%\n",
    "- False positive rate (incorrect dangerous rating): 8% white, 22% Black\n",
    "\n",
    "**Task**:  \n",
    "1. Identify which fairness criterion this violates\n",
    "2. Explain the real-world harm\n",
    "3. Propose three mitigation strategies (pre/in/post-processing)\n",
    "4. What trade-offs would you accept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Bias Detection Analysis\n",
    "\nbias_analysis = {\n    'violated_criterion': \"[Which fairness metric is violated?]\",\n    'explanation': \"[Why is this a problem?]\",\n    \n    'real_world_harm': \"\"\"[Describe concrete harms:\n        - Black defendants incorrectly marked as dangerous\n        - False positives prevent release for less dangerous individuals\n        - Perpetuates racial disparities in criminal justice]\"\"\",\n    \n    'mitigation_strategies': {\n        'preprocessing': \"[Data collection or feature engineering approaches]\",\n        'in_processing': \"[Model training modifications]\",\n        'post_processing': \"[Prediction adjustment techniques]\"\n    },\n    \n    'acceptable_tradeoffs': \"[What performance loss is acceptable to achieve fairness?]\"\n}\n\nprint(\"\\nEXERCISE 4: BIAS IN CRIMINAL JUSTICE\")\nprint(\"=\"*70)\nprint(\"\\nThis is a real issue: COMPAS algorithm showed similar bias.\")\nprint(\"\\nAnalyze this scenario in the bias_analysis dictionary.\")\nprint(\"\\nConsider:\")\nprint(\"  - Demographic parity vs equalized odds trade-off\")\nprint(\"  - Cost of different types of errors\")\nprint(\"  - Whether perfect fairness is even possible\")\nprint(\"  - Who decides the acceptable trade-off?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Professional Ethics Codes and EU Guidelines\n",
    "\n",
    "Multiple professional organizations have established ethics codes for AI and research:\n",
    "\n",
    "### Major Ethics Codes and Standards\n",
    "\n",
    "| Organization | Code | Key Focus |\n",
    "|--------------|------|----------|\n",
    "| **ACM** | Code of Ethics and Professional Conduct | Software engineering ethics |\n",
    "| **IEEE** | Ethically Aligned Design | AI and autonomous systems |\n",
    "| **UNESCO** | Recommendations on AI Ethics | Global perspective |\n",
    "| **OECD** | AI Principles | Cross-national guidelines |\n",
    "| **EU** | Ethics Guidelines for Trustworthy AI | GDPR + fairness |\n",
    "\n",
    "### EU Ethics Guidelines: Seven Key Requirements\n",
    "\n",
    "The European Commission's guidelines require:\n",
    "\n",
    "1. **Human Agency and Oversight**: Humans maintain meaningful control\n",
    "2. **Technical Robustness and Safety**: Systems are secure and reliable\n",
    "3. **Privacy and Data Governance**: GDPR + data quality\n",
    "4. **Transparency**: Explainability and communication\n",
    "5. **Diversity, Non-Discrimination, and Fairness**: Representation and equity\n",
    "6. **Accountability**: Responsibility mechanisms\n",
    "7. **Environmental and Societal Well-being**: Broader impact consideration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "### Five Core Ethical Principles\n",
    "\n",
    "âœ… **Fairness**: Systematically audit for bias; track disparate impact metrics\n",
    "\n",
    "âœ… **Transparency**: Explain model decisions to stakeholders; document assumptions\n",
    "\n",
    "âœ… **Accountability**: Assign clear responsibility; maintain audit trails\n",
    "\n",
    "âœ… **Privacy**: Implement privacy by design; enforce GDPR compliance\n",
    "\n",
    "âœ… **Beneficence**: Balance benefits and harms; protect vulnerable populations\n",
    "\n",
    "### IRB Requirements\n",
    "\n",
    "âœ… IRB review required when research involves **human subjects + intervention/interaction + PII**\n",
    "\n",
    "âœ… Exemption categories exist (secondary analysis of de-identified data)\n",
    "\n",
    "âœ… Three review levels: Exempt â†’ Expedited â†’ Full Board (by risk level)\n",
    "\n",
    "âœ… Different standards apply globally (GDPR in EU, Common Rule in US)\n",
    "\n",
    "### GDPR Compliance\n",
    "\n",
    "âœ… Applies to any organization processing EEA residents' data\n",
    "\n",
    "âœ… Consent must be freely given, specific, informed, unambiguous, revocable\n",
    "\n",
    "âœ… Privacy by design: minimize, encrypt, access control, retention limits\n",
    "\n",
    "âœ… Violating GDPR costs up to â‚¬20M or 4% of global revenue\n",
    "\n",
    "### Bias Detection and Mitigation\n",
    "\n",
    "âœ… Multiple fairness metrics (demographic parity, equalized odds) capture different aspects\n",
    "\n",
    "âœ… Three stages: pre-processing (data), in-processing (training), post-processing (predictions)\n",
    "\n",
    "âœ… Fairness metrics often conflict; choose the right definition for your use case\n",
    "\n",
    "âœ… Regular auditing and monitoring are essential ongoing practices\n",
    "\n",
    "### Practical Implementation\n",
    "\n",
    "âœ… Use ethical checklists before deployment (like the examples in Exercises 1-4)\n",
    "\n",
    "âœ… Document all ethical decisions and trade-offs\n",
    "\n",
    "âœ… Build diverse teams; include affected stakeholders in decisions\n",
    "\n",
    "âœ… Plan for monitoring bias and harms post-deployment\n",
    "\n",
    "âœ… Remember: ethical research is not about perfect solutions, but thoughtful choices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next?\n",
    "\n",
    "**Module 10: Research Communication** will cover:\n",
    "- Writing research papers with integrity\n",
    "- Creating transparent visualizations\n",
    "- Disclosing limitations and conflicts of interest\n",
    "- Engaging with policy makers and public audiences\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "### Books\n",
    "- \"Ethics for AI\" by Shannon Vallor\n",
    "- \"Fairness and Machine Learning\" by Barocas, Hardt, Narayanan (free online)\n",
    "- \"The Ethical Algorithm\" by Kearns & Roth\n",
    "\n",
    "### Tools\n",
    "- **Fairlearn**: Microsoft's fairness library\n",
    "- **AIF360**: IBM's algorithmic fairness toolkit\n",
    "- **Lime/SHAP**: Model explainability tools\n",
    "\n",
    "### Official Guidelines\n",
    "- EU Ethics Guidelines for Trustworthy AI\n",
    "- IEEE Ethically Aligned Design\n",
    "- ACM Code of Ethics and Professional Conduct\n",
    "- GDPR Official Text (gdpr-info.eu)\n",
    "\n",
    "### Case Studies\n",
    "- COMPAS algorithm: Racial bias in recidivism prediction\n",
    "- Amazon hiring AI: Gender bias in resume screening\n",
    "- Facial recognition: Disparate performance across race/gender\n",
    "- Loan approval systems: Redlining patterns in modern ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Assessment\n",
    "\n",
    "Before moving to Module 10, ensure you can:\n",
    "\n",
    "- [ ] Explain the five core ethical principles and give examples of each\n",
    "- [ ] Determine whether research requires IRB review and at what level\n",
    "- [ ] Identify when GDPR applies and implement compliant consent processes\n",
    "- [ ] Calculate fairness metrics (demographic parity, equalized odds)\n",
    "- [ ] Detect bias across demographic groups in ML models\n",
    "- [ ] Propose bias mitigation strategies at pre/in/post-processing stages\n",
    "- [ ] Implement privacy by design principles in systems\n",
    "- [ ] Conduct ethical analysis using frameworks like the five principles\n",
    "- [ ] Recognize fairness metric trade-offs and make intentional choices\n",
    "- [ ] Apply professional ethics codes to real research scenarios\n",
    "\n",
    "If you can confidently check all boxes, you're ready for Module 10! ðŸŽ‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
