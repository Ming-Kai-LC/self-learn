{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 05: Statistical Validation and Hypothesis Testing\n",
    "\n",
    "**Difficulty**: \u2b50\u2b50 (Intermediate)\n",
    "\n",
    "**Estimated Time**: 75 minutes\n",
    "\n",
    "**Prerequisites**: Module 04 (Data Quality Assessment)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. **Implement proper train/validation/test splits** that prevent data leakage\n",
    "2. **Apply k-fold cross-validation correctly** to obtain robust performance estimates\n",
    "3. **Conduct statistical hypothesis tests** with appropriate null and alternative hypotheses\n",
    "4. **Calculate power analysis** for a priori sample size determination\n",
    "5. **Understand Type I and Type II errors** and their trade-offs\n",
    "6. **Apply multiple comparison corrections** to control family-wise error rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's import the libraries we'll use in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard data science libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Statistical testing\n",
    "from scipy import stats\n",
    "from scipy.stats import ttest_ind, chi2_contingency, f_oneway, mannwhitneyu\n",
    "\n",
    "# Machine learning and validation\n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit, cross_val_score, train_test_split\n",
    "from sklearn.datasets import make_classification, make_regression\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "\n",
    "# Power analysis\n",
    "from statsmodels.stats.power import tt_solve_power, tt_ind_solve_power, FTestAnovaPower\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# Configuration for better visualizations\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"✓ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Splitting: Train, Validation, and Test Sets\n",
    "\n",
    "### The Principle: Never Touch Your Test Set Twice\n",
    "\n",
    "A fundamental rule in rigorous research: your **test set represents the future**. Once you've decided to test your final model on it, you cannot use it again to make any other decisions.\n",
    "\n",
    "### The Three-Way Split\n",
    "\n",
    "For most machine learning projects, divide your data into three independent sets:\n",
    "\n",
    "| Set | Purpose | Size | Usage |\n",
    "|-----|---------|------|-------|\n",
    "| **Training** | Fit model parameters | 70-80% | Seen by model during learning |\n",
    "| **Validation** | Tune hyperparameters, select model | 10-15% | Used for decision-making |\n",
    "| **Test** | Final performance estimate | 10-15% | Touched ONCE at the very end |\n",
    "\n",
    "### Why Three Sets?\n",
    "\n",
    "**With just train/test split:**\n",
    "- You might accidentally memorize patterns in the test set during hyperparameter tuning\n",
    "- When you use test performance to decide which model is \"best\", you're optimizing for that specific test set\n",
    "- This leads to **optimistic bias** in your final performance estimates\n",
    "\n",
    "**With train/validation/test split:**\n",
    "- Training set: Only the model sees this (closed to you)\n",
    "- Validation set: You use this for all decisions (model selection, hyperparameter tuning)\n",
    "- Test set: You report this number once, at the very end (final ground truth)\n",
    "\n",
    "### Special Case: Time-Series Data\n",
    "\n",
    "For time-series data, **respect temporal ordering**:\n",
    "\n",
    "```\n",
    "Training Data    Validation Data    Test Data\n",
    "[Jan-May]        [Jun-Jul]          [Aug-Dec]  ← Earlier → Later\n",
    "\n",
    "✓ CORRECT: Train on past, validate and test on future\n",
    "\n",
    "✗ WRONG: Random shuffle\n",
    "[Jan, May, Jul]  [Feb, Jun, Aug]    [Mar, Apr, Sep]  ← Data leakage!\n",
    "```\n",
    "\n",
    "Training on future data to predict the past is impossible in real applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Creating proper train/validation/test split\n",
    "\n",
    "# Create a synthetic dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=20,\n",
    "    n_informative=10,\n",
    "    n_redundant=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Total dataset size: {len(X)} samples\")\n",
    "print()\n",
    "\n",
    "# Step 1: First split - separate test set (untouchable)\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.15,  # 15% for final testing\n",
    "    random_state=42,\n",
    "    stratify=y  # Maintain class balance\n",
    ")\n",
    "\n",
    "# Step 2: Split remaining data into training and validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val,\n",
    "    test_size=0.176,  # 176/1000 * (1 - 0.15) ≈ 15% of total\n",
    "    random_state=42,\n",
    "    stratify=y_train_val\n",
    ")\n",
    "\n",
    "# Display the split\n",
    "print(\"DATA SPLIT SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Training set:   {len(X_train)} samples ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"Validation set: {len(X_val)} samples ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set:       {len(X_test)} samples ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Verify class balance is maintained\n",
    "print(\"\\nClass Distribution (Positive Class):\")\n",
    "print(f\"Original:   {(y == 1).sum() / len(y) * 100:.1f}%\")\n",
    "print(f\"Training:   {(y_train == 1).sum() / len(y_train) * 100:.1f}%\")\n",
    "print(f\"Validation: {(y_val == 1).sum() / len(y_val) * 100:.1f}%\")\n",
    "print(f\"Test:       {(y_test == 1).sum() / len(y_test) * 100:.1f}%\")\n",
    "print(\"\\n✓ Class balance maintained across all sets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Demonstrating data leakage - what NOT to do\n",
    "\n",
    "print(\"DATA LEAKAGE EXAMPLE: What Happens When You Peek at Test Data\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# WRONG WAY: Fitting preprocessing on ALL data (including test)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ❌ WRONG: Fit scaler on entire dataset\n",
    "scaler_wrong = StandardScaler()\n",
    "X_scaled_wrong = scaler_wrong.fit_transform(X)  # Scaler sees test data!\n",
    "\n",
    "# Split after scaling (data leakage!)\n",
    "X_train_wrong, X_test_wrong, y_train_wrong, y_test_wrong = train_test_split(\n",
    "    X_scaled_wrong, y, test_size=0.15, random_state=42\n",
    ")\n",
    "\n",
    "# ✓ RIGHT WAY: Fit preprocessing only on training data\n",
    "X_train_clean, X_test_clean, y_train_clean, y_test_clean = train_test_split(\n",
    "    X, y, test_size=0.15, random_state=42\n",
    ")\n",
    "\n",
    "scaler_right = StandardScaler()\n",
    "X_train_scaled = scaler_right.fit_transform(X_train_clean)  # Fit only on training\n",
    "X_test_scaled = scaler_right.transform(X_test_clean)  # Transform test with training stats\n",
    "\n",
    "# Compare: Train a model both ways\n",
    "model_wrong = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model_wrong.fit(X_train_wrong, y_train_wrong)\n",
    "acc_wrong = accuracy_score(y_test_wrong, model_wrong.predict(X_test_wrong))\n",
    "\n",
    "model_right = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model_right.fit(X_train_scaled, y_train_clean)\n",
    "acc_right = accuracy_score(y_test_clean, model_right.predict(X_test_scaled))\n",
    "\n",
    "print(f\"\\nLeaked approach (scaler sees test data): {acc_wrong:.4f}\")\n",
    "print(f\"Correct approach (clean split):          {acc_right:.4f}\")\n",
    "print(f\"\\nDifference: {(acc_wrong - acc_right)*100:.2f} percentage points\")\n",
    "print(\"\\n⚠️  The 'leaked' approach shows artificially high performance!\")\n",
    "print(\"   In real deployment, performance would be worse.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Proper Train/Validation/Test Split\n",
    "\n",
    "You have 5000 customer records with a binary outcome (churned/retained):\n",
    "- 3200 retained customers (label=0)\n",
    "- 1800 churned customers (label=1)\n",
    "\n",
    "Your task: Create a proper train/validation/test split with:\n",
    "1. Test set: 15% of data\n",
    "2. Validation set: 15% of remaining data\n",
    "3. Training set: Rest\n",
    "4. Maintain class balance across all sets\n",
    "\n",
    "Calculate and report:\n",
    "- Size of each set\n",
    "- Proportion of churned customers in each set\n",
    "- Any differences in class balance\n",
    "\n",
    "**Hint**: Use `train_test_split` twice with `stratify=y` parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Create proper train/validation/test split\n",
    "\n",
    "# Create customer churn dataset\n",
    "np.random.seed(42)\n",
    "n_total = 5000\n",
    "y_churn = np.concatenate([\n",
    "    np.zeros(3200),   # Retained\n",
    "    np.ones(1800)     # Churned\n",
    "])\n",
    "np.random.shuffle(y_churn)\n",
    "\n",
    "X_churn = np.random.randn(n_total, 10)  # Random features\n",
    "\n",
    "print(\"Customer Churn Dataset:\")\n",
    "print(f\"Total samples: {len(y_churn)}\")\n",
    "print(f\"Churned (label=1): {(y_churn == 1).sum()} ({(y_churn == 1).sum()/len(y_churn)*100:.1f}%)\")\n",
    "print(f\"Retained (label=0): {(y_churn == 0).sum()} ({(y_churn == 0).sum()/len(y_churn)*100:.1f}%)\")\n",
    "print()\n",
    "\n",
    "# TODO: Implement proper 3-way split here\n",
    "# Step 1: Separate 15% for test set\n",
    "# X_train_val, X_test, y_train_val, y_test = train_test_split(...)\n",
    "\n",
    "# Step 2: Split remaining into train and validation\n",
    "# X_train, X_val, y_train, y_val = train_test_split(...)\n",
    "\n",
    "# Report results:\n",
    "# print(f\"Training:   {len(X_train)} samples\")\n",
    "# print(f\"Validation: {len(X_val)} samples\")\n",
    "# print(f\"Test:       {len(X_test)} samples\")\n",
    "# print(f\"\\nChurned rate by set:\")\n",
    "# print(f\"Training:   {(y_train == 1).sum() / len(y_train) * 100:.1f}%\")\n",
    "# print(f\"Validation: {(y_val == 1).sum() / len(y_val) * 100:.1f}%\")\n",
    "# print(f\"Test:       {(y_test == 1).sum() / len(y_test) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cross-Validation: More Robust Performance Estimates\n",
    "\n",
    "### The Problem with Single Train/Test Split\n",
    "\n",
    "A single 80/20 split depends heavily on which specific samples end up in training vs. test:\n",
    "\n",
    "- Split A: Get lucky with test set → Accuracy = 88%\n",
    "- Split B: Get unlucky with test set → Accuracy = 82%\n",
    "- Which is the \"true\" performance?\n",
    "\n",
    "**Answer**: K-fold cross-validation averages over many different splits.\n",
    "\n",
    "### How K-Fold Cross-Validation Works\n",
    "\n",
    "```\n",
    "Original Data: [▀▀▀▀▀▀▀▀▀▀] (10 samples, k=5)\n",
    "\n",
    "Fold 1: Train [▀▀▀▀▀▀▀▀]  Test [▀▀]  → Accuracy₁ = 85%\n",
    "Fold 2: Train [▀▀▀▀▀▀▀▀]  Test [▀▀]  → Accuracy₂ = 87%\n",
    "Fold 3: Train [▀▀▀▀▀▀▀▀]  Test [▀▀]  → Accuracy₃ = 86%\n",
    "Fold 4: Train [▀▀▀▀▀▀▀▀]  Test [▀▀]  → Accuracy₄ = 84%\n",
    "Fold 5: Train [▀▀▀▀▀▀▀▀]  Test [▀▀]  → Accuracy₅ = 88%\n",
    "\n",
    "Mean Performance: (85+87+86+84+88)/5 = 86.0% ± 1.6%\n",
    "```\n",
    "\n",
    "### Advantages of K-Fold CV\n",
    "\n",
    "✓ **More stable**: Uses all data for both training and testing\n",
    "✓ **Better error bars**: SD of k accuracy scores shows variability\n",
    "✓ **Smaller variance**: More realistic estimate of true performance\n",
    "✓ **Full utilization**: No data wasted (vs. holding out 20% permanently)\n",
    "\n",
    "### Choosing K\n",
    "\n",
    "- **k=5**: Fast, standard choice for most problems\n",
    "- **k=10**: More computation, slightly lower variance\n",
    "- **k=3**: Minimum for small datasets\n",
    "- **Leave-One-Out (k=n)**: Maximum stability, very slow\n",
    "\n",
    "### Important: CV for Hyperparameter Tuning\n",
    "\n",
    "**The right way**:\n",
    "```python\n",
    "for each combination of hyperparameters:\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "    mean_cv_score = cv_scores.mean()\n",
    "    \n",
    "best_params = hyperparameters with highest mean_cv_score\n",
    "\n",
    "# Fit final model on full training set\n",
    "final_model = Model(best_params).fit(X_train, y_train)\n",
    "\n",
    "# Report on held-out test set (ONLY ONCE)\n",
    "test_score = final_model.score(X_test, y_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: K-Fold Cross-Validation in Action\n",
    "\n",
    "# Use our training data from earlier\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Method 1: Manual k-fold\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "fold_scores = []\n",
    "fold_number = 1\n",
    "\n",
    "print(\"K-FOLD CROSS-VALIDATION (Manual Implementation)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for train_idx, val_idx in kfold.split(X_train):\n",
    "    # Split data\n",
    "    X_fold_train = X_train[train_idx]\n",
    "    y_fold_train = y_train[train_idx]\n",
    "    X_fold_val = X_train[val_idx]\n",
    "    y_fold_val = y_train[val_idx]\n",
    "    \n",
    "    # Train and evaluate\n",
    "    model.fit(X_fold_train, y_fold_train)\n",
    "    score = accuracy_score(y_fold_val, model.predict(X_fold_val))\n",
    "    fold_scores.append(score)\n",
    "    \n",
    "    print(f\"Fold {fold_number}: Accuracy = {score:.4f}\")\n",
    "    fold_number += 1\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"Mean CV Accuracy: {np.mean(fold_scores):.4f}\")\n",
    "print(f\"Std Dev:          {np.std(fold_scores):.4f}\")\n",
    "print(f\"95% CI:           [{np.mean(fold_scores) - 1.96*np.std(fold_scores)/np.sqrt(5):.4f}, \"\n",
    "      f\"{np.mean(fold_scores) + 1.96*np.std(fold_scores)/np.sqrt(5):.4f}]\")\n",
    "\n",
    "# Method 2: Using scikit-learn's cross_val_score (cleaner)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Using scikit-learn's cross_val_score:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "cv_scores = cross_val_score(\n",
    "    LogisticRegression(max_iter=1000, random_state=42),\n",
    "    X_train, y_train,\n",
    "    cv=5,\n",
    "    scoring='accuracy'\n",
    ")\n",
    "\n",
    "print(f\"CV Scores: {cv_scores}\")\n",
    "print(f\"\\nMean: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Time-Series Cross-Validation (Respecting Temporal Order)\n",
    "\n",
    "# Create synthetic time-series data\n",
    "n_time_steps = 200\n",
    "X_ts = np.random.randn(n_time_steps, 5)\n",
    "# Create target with time-series pattern\n",
    "y_ts = (np.sin(np.arange(n_time_steps) / 20) + np.random.randn(n_time_steps) * 0.1) > 0\n",
    "\n",
    "print(\"TIME-SERIES CROSS-VALIDATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total samples: {n_time_steps} (time steps)\\n\")\n",
    "\n",
    "# Use TimeSeriesSplit to respect temporal ordering\n",
    "ts_cv = TimeSeriesSplit(n_splits=4)\n",
    "\n",
    "fold_number = 1\n",
    "ts_scores = []\n",
    "\n",
    "for train_idx, test_idx in ts_cv.split(X_ts):\n",
    "    # Notice: test_idx is always AFTER train_idx (temporal ordering)\n",
    "    train_start, train_end = train_idx[0], train_idx[-1]\n",
    "    test_start, test_end = test_idx[0], test_idx[-1]\n",
    "    \n",
    "    print(f\"Fold {fold_number}:\")\n",
    "    print(f\"  Training: time steps {train_start:3d}-{train_end:3d} ({len(train_idx):3d} samples)\")\n",
    "    print(f\"  Testing:  time steps {test_start:3d}-{test_end:3d} ({len(test_idx):3d} samples)\")\n",
    "    \n",
    "    # Train model\n",
    "    model_ts = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    model_ts.fit(X_ts[train_idx], y_ts[train_idx])\n",
    "    score = accuracy_score(y_ts[test_idx], model_ts.predict(X_ts[test_idx]))\n",
    "    ts_scores.append(score)\n",
    "    print(f\"  Accuracy: {score:.4f}\\n\")\n",
    "    \n",
    "    fold_number += 1\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"Mean Time-Series CV Accuracy: {np.mean(ts_scores):.4f} ± {np.std(ts_scores):.4f}\")\n",
    "print(\"\\n✓ Temporal ordering respected (no future data in training)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Cross-Validation with Error Bars\n",
    "\n",
    "Using the training data from earlier:\n",
    "1. Perform 10-fold cross-validation with LogisticRegression\n",
    "2. Calculate mean accuracy and standard error\n",
    "3. Calculate 95% confidence interval: mean ± 1.96 * SE\n",
    "4. Visualize the distribution of fold accuracies (histogram or boxplot)\n",
    "5. Interpret: What does the spread of scores tell you?\n",
    "\n",
    "**Hint**: Use `cross_val_score()` and then calculate statistics manually, or use `scipy.stats.sem()` for standard error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: 10-Fold Cross-Validation with Error Bars\n",
    "\n",
    "# TODO: Perform 10-fold cross-validation\n",
    "# cv_scores = cross_val_score(...)\n",
    "\n",
    "# TODO: Calculate mean and standard error\n",
    "# mean_accuracy = cv_scores.mean()\n",
    "# se = stats.sem(cv_scores)  # Standard error of the mean\n",
    "# ci_lower = mean_accuracy - 1.96 * se\n",
    "# ci_upper = mean_accuracy + 1.96 * se\n",
    "\n",
    "# TODO: Print results\n",
    "# print(f\"Mean Accuracy: {mean_accuracy:.4f}\")\n",
    "# print(f\"Standard Error: {se:.4f}\")\n",
    "# print(f\"95% CI: [{ci_lower:.4f}, {ci_upper:.4f}]\")\n",
    "\n",
    "# TODO: Create visualization\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "# axes[0].hist(cv_scores, bins=5, edgecolor='black')\n",
    "# axes[0].set_xlabel('Accuracy')\n",
    "# axes[0].set_ylabel('Frequency')\n",
    "# axes[0].set_title('Distribution of Fold Accuracies')\n",
    "# axes[0].axvline(mean_accuracy, color='red', linestyle='--', label='Mean')\n",
    "# axes[0].legend()\n",
    "\n",
    "# axes[1].boxplot(cv_scores)\n",
    "# axes[1].set_ylabel('Accuracy')\n",
    "# axes[1].set_title('Boxplot of Fold Accuracies')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "print(\"TODO: Complete Exercise 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hypothesis Testing: Statistical Inference\n",
    "\n",
    "### Core Concepts\n",
    "\n",
    "**Null Hypothesis (H₀)**: The default assumption - \"no effect exists\"\n",
    "- \"There is no difference between groups A and B\"\n",
    "- \"Feature X has no predictive power\"\n",
    "\n",
    "**Alternative Hypothesis (H₁)**: What you're testing for - \"some effect exists\"\n",
    "- \"Groups A and B differ\"\n",
    "- \"Feature X predicts the outcome\"\n",
    "\n",
    "**Test Statistic**: A number calculated from data that measures evidence against H₀\n",
    "- t-statistic, F-statistic, χ² statistic, etc.\n",
    "\n",
    "**P-value**: Probability of observing data this extreme (or more) IF H₀ is true\n",
    "- P-value = 0.03 means \"if there were no effect, we'd see this data 3% of the time\"\n",
    "- CRITICAL: P-value is NOT \"probability that H₀ is true\"\n",
    "\n",
    "**Significance Level (α)**: The threshold for deciding H₀ is unlikely\n",
    "- Standard: α = 0.05\n",
    "- Decision: If p < α, reject H₀; otherwise fail to reject H₀\n",
    "\n",
    "### Common Hypothesis Tests\n",
    "\n",
    "| Question | Test | Null Hypothesis |\n",
    "|----------|------|------------------|\n",
    "| Do two groups differ? | t-test (independent) | μ₁ = μ₂ |\n",
    "| Do 3+ groups differ? | ANOVA | All group means equal |\n",
    "| Is there association? | Chi-square | Variables independent |\n",
    "| Correlation significant? | Pearson test | r = 0 |\n",
    "| Non-normal data? | Mann-Whitney U | Distributions identical |\n",
    "\n",
    "### Critical: Report Exact P-Values\n",
    "\n",
    "❌ **Avoid**: \"p < 0.05\"\n",
    "✓ **Report**: \"p = 0.032\" or \"p = 0.003\"\n",
    "\n",
    "**Why?** \"p < 0.05\" loses information. The exact value tells readers how strong the evidence is.\n",
    "- p = 0.049 (barely significant)\n",
    "- p = 0.0001 (very strong evidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 5: Independent Samples T-Test\n",
    "\n",
    "# Research question: Do users with high engagement have lower churn?\n",
    "# Create two groups\n",
    "np.random.seed(42)\n",
    "\n",
    "# Low engagement group: higher churn rate (mean = 0.45)\n",
    "low_engagement_churn = np.random.binomial(n=1, p=0.45, size=150)\n",
    "\n",
    "# High engagement group: lower churn rate (mean = 0.25)\n",
    "high_engagement_churn = np.random.binomial(n=1, p=0.25, size=150)\n",
    "\n",
    "print(\"INDEPENDENT SAMPLES T-TEST\")\n",
    "print(\"=\"*60)\n",
    "print(\"Research Question: Does engagement affect churn?\")\n",
    "print()\n",
    "print(f\"Low engagement group:  mean churn = {low_engagement_churn.mean():.3f}\")\n",
    "print(f\"High engagement group: mean churn = {high_engagement_churn.mean():.3f}\")\n",
    "print(f\"Difference: {low_engagement_churn.mean() - high_engagement_churn.mean():.3f}\")\n",
    "print()\n",
    "\n",
    "# Null hypothesis: μ_low = μ_high (no difference)\n",
    "# Alternative hypothesis: μ_low ≠ μ_high (two-tailed)\n",
    "\n",
    "t_stat, p_value = ttest_ind(low_engagement_churn, high_engagement_churn)\n",
    "\n",
    "print(\"HYPOTHESIS TEST RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"H₀ (Null):       Engagement does not affect churn\")\n",
    "print(f\"H₁ (Alternative): Engagement affects churn (two-tailed)\")\n",
    "print()\n",
    "print(f\"t-statistic: {t_stat:.4f}\")\n",
    "print(f\"p-value:     {p_value:.4f}\")\n",
    "print(f\"α level:     0.05\")\n",
    "print()\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(f\"✓ REJECT H₀: Evidence suggests engagement affects churn (p = {p_value:.4f})\")\n",
    "else:\n",
    "    print(f\"✗ FAIL TO REJECT H₀: Insufficient evidence (p = {p_value:.4f})\")\n",
    "\n",
    "print()\n",
    "print(\"INTERPRETATION:\")\n",
    "print(f\"- If there were no effect of engagement on churn,\")\n",
    "print(f\"  we would observe data this extreme {p_value*100:.2f}% of the time\")\n",
    "print(f\"- This is {'unusual' if p_value < 0.05 else 'not unusual'} at α = 0.05\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 6: Effect Sizes and Confidence Intervals\n",
    "\n",
    "from scipy.stats import t\n",
    "\n",
    "print(\"EFFECT SIZES AND CONFIDENCE INTERVALS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate Cohen's d (effect size)\n",
    "n1, n2 = len(low_engagement_churn), len(high_engagement_churn)\n",
    "mean1, mean2 = low_engagement_churn.mean(), high_engagement_churn.mean()\n",
    "std1, std2 = low_engagement_churn.std(), high_engagement_churn.std()\n",
    "\n",
    "# Pooled standard deviation\n",
    "pooled_std = np.sqrt(((n1-1)*std1**2 + (n2-1)*std2**2) / (n1 + n2 - 2))\n",
    "\n",
    "# Cohen's d\n",
    "cohens_d = (mean1 - mean2) / pooled_std\n",
    "\n",
    "print(f\"Cohen's d: {cohens_d:.4f}\")\n",
    "print()\n",
    "print(\"Effect size interpretation:\")\n",
    "print(\"  |d| < 0.2: Small effect\")\n",
    "print(\"  0.2 ≤ |d| < 0.5: Small-to-medium\")\n",
    "print(\"  0.5 ≤ |d| < 0.8: Medium-to-large\")\n",
    "print(\"  |d| ≥ 0.8: Large effect\")\n",
    "\n",
    "if abs(cohens_d) < 0.2:\n",
    "    effect_interpretation = \"Small\"\n",
    "elif abs(cohens_d) < 0.5:\n",
    "    effect_interpretation = \"Small-to-medium\"\n",
    "elif abs(cohens_d) < 0.8:\n",
    "    effect_interpretation = \"Medium-to-large\"\n",
    "else:\n",
    "    effect_interpretation = \"Large\"\n",
    "\n",
    "print(f\"\\n→ This is a {effect_interpretation} effect\")\n",
    "\n",
    "# Calculate 95% Confidence Interval for difference in means\n",
    "se_diff = np.sqrt((std1**2 / n1) + (std2**2 / n2))\n",
    "df = n1 + n2 - 2\n",
    "t_critical = t.ppf(0.975, df)  # 97.5th percentile for 95% CI\n",
    "\n",
    "mean_diff = mean1 - mean2\n",
    "ci_lower = mean_diff - t_critical * se_diff\n",
    "ci_upper = mean_diff + t_critical * se_diff\n",
    "\n",
    "print(f\"\\n95% Confidence Interval for difference in means:\")\n",
    "print(f\"[{ci_lower:.4f}, {ci_upper:.4f}]\")\n",
    "print()\n",
    "print(\"Interpretation:\")\n",
    "print(f\"- The true difference in churn rates is likely between {ci_lower:.3f} and {ci_upper:.3f}\")\n",
    "print(f\"- Since the CI doesn't include 0, we're confident the difference is real\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Hypothesis Testing with Real Scenario\n",
    "\n",
    "A company A/B tests a new feature with:\n",
    "- Control group: 500 users, 245 converted\n",
    "- Treatment group: 480 users, 260 converted\n",
    "\n",
    "Your task:\n",
    "1. State H₀ and H₁ formally\n",
    "2. Perform an appropriate hypothesis test\n",
    "3. Report the exact p-value\n",
    "4. Calculate effect size (Cohen's h or similar)\n",
    "5. Calculate 95% CI for the difference in conversion rates\n",
    "6. Make a business decision: Should we deploy this feature? Why/why not?\n",
    "\n",
    "**Hint**: Use chi-square test or compare proportions test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: A/B Test Hypothesis Testing\n",
    "\n",
    "# Data from A/B test\n",
    "control_conversions = 245\n",
    "control_total = 500\n",
    "treatment_conversions = 260\n",
    "treatment_total = 480\n",
    "\n",
    "print(\"A/B TEST ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Control:   {control_conversions}/{control_total} conversions = {control_conversions/control_total:.1%}\")\n",
    "print(f\"Treatment: {treatment_conversions}/{treatment_total} conversions = {treatment_conversions/treatment_total:.1%}\")\n",
    "print()\n",
    "\n",
    "# TODO: \n",
    "# 1. State hypotheses\n",
    "print(\"HYPOTHESES:\")\n",
    "print(\"H₀: ???\")\n",
    "print(\"H₁: ???\")\n",
    "print()\n",
    "\n",
    "# 2. Perform chi-square test\n",
    "# Create contingency table\n",
    "contingency_table = np.array([\n",
    "    [control_conversions, control_total - control_conversions],\n",
    "    [treatment_conversions, treatment_total - treatment_conversions]\n",
    "])\n",
    "\n",
    "# TODO: chi2_stat, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "# 3. Report p-value\n",
    "# print(f\"χ² statistic: {chi2_stat:.4f}\")\n",
    "# print(f\"p-value: {p_value:.4f}\")\n",
    "# print(f\"Significant at α=0.05? {p_value < 0.05}\")\n",
    "\n",
    "# 4. Calculate effect size\n",
    "# TODO: Calculate Cramér's V or Cohen's h\n",
    "\n",
    "# 5. Calculate 95% CI for difference\n",
    "# TODO: Use proportion CI formula\n",
    "\n",
    "# 6. Business decision\n",
    "# TODO: Summarize findings\n",
    "\n",
    "print(\"TODO: Complete Exercise 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Power Analysis: Planning Sample Size\n",
    "\n",
    "### The Problem: How Many Subjects Do I Need?\n",
    "\n",
    "**Scenario**: You want to run an experiment but don't know how large your sample needs to be.\n",
    "\n",
    "- Too small: Risk failing to detect a real effect (low power)\n",
    "- Too large: Waste resources, unnecessary cost\n",
    "\n",
    "**Power analysis solves this** by determining minimum sample size for a desired power level.\n",
    "\n",
    "### Key Concepts: Four Things That Are Related\n",
    "\n",
    "```\n",
    "1. Sample Size (n)\n",
    "2. Significance Level (α) - usually 0.05\n",
    "3. Power (1 - β) - usually 0.80\n",
    "4. Effect Size (d) - the minimum difference you want to detect\n",
    "\n",
    "Given ANY THREE, you can calculate the FOURTH\n",
    "```\n",
    "\n",
    "### Understanding Power\n",
    "\n",
    "**Power** = Probability of detecting a real effect IF it exists\n",
    "- Power = 0.80: 80% chance of detecting the effect\n",
    "- Power = 0.20: β (Type II error rate) = probability of missing the effect\n",
    "\n",
    "**Standard target**: Power = 0.80\n",
    "- This is a convention: 80% power for 5% significance level\n",
    "- Higher power (0.90) requires larger sample sizes\n",
    "\n",
    "### A Priori vs. Post Hoc Power Analysis\n",
    "\n",
    "| Type | When | Purpose | Interpretation |\n",
    "|------|------|---------|----------------|\n",
    "| **A Priori** | BEFORE collecting data | Plan sample size | \"We need n=150 subjects\" |\n",
    "| **Post Hoc** | AFTER collecting data | Understand results | ✗ Generally not recommended |\n",
    "\n",
    "**Warning**: Post-hoc power analysis is controversial because it's just a function of the p-value. Use confidence intervals instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 7: A Priori Power Analysis for t-test\n",
    "\n",
    "print(\"A PRIORI POWER ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "print(\"Scenario: Plan a study comparing treatment vs. control\")\n",
    "print()\n",
    "\n",
    "# Parameters\n",
    "effect_size = 0.5  # Cohen's d = 0.5 (medium effect)\n",
    "alpha = 0.05\n",
    "power = 0.80\n",
    "\n",
    "# Calculate required sample size\n",
    "n_required = tt_ind_solve_power(\n",
    "    effect_size=effect_size,\n",
    "    nobs1=None,  # What we're solving for\n",
    "    alpha=alpha,\n",
    "    power=power,\n",
    "    ratio=1.0,  # Equal sample sizes\n",
    "    alternative='two-sided'\n",
    ")\n",
    "\n",
    "print(f\"Effect size (Cohen's d): {effect_size}\")\n",
    "print(f\"Significance level (α):  {alpha}\")\n",
    "print(f\"Desired power:           {power}\")\n",
    "print(f\"\\n→ Required sample size per group: {np.ceil(n_required):.0f}\")\n",
    "print(f\"→ Total sample size: {np.ceil(n_required * 2):.0f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SENSITIVITY ANALYSIS: How does sample size change?\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Vary effect size\n",
    "effect_sizes = [0.2, 0.5, 0.8]  # Small, medium, large\n",
    "sample_sizes = []\n",
    "\n",
    "for es in effect_sizes:\n",
    "    n = tt_ind_solve_power(effect_size=es, nobs1=None, alpha=0.05, \n",
    "                          power=0.80, alternative='two-sided')\n",
    "    sample_sizes.append(n)\n",
    "    effect_label = {0.2: \"Small\", 0.5: \"Medium\", 0.8: \"Large\"}[es]\n",
    "    print(f\"Effect size = {es} ({effect_label:7s}): n = {np.ceil(n):.0f} per group\")\n",
    "\n",
    "# Vary power\n",
    "print(\"\\nVarying power (effect size = 0.5):\")\n",
    "powers = [0.80, 0.90, 0.95]\n",
    "\n",
    "for pw in powers:\n",
    "    n = tt_ind_solve_power(effect_size=0.5, nobs1=None, alpha=0.05,\n",
    "                          power=pw, alternative='two-sided')\n",
    "    print(f\"Power = {pw}: n = {np.ceil(n):.0f} per group\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 8: Power Curves - Visualizing Sample Size Trade-offs\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left plot: Sample size vs. Effect size\n",
    "effect_sizes = np.linspace(0.1, 1.0, 50)\n",
    "sample_sizes_by_effect = []\n",
    "\n",
    "for es in effect_sizes:\n",
    "    n = tt_ind_solve_power(effect_size=es, nobs1=None, alpha=0.05,\n",
    "                          power=0.80, alternative='two-sided')\n",
    "    sample_sizes_by_effect.append(n)\n",
    "\n",
    "axes[0].plot(effect_sizes, sample_sizes_by_effect, linewidth=2.5, color='steelblue')\n",
    "axes[0].scatter([0.2, 0.5, 0.8], \n",
    "               [tt_ind_solve_power(es, None, 0.05, 0.80, alternative='two-sided')\n",
    "                for es in [0.2, 0.5, 0.8]],\n",
    "               s=100, color='red', zorder=5, label='Standard effects')\n",
    "axes[0].set_xlabel('Effect Size (Cohen\\'s d)', fontsize=11, fontweight='bold')\n",
    "axes[0].set_ylabel('Sample Size per Group', fontsize=11, fontweight='bold')\n",
    "axes[0].set_title('Sample Size vs. Effect Size\\n(Power = 0.80, α = 0.05)', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].legend()\n",
    "\n",
    "# Right plot: Sample size vs. Power\n",
    "powers = np.linspace(0.50, 0.99, 50)\n",
    "sample_sizes_by_power = []\n",
    "\n",
    "for pw in powers:\n",
    "    n = tt_ind_solve_power(effect_size=0.5, nobs1=None, alpha=0.05,\n",
    "                          power=pw, alternative='two-sided')\n",
    "    sample_sizes_by_power.append(n)\n",
    "\n",
    "axes[1].plot(powers, sample_sizes_by_power, linewidth=2.5, color='coral')\n",
    "axes[1].axvline(x=0.80, color='green', linestyle='--', linewidth=2, label='Standard (80%)')\n",
    "axes[1].axhline(y=tt_ind_solve_power(0.5, None, 0.05, 0.80, alternative='two-sided'),\n",
    "                color='green', linestyle='--', linewidth=2, alpha=0.5)\n",
    "axes[1].scatter([0.80], \n",
    "               [tt_ind_solve_power(0.5, None, 0.05, 0.80, alternative='two-sided')],\n",
    "               s=100, color='red', zorder=5)\n",
    "axes[1].set_xlabel('Power (1 - β)', fontsize=11, fontweight='bold')\n",
    "axes[1].set_ylabel('Sample Size per Group', fontsize=11, fontweight='bold')\n",
    "axes[1].set_title('Sample Size vs. Power\\n(Effect Size = 0.5, α = 0.05)',\n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key Insights:\")\n",
    "print(\"- Smaller effects require much larger sample sizes\")\n",
    "print(\"- Higher power requirements increase sample size\")\n",
    "print(\"- Sample size scales with 1/d² (very nonlinear)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Type I and Type II Errors\n",
    "\n",
    "In hypothesis testing, two types of mistakes are possible:\n",
    "\n",
    "```\n",
    "                 H₀ is TRUE    H₀ is FALSE\n",
    "Reject H₀        Type I Error  ✓ Correct\n",
    "                 (α)           (Power = 1-β)\n",
    "\n",
    "Fail to Reject   ✓ Correct     Type II Error\n",
    "H₀               (1-α)         (β)\n",
    "```\n",
    "\n",
    "**Type I Error (α = \"False Positive\")**:\n",
    "- We claim an effect exists when it actually doesn't\n",
    "- Example: \"This drug works\" when it doesn't\n",
    "- Probability: α (usually set to 0.05)\n",
    "- Medical analogy: False diagnosis of illness\n",
    "\n",
    "**Type II Error (β = \"False Negative\")**:\n",
    "- We fail to detect an effect that actually exists\n",
    "- Example: \"This drug doesn't work\" when it does\n",
    "- Probability: β (we set power = 1 - β, usually 0.80)\n",
    "- Medical analogy: Missing real illness\n",
    "\n",
    "### The Trade-off\n",
    "\n",
    "- **Lower α** (stricter criteria) → fewer false positives, but more false negatives\n",
    "- **Higher power** (detect effects) → requires larger sample size\n",
    "- In many fields, Type I errors are considered worse than Type II\n",
    "  - But in medical screening, missing disease (Type II) is often worse\n",
    "\n",
    "### Example: Drug Approval\n",
    "\n",
    "- **H₀**: Drug is ineffective\n",
    "- **H₁**: Drug is effective\n",
    "- **Type I Error**: Approve ineffective drug (patient harm)\n",
    "- **Type II Error**: Reject effective drug (patients don't get help)\n",
    "- **Solution**: Require strong evidence (low α = 0.01), but also high power (0.90+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 9: Type I and Type II Error Visualization\n",
    "\n",
    "# Visualize Type I and Type II errors\n",
    "from scipy.stats import norm\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Distributions under H₀ and H₁\n",
    "x = np.linspace(-4, 6, 1000)\n",
    "h0_dist = norm(loc=0, scale=1)  # Null hypothesis distribution\n",
    "h1_dist = norm(loc=2, scale=1)  # Alternative hypothesis (effect size d=2)\n",
    "\n",
    "alpha = 0.05\n",
    "critical_value = norm.ppf(1 - alpha/2)  # Two-tailed\n",
    "\n",
    "# Left plot: Type I Error\n",
    "axes[0].plot(x, h0_dist.pdf(x), 'b-', linewidth=2, label='Distribution under H₀')\n",
    "axes[0].fill_between(x[x > critical_value], 0, h0_dist.pdf(x[x > critical_value]),\n",
    "                      alpha=0.3, color='red', label=f'Type I Error (α = {alpha})')\n",
    "axes[0].axvline(critical_value, color='red', linestyle='--', linewidth=2)\n",
    "axes[0].set_xlabel('Test Statistic', fontsize=11)\n",
    "axes[0].set_ylabel('Probability Density', fontsize=11)\n",
    "axes[0].set_title('Type I Error: False Positive\\n(Reject H₀ when it\\'s true)', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[0].legend(loc='upper right')\n",
    "axes[0].set_ylim([0, 0.5])\n",
    "\n",
    "# Right plot: Type II Error and Power\n",
    "axes[1].plot(x, h0_dist.pdf(x), 'b-', linewidth=2, label='Under H₀ (no effect)')\n",
    "axes[1].plot(x, h1_dist.pdf(x), 'g-', linewidth=2, label='Under H₁ (effect exists)')\n",
    "\n",
    "# Type II Error region\n",
    "axes[1].fill_between(x[x < critical_value], 0, h1_dist.pdf(x[x < critical_value]),\n",
    "                      alpha=0.3, color='orange', label=f'Type II Error (β)')\n",
    "\n",
    "# Power region\n",
    "axes[1].fill_between(x[x > critical_value], 0, h1_dist.pdf(x[x > critical_value]),\n",
    "                      alpha=0.3, color='green', label=f'Power (1 - β)')\n",
    "\n",
    "axes[1].axvline(critical_value, color='red', linestyle='--', linewidth=2)\n",
    "axes[1].set_xlabel('Test Statistic', fontsize=11)\n",
    "axes[1].set_ylabel('Probability Density', fontsize=11)\n",
    "axes[1].set_title('Type II Error vs. Power\\n(With true effect present)',\n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[1].legend(loc='upper right')\n",
    "axes[1].set_ylim([0, 0.5])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate actual error rates for this example\n",
    "type1_rate = alpha\n",
    "type2_rate = h1_dist.cdf(critical_value)\n",
    "power = 1 - type2_rate\n",
    "\n",
    "print(\"ERROR RATE SUMMARY (for effect size d=2)\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Type I Error Rate (α):        {type1_rate:.1%}\")\n",
    "print(f\"Type II Error Rate (β):       {type2_rate:.1%}\")\n",
    "print(f\"Power (1 - β):                {power:.1%}\")\n",
    "print()\n",
    "print(\"Interpretation:\")\n",
    "print(f\"- 5% chance of false positive\")\n",
    "print(f\"- {type2_rate:.1%} chance of false negative\")\n",
    "print(f\"- {power:.1%} chance of detecting the effect if it exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multiple Comparisons Problem\n",
    "\n",
    "### The Issue: Testing Many Hypotheses\n",
    "\n",
    "Imagine you test 20 different hypotheses, all with α = 0.05:\n",
    "\n",
    "```\n",
    "Probability of at least one false positive = 1 - (0.95)^20 = 0.64\n",
    "\n",
    "64% chance you'll find a \"significant\" result even if there are\n",
    "NO true effects at all!\n",
    "```\n",
    "\n",
    "This is the **multiple comparisons problem**.\n",
    "\n",
    "### When Do You Have Multiple Comparisons?\n",
    "\n",
    "✓ Testing many features in a model (feature selection)\n",
    "✓ Comparing multiple groups (ANOVA followed by pairwise tests)\n",
    "✓ Testing many outcome variables\n",
    "✓ Testing at multiple time points\n",
    "✓ Exploratory analysis with many hypotheses\n",
    "\n",
    "### Solutions: Correction Methods\n",
    "\n",
    "| Method | How It Works | When to Use | Trade-off |\n",
    "|--------|-------------|-------------|----------|\n",
    "| **Bonferroni** | Divide α by m: α_adj = α/m | Few comparisons (m < 20) | Very conservative, loses power |\n",
    "| **Holm-Bonferroni** | Stepwise Bonferroni | Few comparisons | Less conservative than Bonferroni |\n",
    "| **False Discovery Rate (FDR)** | Control proportion of false positives | Many comparisons | More powerful than Bonferroni |\n",
    "| **None** | Report with multiplicity warning | Pre-specified hypothesis | Only if justified |\n",
    "\n",
    "### Bonferroni Correction\n",
    "\n",
    "**Simple rule**: Divide your significance level by the number of tests\n",
    "\n",
    "```\n",
    "Instead of:  α = 0.05\n",
    "Use:         α_adjusted = 0.05 / m, where m = number of tests\n",
    "\n",
    "Example: 10 tests\n",
    "α_adjusted = 0.05 / 10 = 0.005\n",
    "\n",
    "Only reject H₀ if p < 0.005 (much stricter)\n",
    "```\n",
    "\n",
    "**Advantage**: Simple, easy to explain\n",
    "**Disadvantage**: Very conservative (loses power to detect real effects)\n",
    "\n",
    "### False Discovery Rate (FDR)\n",
    "\n",
    "**Idea**: Instead of controlling probability of even one false positive (family-wise error), control the **proportion** of false positives among all significant findings.\n",
    "\n",
    "**Example**:\n",
    "- Test 100 features\n",
    "- Find 20 significant at FDR q = 0.05\n",
    "- Expected: ~1 false positive among the 20 (5%)\n",
    "\n",
    "**Advantage**: More powerful than Bonferroni\n",
    "**Disadvantage**: Slightly more complex to interpret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 10: Multiple Comparisons Problem\n",
    "\n",
    "print(\"MULTIPLE COMPARISONS PROBLEM DEMONSTRATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Simulate testing 20 features on random data (NO TRUE EFFECTS)\n",
    "np.random.seed(42)\nn_tests = 20\nn_features = 20\nn_samples = 100\n",
    "\n",
    "# Create random features and random target (no correlation)\n",
    "X_random = np.random.randn(n_samples, n_features)\n",
    "y_random = np.random.randn(n_samples)\n",
    "\n",
    "# Test each feature for correlation\n",
    "p_values = []\n",
    "for i in range(n_features):\n",
    "    # Correlation test\n",
    "    r = np.corrcoef(X_random[:, i], y_random)[0, 1]\n",
    "    t_stat = r * np.sqrt(n_samples - 2) / np.sqrt(1 - r**2)\n",
    "    p_value = 2 * (1 - stats.t.cdf(abs(t_stat), n_samples - 2))\n",
    "    p_values.append(p_value)\n",
    "\n",
    "p_values = np.array(p_values)\n",
    "\n",
    "# Count false positives at α=0.05 (without correction)\n",
    "false_positives_uncorrected = (p_values < 0.05).sum()\n",
    "\n",
    "print(f\"Testing {n_features} features on random data (NO true effects)\")\n",
    "print()\n",
    "print(f\"Uncorrected α = 0.05:\")\n",
    "print(f\"  Number of 'significant' results: {false_positives_uncorrected} out of {n_features}\")\n",
    "print(f\"  {false_positives_uncorrected/n_features*100:.1f}% false positive rate\")\n",
    "print()\n",
    "print(f\"Expected false positives (random chance): {n_features * 0.05:.1f}\")\n",
    "print(f\"Theoretical risk of ≥1 false positive: {(1 - (1-0.05)**n_features)*100:.1f}%\")\n",
    "\n",
    "# Apply Bonferroni correction\n",
    "bonferroni_alpha = 0.05 / n_features\n",
    "false_positives_bonf = (p_values < bonferroni_alpha).sum()\n",
    "\n",
    "print(f\"\\nBonferroni corrected α = 0.05/{n_features} = {bonferroni_alpha:.4f}:\")\n",
    "print(f\"  Number of 'significant' results: {false_positives_bonf} out of {n_features}\")\n",
    "print()\n",
    "\n",
    "# Apply FDR correction (Benjamini-Hochberg)\n",
    "reject, p_adjust_fdr, _, _ = multipletests(p_values, method='fdr_bh', alpha=0.05)\n",
    "false_positives_fdr = reject.sum()\n",
    "\n",
    "print(f\"FDR (Benjamini-Hochberg) with q = 0.05:\")\n",
    "print(f\"  Number of 'significant' results: {false_positives_fdr} out of {n_features}\")\n",
    "print()\n",
    "\n",
    "print(\"COMPARISON:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"No correction:      {false_positives_uncorrected} false positives (danger!)\")\n",
    "print(f\"Bonferroni:         {false_positives_bonf} false positives (safe, but conservative)\")\n",
    "print(f\"FDR:                {false_positives_fdr} false positives (balanced)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 11: Applying Multiple Comparison Corrections\n",
    "\n",
    "print(\"PRACTICAL EXAMPLE: Feature Selection with Multiple Comparisons\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create realistic data: some features have real effect, most don't\n",
    "np.random.seed(42)\nn_samples = 200\nn_features = 30\n",
    "\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "\n",
    "# Only first 3 features have real effect\n",
    "y = (X[:, 0] + 0.5*X[:, 1] - 0.7*X[:, 2] + 0.3*np.random.randn(n_samples)) > 0\n",
    "\n",
    "# Calculate p-values for each feature\n",
    "p_values = []\n",
    "correlations = []\n",
    "\n",
    "for i in range(n_features):\n",
    "    # T-test: feature vs. outcome\n",
    "    group_0 = X[y == 0, i]\n",
    "    group_1 = X[y == 1, i]\n",
    "    _, p_val = ttest_ind(group_0, group_1)\n",
    "    p_values.append(p_val)\n",
    "    correlations.append(np.corrcoef(X[:, i], y)[0, 1])\n",
    "\n",
    "p_values = np.array(p_values)\n",
    "correlations = np.array(correlations)\n",
    "\n",
    "# Sort by p-value\n",
    "sorted_idx = np.argsort(p_values)\n",
    "p_sorted = p_values[sorted_idx]\n",
    "feature_idx_sorted = sorted_idx[:10]  # Show top 10\n",
    "\n",
    "print(\"Top 10 Most Significant Features:\")\n",
    "print()\n",
    "print(\"Feature  Raw p-value  Bonferroni  FDR      Significant?\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Apply corrections\n",
    "bonf_threshold = 0.05 / n_features\n",
    "reject_fdr, p_fdr, _, _ = multipletests(p_values, method='fdr_bh', alpha=0.05)\n",
    "\n",
    "for rank, feat_idx in enumerate(feature_idx_sorted, 1):\n",
    "    p_raw = p_values[feat_idx]\n",
    "    significant_raw = \"✓\" if p_raw < 0.05 else \"\"\n",
    "    significant_bonf = \"✓\" if p_raw < bonf_threshold else \"\"\n",
    "    significant_fdr = \"✓\" if reject_fdr[feat_idx] else \"\"\n",
    "    \n",
    "    print(f\"{feat_idx:3d}      {p_raw:.4f}      {significant_bonf}         {significant_fdr}      \"\n",
    "          f\"Real effect: {feat_idx < 3}\")\n",
    "\n",
    "print()\n",
    "print(f\"Bonferroni threshold (α/m): {bonf_threshold:.4f}\")\n",
    "print()\n",
    "print(\"INTERPRETATION:\")\n",
    "print(\"- Without correction: 7 features appear 'significant'\")\n",
    "print(f\"- Bonferroni: {(p_sorted[:10] < bonf_threshold).sum()} remain significant (very strict)\")\n",
    "print(f\"- FDR: {reject_fdr[feature_idx_sorted].sum()} remain significant (balanced)\")\n",
    "print()\n",
    "print(\"Best practice: Use FDR for exploratory analysis, Bonferroni for\")\n",
    "print(\"few pre-specified hypotheses.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary: Putting It All Together\n",
    "\n",
    "### The Complete Validation Pipeline\n",
    "\n",
    "```\n",
    "1. SPLIT DATA\n",
    "   ├── Train (70-80%)\n",
    "   ├── Validation (10-15%)\n",
    "   └── Test (10-15%)\n",
    "\n",
    "2. CROSS-VALIDATE\n",
    "   ├── K-fold CV on training data\n",
    "   ├── Estimate performance ± confidence interval\n",
    "   └── Tune hyperparameters on CV scores\n",
    "\n",
    "3. HYPOTHESIS TEST\n",
    "   ├── State H₀ and H₁\n",
    "   ├── Choose appropriate test\n",
    "   ├── Calculate exact p-value\n",
    "   ├── Report effect size and confidence interval\n",
    "   └── Interpret in context\n",
    "\n",
    "4. ACCOUNT FOR MULTIPLICITY\n",
    "   ├── Count number of tests\n",
    "   ├── Apply appropriate correction (Bonferroni or FDR)\n",
    "   └── Report adjusted p-values\n",
    "\n",
    "5. FINAL EVALUATION\n",
    "   ├── Fit final model on training + validation\n",
    "   ├── Report performance on test set (ONCE)\n",
    "   └── Don't revisit test set\n",
    "```\n",
    "\n",
    "### Key Principles\n",
    "\n",
    "✅ **Data integrity**: Separate train/val/test, fit preprocessing only on training data\n",
    "\n",
    "✅ **Robust estimates**: Use cross-validation to get mean ± SE, not point estimates\n",
    "\n",
    "✅ **Statistical rigor**: Report exact p-values, effect sizes, confidence intervals\n",
    "\n",
    "✅ **Multiplicity awareness**: Apply corrections when testing multiple hypotheses\n",
    "\n",
    "✅ **Planning**: Use power analysis to determine sample size A PRIORI\n",
    "\n",
    "✅ **Reproducibility**: Set random seeds, document decisions, make code transparent\n",
    "\n",
    "### Common Mistakes to Avoid\n",
    "\n",
    "❌ Using test set performance to choose model or tune hyperparameters\n",
    "\n",
    "❌ Running many tests and only reporting significant ones (p-hacking)\n",
    "\n",
    "❌ Ignoring multiple comparisons and using α = 0.05 for 20 tests\n",
    "\n",
    "❌ Fitting preprocessing on entire dataset then splitting\n",
    "\n",
    "❌ Using single train/test split as final performance estimate\n",
    "\n",
    "❌ Reporting only p-values without effect sizes or confidence intervals\n",
    "\n",
    "❌ Claiming causation from correlational analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checklist: Statistical Validation Best Practices\n",
    "\n",
    "Before finalizing your analysis, confirm:\n",
    "\n",
    "### Data Management\n",
    "- [ ] Data split into train/validation/test sets (70/15/15)\n",
    "- [ ] Test set untouched during model development\n",
    "- [ ] Preprocessing fit only on training data\n",
    "- [ ] Temporal ordering respected (if time-series)\n",
    "- [ ] Class balance maintained across sets (if classification)\n",
    "\n",
    "### Cross-Validation\n",
    "- [ ] K-fold CV used to estimate performance\n",
    "- [ ] Standard error calculated from fold scores\n",
    "- [ ] 95% confidence interval reported\n",
    "- [ ] Hyperparameters tuned using CV, not test set\n",
    "\n",
    "### Hypothesis Testing\n",
    "- [ ] Hypotheses stated before seeing data\n",
    "- [ ] Appropriate test selected for data type\n",
    "- [ ] Exact p-value reported (not p < 0.05)\n",
    "- [ ] Effect size calculated\n",
    "- [ ] 95% confidence interval provided\n",
    "\n",
    "### Multiple Comparisons\n",
    "- [ ] Number of tests identified\n",
    "- [ ] Appropriate correction applied (Bonferroni for few, FDR for many)\n",
    "- [ ] Adjusted p-values reported\n",
    "\n",
    "### Final Reporting\n",
    "- [ ] Power analysis documented (if applicable)\n",
    "- [ ] Sample size justified\n",
    "- [ ] Random seeds set for reproducibility\n",
    "- [ ] All decisions documented\n",
    "- [ ] Results communicate uncertainty (not just point estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Assessment\n",
    "\n",
    "Before moving to Module 06, ensure you can:\n",
    "\n",
    "- [ ] Explain why separate test sets are necessary\n",
    "- [ ] Implement proper train/validation/test split\n",
    "- [ ] Identify and prevent data leakage\n",
    "- [ ] Use k-fold cross-validation correctly\n",
    "- [ ] Calculate standard error from CV folds\n",
    "- [ ] State null and alternative hypotheses formally\n",
    "- [ ] Choose appropriate statistical test\n",
    "- [ ] Interpret p-values correctly\n",
    "- [ ] Calculate and interpret effect sizes\n",
    "- [ ] Compute confidence intervals\n",
    "- [ ] Conduct a priori power analysis\n",
    "- [ ] Distinguish Type I and Type II errors\n",
    "- [ ] Apply Bonferroni and FDR corrections\n",
    "- [ ] Explain when multiple comparisons corrections are needed\n",
    "\n",
    "If you can confidently check all boxes, you're ready for Module 06: Causal Inference Fundamentals! 🎉"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "### Books\n",
    "- \"Statistical Rethinking\" by Richard McElreath (Bayesian approach)\n",
    "- \"The Book of Why\" by Judea Pearl (causal inference)\n",
    "- \"An Introduction to Statistical Learning\" by James et al. (practical ML)\n",
    "\n",
    "### Online Courses\n",
    "- MIT OpenCourseWare: \"Statistical Method in Biology\"\n",
    "- Coursera: \"Statistics with Python\" specialization\n",
    "- EdX: \"Causal Inference\" bootcamp\n",
    "\n",
    "### Papers & Guidelines\n",
    "- \"The ASA Statement on p-Values\" (American Statistical Association)\n",
    "- \"Controlling the False Discovery Rate\" (Benjamini & Hochberg, 1995)\n",
    "- NeurIPS Reproducibility Checklist\n",
    "\n",
    "### Software & Tools\n",
    "- `scipy.stats`: Statistical tests\n",
    "- `statsmodels`: Power analysis and advanced statistics\n",
    "- `scikit-learn`: Cross-validation and model selection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
