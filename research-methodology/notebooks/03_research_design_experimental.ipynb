{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 03: Research Design - Experimental vs Observational\n",
    "\n",
    "**Difficulty**: \u2b50\u2b50 (Intermediate)\n",
    "\n",
    "**Estimated Time**: 60 minutes\n",
    "\n",
    "**Prerequisites**: [Module 00: Introduction to Research Methodology](00_introduction_research_methodology.ipynb), [Module 02: Research Foundations and Paradigms](02_research_foundations_paradigms.ipynb)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Compare experimental vs observational study designs and explain their trade-offs\n",
    "2. Apply core principles of experimental design: randomization, replication, and control\n",
    "3. Design factorial experiments and interpret interaction effects\n",
    "4. Understand and navigate the hierarchy of evidence\n",
    "5. Recognize when observational studies are necessary and how to strengthen causal inference\n",
    "6. Calculate statistical power and determine necessary sample sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's import the libraries we'll use throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard data science libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# For experimental design and power analysis\n",
    "from statsmodels.stats.power import FTestPower, tt_solve_power\n",
    "from itertools import product\n",
    "\n",
    "# Configuration for better visualizations\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"âœ“ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Fundamental Research Design Choice\n",
    "\n",
    "The most important decision in planning research is choosing between:\n",
    "\n",
    "1. **Experimental Design** - The researcher manipulates variables\n",
    "2. **Observational Design** - The researcher observes naturally occurring variation\n",
    "\n",
    "This choice determines:\n",
    "- What causal claims you can make\n",
    "- How many resources you need\n",
    "- What ethical constraints apply\n",
    "- How strong your evidence will be\n",
    "\n",
    "### Experimental Design\n",
    "\n",
    "**Key characteristic**: The researcher *controls* and *manipulates* the independent variable(s)\n",
    "\n",
    "**What this enables**:\n",
    "- Making causal claims with confidence\n",
    "- Isolating specific effects\n",
    "- Controlling confounding variables\n",
    "- Replicating conditions precisely\n",
    "\n",
    "**What it requires**:\n",
    "- Ability to assign participants/units to conditions\n",
    "- Ethical approval (sometimes)\n",
    "- Controlled environment\n",
    "\n",
    "**Example**: Testing whether a new website design improves user engagement by randomly assigning visitors to current (control) or new (treatment) design\n",
    "\n",
    "### Observational Design\n",
    "\n",
    "**Key characteristic**: The researcher *observes* naturally occurring variation without manipulation\n",
    "\n",
    "**When it's necessary**:\n",
    "- Randomization is unethical (e.g., smoking effects)\n",
    "- Long time horizons (decades of follow-up)\n",
    "- Rare outcomes (waiting for events to occur naturally)\n",
    "- Studying existing policies or phenomena\n",
    "\n",
    "**The challenge**: Causal inference from observational data requires careful statistical work\n",
    "\n",
    "**Example**: Studying the effect of smoking on health by comparing smokers vs non-smokers in existing data\n",
    "\n",
    "### Key Distinction: Internal vs External Validity\n",
    "\n",
    "| Validity Type | Meaning | Experiments | Observational |\n",
    "|---|---|---|---|\n",
    "| **Internal** | Can we confidently claim causation? | Typically HIGH | Typically LOW |\n",
    "| **External** | Do results generalize to real-world? | May be LIMITED (lab setting) | Often HIGHER (natural conditions) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Core Principles of Experimental Design\n",
    "\n",
    "Rigorous experimental design rests on three fundamental principles:\n",
    "\n",
    "### Principle 1: Randomization\n",
    "\n",
    "**Purpose**: Eliminate systematic bias in assigning units to treatment conditions\n",
    "\n",
    "**How it works**: Each unit has an equal probability of receiving each treatment\n",
    "\n",
    "**Why it matters**:\n",
    "- Distributes confounding variables equally across groups\n",
    "- Creates comparable groups except for the treatment\n",
    "- Enables statistical inference\n",
    "\n",
    "**Different randomization approaches**:\n",
    "- **Simple random assignment**: Each unit independently assigned with fixed probability\n",
    "- **Stratified randomization**: Randomize within subgroups, then combine\n",
    "- **Block randomization**: Ensure equal group sizes in blocks\n",
    "\n",
    "### Principle 2: Replication\n",
    "\n",
    "**Purpose**: Observe the effect across multiple units to reduce noise\n",
    "\n",
    "**Two types**:\n",
    "1. **Within-experiment replication**: Multiple observations per condition\n",
    "2. **Across-experiment replication**: Repeating the entire experiment\n",
    "\n",
    "**Why it matters**:\n",
    "- Larger sample sizes â†’ more precise estimates\n",
    "- Allows assessment of statistical significance\n",
    "- Reduces impact of random variation\n",
    "\n",
    "### Principle 3: Control\n",
    "\n",
    "**Purpose**: Hold constant variables that might affect outcomes\n",
    "\n",
    "**Methods**:\n",
    "1. **Holding variables constant**: Use identical conditions for all units\n",
    "2. **Matching**: Pair similar units, give different treatments\n",
    "3. **Blocking**: Group similar units, randomize treatments within groups\n",
    "4. **Statistical adjustment**: Use covariates in analysis\n",
    "\n",
    "**Why it matters**: Reduces noise, makes effects clearer, increases statistical power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstration: Impact of These Principles\n",
    "\n",
    "Let's simulate a simple experiment and show how these principles matter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a simple learning experiment\n",
    "# Question: Does using active recall improve learning compared to passive reading?\n",
    "\n",
    "# True effect: Active recall helps people remember 15% more on average\n",
    "true_effect = 0.15\n",
    "\n",
    "# Simulate different sample sizes\n",
    "sample_sizes = [10, 30, 100, 300]\n",
    "results_summary = []\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, n_per_group in enumerate(sample_sizes):\n",
    "    # Simulate experiment: compare passive reading (control) vs active recall (treatment)\n",
    "    # Both groups start with same baseline, treatment gets +15% boost\n",
    "    control_scores = np.random.normal(65, 10, n_per_group)\n",
    "    treatment_scores = np.random.normal(65 + 15, 10, n_per_group)\n",
    "    \n",
    "    # Statistical test\n",
    "    t_stat, p_value = stats.ttest_ind(treatment_scores, control_scores)\n",
    "    effect_size = (treatment_scores.mean() - control_scores.mean()) / np.sqrt((np.var(control_scores) + np.var(treatment_scores))/2)\n",
    "    \n",
    "    # Store results\n",
    "    results_summary.append({\n",
    "        'Sample Size': n_per_group,\n",
    "        'Control Mean': control_scores.mean(),\n",
    "        'Treatment Mean': treatment_scores.mean(),\n",
    "        'Observed Effect': treatment_scores.mean() - control_scores.mean(),\n",
    "        'P-value': p_value,\n",
    "        'Significant': p_value < 0.05,\n",
    "        'Effect Size (Cohen\\'s d)': effect_size\n",
    "    })\n",
    "    \n",
    "    # Visualize\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Create violin plots\n",
    "    parts = ax.violinplot([control_scores, treatment_scores], positions=[1, 2], showmeans=True)\n",
    "    ax.set_xticks([1, 2])\n",
    "    ax.set_xticklabels(['Control\\n(Passive Reading)', 'Treatment\\n(Active Recall)'])\n",
    "    ax.set_ylabel('Test Score', fontsize=11)\n",
    "    ax.set_title(f'Sample Size: n={n_per_group} per group\\np-value: {p_value:.4f} {\"âœ“ Significant\" if p_value < 0.05 else \"âœ— Not significant\"}',\n",
    "                 fontsize=11, fontweight='bold')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    ax.set_ylim(30, 100)\n",
    "\n",
    "plt.suptitle('Impact of Replication (Sample Size) on Detecting a True Effect',\n",
    "             fontsize=14, fontweight='bold', y=1.00)\nplt.tight_layout()\nplt.show()\n",
    "\n",
    "# Display results table\n",
    "results_df = pd.DataFrame(results_summary)\nprintcolumns_display = ['Sample Size', 'Control Mean', 'Treatment Mean', 'Observed Effect', 'P-value', 'Significant']\nprint(\"\\nðŸ“Š Effect of Replication (Sample Size) on Statistical Power:\")\nprint(\"=\"*90)\nprint(results_df[print_columns_display].to_string(index=False))\nprint(\"\\nðŸ’¡ Key insight: Larger samples increase statistical power to detect real effects!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Experimental Designs: From Simple to Complex\n",
    "\n",
    "### 3.1 Simple Two-Group Design (A/B Test)\n",
    "\n",
    "The simplest experimental design:\n",
    "\n",
    "```\n",
    "RANDOMIZE â†’ CONTROL GROUP (No intervention)\n",
    "         â†’ TREATMENT GROUP (Intervention)\n",
    "         â†’ MEASURE & COMPARE\n",
    "```\n",
    "\n",
    "**Example use cases**:\n",
    "- Website button color (blue vs red) and click-through rate\n",
    "- Email subject lines and open rates\n",
    "- Price variations and purchase conversion\n",
    "\n",
    "**Advantages**:\n",
    "- Simple to implement and analyze\n",
    "- Clear interpretation\n",
    "\n",
    "**Disadvantages**:\n",
    "- Can only test one variable at a time\n",
    "- Less efficient for studying multiple factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation: A/B Test for Website Conversion\n",
    "\n",
    "Suppose we want to test whether a new checkout button design improves conversion rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A/B Test Simulation: Checkout Button Design\n",
    "\n",
    "def simulate_ab_test(control_conversion_rate, treatment_effect, sample_size_per_group, iterations=1000):\n",
    "    \"\"\"\n",
    "    Simulate multiple A/B tests to understand variability and power.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    control_conversion_rate : float\n",
    "        Baseline conversion rate (0 to 1)\n",
    "    treatment_effect : float\n",
    "        Absolute increase in conversion rate for treatment\n",
    "    sample_size_per_group : int\n",
    "        Number of users per group\n",
    "    iterations : int\n",
    "        Number of simulated A/B tests\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Summary statistics and power analysis\n",
    "    \"\"\"\n",
    "    treatment_conversion_rate = control_conversion_rate + treatment_effect\n",
    "    \n",
    "    p_values = []\n",
    "    observed_effects = []\n",
    "    \n",
    "    for _ in range(iterations):\n",
    "        # Simulate users and conversions\n",
    "        control_conversions = np.random.binomial(\n",
    "            n=sample_size_per_group,\n",
    "            p=control_conversion_rate\n",
    "        )\n",
    "        \n",
    "        treatment_conversions = np.random.binomial(\n",
    "            n=sample_size_per_group,\n",
    "            p=treatment_conversion_rate\n",
    "        )\n",
    "        \n",
    "        # Chi-square test for independence\n",
    "        contingency_table = np.array([\n",
    "            [control_conversions, sample_size_per_group - control_conversions],\n",
    "            [treatment_conversions, sample_size_per_group - treatment_conversions]\n",
    "        ])\n",
    "        \n",
    "        chi2, p_value, dof, expected = stats.chi2_contingency(contingency_table)\n",
    "        p_values.append(p_value)\n",
    "        \n",
    "        observed_effect = (treatment_conversions / sample_size_per_group) - (control_conversions / sample_size_per_group)\n",
    "        observed_effects.append(observed_effect)\n",
    "    \n",
    "    # Calculate statistical power\n",
    "    power = np.mean(np.array(p_values) < 0.05)\n",
    "    \n",
    "    return {\n",
    "        'true_effect': treatment_effect,\n",
    "        'sample_size_per_group': sample_size_per_group,\n",
    "        'statistical_power': power,\n",
    "        'p_values': np.array(p_values),\n",
    "        'observed_effects': np.array(observed_effects),\n",
    "        'mean_p_value': np.mean(p_values),\n",
    "        'mean_observed_effect': np.mean(observed_effects)\n",
    "    }\n",
    "\n",
    "# Run simulations with different sample sizes\n",
    "baseline_conversion = 0.05  # 5% baseline\n",
    "true_effect = 0.02  # 2 percentage point improvement (relative: 40% improvement)\n",
    "\n",
    "test_sizes = [100, 500, 2000, 5000]\n",
    "power_results = []\n",
    "\n",
    "print(\"ðŸ“Š A/B Test Power Analysis (1000 simulations each)\")\nprint(\"=\"*80)\nprint(f\"Baseline conversion rate: {baseline_conversion*100:.1f}%\")\nprint(f\"Expected treatment effect: +{true_effect*100:.2f} percentage points\")\nprint(\"\\n\" + \"-\"*80)\n\nfor sample_size in test_sizes:\n",
    "    result = simulate_ab_test(baseline_conversion, true_effect, sample_size)\n",
    "    power_results.append(result)\n",
    "    \n",
    "    print(f\"\\nSample Size: {sample_size} per group (Total: {sample_size*2})\")\n",
    "    print(f\"  Statistical Power: {result['statistical_power']:.1%}\")\n",
    "    print(f\"  (Power = probability of detecting true effect)\")\n",
    "\nprint(\"\\n\" + \"=\"*80)\nprint(\"ðŸ’¡ Key insight: Larger samples = higher power to detect real effects!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing A/B Test Power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the relationship between sample size and power\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Power curve\n",
    "ax1 = axes[0]\nsample_sizes_detailed = np.arange(50, 5000, 50)\npowers = []\n\nfor n in sample_sizes_detailed:\n",
    "    result = simulate_ab_test(baseline_conversion, true_effect, int(n), iterations=500)\n",
    "    powers.append(result['statistical_power'])\n",
    "\nax1.plot(sample_sizes_detailed, powers, linewidth=3, color='darkblue', label='Power curve')\nax1.axhline(y=0.8, color='red', linestyle='--', linewidth=2, label='Target power = 80%')\nax1.fill_between(sample_sizes_detailed, 0.8, 1, alpha=0.2, color='green', label='Acceptable power')\nax1.set_xlabel('Sample Size per Group', fontsize=12)\nax1.set_ylabel('Statistical Power', fontsize=12)\nax1.set_title('A/B Test: Sample Size vs Statistical Power', fontsize=13, fontweight='bold')\nax1.set_ylim(0, 1)\nax1.legend(fontsize=11)\nax1.grid(alpha=0.3)\n\n# Plot 2: Distribution of observed effects\nax2 = axes[1]\ncolors = ['lightcoral', 'gold', 'lightgreen', 'lightblue']\n\nfor idx, result in enumerate(power_results):\n    ax2.hist(result['observed_effects'], bins=30, alpha=0.5, label=f\"n={result['sample_size_per_group']}\",\n             color=colors[idx], edgecolor='black')\n\nax2.axvline(x=true_effect, color='red', linestyle='--', linewidth=2, label=f'True effect ({true_effect*100:.2f}%)')\nax2.set_xlabel('Observed Effect Size', fontsize=12)\nax2.set_ylabel('Frequency', fontsize=12)\nax2.set_title('Distribution of Observed Effects Across Simulations', fontsize=13, fontweight='bold')\nax2.legend(fontsize=10)\nax2.grid(axis='y', alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nðŸ“ˆ Key findings from A/B test simulation:\")\nprint(\"   - Larger samples: narrower distribution (more consistent results)\")\nprint(\"   - Larger samples: higher power to detect true effect\")\nprint(\"   - For 80% power, we need ~1500 users per group for this scenario\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Factorial Designs (Testing Multiple Factors)\n",
    "\n",
    "**When to use**: Testing multiple factors simultaneously and their interactions\n",
    "\n",
    "**Structure**: All combinations of factor levels\n",
    "\n",
    "**Example**: 2Ã—2 factorial design\n",
    "```\n",
    "Factor A: Button Color (Blue vs Red)\n",
    "Factor B: Button Text (\"Buy Now\" vs \"Add to Cart\")\n",
    "\n",
    "Results in 4 experimental conditions:\n",
    "1. Blue button + \"Buy Now\"\n",
    "2. Blue button + \"Add to Cart\"\n",
    "3. Red button + \"Buy Now\"\n",
    "4. Red button + \"Add to Cart\"\n",
    "```\n",
    "\n",
    "**Efficiency**: Testing 2 factors in one study instead of 2 separate studies\n",
    "- Main effects: Effect of Factor A alone, Effect of Factor B alone\n",
    "- Interaction effect: Does the effect of A depend on B? (e.g., red works better with \"Buy Now\" but blue works better with \"Add to Cart\")\n",
    "\n",
    "**Advantages**:\n",
    "- More efficient than one-factor-at-a-time\n",
    "- Can detect interaction effects\n",
    "- More realistic (factors usually interact in practice)\n",
    "\n",
    "**Disadvantages**:\n",
    "- More complex to analyze\n",
    "- Larger sample sizes needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation: 2Ã—2 Factorial Design\n",
    "\n",
    "Let's design an experiment testing two factors in online learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2x2 Factorial Design: Online Learning Study\n",
    "# Factor A: Practice type (Spaced vs Massed)\n",
    "# Factor B: Feedback type (Immediate vs Delayed)\n",
    "\n",
    "# Generate simulated data\n",
    "np.random.seed(42)\n",
    "\n",
    "# True effects (test scores out of 100)\n",
    "baseline = 65\n",
    "effect_spacing = 8          # Spaced practice advantage\n",
    "effect_feedback = 6         # Immediate feedback advantage\n",
    "interaction_effect = -4     # Interaction: spaced + immediate is worse than expected\n",
    "\n",
    "n_per_condition = 50  # Sample size per condition\n",
    "noise_sd = 8\n",
    "\n",
    "# Create all four conditions\n",
    "conditions = {\n",
    "    'Massed + Delayed': baseline,\n",
    "    'Massed + Immediate': baseline + effect_feedback,\n",
    "    'Spaced + Delayed': baseline + effect_spacing,\n",
    "    'Spaced + Immediate': baseline + effect_spacing + effect_feedback + interaction_effect\n",
    "}\n",
    "\n",
    "# Simulate data\n",
    "factorial_data = []\n",
    "for condition, mean_score in conditions.items():\n",
    "    scores = np.random.normal(mean_score, noise_sd, n_per_condition)\n",
    "    \n",
    "    # Parse condition into factors\n",
    "    spacing, feedback = condition.split(' + ')\n",
    "    \n",
    "    for score in scores:\n",
    "        factorial_data.append({\n",
    "            'Practice_Type': spacing,\n",
    "            'Feedback_Type': feedback,\n",
    "            'Test_Score': score,\n",
    "            'Condition': condition\n",
    "        })\n",
    "\nfactorial_df = pd.DataFrame(factorial_data)\n",
    "\n",
    "# Analyze results\nprint(\"\\nðŸ“Š 2Ã—2 Factorial Design Results: Learning Study\")\nprint(\"=\"*70)\nprint(\"\\nMean Test Scores by Condition:\")\nprint(\"-\"*70)\n\nsummary_stats = factorial_df.groupby('Condition')['Test_Score'].agg(['mean', 'std', 'count'])\nprint(summary_stats.round(2))\n\n# Calculate main effects\nprint(\"\\n\\nMain Effects Analysis:\")\nprint(\"-\"*70)\n\nspaced_mean = factorial_df[factorial_df['Practice_Type'] == 'Spaced']['Test_Score'].mean()\nmassed_mean = factorial_df[factorial_df['Practice_Type'] == 'Massed']['Test_Score'].mean()\nprint(f\"Effect of Practice Type:\")\nprint(f\"  Spaced:  {spaced_mean:.2f}\")\nprint(f\"  Massed:  {massed_mean:.2f}\")\nprint(f\"  Difference: {spaced_mean - massed_mean:.2f} points (Spaced is better)\")\n\nimmediate_mean = factorial_df[factorial_df['Feedback_Type'] == 'Immediate']['Test_Score'].mean()\ndelayed_mean = factorial_df[factorial_df['Feedback_Type'] == 'Delayed']['Test_Score'].mean()\nprint(f\"\\nEffect of Feedback Type:\")\nprint(f\"  Immediate: {immediate_mean:.2f}\")\nprint(f\"  Delayed:   {delayed_mean:.2f}\")\nprint(f\"  Difference: {immediate_mean - delayed_mean:.2f} points (Immediate is better)\")\n\nprint(\"\\n\\nInteraction Effect:\")\nprint(\"-\"*70)\nprint(\"\\nDoes the benefit of spaced practice depend on feedback type?\")\n\nfor feedback_type in ['Immediate', 'Delayed']:\n",
    "    subset = factorial_df[factorial_df['Feedback_Type'] == feedback_type]\n",
    "    spaced_effect = subset[subset['Practice_Type'] == 'Spaced']['Test_Score'].mean() - \\\n",
    "                    subset[subset['Practice_Type'] == 'Massed']['Test_Score'].mean()\n",
    "    print(f\"\\n  With {feedback_type} feedback:\")\n",
    "    print(f\"    Spaced advantage: {spaced_effect:.2f} points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Factorial Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the 2x2 factorial design\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Box plot showing all four conditions\n",
    "ax1 = axes[0]\nfactorial_df_sorted = factorial_df.sort_values('Condition')\nsns.boxplot(data=factorial_df_sorted, x='Practice_Type', y='Test_Score', \n            hue='Feedback_Type', ax=ax1, palette='Set2')\nax1.set_xlabel('Practice Type', fontsize=12)\nax1.set_ylabel('Test Score', fontsize=12)\nax1.set_title('2Ã—2 Factorial Design: Main Effects\\nand Interaction', fontsize=13, fontweight='bold')\nax1.legend(title='Feedback Type', fontsize=11)\nax1.grid(axis='y', alpha=0.3)\n\n# Plot 2: Interaction plot (line plot)\nax2 = axes[1]\n\n# Calculate means for each combination\nfor practice in ['Massed', 'Spaced']:\n",
    "    means = []\n",
    "    feedback_types = ['Delayed', 'Immediate']\n",
    "    \n",
    "    for feedback in feedback_types:\n",
    "        subset = factorial_df[(factorial_df['Practice_Type'] == practice) & \n",
    "                             (factorial_df['Feedback_Type'] == feedback)]\n",
    "        means.append(subset['Test_Score'].mean())\n",
    "    \n",
    "    ax2.plot(feedback_types, means, 'o-', linewidth=2.5, markersize=10, \n",
    "             label=f'{practice}', markerfacecolor='white', markeredgewidth=2)\n",
    "\nax2.set_xlabel('Feedback Type', fontsize=12)\nax2.set_ylabel('Mean Test Score', fontsize=12)\nax2.set_title('Interaction Plot\\n(Non-parallel lines indicate interaction)', \n              fontsize=13, fontweight='bold')\nax2.legend(title='Practice Type', fontsize=11)\nax2.grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nðŸ’¡ Interpretation of interaction plot:\")\nprint(\"   - Non-parallel lines indicate an interaction effect\")\nprint(\"   - Spaced practice helps more with Delayed feedback\")\nprint(\"   - The benefit differs depending on feedback timing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Blocking Design (Controlling for Known Confounders)\n",
    "\n",
    "**When to use**: When you know that some variable will affect outcomes but is not your focus\n",
    "\n",
    "**How it works**:\n",
    "1. Divide units into blocks based on the confounding variable\n",
    "2. Randomize treatments within each block\n",
    "3. Analyze results accounting for blocks\n",
    "\n",
    "**Example**: Testing a new teaching method across schools\n",
    "- Schools vary in quality (confounding variable)\n",
    "- Solution: Block by school, randomize method within each school\n",
    "- Result: Compare treatment vs control in each school, then combine estimates\n",
    "\n",
    "**Advantages**:\n",
    "- Reduces unexplained variation\n",
    "- Increases precision of estimates\n",
    "- Improves statistical power\n",
    "- Controls known confounders elegantly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blocking Design Example: Testing new teaching method across schools\n",
    "\n",
    "# Scenario: Schools differ in quality\n",
    "schools = ['School A (Low)', 'School B (Medium)', 'School C (High)']\n",
    "school_effects = [15, 55, 75]  # Baseline test scores\n",
    "treatment_effect = 5  # New method adds 5 points on average\n",
    "\n",
    "# Create blocked design\n",
    "blocked_data = []\n",
    "n_per_condition_per_block = 25  # 25 students per condition per school\n",
    "\n",
    "for school_idx, school_name in enumerate(schools):\n",
    "    baseline_score = school_effects[school_idx]\n",
    "    \n",
    "    # Control group in this school\n",
    "    control_scores = np.random.normal(baseline_score, 5, n_per_condition_per_block)\n",
    "    \n",
    "    # Treatment group in this school\n",
    "    treatment_scores = np.random.normal(baseline_score + treatment_effect, 5, n_per_condition_per_block)\n",
    "    \n",
    "    for score in control_scores:\n",
    "        blocked_data.append({\n",
    "            'School': school_name,\n",
    "            'Condition': 'Control (Traditional)',\n",
    "            'Test_Score': score\n",
    "        })\n",
    "    \n",
    "    for score in treatment_scores:\n",
    "        blocked_data.append({\n",
    "            'School': school_name,\n",
    "            'Condition': 'Treatment (New Method)',\n",
    "            'Test_Score': score\n",
    "        })\n",
    "\nblocked_df = pd.DataFrame(blocked_data)\n",
    "\nprint(\"\\nðŸ“Š Blocking Design: Effect of School Quality\")\nprint(\"=\"*70)\nprint(\"\\nResults by School and Condition:\")\nprint(\"-\"*70)\n\nresults_by_school = blocked_df.groupby(['School', 'Condition'])['Test_Score'].agg(['mean', 'std', 'count'])\nprint(results_by_school.round(2))\n\nprint(\"\\n\\nTreatment Effect within Each Block (School):\")\nprint(\"-\"*70)\n\nblock_effects = []\nfor school in schools:\n",
    "    school_data = blocked_df[blocked_df['School'] == school]\n",
    "    control_mean = school_data[school_data['Condition'] == 'Control (Traditional)']['Test_Score'].mean()\n",
    "    treatment_mean = school_data[school_data['Condition'] == 'Treatment (New Method)']['Test_Score'].mean()\n",
    "    effect = treatment_mean - control_mean\n",
    "    block_effects.append(effect)\n",
    "    print(f\"\\n{school}:\")\n",
    "    print(f\"  Control mean:    {control_mean:.2f}\")\n",
    "    print(f\"  Treatment mean:  {treatment_mean:.2f}\")\n",
    "    print(f\"  Effect:          {effect:.2f} points\")\n",
    "\nprint(f\"\\n\\nOverall Average Treatment Effect: {np.mean(block_effects):.2f} points\")\nprint(\"(Combining effects across all schools)\")\n\nprint(\"\\n\\nðŸ’¡ Key advantage of blocking:\")\nprint(\"   - School quality controlled for\")\nprint(\"   - Cleaner estimate of method effect\")\nprint(\"   - Each school's data used to estimate effect\")\nprint(\"   - Better precision than ignoring school differences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Blocking Effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare blocked vs unblocked analysis\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Unblocked view (ignoring school)\n",
    "ax1 = axes[0]\ndata_unblocked = blocked_df.groupby('Condition')['Test_Score'].apply(list)\ncolors_unblocked = ['lightcoral', 'lightgreen']\nfor idx, condition in enumerate(['Control (Traditional)', 'Treatment (New Method)']):\n",
    "    scores = blocked_df[blocked_df['Condition'] == condition]['Test_Score'].values\n",
    "    ax1.scatter([idx]*len(scores), scores, alpha=0.3, s=50, color=colors_unblocked[idx])\n",
    "    ax1.plot([idx, idx], [scores.mean() - 1.96*scores.std()/np.sqrt(len(scores)),\n",
    "                           scores.mean() + 1.96*scores.std()/np.sqrt(len(scores))],\n",
    "             'k-', linewidth=3)\n",
    "    ax1.scatter(idx, scores.mean(), s=200, color=colors_unblocked[idx], \n",
    "               edgecolor='black', linewidth=2, zorder=5)\n",
    "\nax1.set_xticks([0, 1])\nax1.set_xticklabels(['Control', 'Treatment'])\nax1.set_ylabel('Test Score', fontsize=12)\nax1.set_title('Unblocked Analysis\\n(Ignoring school differences)', fontsize=13, fontweight='bold')\nax1.set_ylim(0, 100)\nax1.grid(axis='y', alpha=0.3)\n\n# Plot 2: Blocked view (accounting for school)\nax2 = axes[1]\nfor school_idx, school in enumerate(schools):\n",
    "    for cond_idx, condition in enumerate(['Control (Traditional)', 'Treatment (New Method)']):\n",
    "        school_cond_data = blocked_df[(blocked_df['School'] == school) & \n",
    "                                      (blocked_df['Condition'] == condition)]['Test_Score']\n",
    "        x_pos = school_idx + (cond_idx - 0.5) * 0.3\n",
    "        color = ['lightcoral', 'lightgreen'][cond_idx]\n",
    "        ax2.scatter([x_pos]*len(school_cond_data), school_cond_data, alpha=0.3, s=50, color=color)\n",
    "        ax2.scatter(x_pos, school_cond_data.mean(), s=100, color=color, \n",
    "                   edgecolor='black', linewidth=2, zorder=5)\n",
    "\n# Connect control to treatment within each school\nfor school_idx, school in enumerate(schools):\n",
    "    control_mean = blocked_df[(blocked_df['School'] == school) & \n",
    "                             (blocked_df['Condition'] == 'Control (Traditional)')]['Test_Score'].mean()\n",
    "    treatment_mean = blocked_df[(blocked_df['School'] == school) & \n",
    "                               (blocked_df['Condition'] == 'Treatment (New Method)')]['Test_Score'].mean()\n",
    "    ax2.plot([school_idx - 0.15, school_idx + 0.15], [control_mean, treatment_mean], \n",
    "            'k--', linewidth=2, alpha=0.7)\n",
    "\nax2.set_xticks([0, 1, 2])\nax2.set_xticklabels(schools)\nax2.set_ylabel('Test Score', fontsize=12)\nax2.set_title('Blocked Analysis\\n(Accounting for school differences)', fontsize=13, fontweight='bold')\nax2.set_ylim(0, 100)\nax2.grid(axis='y', alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nðŸ“ˆ Visualization shows:\")\nprint(\"   - Left: Overall effect appears variable (high noise)\")\nprint(\"   - Right: Effect is consistent within each school (less noise)\")\nprint(\"   - Blocking reduces noise and improves clarity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Observational Studies: When Randomization Isn't Possible\n",
    "\n",
    "### When Experiments Aren't Feasible\n",
    "\n",
    "Sometimes randomization is **impossible, impractical, or unethical**:\n",
    "\n",
    "1. **Unethical**: Can't randomly assign people to smoke\n",
    "2. **Impractical**: Can't wait decades for long-term outcomes\n",
    "3. **Rare events**: Waiting for outcome to occur naturally\n",
    "4. **Historical**: Can only study what has already happened\n",
    "5. **Policy**: Testing laws that already exist\n",
    "\n",
    "### The Fundamental Problem: Causality Without Randomization\n",
    "\n",
    "In observational studies, groups differ on many dimensions:\n",
    "\n",
    "```\n",
    "Example: Smoking and health\n",
    "\n",
    "Smokers vs Non-smokers differ in:\n",
    "- Smoking âœ“ (treatment)\n",
    "- Age (confound)\n",
    "- Socioeconomic status (confound)\n",
    "- Diet (confound)\n",
    "- Exercise (confound)\n",
    "- Genetics (confound)\n",
    "- And many unmeasured factors...\n",
    "\n",
    "Observed difference = TRUE EFFECT + CONFOUNDING BIAS\n",
    "```\n",
    "\n",
    "### Strategies to Strengthen Causal Inference\n",
    "\n",
    "**1. Matching**\n",
    "- Pair treated and untreated units that are similar on observed confounders\n",
    "- Compare outcomes within pairs\n",
    "- Limitation: Can't account for unmeasured confounders\n",
    "\n",
    "**2. Regression Adjustment**\n",
    "- Include confounding variables as covariates in regression\n",
    "- Estimates treatment effect controlling for confounders\n",
    "- Limitation: Linear relationships may not fit; unmeasured confounders ignored\n",
    "\n",
    "**3. Instrumental Variables**\n",
    "- Find a variable that affects treatment but not outcome directly\n",
    "- Use it to estimate causal effect\n",
    "- Example: Proximity to college used as instrument for college attendance\n",
    "\n",
    "**4. Regression Discontinuity**\n",
    "- When treatment assignment depends on a cutoff (e.g., passing score)\n",
    "- Compare units just above and below cutoff\n",
    "- Example: Did students just passing a test benefit differently than those just failing?\n",
    "\n",
    "**5. Difference-in-Differences**\n",
    "- Compare groups before and after a policy change\n",
    "- Differences in pre-trends suggest confounding\n",
    "- Example: Compare income growth in treated vs control states before/after policy\n",
    "\n",
    "### Limitations of Observational Studies\n",
    "\n",
    "Even with these methods, observational studies cannot fully address:\n",
    "- **Unmeasured confounding**: Variables you didn't measure\n",
    "- **Selection bias**: How people selected into treatment\n",
    "- **Reverse causality**: Does X cause Y or does Y cause X?\n",
    "\n",
    "This is why experiments, when feasible, provide stronger evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hierarchy of Evidence\n",
    "\n",
    "Different study designs provide different levels of evidence for causal claims.\n",
    "\n",
    "### The Evidence Hierarchy (Strongest to Weakest)\n",
    "\n",
    "```\n",
    "    â•‘  LEVEL 1 (STRONGEST)\n",
    "    â•‘  Systematic Reviews & Meta-Analyses\n",
    "    â•‘  (Combining results from multiple RCTs)\n",
    "    â•‘\n",
    "    â•‘  LEVEL 2\n",
    "    â•‘  Randomized Controlled Trials (RCTs)\n",
    "    â•‘  (Gold standard for individual studies)\n",
    "    â•‘\n",
    "    â•‘  LEVEL 3\n",
    "    â•‘  Cohort Studies\n",
    "    â•‘  (Follow groups over time; stronger observational design)\n",
    "    â•‘\n",
    "    â•‘  LEVEL 4\n",
    "    â•‘  Case-Control Studies\n",
    "    â•‘  (Retrospective; comparing those with vs without outcome)\n",
    "    â•‘\n",
    "    â•‘  LEVEL 5\n",
    "    â•‘  Case Series / Case Reports\n",
    "    â•‘  (Weakest: describing individual cases)\n",
    "    â•‘\n",
    "    â•‘  LEVEL 6 (WEAKEST)\n",
    "    â•‘  Expert Opinion, Anecdotes\n",
    "    â•‘  (Subjective; prone to bias)\n",
    "    â–¼\n",
    "```\n",
    "\n",
    "### Understanding Each Level\n",
    "\n",
    "| Level | Design | Example | Strength | Limitation |\n",
    "|-------|--------|---------|----------|------------|\n",
    "| **Systematic Review** | Combines RCTs | \"Meta-analysis of 50 depression trials\" | Strongest evidence | Time-consuming |\n",
    "| **RCT** | Randomized experiment | \"Patients randomly assigned to drug or placebo\" | Gold standard | Can't always do |\n",
    "| **Cohort** | Follow exposed/unexposed forward | \"Track smokers vs non-smokers for 10 years\" | Can measure incidence | Confounding possible |\n",
    "| **Case-Control** | Compare cases/controls backward | \"Compare lung cancer patients to controls, ask about smoking\" | Efficient for rare diseases | Recall bias |\n",
    "| **Case Series** | Describe cases | \"Here are 5 patients with unusual symptoms\" | Shows what's possible | Anecdotal |\n",
    "| **Opinion** | Expert judgment | \"I think this works based on experience\" | Generates hypotheses | Highly subjective |\n",
    "\n",
    "### Why the Hierarchy Matters\n",
    "\n",
    "**Different evidence levels for different questions**:\n",
    "- \"Does this treatment cause recovery?\" â†’ Needs RCT\n",
    "- \"How common is this condition?\" â†’ Cohort study sufficient\n",
    "- \"What are side effects?\" â†’ Case reports valuable\n",
    "- \"What's the best current understanding?\" â†’ Systematic review\n",
    "\n",
    "**Real-world example**: COVID-19 vaccine safety\n",
    "1. Case reports â†’ Noticed blood clotting in rare cases\n",
    "2. Case series â†’ Confirmed pattern across multiple cases\n",
    "3. Cohort study â†’ Estimated frequency in general population\n",
    "4. RCT analysis â†’ Compared rates in vaccinated vs unvaccinated\n",
    "5. Meta-analysis â†’ Combined evidence across countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization of evidence hierarchy\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Left plot: Pyramid showing study types and evidence strength\nlevels = ['Systematic Review\\n& Meta-Analysis', 'RCTs', 'Cohort Studies', \n          'Case-Control Studies', 'Case Series / Reports', 'Expert Opinion']\nheights = [1, 1.5, 2, 2, 2.5, 3]\ncolors_evidence = ['#2ecc71', '#27ae60', '#f39c12', '#e67e22', '#e74c3c', '#c0392b']\n\ny_position = 0\nfor idx, (level, height, color) in enumerate(zip(levels, heights, colors_evidence)):\n",
    "    ax1.barh(y_position, 10, height=height, color=color, edgecolor='black', linewidth=2)\n",
    "    ax1.text(5, y_position, level, ha='center', va='center', fontsize=11, \n",
    "            fontweight='bold', color='white')\n",
    "    y_position += height\n",
    "\nax1.set_ylim(0, sum(heights))\nax1.set_xlim(0, 10)\nax1.set_xlabel('Evidence Strength â†’', fontsize=12, fontweight='bold')\nax1.set_title('Hierarchy of Evidence for Causal Claims\\n(Green = Strong, Red = Weak)', \n             fontsize=13, fontweight='bold')\nax1.set_yticks([])\nax1.spines['top'].set_visible(False)\nax1.spines['right'].set_visible(False)\nax1.spines['bottom'].set_visible(False)\nax1.spines['left'].set_visible(False)\n\n# Right plot: Characteristics comparison\nchar_data = {\n",
    "    'Can infer causation': [95, 80, 40, 30, 20, 10],\n",
    "    'Free from bias': [90, 85, 50, 40, 30, 15],\n",
    "    'Generalizable': [80, 70, 75, 60, 50, 40],\n",
    "    'Quick/feasible': [40, 50, 70, 80, 85, 95]\n",
    "}\n",
    "\nlevels_short = ['Sys Rev', 'RCT', 'Cohort', 'Case-Ctrl', 'Series', 'Opinion']\nx = np.arange(len(levels_short))\nwidth = 0.2\n",
    "\nfor idx, (characteristic, values) in enumerate(char_data.items()):\n",
    "    ax2.bar(x + idx*width, values, width, label=characteristic)\n",
    "\nax2.set_xlabel('Study Type', fontsize=12)\nax2.set_ylabel('Rating (0-100)', fontsize=12)\nax2.set_title('Comparison of Study Characteristics', fontsize=13, fontweight='bold')\nax2.set_xticks(x + width * 1.5)\nax2.set_xticklabels(levels_short, rotation=45, ha='right')\nax2.legend(fontsize=10)\nax2.set_ylim(0, 100)\nax2.grid(axis='y', alpha=0.3)\n",
    "\nplt.tight_layout()\nplt.show()\n",
    "\nprint(\"\\nðŸ“Š Trade-offs in Study Design:\")\nprint(\"=\"*70)\nprint(\"\\nSystematic Reviews & RCTs:\")\nprint(\"  âœ“ Strongest causal evidence\")\nprint(\"  âœ— Time-consuming and expensive\")\nprint(\"  âœ— May not generalize perfectly\")\n",
    "print(\"\\nCohort Studies:\")\nprint(\"  âœ“ Measure incidence (outcomes develop during study)\")\nprint(\"  âœ“ Faster than RCTs for some questions\")\nprint(\"  âœ— Cannot definitively prove causation\")\nprint(\"\\nCase-Control Studies:\")\nprint(\"  âœ“ Efficient for rare outcomes\")\nprint(\"  âœ“ Can study historical data\")\nprint(\"  âœ— Prone to recall bias and confounding\")\nprint(\"\\nCase Series:\")\nprint(\"  âœ“ Identifies unusual patterns\")\nprint(\"  âœ“ Fast to generate hypotheses\")\nprint(\"  âœ— Cannot determine if outcome is causal effect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Statistical Power: Planning Your Study\n",
    "\n",
    "### What is Statistical Power?\n",
    "\n",
    "**Power** = Probability of detecting a real effect if it exists\n",
    "\n",
    "- Power = 0.80 means 80% chance of finding significant result (if true effect exists)\n",
    "- Power = 0.20 means 20% chance (4 times more likely to miss real effect)\n",
    "\n",
    "### Why Power Matters\n",
    "\n",
    "A study with low power wastes resources:\n",
    "- Spend money and time but don't detect the effect\n",
    "- Conclude \"no difference\" when effect might exist\n",
    "- Contribute to false negatives in literature\n",
    "\n",
    "### Factors Affecting Power\n",
    "\n",
    "1. **Sample Size**: Larger samples â†’ Higher power\n",
    "2. **Effect Size**: Larger effects â†’ Higher power\n",
    "3. **Significance Level (Î±)**: Standard is 0.05; stricter = lower power\n",
    "4. **Variability**: More noise â†’ Lower power\n",
    "5. **Study Design**: Blocking/matching â†’ Higher power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Power analysis for common scenarios\nfrom scipy.stats import norm\n",
    "\n",
    "# Function to estimate sample size needed for desired power\n",
    "def estimate_sample_size_for_power(effect_size, power=0.8, alpha=0.05, design='independent_ttest'):\n",
    "    \"\"\"\n",
    "    Estimate sample size needed to achieve desired statistical power.\n",
    "    \n",
    "    For independent samples t-test:\n",
    "    n = 2 * ((z_alpha/2 + z_power) / effect_size) ** 2\n",
    "    \"\"\"\n",
    "    # Critical values\n",
    "    z_alpha = norm.ppf(1 - alpha/2)  # Two-tailed\n",
    "    z_power = norm.ppf(power)\n",
    "    \n",
    "    # Calculate n per group\n",
    "    n_per_group = 2 * ((z_alpha + z_power) / effect_size) ** 2\n",
    "    \n",
    "    return int(np.ceil(n_per_group))\n",
    "\n",
    "# Create power analysis table\nprint(\"\\nðŸ“Š Sample Size Requirements for Different Effect Sizes\")\nprint(\"=\"*80)\nprint(\"\\nTo achieve 80% power with Î±=0.05 (two-tailed test):\")\nprint(\"-\"*80)\n",
    "\neffect_sizes = [0.2, 0.5, 0.8]  # Small, medium, large\neffect_labels = ['Small (0.2)', 'Medium (0.5)', 'Large (0.8)']\n",
    "\nsample_size_table = []\nfor effect_size, label in zip(effect_sizes, effect_labels):\n",
    "    n_per_group = estimate_sample_size_for_power(effect_size, power=0.80, alpha=0.05)\n",
    "    total_n = n_per_group * 2\n",
    "    sample_size_table.append({\n",
    "        'Effect Size': label,\n",
    "        'Per Group': n_per_group,\n",
    "        'Total': total_n\n",
    "    })\n",
    "    print(f\"\\n{label}:\")\n",
    "    print(f\"  Sample size per group: {n_per_group}\")\n",
    "    print(f\"  Total sample size: {total_n}\")\n",
    "\nprint(\"\\n\" + \"-\"*80)\nprint(\"\\nKey insight: Smaller effects require larger samples\")\nprint(\"  - Large effect: ~64 total\")\nprint(\"  - Medium effect: ~128 total\")\nprint(\"  - Small effect: ~784 total\")\nprint(\"  - Therefore: Research design should target meaningful effects!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Power Analysis Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize relationship between effect size, sample size, and power\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Power vs Sample Size for different effect sizes\nax1 = axes[0]\n",
    "\nsample_sizes_range = np.arange(20, 500, 10)\neffect_sizes_range = [0.2, 0.5, 0.8]\ncolors_effect = ['red', 'orange', 'green']\n",
    "\nfor effect_size, color in zip(effect_sizes_range, colors_effect):\n",
    "    powers = []\n",
    "    for n in sample_sizes_range:\n",
    "        # Calculate power using normal approximation\n",
    "        z_alpha = norm.ppf(1 - 0.05/2)\n",
    "        z_stat = effect_size * np.sqrt(n/2) / 2\n",
    "        power_val = 1 - norm.cdf(z_alpha - z_stat)\n",
    "        powers.append(power_val)\n",
    "    \n",
    "    label = f'Effect size = {effect_size}'\n",
    "    ax1.plot(sample_sizes_range, powers, linewidth=2.5, label=label, color=color)\n",
    "\nax1.axhline(y=0.8, color='black', linestyle='--', linewidth=2, label='Target power = 80%')\nax1.set_xlabel('Sample Size (per group)', fontsize=12)\nax1.set_ylabel('Statistical Power', fontsize=12)\nax1.set_title('Power Analysis: Effect Size vs Sample Size', fontsize=13, fontweight='bold')\nax1.set_ylim(0, 1)\nax1.legend(fontsize=11)\nax1.grid(alpha=0.3)\n",
    "\n# Plot 2: Sample size needed by scenario\nax2 = axes[1]\n",
    "\nscenarios = ['Email\\n(Small effect)', 'Website\\n(Medium effect)', 'Treatment\\n(Large effect)']\nsample_sizes_by_scenario = [\n",
    "    estimate_sample_size_for_power(0.2),\n",
    "    estimate_sample_size_for_power(0.5),\n",
    "    estimate_sample_size_for_power(0.8)\n",
    "]\n",
    "\ncolors_scenarios = ['#3498db', '#e74c3c', '#2ecc71']\nbars = ax2.bar(scenarios, [n*2 for n in sample_sizes_by_scenario], color=colors_scenarios, edgecolor='black', linewidth=2)\n",
    "\nax2.set_ylabel('Total Sample Size Needed', fontsize=12)\nax2.set_title('Sample Size Requirements by Research Domain\\n(For 80% power, Î±=0.05)', fontsize=13, fontweight='bold')\nax2.set_ylim(0, 1000)\n",
    "\n# Add value labels on bars\nfor bar, n in zip(bars, sample_sizes_by_scenario):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'n = {int(height)}',\n",
    "            ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\nax2.grid(axis='y', alpha=0.3)\n",
    "\nplt.tight_layout()\nplt.show()\n",
    "\nprint(\"\\nðŸ’¡ Practical implications:\")\nprint(\"   - Small effects (email variants): Need large samples (~400 per group)\")\nprint(\"   - Medium effects (website changes): Moderate samples (~64 per group)\")\nprint(\"   - Large effects (new treatment): Smaller samples (~26 per group)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Exercises\n",
    "\n",
    "### Exercise 1: Design Choice Decision\n",
    "\n",
    "For each research question, decide whether to use experimental or observational design and justify your choice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Design choice\n",
    "\nresearch_questions = [\n",
    "    {\n",
    "        'Q': 'Does caffeine consumption affect sleep quality?',\n",
    "        'Your Answer': '???'\n",
    "    },\n",
    "    {\n",
    "        'Q': 'Does vitamin D supplementation prevent COVID-19?',\n",
    "        'Your Answer': '???'\n",
    "    },\n",
    "    {\n",
    "        'Q': 'What percentage of adults have hypertension?',\n",
    "        'Your Answer': '???'\n",
    "    },\n",
    "    {\n",
    "        'Q': 'Does this new drug reduce symptoms better than placebo?',\n",
    "        'Your Answer': '???'\n",
    "    },\n",
    "    {\n",
    "        'Q': 'Do college graduates earn more than high school graduates?',\n",
    "        'Your Answer': '???'\n",
    "    }\n",
    "]\n",
    "\nprint(\"\\nðŸ“‹ Exercise 1: Research Design Choices\")\nprint(\"=\"*80)\nprint(\"\\nFor each question, choose:\")\nprint(\"  A) Experimental (can randomize)\")\nprint(\"  B) Observational (must observe naturally)\")\nprint(\"  C) Either (could work both ways)\")\nprint(\"-\"*80)\n",
    "\nfor i, question_dict in enumerate(research_questions, 1):\n",
    "    print(f\"\\n{i}. {question_dict['Q']}\")\n",
    "    print(f\"   Your choice: ___\")\n",
    "    print(f\"   Reasoning: ___\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Factorial Design Planning\n",
    "\n",
    "You're designing an experiment to optimize an online learning platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Factorial design planning\n",
    "\nprint(\"\\n\\nðŸ“‹ Exercise 2: Factorial Design Planning\")\nprint(\"=\"*80)\nprint(\"\"\"\nScenario: You want to optimize an online learning platform by testing:\n- Video presentation (Traditional vs Interactive)\n- Quiz timing (After lesson vs During lesson)\n\nYou have 1000 students available for the study.\n\nQuestions:\n1. How many experimental conditions will you create?\n   Answer: ___\n\n2. How many students should you assign per condition for 125 per group?\n   Answer: ___\n\n3. What is the main effect of quiz timing?\n   Operationalization: ___\n\n4. What would indicate an interaction effect?\n   Example: ___\n\n5. Why might interaction effects be important practically?\n   Answer: ___\n\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Causal Inference Challenge\n",
    "\n",
    "Interpret the following observational study with confounding in mind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Causal inference challenge\n",
    "\nprint(\"\\n\\nðŸ“‹ Exercise 3: Observational Study Analysis\")\nprint(\"=\"*80)\nprint(\"\"\"\nStudy Finding: \"People who own more books have higher incomes.\"\n\nInterpretation Exercise:\n\n1. What is the observed association?\n   Answer: ___\n\n2. What potential confounding variables might explain this relationship?\n   (List at least 3)\n   Answer: ___\n\n3. Why can't we conclude \"buying books makes you richer\"?\n   Answer: ___\n\n4. What research design would strengthen causal claims?\n   Answer: ___\n\n5. Suggest 3 methods to improve causal inference in this observational study:\n   Method 1: ___\n   Method 2: ___\n   Method 3: ___\n\"\"\")\n",
    "\nprint(\"\\n\\nHints:\")\nprint(\"  - Consider what types of people buy books\")\nprint(\"  - Think about reverse causality\")\nprint(\"  - Consider parental education, wealth, profession\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "âœ… **Experimental vs Observational** - Experiments enable causal claims; observational studies are necessary when randomization isn't feasible\n",
    "\n",
    "âœ… **Core Principles** - Randomization, replication, and control are the foundation of experimental design\n",
    "\n",
    "âœ… **Designs Range from Simple to Complex**:\n",
    "- Two-group A/B tests\n",
    "- Factorial designs for multiple factors\n",
    "- Blocked designs to control known confounders\n",
    "\n",
    "âœ… **Observational Studies** - Can be strengthened through matching, regression adjustment, instrumental variables, and regression discontinuity\n",
    "\n",
    "âœ… **Hierarchy of Evidence** - Different designs provide different levels of evidence; systematic reviews are strongest, expert opinion weakest\n",
    "\n",
    "âœ… **Statistical Power** - Essential planning tool; larger samples, larger effects, and better designs increase power\n",
    "\n",
    "âœ… **Trade-offs Exist** - Strong causal evidence vs practical feasibility, internal validity vs external validity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next?\n",
    "\n",
    "In **Module 04: Sample Size and Power**, you'll learn:\n",
    "- Detailed power calculations for different study designs\n",
    "- How to plan sample sizes before conducting research\n",
    "- Sensitivity analysis for study planning\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- **Book**: \"Design of Experiments\" by Douglas C. Montgomery\n",
    "- **Book**: \"The Book of Why\" by Judea Pearl (causal inference)\n",
    "- **Online**: G*Power software for power analysis (free)\n",
    "- **Paper**: \"Randomized Controlled Trials\" review articles\n",
    "- **Paper**: \"Strengthening Causal Inference in Observational Studies\" (Rotnitzky et al.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Assessment\n",
    "\n",
    "Before moving to Module 04, ensure you can:\n",
    "\n",
    "- [ ] Explain the difference between experimental and observational designs\n",
    "- [ ] Describe the three core principles: randomization, replication, control\n",
    "- [ ] Design a simple A/B test with appropriate sample size\n",
    "- [ ] Plan and interpret a factorial experiment\n",
    "- [ ] Explain what blocking is and when to use it\n",
    "- [ ] Identify confounding variables in observational studies\n",
    "- [ ] Describe the hierarchy of evidence and what it means\n",
    "- [ ] Calculate statistical power for a given sample size\n",
    "- [ ] Determine sample size needed for desired power\n",
    "- [ ] Recognize trade-offs between internal and external validity\n",
    "\n",
    "If you can confidently check all boxes, you're ready for Module 04! ðŸŽ‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
