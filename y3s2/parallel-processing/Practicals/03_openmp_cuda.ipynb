{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical 3: Memory Management with OpenMP and CUDA\n",
    "\n",
    "**Course**: BMCS3003 Distributed Systems and Parallel Computing\n",
    "\n",
    "**Difficulty**: ⭐⭐ (Intermediate)\n",
    "\n",
    "**Estimated Time**: 120 minutes\n",
    "\n",
    "**Prerequisites**:\n",
    "- Basic understanding of parallel programming concepts\n",
    "- C/C++ programming experience\n",
    "- Familiarity with threading from Practical 2\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this practical, you will be able to:\n",
    "\n",
    "1. Configure and use OpenMP in Visual Studio for parallel programming\n",
    "2. Understand and implement shared vs private data in parallel regions\n",
    "3. Apply OpenMP directives for loop parallelization and reduction operations\n",
    "4. Identify and prevent false sharing in multi-threaded programs\n",
    "5. Implement parallel algorithms (numerical integration, matrix multiplication)\n",
    "6. Use CUDA for GPU-accelerated computing (optional based on hardware)\n",
    "7. Profile and analyze parallel program performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Introduction to OpenMP](#section1)\n",
    "2. [OpenMP Configuration](#section2)\n",
    "3. [Shared vs Private Data](#section3)\n",
    "4. [Parallel Numerical Integration](#section4)\n",
    "5. [False Sharing and Cache Line Padding](#section5)\n",
    "6. [Matrix Multiplication with OpenMP](#section6)\n",
    "7. [Introduction to CUDA](#section7)\n",
    "8. [CUDA Vector Addition](#section8)\n",
    "9. [Performance Profiling](#section9)\n",
    "10. [Summary](#section10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "## 1. Introduction to OpenMP\n",
    "\n",
    "### What is OpenMP?\n",
    "\n",
    "**OpenMP** (Open Multi-Processing) is an API that supports multi-platform shared-memory parallel programming in C, C++, and Fortran.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Compiler directives**: Simple pragmas to parallelize code\n",
    "- **Fork-join model**: Master thread forks worker threads\n",
    "- **Shared memory**: All threads access same memory space\n",
    "- **Easy to learn**: Incremental parallelization\n",
    "\n",
    "### OpenMP vs Manual Threading\n",
    "\n",
    "| Aspect | Manual Threading | OpenMP |\n",
    "|--------|------------------|--------|\n",
    "| Code complexity | High | Low |\n",
    "| Lines of code | Many | Few |\n",
    "| Thread management | Manual | Automatic |\n",
    "| Portability | Platform-specific | Cross-platform |\n",
    "| Learning curve | Steep | Gentle |\n",
    "\n",
    "### Fork-Join Execution Model\n",
    "\n",
    "```\n",
    "MASTER THREAD\n",
    "    |\n",
    "    |  #pragma omp parallel\n",
    "    |\n",
    "    +------- FORK -------+\n",
    "    |                    |\n",
    " Thread 0    Thread 1   Thread 2   Thread 3\n",
    "    |           |          |          |\n",
    "    | Parallel  |          |          |\n",
    "    | Region    |          |          |\n",
    "    |           |          |          |\n",
    "    +------- JOIN --------+\n",
    "    |\n",
    " MASTER THREAD\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "## 2. OpenMP Configuration in Visual Studio\n",
    "\n",
    "### Step-by-Step Configuration\n",
    "\n",
    "#### Step 1: Open Project Properties\n",
    "- Right-click on your project in Solution Explorer\n",
    "- Select **Properties**\n",
    "\n",
    "#### Step 2: Navigate to OpenMP Settings\n",
    "```\n",
    "Configuration Properties\n",
    "  └─ C/C++\n",
    "      └─ Language\n",
    "          └─ Open MP Support\n",
    "```\n",
    "\n",
    "#### Step 3: Enable OpenMP\n",
    "- Set **Open MP Support** to **Yes (/openmp)**\n",
    "- Click **Apply** and **OK**\n",
    "\n",
    "#### Step 4: Verify Platform Configuration\n",
    "- Ensure the **Platform** dropdown matches your build configuration\n",
    "- **Win32** = **x86** (32-bit)\n",
    "- **x64** = 64-bit\n",
    "\n",
    "### Visual Studio Configuration Dialog\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────┐\n",
    "│ Configuration: Active(Debug)    Platform: x64       │\n",
    "├─────────────────────────────────────────────────────┤\n",
    "│ ▶ Configuration Properties                          │\n",
    "│   ▶ General                                         │\n",
    "│   ▶ C/C++                                           │\n",
    "│     ▼ Language                                      │\n",
    "│         Open MP Support: Yes (/openmp)    ◄─────── │\n",
    "│         C++ Language Standard: Default              │\n",
    "│         Conformance mode: Yes (/permissive-)        │\n",
    "└─────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Linux/macOS Configuration\n",
    "\n",
    "```bash\n",
    "# GCC\n",
    "g++ -fopenmp program.cpp -o program\n",
    "\n",
    "# Clang\n",
    "clang++ -fopenmp program.cpp -o program\n",
    "\n",
    "# Run\n",
    "./program\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Hello World with OpenMP\n",
    "\n",
    "**Objective**: Verify OpenMP is configured correctly by creating a simple parallel program.\n",
    "\n",
    "#### Code (P3Q1.cpp)\n",
    "\n",
    "```cpp\n",
    "#include <omp.h>\n",
    "#include <iostream>\n",
    "\n",
    "int main() {\n",
    "    // Set number of threads (optional)\n",
    "    omp_set_num_threads(4);\n",
    "    \n",
    "    // Parallel region starts here\n",
    "    #pragma omp parallel\n",
    "    {\n",
    "        // Get thread ID (0 to N-1)\n",
    "        int thread_id = omp_get_thread_num();\n",
    "        \n",
    "        // Get total number of threads\n",
    "        int num_threads = omp_get_num_threads();\n",
    "        \n",
    "        // Each thread prints its ID\n",
    "        printf(\"Hello(%d) World(%d)\\n\", thread_id, thread_id);\n",
    "    }\n",
    "    // Implicit barrier - all threads join here\n",
    "    \n",
    "    std::cout << \"All threads completed!\" << std::endl;\n",
    "    \n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "#### Expected Output\n",
    "\n",
    "```\n",
    "Hello(1) World(1)\n",
    "Hello(0) World(0)\n",
    "Hello(2) World(2)\n",
    "Hello(5) World(5)\n",
    "Hello(3) World(3)\n",
    "Hello(4) World(4)\n",
    "Hello(7) World(7)\n",
    "Hello(6) World(6)\n",
    "All threads completed!\n",
    "```\n",
    "\n",
    "**Note**: The output order is non-deterministic because threads execute in parallel!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Code\n",
    "\n",
    "```cpp\n",
    "#pragma omp parallel\n",
    "{\n",
    "    // This code is executed by ALL threads\n",
    "}\n",
    "```\n",
    "\n",
    "#### Key OpenMP Functions\n",
    "\n",
    "| Function | Description | Return Type |\n",
    "|----------|-------------|-------------|\n",
    "| `omp_get_thread_num()` | Current thread ID (0 to N-1) | int |\n",
    "| `omp_get_num_threads()` | Total number of threads | int |\n",
    "| `omp_get_max_threads()` | Max threads available | int |\n",
    "| `omp_set_num_threads(n)` | Set number of threads | void |\n",
    "| `omp_get_wtime()` | Wall clock time in seconds | double |\n",
    "\n",
    "### Common Issues and Solutions\n",
    "\n",
    "#### Issue 1: OpenMP not recognized\n",
    "```\n",
    "Error: #include <omp.h> not found\n",
    "```\n",
    "**Solution**: Enable OpenMP in project properties\n",
    "\n",
    "#### Issue 2: Single thread execution\n",
    "```\n",
    "Hello(0) World(0)\n",
    "All threads completed!\n",
    "```\n",
    "**Solution**: \n",
    "- Check OpenMP is enabled in build configuration\n",
    "- Verify `/openmp` flag is set\n",
    "- Try explicitly setting thread count: `omp_set_num_threads(4)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>\n",
    "## 3. Shared vs Private Data in OpenMP\n",
    "\n",
    "### Data Scoping Rules\n",
    "\n",
    "In OpenMP, variables can be:\n",
    "1. **Shared**: All threads access the SAME memory location\n",
    "2. **Private**: Each thread has its OWN copy\n",
    "\n",
    "### Default Scoping Rules\n",
    "\n",
    "```cpp\n",
    "int global = 5;        // SHARED by default\n",
    "\n",
    "#pragma omp parallel\n",
    "{\n",
    "    int local = 10;    // PRIVATE (declared inside parallel region)\n",
    "    global++;          // All threads modify SAME variable\n",
    "    local++;           // Each thread modifies its OWN copy\n",
    "}\n",
    "```\n",
    "\n",
    "### Shared Data Example\n",
    "\n",
    "```cpp\n",
    "#include <omp.h>\n",
    "#include <iostream>\n",
    "\n",
    "int main() {\n",
    "    int x = 5;  // Declared outside parallel region → SHARED\n",
    "    \n",
    "    #pragma omp parallel\n",
    "    {\n",
    "        x = x + 1;  // All threads modify the SAME x\n",
    "        printf(\"Shared: x is %d\\n\", x);\n",
    "    }\n",
    "    \n",
    "    printf(\"Final x: %d\\n\", x);\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "#### Output (8 threads)\n",
    "```\n",
    "Shared: x is 8\n",
    "Shared: x is 6\n",
    "Shared: x is 7\n",
    "Shared: x is 9\n",
    "Shared: x is 9\n",
    "Shared: x is 10\n",
    "Shared: x is 11\n",
    "Shared: x is 12\n",
    "Final x: 12\n",
    "```\n",
    "\n",
    "**Important**: Values are unpredictable due to **race condition**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Private Data Examples\n",
    "\n",
    "#### Method 1: Declare inside parallel region\n",
    "\n",
    "```cpp\n",
    "int x = 5;  // Outer variable\n",
    "\n",
    "#pragma omp parallel\n",
    "{\n",
    "    int x;   // NEW variable, shadows outer x\n",
    "    x = 3;   // Each thread has its own x\n",
    "    printf(\"Local: x is %d\\n\", x);\n",
    "}\n",
    "\n",
    "printf(\"After: x is still %d\\n\", x);  // Outer x unchanged\n",
    "```\n",
    "\n",
    "#### Output\n",
    "```\n",
    "Local: x is 3\n",
    "Local: x is 3\n",
    "Local: x is 3\n",
    "Local: x is 3\n",
    "...\n",
    "After: x is still 5\n",
    "```\n",
    "\n",
    "#### Method 2: Use `private` clause\n",
    "\n",
    "```cpp\n",
    "int x = 5;\n",
    "\n",
    "#pragma omp parallel private(x)\n",
    "{\n",
    "    // x is private, but UNINITIALIZED!\n",
    "    x = omp_get_thread_num();\n",
    "    printf(\"Private: x is %d\\n\", x);\n",
    "}\n",
    "\n",
    "printf(\"After: x is %d\\n\", x);  // Still 5\n",
    "```\n",
    "\n",
    "#### Output\n",
    "```\n",
    "Private: x is 0\n",
    "Private: x is 1\n",
    "Private: x is 2\n",
    "Private: x is 3\n",
    "...\n",
    "After: x is 5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dangerous Private Variable Example\n",
    "\n",
    "```cpp\n",
    "int x = 5;\n",
    "\n",
    "#pragma omp parallel private(x)\n",
    "{\n",
    "    x = x + 1;  // DANGEROUS! x is uninitialized\n",
    "    printf(\"Private: x is %d\\n\", x);\n",
    "}\n",
    "\n",
    "printf(\"After: x is %d\\n\", x);  // Also dangerous\n",
    "```\n",
    "\n",
    "#### Output (undefined behavior)\n",
    "```\n",
    "Private: x is 6     ← Thread 0 read outer x before fork\n",
    "Private: x is 13    ← Garbage value\n",
    "Private: x is 9     ← Garbage value\n",
    "Private: x is 7     ← Garbage value\n",
    "...\n",
    "After: x is 13      ← Value after parallel region is undefined\n",
    "```\n",
    "\n",
    "### Data Clause Modifiers\n",
    "\n",
    "| Clause | Description | Initialization | After Region |\n",
    "|--------|-------------|----------------|--------------|\n",
    "| `shared(x)` | All threads share x | Same as before | Updated |\n",
    "| `private(x)` | Each thread has copy | Undefined | Undefined |\n",
    "| `firstprivate(x)` | Private, initialized | Copy of original | Undefined |\n",
    "| `lastprivate(x)` | Private | Undefined | Last iteration value |\n",
    "\n",
    "#### firstprivate Example\n",
    "\n",
    "```cpp\n",
    "int x = 5;\n",
    "\n",
    "#pragma omp parallel firstprivate(x)\n",
    "{\n",
    "    x = x + omp_get_thread_num();  // Initialized to 5\n",
    "    printf(\"Thread %d: x = %d\\n\", omp_get_thread_num(), x);\n",
    "}\n",
    "\n",
    "// x is still 5 here\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>\n",
    "## 4. Question 2: Parallel Numerical Integration (Pi Calculation)\n",
    "\n",
    "### The Problem\n",
    "\n",
    "Calculate π using numerical integration:\n",
    "\n",
    "$$\\pi = \\int_0^1 \\frac{4.0}{1+x^2} dx$$\n",
    "\n",
    "### Approximation Method\n",
    "\n",
    "Divide the integral into rectangles:\n",
    "\n",
    "$$\\sum_{i=0}^{N} \\Delta x \\cdot F(x_i) \\approx \\pi$$\n",
    "\n",
    "Where:\n",
    "- $\\Delta x$ = width of each rectangle\n",
    "- $F(x_i) = \\frac{4.0}{1+x_i^2}$ = height at point $x_i$\n",
    "\n",
    "### Visual Representation\n",
    "\n",
    "```\n",
    "F(x) = 4.0/(1+x²)\n",
    "  |\n",
    "4 ├─┐\n",
    "  │ │█\n",
    "3 │ │█\n",
    "  │ │█\n",
    "2 │ │█ █\n",
    "  │ │███ █\n",
    "1 │ │█████ █\n",
    "  │ │████████\n",
    "0 └─┴────────┴─→ x\n",
    "  0          1\n",
    "```\n",
    "\n",
    "Each rectangle approximates a small part of the curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serial Implementation (P3Q2.cpp)\n",
    "\n",
    "```cpp\n",
    "#include <iostream>\n",
    "#include \"omp.h\"\n",
    "\n",
    "static long num_steps = 100000;\n",
    "double step;\n",
    "\n",
    "int main() {\n",
    "    int i;\n",
    "    double x, pi, sum = 0.0;\n",
    "    \n",
    "    // Calculate step size\n",
    "    step = 1.0 / (double)num_steps;\n",
    "    \n",
    "    // Start timing\n",
    "    double start_time = omp_get_wtime();\n",
    "    \n",
    "    // Calculate sum of all rectangles\n",
    "    for (i = 0; i < num_steps; i++) {\n",
    "        x = (i + 0.5) * step;              // Midpoint of rectangle\n",
    "        sum = sum + 4.0 / (1.0 + x * x);   // Height of rectangle\n",
    "    }\n",
    "    \n",
    "    // Multiply by width to get area\n",
    "    pi = step * sum;\n",
    "    \n",
    "    double end_time = omp_get_wtime();\n",
    "    \n",
    "    printf(\"Pi = %.10f\\n\", pi);\n",
    "    printf(\"Time = %.6f seconds\\n\", end_time - start_time);\n",
    "    \n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "#### Output (Serial)\n",
    "```\n",
    "Pi = 3.1415926536\n",
    "Time = 0.002341 seconds\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A: Basic Parallel Version\n",
    "\n",
    "**Challenge**: Parallelize using `#pragma omp parallel`\n",
    "\n",
    "```cpp\n",
    "#include <iostream>\n",
    "#include <omp.h>\n",
    "\n",
    "static long num_steps = 10000000;  // Increased for better measurement\n",
    "double step;\n",
    "\n",
    "int main() {\n",
    "    int i;\n",
    "    double x, pi;\n",
    "    double sum[16];  // Array to store partial sums (max 16 threads)\n",
    "    \n",
    "    step = 1.0 / (double)num_steps;\n",
    "    \n",
    "    double start_time = omp_get_wtime();\n",
    "    \n",
    "    #pragma omp parallel\n",
    "    {\n",
    "        int id = omp_get_thread_num();\n",
    "        int num_threads = omp_get_num_threads();\n",
    "        \n",
    "        // Each thread gets its own sum\n",
    "        sum[id] = 0.0;\n",
    "        \n",
    "        // Divide work among threads\n",
    "        for (i = id; i < num_steps; i += num_threads) {\n",
    "            x = (i + 0.5) * step;\n",
    "            sum[id] += 4.0 / (1.0 + x * x);\n",
    "        }\n",
    "    }\n",
    "    // Implicit barrier here\n",
    "    \n",
    "    // Combine partial sums\n",
    "    double total_sum = 0.0;\n",
    "    for (i = 0; i < omp_get_max_threads(); i++) {\n",
    "        total_sum += sum[i];\n",
    "    }\n",
    "    \n",
    "    pi = step * total_sum;\n",
    "    \n",
    "    double end_time = omp_get_wtime();\n",
    "    \n",
    "    printf(\"Pi = %.10f\\n\", pi);\n",
    "    printf(\"Time = %.6f seconds\\n\", end_time - start_time);\n",
    "    printf(\"Threads = %d\\n\", omp_get_max_threads());\n",
    "    \n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "#### Work Distribution\n",
    "\n",
    "```\n",
    "Total iterations: 0 1 2 3 4 5 6 7 8 9 10 11 ...\n",
    "\n",
    "Thread 0: 0   4   8   12  ...  (i += 4)\n",
    "Thread 1:   1   5   9   13 ...\n",
    "Thread 2:     2   6   10  14 ...\n",
    "Thread 3:       3   7   11 15 ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section5'></a>\n",
    "## 5. Part B: False Sharing Problem\n",
    "\n",
    "### What is False Sharing?\n",
    "\n",
    "**False sharing** occurs when threads on different processors modify variables that reside on the same cache line.\n",
    "\n",
    "### Cache Line Basics\n",
    "\n",
    "- CPU caches work with **cache lines** (typically 64 bytes)\n",
    "- When one core modifies data, the entire cache line is invalidated on other cores\n",
    "- Even if threads access different variables!\n",
    "\n",
    "### False Sharing Illustration\n",
    "\n",
    "```\n",
    "Array: sum[16]  (each double = 8 bytes)\n",
    "\n",
    "Cache Line (64 bytes):\n",
    "┌────────────────────────────────────────────────┐\n",
    "│ sum[0] sum[1] sum[2] sum[3] sum[4] sum[5] ... │\n",
    "└────────────────────────────────────────────────┘\n",
    "     ↑      ↑      ↑      ↑\n",
    "  Thread0 Thread1 Thread2 Thread3\n",
    "```\n",
    "\n",
    "**Problem**: \n",
    "- Thread 0 modifies `sum[0]`\n",
    "- Entire cache line invalidated on all cores\n",
    "- Thread 1, 2, 3 must reload cache line\n",
    "- Massive performance loss!\n",
    "\n",
    "### Solution: Cache Line Padding\n",
    "\n",
    "Give each thread its own cache line:\n",
    "\n",
    "```\n",
    "Cache Line 0 (64 bytes):\n",
    "┌────────────────────────────────────────────────┐\n",
    "│ sum[0][0]    padding (56 bytes)                │  ← Thread 0\n",
    "└────────────────────────────────────────────────┘\n",
    "\n",
    "Cache Line 1 (64 bytes):\n",
    "┌────────────────────────────────────────────────┐\n",
    "│ sum[1][0]    padding (56 bytes)                │  ← Thread 1\n",
    "└────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation with Padding\n",
    "\n",
    "```cpp\n",
    "#include <iostream>\n",
    "#include <omp.h>\n",
    "\n",
    "static long num_steps = 10000000;\n",
    "double step;\n",
    "\n",
    "#define PAD 8  // 64 bytes / 8 bytes per double = 8 doubles\n",
    "\n",
    "int main() {\n",
    "    int i;\n",
    "    double x, pi;\n",
    "    double sum[16][PAD];  // 2D array with padding\n",
    "    \n",
    "    step = 1.0 / (double)num_steps;\n",
    "    \n",
    "    double start_time = omp_get_wtime();\n",
    "    \n",
    "    #pragma omp parallel\n",
    "    {\n",
    "        int id = omp_get_thread_num();\n",
    "        int num_threads = omp_get_num_threads();\n",
    "        \n",
    "        // Initialize first element of row\n",
    "        sum[id][0] = 0.0;\n",
    "        \n",
    "        for (i = id; i < num_steps; i += num_threads) {\n",
    "            x = (i + 0.5) * step;\n",
    "            sum[id][0] += 4.0 / (1.0 + x * x);  // Access [id][0] only\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    // Combine partial sums\n",
    "    double total_sum = 0.0;\n",
    "    for (i = 0; i < omp_get_max_threads(); i++) {\n",
    "        total_sum += sum[i][0];\n",
    "    }\n",
    "    \n",
    "    pi = step * total_sum;\n",
    "    \n",
    "    double end_time = omp_get_wtime();\n",
    "    \n",
    "    printf(\"Pi = %.10f\\n\", pi);\n",
    "    printf(\"Time with padding = %.6f seconds\\n\", end_time - start_time);\n",
    "    printf(\"Threads = %d\\n\", omp_get_max_threads());\n",
    "    \n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "### Performance Comparison\n",
    "\n",
    "**Configuration**: 8 threads, 10,000,000 steps\n",
    "\n",
    "| Implementation | Time (ms) | Speedup | Notes |\n",
    "|----------------|-----------|---------|-------|\n",
    "| Serial | 45.2 | 1.0x | Baseline |\n",
    "| Parallel (no pad) | 12.8 | 3.5x | False sharing! |\n",
    "| Parallel (padded) | 6.1 | 7.4x | Good speedup |\n",
    "\n",
    "**Improvement**: ~2x faster with padding!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C: Using Reduction Clause\n",
    "\n",
    "OpenMP provides a **reduction** clause that handles this automatically!\n",
    "\n",
    "```cpp\n",
    "#include <iostream>\n",
    "#include <omp.h>\n",
    "\n",
    "static long num_steps = 10000000;\n",
    "double step;\n",
    "\n",
    "int main() {\n",
    "    int i;\n",
    "    double x, pi, sum = 0.0;\n",
    "    \n",
    "    step = 1.0 / (double)num_steps;\n",
    "    \n",
    "    double start_time = omp_get_wtime();\n",
    "    \n",
    "    // Parallel loop with reduction\n",
    "    #pragma omp parallel for reduction(+:sum)\n",
    "    for (i = 0; i < num_steps; i++) {\n",
    "        x = (i + 0.5) * step;\n",
    "        sum += 4.0 / (1.0 + x * x);\n",
    "    }\n",
    "    \n",
    "    pi = step * sum;\n",
    "    \n",
    "    double end_time = omp_get_wtime();\n",
    "    \n",
    "    printf(\"Pi = %.10f\\n\", pi);\n",
    "    printf(\"Time with reduction = %.6f seconds\\n\", end_time - start_time);\n",
    "    printf(\"Threads = %d\\n\", omp_get_max_threads());\n",
    "    \n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "### How Reduction Works\n",
    "\n",
    "```cpp\n",
    "#pragma omp parallel for reduction(+:sum)\n",
    "```\n",
    "\n",
    "1. Each thread gets a **private copy** of `sum` (initialized to 0)\n",
    "2. Threads compute their partial sums independently\n",
    "3. At the end, OpenMP **combines** all partial sums: `sum = sum0 + sum1 + sum2 + ...`\n",
    "\n",
    "### Reduction Operations\n",
    "\n",
    "| Operator | Description | Initial Value |\n",
    "|----------|-------------|---------------|\n",
    "| `+` | Addition | 0 |\n",
    "| `*` | Multiplication | 1 |\n",
    "| `-` | Subtraction | 0 |\n",
    "| `&` | Bitwise AND | ~0 |\n",
    "| `|` | Bitwise OR | 0 |\n",
    "| `^` | Bitwise XOR | 0 |\n",
    "| `&&` | Logical AND | 1 |\n",
    "| `||` | Logical OR | 0 |\n",
    "\n",
    "### Why Reduction is Best\n",
    "\n",
    "1. **Simplest code**: Looks almost like serial version\n",
    "2. **No false sharing**: OpenMP handles it internally\n",
    "3. **Optimal performance**: Uses efficient reduction algorithms\n",
    "4. **Less error-prone**: No manual thread management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion: Why Different Results?\n",
    "\n",
    "Running with different thread counts may give slightly different π values:\n",
    "\n",
    "```\n",
    "Threads = 1: π = 3.141592653590\n",
    "Threads = 2: π = 3.141592653589\n",
    "Threads = 4: π = 3.141592653591\n",
    "Threads = 8: π = 3.141592653590\n",
    "```\n",
    "\n",
    "**Reasons**:\n",
    "\n",
    "1. **Floating-point rounding**: Addition order affects rounding errors\n",
    "   - (a + b) + c ≠ a + (b + c) for floating-point\n",
    "\n",
    "2. **Different summation order**: \n",
    "   - 1 thread: sum[0] + sum[1] + sum[2] + ...\n",
    "   - 4 threads: (sum[0]+sum[1]) + (sum[2]+sum[3]) + ...\n",
    "\n",
    "3. **Not an error**: All results are correct to many decimal places\n",
    "\n",
    "4. **Solution**: Use higher precision (e.g., `long double`) or Kahan summation algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section6'></a>\n",
    "## 6. Question 3: Matrix Multiplication\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "Parallelize matrix multiplication: **C = A × B**\n",
    "\n",
    "```\n",
    "A[N×M] × B[M×P] = C[N×P]\n",
    "```\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "```cpp\n",
    "for (i = 0; i < N; i++) {\n",
    "    for (j = 0; j < P; j++) {\n",
    "        C[i][j] = 0;\n",
    "        for (k = 0; k < M; k++) {\n",
    "            C[i][j] += A[i][k] * B[k][j];\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "### Serial Implementation (P3Q3.c)\n",
    "\n",
    "```cpp\n",
    "#include <iostream>\n",
    "#include <omp.h>\n",
    "#include <cstdlib>\n",
    "\n",
    "#define N 1000  // Matrix size\n",
    "\n",
    "double A[N][N], B[N][N], C[N][N];\n",
    "\n",
    "void initialize() {\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        for (int j = 0; j < N; j++) {\n",
    "            A[i][j] = rand() % 10;\n",
    "            B[i][j] = rand() % 10;\n",
    "            C[i][j] = 0;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "void multiply_serial() {\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        for (int j = 0; j < N; j++) {\n",
    "            for (int k = 0; k < N; k++) {\n",
    "                C[i][j] += A[i][k] * B[k][j];\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    initialize();\n",
    "    \n",
    "    double start = omp_get_wtime();\n",
    "    multiply_serial();\n",
    "    double end = omp_get_wtime();\n",
    "    \n",
    "    printf(\"Serial time: %.6f seconds\\n\", end - start);\n",
    "    \n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel Implementation\n",
    "\n",
    "**Key insight**: Outer loop iterations are independent!\n",
    "\n",
    "```cpp\n",
    "void multiply_parallel() {\n",
    "    int i, j, k;\n",
    "    \n",
    "    // Parallelize outer loop\n",
    "    // i, j, k are private by default (loop variables)\n",
    "    #pragma omp parallel for private(j, k)\n",
    "    for (i = 0; i < N; i++) {\n",
    "        for (j = 0; j < N; j++) {\n",
    "            for (k = 0; k < N; k++) {\n",
    "                C[i][j] += A[i][k] * B[k][j];\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "### Why `private(j, k)`?\n",
    "\n",
    "- **i**: Already private (loop variable of parallel for)\n",
    "- **j, k**: Must be explicitly declared private\n",
    "- **A, B, C**: Shared (declared outside)\n",
    "\n",
    "### Work Distribution\n",
    "\n",
    "```\n",
    "Matrix C (N rows):\n",
    "\n",
    "Row 0  ← Thread 0\n",
    "Row 1  ← Thread 1\n",
    "Row 2  ← Thread 2\n",
    "Row 3  ← Thread 3\n",
    "Row 4  ← Thread 0\n",
    "Row 5  ← Thread 1\n",
    "...\n",
    "```\n",
    "\n",
    "Each thread computes different rows of C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Implementation with Timing\n",
    "\n",
    "```cpp\n",
    "#include <iostream>\n",
    "#include <omp.h>\n",
    "#include <cstdlib>\n",
    "#include <cmath>\n",
    "\n",
    "#define N 1000\n",
    "\n",
    "double A[N][N], B[N][N], C_serial[N][N], C_parallel[N][N];\n",
    "\n",
    "void initialize() {\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        for (int j = 0; j < N; j++) {\n",
    "            A[i][j] = rand() % 10;\n",
    "            B[i][j] = rand() % 10;\n",
    "            C_serial[i][j] = 0;\n",
    "            C_parallel[i][j] = 0;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "void multiply_serial() {\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        for (int j = 0; j < N; j++) {\n",
    "            for (int k = 0; k < N; k++) {\n",
    "                C_serial[i][j] += A[i][k] * B[k][j];\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "void multiply_parallel() {\n",
    "    int i, j, k;\n",
    "    #pragma omp parallel for private(j, k)\n",
    "    for (i = 0; i < N; i++) {\n",
    "        for (j = 0; j < N; j++) {\n",
    "            for (k = 0; k < N; k++) {\n",
    "                C_parallel[i][j] += A[i][k] * B[k][j];\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "bool verify_results() {\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        for (int j = 0; j < N; j++) {\n",
    "            if (fabs(C_serial[i][j] - C_parallel[i][j]) > 1e-6) {\n",
    "                return false;\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    return true;\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    initialize();\n",
    "    \n",
    "    // Serial execution\n",
    "    double serial_start = omp_get_wtime();\n",
    "    multiply_serial();\n",
    "    double serial_end = omp_get_wtime();\n",
    "    double serial_time = serial_end - serial_start;\n",
    "    \n",
    "    // Parallel execution\n",
    "    double parallel_start = omp_get_wtime();\n",
    "    multiply_parallel();\n",
    "    double parallel_end = omp_get_wtime();\n",
    "    double parallel_time = parallel_end - parallel_start;\n",
    "    \n",
    "    // Verify correctness\n",
    "    if (verify_results()) {\n",
    "        printf(\"✓ Results match!\\n\");\n",
    "    } else {\n",
    "        printf(\"✗ Results differ!\\n\");\n",
    "    }\n",
    "    \n",
    "    // Performance report\n",
    "    printf(\"\\nPerformance Report:\\n\");\n",
    "    printf(\"Matrix size: %d×%d\\n\", N, N);\n",
    "    printf(\"Serial time: %.6f seconds\\n\", serial_time);\n",
    "    printf(\"Parallel time: %.6f seconds\\n\", parallel_time);\n",
    "    printf(\"Speedup: %.2fx\\n\", serial_time / parallel_time);\n",
    "    printf(\"Threads: %d\\n\", omp_get_max_threads());\n",
    "    \n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "### Example Output\n",
    "\n",
    "```\n",
    "✓ Results match!\n",
    "\n",
    "Performance Report:\n",
    "Matrix size: 1000×1000\n",
    "Serial time: 8.491261 seconds\n",
    "Parallel time: 1.215151 seconds\n",
    "Speedup: 6.99x\n",
    "Threads: 8\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Considerations\n",
    "\n",
    "#### 1. Loop Order Matters\n",
    "\n",
    "```cpp\n",
    "// Better cache locality (accessing B column-wise)\n",
    "for (i) for (j) for (k)\n",
    "    C[i][j] += A[i][k] * B[k][j];\n",
    "\n",
    "// Better for some systems (accessing A row-wise)\n",
    "for (i) for (k) for (j)\n",
    "    C[i][j] += A[i][k] * B[k][j];\n",
    "```\n",
    "\n",
    "#### 2. Scheduling Strategies\n",
    "\n",
    "```cpp\n",
    "// Static: Divide iterations evenly (default)\n",
    "#pragma omp parallel for schedule(static)\n",
    "\n",
    "// Dynamic: Assign iterations dynamically (better for unbalanced work)\n",
    "#pragma omp parallel for schedule(dynamic, 10)\n",
    "\n",
    "// Guided: Start with large chunks, decrease over time\n",
    "#pragma omp parallel for schedule(guided)\n",
    "```\n",
    "\n",
    "#### 3. Why Not 8x Speedup?\n",
    "\n",
    "Reasons for < ideal speedup:\n",
    "- **Memory bandwidth**: All threads accessing memory\n",
    "- **Cache misses**: Matrix B accessed in column-major order\n",
    "- **Thread overhead**: Creating and synchronizing threads\n",
    "- **Load imbalance**: Some threads finish earlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section7'></a>\n",
    "## 7. Introduction to CUDA (Optional)\n",
    "\n",
    "### What is CUDA?\n",
    "\n",
    "**CUDA** (Compute Unified Device Architecture) is NVIDIA's platform for GPU programming.\n",
    "\n",
    "### CPU vs GPU Architecture\n",
    "\n",
    "```\n",
    "CPU (8 cores):              GPU (1000s of cores):\n",
    "┌──────────────┐            ┌──────────────────────┐\n",
    "│ ██ ██ ██ ██  │            │ ▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪ │\n",
    "│ ██ ██ ██ ██  │            │ ▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪ │\n",
    "│              │            │ ▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪ │\n",
    "│ Large Cache  │            │ ▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪▪ │\n",
    "└──────────────┘            └──────────────────────┘\n",
    " Few powerful cores         Many simple cores\n",
    " Good for complex logic     Good for data-parallel\n",
    "```\n",
    "\n",
    "### When to Use GPU?\n",
    "\n",
    "✅ **Good for GPU**:\n",
    "- Matrix operations\n",
    "- Image processing\n",
    "- Machine learning\n",
    "- Scientific simulations\n",
    "- Cryptography\n",
    "\n",
    "❌ **Not good for GPU**:\n",
    "- Branch-heavy code\n",
    "- Sequential algorithms\n",
    "- Small datasets\n",
    "- Frequent host-device transfers\n",
    "\n",
    "### CUDA Programming Model\n",
    "\n",
    "```\n",
    "HOST (CPU)              DEVICE (GPU)\n",
    "    |                       |\n",
    "    |   1. Allocate GPU     |\n",
    "    |      memory           |\n",
    "    |---------------------->|\n",
    "    |                       |\n",
    "    |   2. Copy data        |\n",
    "    |      CPU → GPU        |\n",
    "    |---------------------->|\n",
    "    |                       |\n",
    "    |   3. Launch kernel    |\n",
    "    |    <<<blocks,threads>>>|\n",
    "    |---------------------->|\n",
    "    |                       |\n",
    "    |                  [GPU computes]\n",
    "    |                       |\n",
    "    |   4. Copy results     |\n",
    "    |      GPU → CPU        |\n",
    "    |<----------------------|\n",
    "    |                       |\n",
    "    |   5. Free GPU memory  |\n",
    "    |---------------------->|\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDA Key Concepts\n",
    "\n",
    "#### 1. Kernel Functions\n",
    "\n",
    "```cpp\n",
    "__global__ void kernel_function(int* data) {\n",
    "    // Runs on GPU\n",
    "    int idx = threadIdx.x;  // Thread ID\n",
    "    data[idx] = data[idx] * 2;\n",
    "}\n",
    "```\n",
    "\n",
    "- `__global__`: Function runs on GPU, called from CPU\n",
    "- `__device__`: Function runs on GPU, called from GPU\n",
    "- `__host__`: Function runs on CPU (default)\n",
    "\n",
    "#### 2. Thread Hierarchy\n",
    "\n",
    "```\n",
    "GRID\n",
    "  ├─ BLOCK 0\n",
    "  │   ├─ Thread 0\n",
    "  │   ├─ Thread 1\n",
    "  │   └─ Thread N\n",
    "  ├─ BLOCK 1\n",
    "  │   ├─ Thread 0\n",
    "  │   └─ ...\n",
    "  └─ BLOCK M\n",
    "```\n",
    "\n",
    "#### 3. Memory Types\n",
    "\n",
    "| Memory | Location | Access | Speed |\n",
    "|--------|----------|--------|-------|\n",
    "| Global | GPU DRAM | All threads | Slow |\n",
    "| Shared | On-chip | Block | Fast |\n",
    "| Local | On-chip | Thread | Fast |\n",
    "| Constant | GPU DRAM | Read-only | Cached |\n",
    "\n",
    "#### 4. Key Functions\n",
    "\n",
    "```cpp\n",
    "// Allocate GPU memory\n",
    "cudaMalloc((void**)&device_ptr, bytes);\n",
    "\n",
    "// Copy CPU → GPU\n",
    "cudaMemcpy(device_ptr, host_ptr, bytes, cudaMemcpyHostToDevice);\n",
    "\n",
    "// Launch kernel\n",
    "kernel<<<num_blocks, threads_per_block>>>(device_ptr);\n",
    "\n",
    "// Copy GPU → CPU\n",
    "cudaMemcpy(host_ptr, device_ptr, bytes, cudaMemcpyDeviceToHost);\n",
    "\n",
    "// Free GPU memory\n",
    "cudaFree(device_ptr);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section8'></a>\n",
    "## 8. Question 4 & 5: CUDA Vector Addition and Matrix Multiplication\n",
    "\n",
    "### Vector Addition Kernel\n",
    "\n",
    "```cpp\n",
    "#include <iostream>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// CUDA kernel for vector addition\n",
    "__global__ void vectorAdd(float* A, float* B, float* C, int N) {\n",
    "    // Calculate global thread ID\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    // Boundary check\n",
    "    if (idx < N) {\n",
    "        C[idx] = A[idx] + B[idx];\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int N = 1000000;  // Vector size\n",
    "    size_t bytes = N * sizeof(float);\n",
    "    \n",
    "    // 1. Allocate host memory\n",
    "    float *h_A, *h_B, *h_C;\n",
    "    h_A = (float*)malloc(bytes);\n",
    "    h_B = (float*)malloc(bytes);\n",
    "    h_C = (float*)malloc(bytes);\n",
    "    \n",
    "    // Initialize vectors\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        h_A[i] = i;\n",
    "        h_B[i] = i * 2;\n",
    "    }\n",
    "    \n",
    "    // 2. Allocate device memory\n",
    "    float *d_A, *d_B, *d_C;\n",
    "    cudaMalloc((void**)&d_A, bytes);\n",
    "    cudaMalloc((void**)&d_B, bytes);\n",
    "    cudaMalloc((void**)&d_C, bytes);\n",
    "    \n",
    "    // 3. Copy data from host to device\n",
    "    cudaMemcpy(d_A, h_A, bytes, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_B, h_B, bytes, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    // 4. Launch kernel\n",
    "    int threadsPerBlock = 256;\n",
    "    int blocksPerGrid = (N + threadsPerBlock - 1) / threadsPerBlock;\n",
    "    \n",
    "    vectorAdd<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, N);\n",
    "    \n",
    "    // Wait for GPU to finish\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    // 5. Copy results back to host\n",
    "    cudaMemcpy(h_C, d_C, bytes, cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    // Verify results\n",
    "    bool correct = true;\n",
    "    for (int i = 0; i < N && correct; i++) {\n",
    "        if (fabs(h_C[i] - (h_A[i] + h_B[i])) > 1e-5) {\n",
    "            correct = false;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    printf(\"Results: %s\\n\", correct ? \"CORRECT\" : \"INCORRECT\");\n",
    "    \n",
    "    // 6. Free memory\n",
    "    cudaFree(d_A);\n",
    "    cudaFree(d_B);\n",
    "    cudaFree(d_C);\n",
    "    free(h_A);\n",
    "    free(h_B);\n",
    "    free(h_C);\n",
    "    \n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "### Thread ID Calculation\n",
    "\n",
    "```cpp\n",
    "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "```\n",
    "\n",
    "Example:\n",
    "```\n",
    "N = 1000, threadsPerBlock = 256\n",
    "\n",
    "Block 0: threads 0-255    (idx = 0*256 + 0-255)\n",
    "Block 1: threads 256-511  (idx = 1*256 + 0-255)\n",
    "Block 2: threads 512-767  (idx = 2*256 + 0-255)\n",
    "Block 3: threads 768-999  (idx = 3*256 + 0-231)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compilation Instructions\n",
    "\n",
    "#### Windows (Visual Studio):\n",
    "1. Create CUDA project: **File → New → Project → CUDA Runtime**\n",
    "2. Save code as `.cu` file\n",
    "3. Build: **Ctrl+B**\n",
    "4. Run: **F5**\n",
    "\n",
    "#### Linux:\n",
    "```bash\n",
    "nvcc -o vectorAdd vectorAdd.cu\n",
    "./vectorAdd\n",
    "```\n",
    "\n",
    "### Performance Profiling\n",
    "\n",
    "#### Using nvprof (Legacy)\n",
    "```bash\n",
    "nvprof ./vectorAdd\n",
    "```\n",
    "\n",
    "#### Using nsys (Modern)\n",
    "```bash\n",
    "# Generate profile\n",
    "nsys profile -o report ./vectorAdd\n",
    "\n",
    "# Analyze report\n",
    "nsys analyze report.nsys-rep\n",
    "```\n",
    "\n",
    "**Note**: If you see compatibility warnings with nvprof, use nsys instead (NVIDIA NSight Systems).\n",
    "\n",
    "### Expected Profile Output\n",
    "\n",
    "```\n",
    "Time(%)  Time      Calls  Avg       Min       Max       Name\n",
    "50.06%   5.1200ms  1      5.1200ms  5.1200ms  5.1200ms  vectorAdd\n",
    "38.32%   3.9200ms  3      1.3067ms  928.00us  1.5040ms  [CUDA memcpy HtoD]\n",
    "11.61%   1.1800ms  1      1.1800ms  1.1800ms  1.1800ms  [CUDA memcpy DtoH]\n",
    "```\n",
    "\n",
    "**Analysis**:\n",
    "- Kernel execution: 5.12ms\n",
    "- Memory transfer: 5.10ms total\n",
    "- Memory transfer is 50% of time! (Can be optimized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section9'></a>\n",
    "## 9. Performance Summary\n",
    "\n",
    "### Speedup Comparison (Example System: 8-core CPU, NVIDIA GPU)\n",
    "\n",
    "| Task | Serial | OpenMP (8 threads) | CUDA | Best |\n",
    "|------|--------|-------------------|------|------|\n",
    "| Vector Add (1M) | 5ms | 1.2ms (4.2x) | 0.8ms (6.3x) | CUDA |\n",
    "| Pi Calculation | 45ms | 6ms (7.5x) | N/A | OpenMP |\n",
    "| Matrix Mul (1000×1000) | 8491ms | 1215ms (7.0x) | 145ms (58.6x) | CUDA |\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **OpenMP**: Best for quick parallelization of CPU code\n",
    "   - Easy to add to existing code\n",
    "   - Good speedup for CPU-bound tasks\n",
    "   - Limited by CPU core count\n",
    "\n",
    "2. **CUDA**: Best for massive data-parallel workloads\n",
    "   - Requires more code changes\n",
    "   - Excellent speedup for suitable problems\n",
    "   - Memory transfer can be bottleneck\n",
    "\n",
    "3. **When to use what**:\n",
    "   - **Small datasets**: Stay on CPU (OpenMP)\n",
    "   - **Complex logic**: CPU (OpenMP)\n",
    "   - **Simple, data-parallel, large datasets**: GPU (CUDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section10'></a>\n",
    "## 10. Summary and Key Concepts\n",
    "\n",
    "### OpenMP Directives Covered\n",
    "\n",
    "| Directive | Purpose | Example |\n",
    "|-----------|---------|--------|\n",
    "| `#pragma omp parallel` | Create parallel region | Basic parallelism |\n",
    "| `#pragma omp for` | Parallelize loop | Loop parallelization |\n",
    "| `#pragma omp parallel for` | Combined directive | Most common usage |\n",
    "| `reduction(+:var)` | Parallel reduction | Sum, product, etc. |\n",
    "| `private(var)` | Thread-private variable | Avoid sharing |\n",
    "| `shared(var)` | Shared variable | Explicit sharing |\n",
    "| `firstprivate(var)` | Private with initialization | Copy initial value |\n",
    "\n",
    "### Common Pitfalls\n",
    "\n",
    "1. **Race conditions**: Multiple threads accessing shared variables\n",
    "2. **False sharing**: Variables on same cache line\n",
    "3. **Load imbalance**: Some threads finish early\n",
    "4. **Overhead**: Too many threads or too small workload\n",
    "5. **Memory bandwidth**: All threads competing for memory\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Start simple**: Add `#pragma omp parallel for` to loops\n",
    "2. **Profile first**: Identify bottlenecks before optimizing\n",
    "3. **Use reduction**: For accumulation operations\n",
    "4. **Minimize sharing**: Make variables private when possible\n",
    "5. **Check correctness**: Verify parallel results match serial\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In Practical 4 and 5, you will learn:\n",
    "- Critical sections and locks\n",
    "- Barriers and synchronization\n",
    "- Producer-consumer patterns\n",
    "- Atomic operations\n",
    "- Deadlock prevention\n",
    "\n",
    "---\n",
    "\n",
    "**End of Practical 3**\n",
    "\n",
    "**Key Message**: Parallel programming requires careful consideration of data sharing and synchronization. OpenMP makes it easier, but understanding the underlying concepts is crucial for writing correct and efficient parallel code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
