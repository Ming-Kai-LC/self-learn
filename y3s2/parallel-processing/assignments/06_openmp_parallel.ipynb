{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 06: Multi-Core CPU Parallelization with Python\n",
    "\n",
    "**Difficulty**: ⭐⭐⭐\n",
    "\n",
    "**Estimated Time**: 90 minutes\n",
    "\n",
    "**Prerequisites**: \n",
    "- Module 05: Serial Implementation\n",
    "- Understanding of processes vs threads\n",
    "- Basic parallel computing concepts\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Understand OpenMP concepts and how they map to Python multiprocessing\n",
    "2. Implement parallel versions of serial algorithms using multiprocessing\n",
    "3. Use Numba's parallel features for CPU parallelization\n",
    "4. Manage thread synchronization and avoid race conditions\n",
    "5. Implement load balancing strategies for optimal performance\n",
    "6. Measure and analyze speedup from parallelization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from typing import Tuple, List\n",
    "\n",
    "# Try to import numba (for JIT compilation and parallelization)\n",
    "try:\n",
    "    from numba import jit, prange, set_num_threads\n",
    "    NUMBA_AVAILABLE = True\n",
    "    print(\"Numba available - will use for JIT compilation\")\n",
    "except ImportError:\n",
    "    NUMBA_AVAILABLE = False\n",
    "    print(\"Numba not available - install with: pip install numba\")\n",
    "    print(\"Will use multiprocessing only\")\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Get system information\n",
    "num_cores = cpu_count()\n",
    "print(f\"\\nSystem information:\")\n",
    "print(f\"  CPU cores available: {num_cores}\")\n",
    "print(f\"  NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. OpenMP Concepts → Python Equivalents\n",
    "\n",
    "### OpenMP Directives and Python Mapping\n",
    "\n",
    "| OpenMP (C/C++) | Python Equivalent | Purpose |\n",
    "|----------------|-------------------|----------|\n",
    "| `#pragma omp parallel` | `multiprocessing.Pool()` | Create parallel region |\n",
    "| `#pragma omp for` | `pool.map()` | Parallel loop |\n",
    "| `#pragma omp parallel for` | Numba `prange()` | Combined parallel+loop |\n",
    "| `#pragma omp critical` | `multiprocessing.Lock()` | Critical section |\n",
    "| `#pragma omp atomic` | `multiprocessing.Value()` | Atomic operation |\n",
    "| `#pragma omp reduction` | Manual reduction in Pool | Reduction operation |\n",
    "| `num_threads(N)` | `Pool(processes=N)` | Set thread count |\n",
    "\n",
    "### Important Differences\n",
    "\n",
    "- **Python multiprocessing uses processes, not threads** (due to GIL)\n",
    "- **Higher overhead** for process creation vs threads\n",
    "- **No shared memory by default** - data must be serialized\n",
    "- **Numba can use real threads** - better for numerical code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Timing and Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_function(func, *args, num_runs=3, **kwargs):\n",
    "    \"\"\"\n",
    "    Time a function execution with multiple runs.\n",
    "    \"\"\"\n",
    "    times = []\n",
    "    result = None\n",
    "    \n",
    "    for i in range(num_runs):\n",
    "        start = time.perf_counter()\n",
    "        result = func(*args, **kwargs)\n",
    "        end = time.perf_counter()\n",
    "        times.append(end - start)\n",
    "    \n",
    "    mean_time = np.mean(times)\n",
    "    std_time = np.std(times)\n",
    "    \n",
    "    return mean_time, std_time, result\n",
    "\n",
    "\n",
    "def calculate_speedup(serial_time, parallel_time):\n",
    "    \"\"\"\n",
    "    Calculate speedup and efficiency.\n",
    "    \n",
    "    Speedup = T_serial / T_parallel\n",
    "    Efficiency = Speedup / num_cores\n",
    "    \"\"\"\n",
    "    speedup = serial_time / parallel_time\n",
    "    efficiency = speedup / num_cores\n",
    "    return speedup, efficiency\n",
    "\n",
    "\n",
    "# Create test data (reuse from Module 05)\n",
    "def create_test_image(size=(512, 512)):\n",
    "    \"\"\"Create test image for filtering.\"\"\"\n",
    "    height, width = size\n",
    "    image = np.zeros((height, width), dtype=np.uint8)\n",
    "    \n",
    "    for i in range(height):\n",
    "        image[i, :] = int(255 * i / height)\n",
    "    \n",
    "    image[100:200, 100:300] = 200\n",
    "    \n",
    "    center_y, center_x = height // 2, width // 2\n",
    "    radius = 80\n",
    "    y, x = np.ogrid[:height, :width]\n",
    "    mask = (x - center_x)**2 + (y - center_y)**2 <= radius**2\n",
    "    image[mask] = 150\n",
    "    \n",
    "    noise = np.random.randint(-20, 20, size=(height, width))\n",
    "    image = np.clip(image.astype(np.int16) + noise, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    return image\n",
    "\n",
    "print(\"Utilities loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Image Processing with Multiprocessing\n",
    "\n",
    "We'll parallelize image filtering by dividing the image into chunks and processing each chunk on a separate core."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Serial Implementation (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gaussian_kernel(size=5, sigma=1.0):\n",
    "    \"\"\"Create Gaussian kernel.\"\"\"\n",
    "    if size % 2 == 0:\n",
    "        size += 1\n",
    "    ax = np.arange(-size // 2 + 1, size // 2 + 1)\n",
    "    xx, yy = np.meshgrid(ax, ax)\n",
    "    kernel = np.exp(-(xx**2 + yy**2) / (2 * sigma**2))\n",
    "    return kernel / np.sum(kernel)\n",
    "\n",
    "\n",
    "def apply_filter_serial(image, kernel):\n",
    "    \"\"\"Serial convolution implementation.\"\"\"\n",
    "    height, width = image.shape\n",
    "    k_height, k_width = kernel.shape\n",
    "    \n",
    "    pad_h = k_height // 2\n",
    "    pad_w = k_width // 2\n",
    "    \n",
    "    padded = np.pad(image, ((pad_h, pad_h), (pad_w, pad_w)), mode='edge')\n",
    "    output = np.zeros_like(image, dtype=np.float32)\n",
    "    \n",
    "    for i in range(height):\n",
    "        for j in range(width):\n",
    "            region = padded[i:i+k_height, j:j+k_width]\n",
    "            output[i, j] = np.sum(region * kernel)\n",
    "    \n",
    "    return np.clip(output, 0, 255).astype(np.uint8)\n",
    "\n",
    "\n",
    "# Create test image and kernel\n",
    "test_image = create_test_image((512, 512))\n",
    "gaussian_kernel = create_gaussian_kernel(size=5, sigma=1.5)\n",
    "\n",
    "print(f\"Test image shape: {test_image.shape}\")\n",
    "print(f\"Kernel shape: {gaussian_kernel.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Parallel Implementation with Multiprocessing\n",
    "\n",
    "**Strategy**: Divide image into horizontal strips, process each strip in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image_chunk(args):\n",
    "    \"\"\"\n",
    "    Process a chunk of the image.\n",
    "    \n",
    "    This function will be called by each worker process.\n",
    "    \n",
    "    Args:\n",
    "        args: Tuple of (image_chunk, kernel, start_row)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (filtered_chunk, start_row)\n",
    "    \"\"\"\n",
    "    image_chunk, kernel, start_row = args\n",
    "    \n",
    "    height, width = image_chunk.shape\n",
    "    k_height, k_width = kernel.shape\n",
    "    \n",
    "    pad_h = k_height // 2\n",
    "    pad_w = k_width // 2\n",
    "    \n",
    "    # Pad the chunk\n",
    "    padded = np.pad(image_chunk, ((pad_h, pad_h), (pad_w, pad_w)), mode='edge')\n",
    "    output = np.zeros_like(image_chunk, dtype=np.float32)\n",
    "    \n",
    "    # Apply filter\n",
    "    for i in range(height):\n",
    "        for j in range(width):\n",
    "            region = padded[i:i+k_height, j:j+k_width]\n",
    "            output[i, j] = np.sum(region * kernel)\n",
    "    \n",
    "    output = np.clip(output, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    return output, start_row\n",
    "\n",
    "\n",
    "def apply_filter_parallel(image, kernel, num_processes=None):\n",
    "    \"\"\"\n",
    "    Apply filter using multiprocessing.\n",
    "    \n",
    "    Args:\n",
    "        image: Input image\n",
    "        kernel: Convolution kernel\n",
    "        num_processes: Number of processes (default: cpu_count)\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: Filtered image\n",
    "    \"\"\"\n",
    "    if num_processes is None:\n",
    "        num_processes = cpu_count()\n",
    "    \n",
    "    height, width = image.shape\n",
    "    \n",
    "    # Divide image into chunks (horizontal strips)\n",
    "    chunk_size = height // num_processes\n",
    "    \n",
    "    # Prepare chunks with overlap for boundary handling\n",
    "    k_height = kernel.shape[0]\n",
    "    overlap = k_height // 2\n",
    "    \n",
    "    chunks = []\n",
    "    for i in range(num_processes):\n",
    "        start = i * chunk_size\n",
    "        end = start + chunk_size if i < num_processes - 1 else height\n",
    "        \n",
    "        # Add overlap at boundaries\n",
    "        chunk_start = max(0, start - overlap)\n",
    "        chunk_end = min(height, end + overlap)\n",
    "        \n",
    "        chunk = image[chunk_start:chunk_end, :]\n",
    "        chunks.append((chunk, kernel, start))\n",
    "    \n",
    "    # Process chunks in parallel\n",
    "    with Pool(processes=num_processes) as pool:\n",
    "        results = pool.map(process_image_chunk, chunks)\n",
    "    \n",
    "    # Combine results\n",
    "    output = np.zeros_like(image, dtype=np.uint8)\n",
    "    \n",
    "    for result_chunk, start_row in results:\n",
    "        chunk_height = result_chunk.shape[0]\n",
    "        end_row = min(start_row + chunk_height, height)\n",
    "        actual_height = end_row - start_row\n",
    "        output[start_row:end_row, :] = result_chunk[:actual_height, :]\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "print(\"Parallel image processing functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Benchmark and Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"IMAGE PROCESSING BENCHMARK: Serial vs Parallel\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test serial\n",
    "print(\"\\nSerial implementation...\")\n",
    "t_serial, std_serial, result_serial = time_function(\n",
    "    apply_filter_serial, test_image, gaussian_kernel, num_runs=3\n",
    ")\n",
    "print(f\"  Time: {t_serial:.4f} ± {std_serial:.4f} seconds\")\n",
    "\n",
    "# Test parallel with different number of processes\n",
    "parallel_results = []\n",
    "\n",
    "for n_proc in [1, 2, 4, num_cores]:\n",
    "    if n_proc > num_cores:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nParallel with {n_proc} processes...\")\n",
    "    t_parallel, std_parallel, result_parallel = time_function(\n",
    "        apply_filter_parallel, test_image, gaussian_kernel, \n",
    "        num_processes=n_proc, num_runs=3\n",
    "    )\n",
    "    print(f\"  Time: {t_parallel:.4f} ± {std_parallel:.4f} seconds\")\n",
    "    \n",
    "    speedup, efficiency = calculate_speedup(t_serial, t_parallel)\n",
    "    print(f\"  Speedup: {speedup:.2f}x\")\n",
    "    print(f\"  Efficiency: {efficiency*100:.1f}%\")\n",
    "    \n",
    "    parallel_results.append({\n",
    "        'Processes': n_proc,\n",
    "        'Time (s)': t_parallel,\n",
    "        'Speedup': speedup,\n",
    "        'Efficiency (%)': efficiency * 100\n",
    "    })\n",
    "\n",
    "# Verify results are identical\n",
    "are_equal = np.allclose(result_serial, result_parallel, rtol=1e-5)\n",
    "print(f\"\\nResults match: {are_equal}\")\n",
    "\n",
    "# Display table\n",
    "df_image = pd.DataFrame(parallel_results)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY TABLE\")\n",
    "print(\"=\"*70)\n",
    "print(df_image.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize speedup\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Speedup curve\n",
    "plt.subplot(131)\n",
    "processes = df_image['Processes'].values\n",
    "speedups = df_image['Speedup'].values\n",
    "plt.plot(processes, speedups, 'o-', linewidth=2, markersize=8, label='Actual')\n",
    "plt.plot(processes, processes, '--', linewidth=1, color='gray', label='Ideal (linear)')\n",
    "plt.xlabel('Number of Processes')\n",
    "plt.ylabel('Speedup')\n",
    "plt.title('Speedup vs Number of Processes')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "# Plot 2: Efficiency\n",
    "plt.subplot(132)\n",
    "efficiency = df_image['Efficiency (%)'].values\n",
    "plt.plot(processes, efficiency, 'o-', linewidth=2, markersize=8, color='green')\n",
    "plt.axhline(y=100, linestyle='--', color='gray', linewidth=1, label='100% (ideal)')\n",
    "plt.xlabel('Number of Processes')\n",
    "plt.ylabel('Efficiency (%)')\n",
    "plt.title('Parallel Efficiency')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "# Plot 3: Execution time\n",
    "plt.subplot(133)\n",
    "times = df_image['Time (s)'].values\n",
    "plt.plot(processes, times, 'o-', linewidth=2, markersize=8, color='red')\n",
    "plt.axhline(y=t_serial, linestyle='--', color='gray', linewidth=1, label='Serial time')\n",
    "plt.xlabel('Number of Processes')\n",
    "plt.ylabel('Execution Time (seconds)')\n",
    "plt.title('Execution Time vs Processes')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Numba Parallel Implementation\n",
    "\n",
    "Numba can use real threads (not processes) and has lower overhead. Let's compare!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if NUMBA_AVAILABLE:\n",
    "    @jit(nopython=True, parallel=True)\n",
    "    def apply_filter_numba(image, kernel):\n",
    "        \"\"\"\n",
    "        Apply filter using Numba parallel JIT compilation.\n",
    "        \n",
    "        The @jit decorator compiles this to machine code.\n",
    "        parallel=True enables automatic parallelization.\n",
    "        prange() creates parallel loops.\n",
    "        \"\"\"\n",
    "        height, width = image.shape\n",
    "        k_height, k_width = kernel.shape\n",
    "        \n",
    "        pad_h = k_height // 2\n",
    "        pad_w = k_width // 2\n",
    "        \n",
    "        # Manual padding (Numba doesn't support np.pad)\n",
    "        padded = np.zeros((height + 2*pad_h, width + 2*pad_w), dtype=image.dtype)\n",
    "        padded[pad_h:pad_h+height, pad_w:pad_w+width] = image\n",
    "        \n",
    "        # Edge replication for borders\n",
    "        for i in range(pad_h):\n",
    "            padded[i, pad_w:pad_w+width] = image[0, :]\n",
    "            padded[height+pad_h+i, pad_w:pad_w+width] = image[height-1, :]\n",
    "        for j in range(pad_w):\n",
    "            padded[:, j] = padded[:, pad_w]\n",
    "            padded[:, width+pad_w+j] = padded[:, width+pad_w-1]\n",
    "        \n",
    "        output = np.zeros((height, width), dtype=np.float32)\n",
    "        \n",
    "        # PARALLEL LOOP - prange() distributes iterations across threads\n",
    "        for i in prange(height):\n",
    "            for j in range(width):\n",
    "                # Apply kernel\n",
    "                value = 0.0\n",
    "                for ki in range(k_height):\n",
    "                    for kj in range(k_width):\n",
    "                        value += padded[i+ki, j+kj] * kernel[ki, kj]\n",
    "                output[i, j] = value\n",
    "        \n",
    "        # Clip and convert\n",
    "        result = np.empty((height, width), dtype=np.uint8)\n",
    "        for i in prange(height):\n",
    "            for j in range(width):\n",
    "                val = output[i, j]\n",
    "                if val < 0:\n",
    "                    result[i, j] = 0\n",
    "                elif val > 255:\n",
    "                    result[i, j] = 255\n",
    "                else:\n",
    "                    result[i, j] = np.uint8(val)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    print(\"Numba parallel functions compiled!\")\n",
    "    \n",
    "    # Warm-up (first run compiles the function)\n",
    "    print(\"\\nWarming up Numba (first run compiles)...\")\n",
    "    _ = apply_filter_numba(test_image[:100, :100], gaussian_kernel)\n",
    "    print(\"Warm-up complete!\")\n",
    "else:\n",
    "    print(\"Numba not available - skipping Numba benchmarks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if NUMBA_AVAILABLE:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"NUMBA PARALLEL BENCHMARK\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    numba_results = []\n",
    "    \n",
    "    for n_threads in [1, 2, 4, num_cores]:\n",
    "        if n_threads > num_cores:\n",
    "            continue\n",
    "        \n",
    "        set_num_threads(n_threads)\n",
    "        \n",
    "        print(f\"\\nNumba with {n_threads} threads...\")\n",
    "        t_numba, std_numba, result_numba = time_function(\n",
    "            apply_filter_numba, test_image, gaussian_kernel, num_runs=3\n",
    "        )\n",
    "        print(f\"  Time: {t_numba:.4f} ± {std_numba:.4f} seconds\")\n",
    "        \n",
    "        speedup, efficiency = calculate_speedup(t_serial, t_numba)\n",
    "        print(f\"  Speedup vs serial: {speedup:.2f}x\")\n",
    "        print(f\"  Efficiency: {efficiency*100:.1f}%\")\n",
    "        \n",
    "        numba_results.append({\n",
    "            'Threads': n_threads,\n",
    "            'Time (s)': t_numba,\n",
    "            'Speedup': speedup,\n",
    "            'Efficiency (%)': efficiency * 100\n",
    "        })\n",
    "    \n",
    "    # Display comparison\n",
    "    df_numba = pd.DataFrame(numba_results)\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"NUMBA SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(df_numba.to_string(index=False))\n",
    "    \n",
    "    # Compare multiprocessing vs Numba\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"COMPARISON: Multiprocessing vs Numba (max cores)\")\n",
    "    print(\"=\"*70)\n",
    "    mp_time = df_image[df_image['Processes'] == num_cores]['Time (s)'].values[0]\n",
    "    numba_time = df_numba[df_numba['Threads'] == num_cores]['Time (s)'].values[0]\n",
    "    print(f\"Multiprocessing ({num_cores} processes): {mp_time:.4f} s\")\n",
    "    print(f\"Numba ({num_cores} threads):            {numba_time:.4f} s\")\n",
    "    print(f\"Numba advantage:                         {mp_time/numba_time:.2f}x faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Matrix Multiplication Parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_multiply_serial(A, B):\n",
    "    \"\"\"Serial matrix multiplication.\"\"\"\n",
    "    m, n = A.shape\n",
    "    n2, p = B.shape\n",
    "    assert n == n2\n",
    "    \n",
    "    C = np.zeros((m, p), dtype=np.float64)\n",
    "    \n",
    "    for i in range(m):\n",
    "        for j in range(p):\n",
    "            for k in range(n):\n",
    "                C[i, j] += A[i, k] * B[k, j]\n",
    "    \n",
    "    return C\n",
    "\n",
    "\n",
    "def multiply_row_range(args):\n",
    "    \"\"\"\n",
    "    Multiply a range of rows from matrix A with entire matrix B.\n",
    "    \n",
    "    This parallelizes the outer loop of matrix multiplication.\n",
    "    \"\"\"\n",
    "    A_chunk, B, start_row = args\n",
    "    \n",
    "    m_chunk, n = A_chunk.shape\n",
    "    n2, p = B.shape\n",
    "    \n",
    "    C_chunk = np.zeros((m_chunk, p), dtype=np.float64)\n",
    "    \n",
    "    for i in range(m_chunk):\n",
    "        for j in range(p):\n",
    "            for k in range(n):\n",
    "                C_chunk[i, j] += A_chunk[i, k] * B[k, j]\n",
    "    \n",
    "    return C_chunk, start_row\n",
    "\n",
    "\n",
    "def matrix_multiply_parallel(A, B, num_processes=None):\n",
    "    \"\"\"\n",
    "    Parallel matrix multiplication using multiprocessing.\n",
    "    \n",
    "    Divides matrix A into row chunks and processes in parallel.\n",
    "    \"\"\"\n",
    "    if num_processes is None:\n",
    "        num_processes = cpu_count()\n",
    "    \n",
    "    m, n = A.shape\n",
    "    n2, p = B.shape\n",
    "    assert n == n2\n",
    "    \n",
    "    # Divide rows among processes\n",
    "    chunk_size = m // num_processes\n",
    "    \n",
    "    chunks = []\n",
    "    for i in range(num_processes):\n",
    "        start = i * chunk_size\n",
    "        end = start + chunk_size if i < num_processes - 1 else m\n",
    "        chunks.append((A[start:end, :], B, start))\n",
    "    \n",
    "    # Process in parallel\n",
    "    with Pool(processes=num_processes) as pool:\n",
    "        results = pool.map(multiply_row_range, chunks)\n",
    "    \n",
    "    # Combine results\n",
    "    C = np.zeros((m, p), dtype=np.float64)\n",
    "    for C_chunk, start_row in results:\n",
    "        chunk_rows = C_chunk.shape[0]\n",
    "        C[start_row:start_row+chunk_rows, :] = C_chunk\n",
    "    \n",
    "    return C\n",
    "\n",
    "\n",
    "# Create test matrices\n",
    "test_matrix_a = np.random.rand(300, 300)\n",
    "test_matrix_b = np.random.rand(300, 300)\n",
    "\n",
    "print(f\"Test matrices: {test_matrix_a.shape} × {test_matrix_b.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"MATRIX MULTIPLICATION BENCHMARK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Serial baseline\n",
    "print(\"\\nSerial implementation...\")\n",
    "t_serial_mat, _, result_serial_mat = time_function(\n",
    "    matrix_multiply_serial, test_matrix_a, test_matrix_b, num_runs=3\n",
    ")\n",
    "print(f\"  Time: {t_serial_mat:.4f} seconds\")\n",
    "\n",
    "# Parallel versions\n",
    "matrix_parallel_results = []\n",
    "\n",
    "for n_proc in [1, 2, 4, num_cores]:\n",
    "    if n_proc > num_cores:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nParallel with {n_proc} processes...\")\n",
    "    t_parallel_mat, _, result_parallel_mat = time_function(\n",
    "        matrix_multiply_parallel, test_matrix_a, test_matrix_b,\n",
    "        num_processes=n_proc, num_runs=3\n",
    "    )\n",
    "    print(f\"  Time: {t_parallel_mat:.4f} seconds\")\n",
    "    \n",
    "    speedup, efficiency = calculate_speedup(t_serial_mat, t_parallel_mat)\n",
    "    print(f\"  Speedup: {speedup:.2f}x\")\n",
    "    print(f\"  Efficiency: {efficiency*100:.1f}%\")\n",
    "    \n",
    "    matrix_parallel_results.append({\n",
    "        'Processes': n_proc,\n",
    "        'Time (s)': t_parallel_mat,\n",
    "        'Speedup': speedup,\n",
    "        'Efficiency (%)': efficiency * 100\n",
    "    })\n",
    "\n",
    "# Verify correctness\n",
    "matches = np.allclose(result_serial_mat, result_parallel_mat, rtol=1e-10)\n",
    "print(f\"\\nResults match: {matches}\")\n",
    "\n",
    "df_matrix_par = pd.DataFrame(matrix_parallel_results)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(df_matrix_par.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Monte Carlo Parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_pi_serial(num_samples):\n",
    "    \"\"\"Serial Monte Carlo π estimation.\"\"\"\n",
    "    inside_circle = 0\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        x = np.random.random()\n",
    "        y = np.random.random()\n",
    "        \n",
    "        if x*x + y*y <= 1.0:\n",
    "            inside_circle += 1\n",
    "    \n",
    "    return 4.0 * inside_circle / num_samples\n",
    "\n",
    "\n",
    "def estimate_pi_chunk(num_samples):\n",
    "    \"\"\"\n",
    "    Estimate π for a chunk of samples.\n",
    "    \n",
    "    Each worker process runs this independently.\n",
    "    \"\"\"\n",
    "    x = np.random.random(num_samples)\n",
    "    y = np.random.random(num_samples)\n",
    "    inside = np.sum(x*x + y*y <= 1.0)\n",
    "    return inside\n",
    "\n",
    "\n",
    "def estimate_pi_parallel(num_samples, num_processes=None):\n",
    "    \"\"\"\n",
    "    Parallel Monte Carlo π estimation.\n",
    "    \n",
    "    Distributes samples across processes, then combines results.\n",
    "    \"\"\"\n",
    "    if num_processes is None:\n",
    "        num_processes = cpu_count()\n",
    "    \n",
    "    # Divide samples among processes\n",
    "    samples_per_process = num_samples // num_processes\n",
    "    \n",
    "    # Process in parallel\n",
    "    with Pool(processes=num_processes) as pool:\n",
    "        results = pool.map(estimate_pi_chunk, \n",
    "                          [samples_per_process] * num_processes)\n",
    "    \n",
    "    # Combine results (reduction)\n",
    "    total_inside = sum(results)\n",
    "    pi_estimate = 4.0 * total_inside / num_samples\n",
    "    \n",
    "    return pi_estimate\n",
    "\n",
    "\n",
    "print(\"Monte Carlo parallel functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"MONTE CARLO π ESTIMATION BENCHMARK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "num_samples = 10_000_000\n",
    "\n",
    "# Serial\n",
    "print(f\"\\nSerial ({num_samples:,} samples)...\")\n",
    "t_serial_pi, _, pi_serial = time_function(\n",
    "    estimate_pi_serial, num_samples, num_runs=3\n",
    ")\n",
    "print(f\"  Time: {t_serial_pi:.4f} seconds\")\n",
    "print(f\"  π estimate: {pi_serial:.6f}\")\n",
    "print(f\"  Error: {abs(pi_serial - np.pi):.6f}\")\n",
    "\n",
    "# Parallel\n",
    "monte_carlo_results = []\n",
    "\n",
    "for n_proc in [1, 2, 4, num_cores]:\n",
    "    if n_proc > num_cores:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nParallel with {n_proc} processes...\")\n",
    "    t_parallel_pi, _, pi_parallel = time_function(\n",
    "        estimate_pi_parallel, num_samples, num_processes=n_proc, num_runs=3\n",
    "    )\n",
    "    print(f\"  Time: {t_parallel_pi:.4f} seconds\")\n",
    "    print(f\"  π estimate: {pi_parallel:.6f}\")\n",
    "    \n",
    "    speedup, efficiency = calculate_speedup(t_serial_pi, t_parallel_pi)\n",
    "    print(f\"  Speedup: {speedup:.2f}x\")\n",
    "    print(f\"  Efficiency: {efficiency*100:.1f}%\")\n",
    "    \n",
    "    monte_carlo_results.append({\n",
    "        'Processes': n_proc,\n",
    "        'Time (s)': t_parallel_pi,\n",
    "        'Speedup': speedup,\n",
    "        'Efficiency (%)': efficiency * 100,\n",
    "        'π Estimate': pi_parallel\n",
    "    })\n",
    "\n",
    "df_monte = pd.DataFrame(monte_carlo_results)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(df_monte.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Thread Safety and Race Conditions\n",
    "\n",
    "Let's demonstrate common pitfalls in parallel programming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Race condition (INCORRECT CODE - for demonstration)\n",
    "def increment_counter_unsafe(counter, num_increments):\n",
    "    \"\"\"\n",
    "    UNSAFE: Multiple processes modifying shared counter.\n",
    "    This will produce incorrect results!\n",
    "    \"\"\"\n",
    "    for _ in range(num_increments):\n",
    "        counter.value += 1\n",
    "\n",
    "\n",
    "def increment_counter_safe(lock, counter, num_increments):\n",
    "    \"\"\"\n",
    "    SAFE: Uses lock to prevent race conditions.\n",
    "    \"\"\"\n",
    "    for _ in range(num_increments):\n",
    "        with lock:\n",
    "            counter.value += 1\n",
    "\n",
    "\n",
    "print(\"Demonstrating race condition...\\n\")\n",
    "\n",
    "# Create shared counter\n",
    "counter_unsafe = mp.Value('i', 0)  # 'i' = integer\n",
    "counter_safe = mp.Value('i', 0)\n",
    "lock = mp.Lock()\n",
    "\n",
    "num_processes = 4\n",
    "increments_per_process = 10000\n",
    "expected_value = num_processes * increments_per_process\n",
    "\n",
    "# Test unsafe version\n",
    "print(\"Unsafe version (race condition):\")\n",
    "processes = []\n",
    "for _ in range(num_processes):\n",
    "    p = mp.Process(target=increment_counter_unsafe, \n",
    "                   args=(counter_unsafe, increments_per_process))\n",
    "    processes.append(p)\n",
    "    p.start()\n",
    "\n",
    "for p in processes:\n",
    "    p.join()\n",
    "\n",
    "print(f\"  Expected: {expected_value}\")\n",
    "print(f\"  Actual:   {counter_unsafe.value}\")\n",
    "print(f\"  Lost updates: {expected_value - counter_unsafe.value}\")\n",
    "\n",
    "# Test safe version\n",
    "print(\"\\nSafe version (with lock):\")\n",
    "processes = []\n",
    "for _ in range(num_processes):\n",
    "    p = mp.Process(target=increment_counter_safe, \n",
    "                   args=(lock, counter_safe, increments_per_process))\n",
    "    processes.append(p)\n",
    "    p.start()\n",
    "\n",
    "for p in processes:\n",
    "    p.join()\n",
    "\n",
    "print(f\"  Expected: {expected_value}\")\n",
    "print(f\"  Actual:   {counter_safe.value}\")\n",
    "print(f\"  Correct:  {counter_safe.value == expected_value}\")\n",
    "\n",
    "print(\"\\nKey lesson: Always protect shared state with locks!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Load Balancing Strategies\n",
    "\n",
    "Different work distribution strategies affect performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_workload(n):\n",
    "    \"\"\"\n",
    "    Simulate variable computation time.\n",
    "    Some tasks take longer than others.\n",
    "    \"\"\"\n",
    "    # Sleep time proportional to n\n",
    "    time.sleep(0.001 * n)\n",
    "    return n * n\n",
    "\n",
    "\n",
    "# Create tasks with varying workloads\n",
    "tasks = list(range(1, 21))  # 1, 2, 3, ..., 20\n",
    "\n",
    "print(\"Comparing load balancing strategies...\\n\")\n",
    "\n",
    "# Strategy 1: Static partitioning (chunks)\n",
    "print(\"Strategy 1: Static chunks (default)\")\n",
    "start = time.time()\n",
    "with Pool(processes=4) as pool:\n",
    "    results = pool.map(variable_workload, tasks)\n",
    "time_static = time.time() - start\n",
    "print(f\"  Time: {time_static:.3f} seconds\")\n",
    "\n",
    "# Strategy 2: Dynamic (chunksize=1)\n",
    "print(\"\\nStrategy 2: Dynamic (chunksize=1)\")\n",
    "start = time.time()\n",
    "with Pool(processes=4) as pool:\n",
    "    results = pool.map(variable_workload, tasks, chunksize=1)\n",
    "time_dynamic = time.time() - start\n",
    "print(f\"  Time: {time_dynamic:.3f} seconds\")\n",
    "\n",
    "print(f\"\\nDynamic is {time_static/time_dynamic:.2f}x faster!\")\n",
    "print(\"\\nWhy? Dynamic scheduling prevents idle workers.\")\n",
    "print(\"Static: Some workers get long tasks, others finish early and wait.\")\n",
    "print(\"Dynamic: Workers grab new tasks as soon as they finish.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"PARALLEL PROGRAMMING BEST PRACTICES\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "1. CHOOSE THE RIGHT TOOL:\n",
    "   - Multiprocessing: CPU-bound tasks, large data\n",
    "   - Numba: Numerical code, lower overhead\n",
    "   - Threading: I/O-bound tasks only (GIL limitation)\n",
    "\n",
    "2. MINIMIZE COMMUNICATION:\n",
    "   - Pass data to workers once at start\n",
    "   - Avoid frequent synchronization\n",
    "   - Larger chunks = less overhead\n",
    "\n",
    "3. AVOID SHARED STATE:\n",
    "   - Prefer independent tasks\n",
    "   - If sharing needed, use locks/semaphores\n",
    "   - Reduction at end instead of continuous updates\n",
    "\n",
    "4. LOAD BALANCING:\n",
    "   - Equal workload? Use static partitioning\n",
    "   - Variable workload? Use dynamic scheduling\n",
    "   - Monitor worker utilization\n",
    "\n",
    "5. MEASURE EVERYTHING:\n",
    "   - Always compare to serial baseline\n",
    "   - Test with different core counts\n",
    "   - Calculate speedup and efficiency\n",
    "   - Profile to find bottlenecks\n",
    "\n",
    "6. AMDAHL'S LAW:\n",
    "   - Speedup limited by serial portion\n",
    "   - 10% serial code → max 10x speedup\n",
    "   - Focus on parallelizing the bottleneck\n",
    "\n",
    "7. COMMON PITFALLS:\n",
    "   - Race conditions (use locks)\n",
    "   - False sharing (pad shared data)\n",
    "   - Too many processes (overhead dominates)\n",
    "   - Not enough work per task (overhead dominates)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PERFORMANCE SUMMARY (from this module)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nImage Processing ({test_image.shape[0]}x{test_image.shape[1]}):\")\n",
    "print(f\"  Best speedup: {df_image['Speedup'].max():.2f}x with {df_image.loc[df_image['Speedup'].idxmax(), 'Processes']:.0f} processes\")\n",
    "\n",
    "print(f\"\\nMatrix Multiplication ({test_matrix_a.shape[0]}x{test_matrix_a.shape[1]}):\")\n",
    "print(f\"  Best speedup: {df_matrix_par['Speedup'].max():.2f}x with {df_matrix_par.loc[df_matrix_par['Speedup'].idxmax(), 'Processes']:.0f} processes\")\n",
    "\n",
    "print(f\"\\nMonte Carlo ({num_samples:,} samples):\")\n",
    "print(f\"  Best speedup: {df_monte['Speedup'].max():.2f}x with {df_monte.loc[df_monte['Speedup'].idxmax(), 'Processes']:.0f} processes\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"NEXT STEPS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "Module 07: GPU Acceleration with CUDA\n",
    "  - Learn when GPU beats CPU\n",
    "  - Implement CUDA kernels with Numba\n",
    "  - Understand memory hierarchy\n",
    "  - Achieve 10-100x speedups on suitable problems\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Parallel Histogram Calculation\n",
    "\n",
    "Implement parallel histogram computation for image analysis.\n",
    "\n",
    "**Challenge**: How do you combine histograms from different chunks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_histogram_chunk(image_chunk):\n",
    "    \"\"\"\n",
    "    Compute histogram for an image chunk.\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: Histogram (256 bins for 0-255)\n",
    "    \"\"\"\n",
    "    # TODO: Implement histogram computation\n",
    "    # Hint: Use np.bincount() or np.histogram()\n",
    "    pass\n",
    "\n",
    "\n",
    "def compute_histogram_parallel(image, num_processes=None):\n",
    "    \"\"\"\n",
    "    Compute image histogram in parallel.\n",
    "    \n",
    "    Strategy:\n",
    "    1. Divide image into chunks\n",
    "    2. Compute histogram for each chunk\n",
    "    3. Sum all histograms\n",
    "    \"\"\"\n",
    "    # TODO: Implement parallel histogram\n",
    "    pass\n",
    "\n",
    "# Test your implementation\n",
    "# hist = compute_histogram_parallel(test_image, num_processes=4)\n",
    "# plt.bar(range(256), hist)\n",
    "# plt.xlabel('Pixel Value')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.title('Image Histogram (Parallel)')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Parallel Sorting (Merge Sort)\n",
    "\n",
    "Implement parallel merge sort.\n",
    "\n",
    "**Strategy**: Recursively divide array, sort sub-arrays in parallel, merge results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(left, right):\n",
    "    \"\"\"\n",
    "    Merge two sorted arrays.\n",
    "    \"\"\"\n",
    "    # TODO: Implement merge\n",
    "    pass\n",
    "\n",
    "\n",
    "def parallel_merge_sort(arr, num_processes=None):\n",
    "    \"\"\"\n",
    "    Parallel merge sort.\n",
    "    \"\"\"\n",
    "    # TODO: Implement parallel merge sort\n",
    "    # Hint: Divide array, use Pool.map() to sort chunks, merge results\n",
    "    pass\n",
    "\n",
    "# Test\n",
    "# test_arr = np.random.randint(0, 1000, size=10000)\n",
    "# sorted_arr = parallel_merge_sort(test_arr, num_processes=4)\n",
    "# print(f\"Correctly sorted: {np.all(sorted_arr[:-1] <= sorted_arr[1:])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Scalability Analysis\n",
    "\n",
    "Analyze strong vs weak scaling for one of the implemented algorithms.\n",
    "\n",
    "**Strong scaling**: Fixed problem size, increase cores\n",
    "\n",
    "**Weak scaling**: Problem size increases with cores (constant work per core)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement scalability analysis\n",
    "# 1. Choose an algorithm (e.g., Monte Carlo)\n",
    "# 2. Strong scaling: Fix num_samples, vary num_processes\n",
    "# 3. Weak scaling: num_samples proportional to num_processes\n",
    "# 4. Plot both scaling curves\n",
    "\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this module, you learned:\n",
    "\n",
    "1. **Parallel Programming Concepts**\n",
    "   - OpenMP directives → Python equivalents\n",
    "   - Processes vs threads in Python\n",
    "   - Shared memory vs message passing\n",
    "\n",
    "2. **Implementation Techniques**\n",
    "   - Multiprocessing Pool for task parallelism\n",
    "   - Numba parallel loops for data parallelism\n",
    "   - Work distribution strategies\n",
    "\n",
    "3. **Performance Optimization**\n",
    "   - Measuring speedup and efficiency\n",
    "   - Load balancing (static vs dynamic)\n",
    "   - Minimizing communication overhead\n",
    "\n",
    "4. **Common Pitfalls**\n",
    "   - Race conditions and synchronization\n",
    "   - Too much/too little granularity\n",
    "   - Amdahl's law limitations\n",
    "\n",
    "**What's Next?**\n",
    "\n",
    "- **Module 07**: GPU acceleration with CUDA - 10-100x speedups\n",
    "- **Module 08**: Comprehensive benchmarking and analysis\n",
    "- **Module 09**: Advanced optimization techniques"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
