{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 09: Advanced Optimization Techniques\n",
    "\n",
    "**Difficulty**: ⭐⭐⭐\n",
    "\n",
    "**Estimated Time**: 90 minutes\n",
    "\n",
    "**Prerequisites**: \n",
    "- Module 05-08: Serial, Parallel, GPU, Benchmarking\n",
    "- Understanding of performance bottlenecks\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Optimize memory access patterns for cache efficiency\n",
    "2. Implement load balancing strategies for irregular workloads\n",
    "3. Reduce communication overhead in parallel programs\n",
    "4. Apply vectorization techniques for better CPU utilization\n",
    "5. Design hybrid CPU-GPU approaches for maximum performance\n",
    "6. Follow an iterative optimization process with measurable improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from typing import Tuple, List\n",
    "import seaborn as sns\n",
    "\n",
    "# Try importing optimization libraries\n",
    "try:\n",
    "    from numba import jit, prange, set_num_threads\n",
    "    NUMBA_AVAILABLE = True\n",
    "except:\n",
    "    NUMBA_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    from numba import cuda\n",
    "    GPU_AVAILABLE = cuda.is_available()\n",
    "except:\n",
    "    GPU_AVAILABLE = False\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# System info\n",
    "num_cores = cpu_count()\n",
    "print(f\"System Configuration:\")\n",
    "print(f\"  CPU cores: {num_cores}\")\n",
    "print(f\"  Numba available: {NUMBA_AVAILABLE}\")\n",
    "print(f\"  GPU available: {GPU_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cache Optimization: Memory Access Patterns\n",
    "\n",
    "Cache misses can devastate performance. Let's optimize memory access!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_sum_row_major(matrix):\n",
    "    \"\"\"\n",
    "    Sum matrix elements using row-major order (cache-friendly for NumPy).\n",
    "    \n",
    "    NumPy stores arrays in row-major (C) order by default.\n",
    "    Accessing consecutive elements in a row = good cache locality.\n",
    "    \"\"\"\n",
    "    total = 0.0\n",
    "    rows, cols = matrix.shape\n",
    "    \n",
    "    # Outer loop: rows, inner loop: columns\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            total += matrix[i, j]\n",
    "    \n",
    "    return total\n",
    "\n",
    "\n",
    "def matrix_sum_column_major(matrix):\n",
    "    \"\"\"\n",
    "    Sum matrix elements using column-major order (cache-unfriendly).\n",
    "    \n",
    "    Accessing down columns = jumping around in memory.\n",
    "    Each access might miss the cache!\n",
    "    \"\"\"\n",
    "    total = 0.0\n",
    "    rows, cols = matrix.shape\n",
    "    \n",
    "    # Outer loop: columns, inner loop: rows\n",
    "    for j in range(cols):\n",
    "        for i in range(rows):\n",
    "            total += matrix[i, j]\n",
    "    \n",
    "    return total\n",
    "\n",
    "\n",
    "# Benchmark cache effects\n",
    "print(\"=\"*70)\n",
    "print(\"CACHE OPTIMIZATION: Memory Access Patterns\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_matrix = np.random.rand(2000, 2000)\n",
    "print(f\"\\nTest matrix: {test_matrix.shape} ({test_matrix.nbytes/1e6:.2f} MB)\")\n",
    "\n",
    "# Row-major access (cache-friendly)\n",
    "times_row = []\n",
    "for _ in range(5):\n",
    "    start = time.perf_counter()\n",
    "    result_row = matrix_sum_row_major(test_matrix)\n",
    "    times_row.append(time.perf_counter() - start)\n",
    "time_row = np.mean(times_row)\n",
    "\n",
    "print(f\"\\nRow-major access (cache-friendly):\")\n",
    "print(f\"  Time: {time_row:.4f} seconds\")\n",
    "print(f\"  Result: {result_row:.2f}\")\n",
    "\n",
    "# Column-major access (cache-unfriendly)\n",
    "times_col = []\n",
    "for _ in range(5):\n",
    "    start = time.perf_counter()\n",
    "    result_col = matrix_sum_column_major(test_matrix)\n",
    "    times_col.append(time.perf_counter() - start)\n",
    "time_col = np.mean(times_col)\n",
    "\n",
    "print(f\"\\nColumn-major access (cache-unfriendly):\")\n",
    "print(f\"  Time: {time_col:.4f} seconds\")\n",
    "print(f\"  Result: {result_col:.2f}\")\n",
    "\n",
    "# Compare\n",
    "slowdown = time_col / time_row\n",
    "print(f\"\\nColumn-major is {slowdown:.2f}x SLOWER!\")\n",
    "print(f\"\\nWhy? Cache misses!\")\n",
    "print(f\"  Row-major: Sequential access → data prefetched into cache\")\n",
    "print(f\"  Column-major: Strided access → cache line wasted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Cache-Optimized Matrix Transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose_naive(matrix):\n",
    "    \"\"\"Naive transpose - poor cache usage.\"\"\"\n",
    "    rows, cols = matrix.shape\n",
    "    result = np.zeros((cols, rows), dtype=matrix.dtype)\n",
    "    \n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            result[j, i] = matrix[i, j]  # Column-major write!\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def transpose_tiled(matrix, tile_size=32):\n",
    "    \"\"\"\n",
    "    Cache-optimized transpose using tiling.\n",
    "    \n",
    "    Process matrix in tiles that fit in cache.\n",
    "    Within each tile, both reads and writes are cache-friendly.\n",
    "    \"\"\"\n",
    "    rows, cols = matrix.shape\n",
    "    result = np.zeros((cols, rows), dtype=matrix.dtype)\n",
    "    \n",
    "    # Process in tiles\n",
    "    for i in range(0, rows, tile_size):\n",
    "        for j in range(0, cols, tile_size):\n",
    "            # Get tile boundaries\n",
    "            i_end = min(i + tile_size, rows)\n",
    "            j_end = min(j + tile_size, cols)\n",
    "            \n",
    "            # Transpose this tile\n",
    "            for ii in range(i, i_end):\n",
    "                for jj in range(j, j_end):\n",
    "                    result[jj, ii] = matrix[ii, jj]\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# Benchmark transpose methods\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TILED TRANSPOSE FOR CACHE OPTIMIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_size = 2048\n",
    "test_matrix = np.random.rand(test_size, test_size).astype(np.float32)\n",
    "\n",
    "# Naive\n",
    "start = time.time()\n",
    "result_naive = transpose_naive(test_matrix)\n",
    "time_naive = time.time() - start\n",
    "\n",
    "# Tiled\n",
    "start = time.time()\n",
    "result_tiled = transpose_tiled(test_matrix, tile_size=32)\n",
    "time_tiled = time.time() - start\n",
    "\n",
    "# NumPy (highly optimized)\n",
    "start = time.time()\n",
    "result_numpy = test_matrix.T.copy()\n",
    "time_numpy = time.time() - start\n",
    "\n",
    "print(f\"Matrix: {test_size}x{test_size}\\n\")\n",
    "print(f\"Naive transpose:  {time_naive:.4f} seconds\")\n",
    "print(f\"Tiled transpose:  {time_tiled:.4f} seconds ({time_naive/time_tiled:.2f}x faster)\")\n",
    "print(f\"NumPy transpose:  {time_numpy:.4f} seconds ({time_naive/time_numpy:.2f}x faster)\")\n",
    "\n",
    "# Verify correctness\n",
    "assert np.allclose(result_naive, result_tiled)\n",
    "assert np.allclose(result_naive, result_numpy)\n",
    "print(\"\\n✓ All results match!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Balancing: Dynamic Work Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_workload(n):\n",
    "    \"\"\"\n",
    "    Simulate task with variable computation time.\n",
    "    \n",
    "    Task n takes time proportional to n.\n",
    "    This creates load imbalance if statically distributed.\n",
    "    \"\"\"\n",
    "    # Simulate work (compute-bound)\n",
    "    result = 0\n",
    "    for i in range(n * 100000):\n",
    "        result += np.sqrt(i + 1)\n",
    "    return result\n",
    "\n",
    "\n",
    "def static_load_balancing(tasks, num_workers):\n",
    "    \"\"\"\n",
    "    Static partitioning: Divide tasks equally among workers.\n",
    "    \n",
    "    Problem: If tasks have variable time, some workers finish early.\n",
    "    \"\"\"\n",
    "    chunk_size = len(tasks) // num_workers\n",
    "    \n",
    "    with Pool(processes=num_workers) as pool:\n",
    "        # Map assigns fixed chunks to each worker\n",
    "        results = pool.map(variable_workload, tasks)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def dynamic_load_balancing(tasks, num_workers):\n",
    "    \"\"\"\n",
    "    Dynamic scheduling: Workers grab tasks as they become available.\n",
    "    \n",
    "    Uses chunksize=1 so workers get one task at a time.\n",
    "    No idle time - workers always busy!\n",
    "    \"\"\"\n",
    "    with Pool(processes=num_workers) as pool:\n",
    "        # chunksize=1 enables dynamic scheduling\n",
    "        results = pool.map(variable_workload, tasks, chunksize=1)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Benchmark load balancing strategies\n",
    "print(\"=\"*70)\n",
    "print(\"LOAD BALANCING: Static vs Dynamic\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create tasks with variable workload\n",
    "# Tasks 1, 2, 3, ..., 20 (increasing difficulty)\n",
    "tasks = list(range(1, 21))\n",
    "num_workers = 4\n",
    "\n",
    "print(f\"\\nTasks: {tasks}\")\n",
    "print(f\"Workers: {num_workers}\")\n",
    "print(f\"Note: Task difficulty increases linearly\\n\")\n",
    "\n",
    "# Static partitioning\n",
    "print(\"Static partitioning...\")\n",
    "start = time.time()\n",
    "results_static = static_load_balancing(tasks, num_workers)\n",
    "time_static = time.time() - start\n",
    "print(f\"  Time: {time_static:.3f} seconds\")\n",
    "print(f\"  Problem: Worker 4 gets tasks [16,17,18,19,20] - takes longest!\")\n",
    "\n",
    "# Dynamic scheduling\n",
    "print(\"\\nDynamic scheduling...\")\n",
    "start = time.time()\n",
    "results_dynamic = dynamic_load_balancing(tasks, num_workers)\n",
    "time_dynamic = time.time() - start\n",
    "print(f\"  Time: {time_dynamic:.3f} seconds\")\n",
    "print(f\"  Benefit: Workers grab tasks as available - no idle time!\")\n",
    "\n",
    "# Compare\n",
    "speedup = time_static / time_dynamic\n",
    "print(f\"\\nDynamic is {speedup:.2f}x faster!\")\n",
    "print(f\"Savings: {time_static - time_dynamic:.3f} seconds ({(1-1/speedup)*100:.1f}% reduction)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Reducing Communication Overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_small_chunks(data, num_workers):\n",
    "    \"\"\"\n",
    "    Process data in many small chunks.\n",
    "    \n",
    "    High communication overhead: data serialization/deserialization\n",
    "    happens many times.\n",
    "    \"\"\"\n",
    "    chunk_size = 1000  # Small chunks\n",
    "    chunks = [data[i:i+chunk_size] for i in range(0, len(data), chunk_size)]\n",
    "    \n",
    "    def worker_func(chunk):\n",
    "        return np.sum(chunk ** 2)  # Simple computation\n",
    "    \n",
    "    with Pool(processes=num_workers) as pool:\n",
    "        results = pool.map(worker_func, chunks)\n",
    "    \n",
    "    return sum(results)\n",
    "\n",
    "\n",
    "def process_large_chunks(data, num_workers):\n",
    "    \"\"\"\n",
    "    Process data in fewer large chunks.\n",
    "    \n",
    "    Lower communication overhead: data transferred fewer times.\n",
    "    Better computation/communication ratio.\n",
    "    \"\"\"\n",
    "    chunk_size = len(data) // num_workers  # Large chunks\n",
    "    chunks = [data[i:i+chunk_size] for i in range(0, len(data), chunk_size)]\n",
    "    \n",
    "    def worker_func(chunk):\n",
    "        return np.sum(chunk ** 2)\n",
    "    \n",
    "    with Pool(processes=num_workers) as pool:\n",
    "        results = pool.map(worker_func, chunks)\n",
    "    \n",
    "    return sum(results)\n",
    "\n",
    "\n",
    "# Benchmark communication overhead\n",
    "print(\"=\"*70)\n",
    "print(\"COMMUNICATION OVERHEAD: Chunk Size Impact\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "data = np.random.rand(10_000_000)  # 10M elements\n",
    "num_workers = 4\n",
    "\n",
    "print(f\"\\nData size: {len(data):,} elements ({data.nbytes/1e6:.2f} MB)\")\n",
    "print(f\"Workers: {num_workers}\\n\")\n",
    "\n",
    "# Small chunks (high overhead)\n",
    "print(\"Small chunks (1000 elements each)...\")\n",
    "start = time.time()\n",
    "result_small = process_small_chunks(data, num_workers)\n",
    "time_small = time.time() - start\n",
    "num_chunks_small = len(data) // 1000\n",
    "print(f\"  Chunks: {num_chunks_small}\")\n",
    "print(f\"  Time: {time_small:.4f} seconds\")\n",
    "\n",
    "# Large chunks (low overhead)\n",
    "print(f\"\\nLarge chunks ({len(data)//num_workers:,} elements each)...\")\n",
    "start = time.time()\n",
    "result_large = process_large_chunks(data, num_workers)\n",
    "time_large = time.time() - start\n",
    "print(f\"  Chunks: {num_workers}\")\n",
    "print(f\"  Time: {time_large:.4f} seconds\")\n",
    "\n",
    "# Serial for comparison\n",
    "print(\"\\nSerial (no communication)...\")\n",
    "start = time.time()\n",
    "result_serial = np.sum(data ** 2)\n",
    "time_serial = time.time() - start\n",
    "print(f\"  Time: {time_serial:.4f} seconds\")\n",
    "\n",
    "# Analysis\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Large chunks are {time_small/time_large:.2f}x faster than small chunks\")\n",
    "print(f\"\\nWhy?\")\n",
    "print(f\"  - Fewer serialization/deserialization operations\")\n",
    "print(f\"  - Better computation-to-communication ratio\")\n",
    "print(f\"  - Less overhead from process coordination\")\n",
    "print(f\"\\nRule of thumb: Each chunk should take >10ms to process\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Vectorization with NumPy/Numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scalar (slow)\n",
    "def polynomial_scalar(x, coefficients):\n",
    "    \"\"\"\n",
    "    Evaluate polynomial using loops (slow).\n",
    "    \n",
    "    P(x) = c0 + c1*x + c2*x² + c3*x³ + ...\n",
    "    \"\"\"\n",
    "    result = np.zeros_like(x)\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        for j, coef in enumerate(coefficients):\n",
    "            result[i] += coef * (x[i] ** j)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# Vectorized (fast)\n",
    "def polynomial_vectorized(x, coefficients):\n",
    "    \"\"\"\n",
    "    Evaluate polynomial using NumPy vectorization.\n",
    "    \n",
    "    Uses broadcasting and optimized C loops.\n",
    "    \"\"\"\n",
    "    result = np.zeros_like(x)\n",
    "    \n",
    "    for j, coef in enumerate(coefficients):\n",
    "        result += coef * (x ** j)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "if NUMBA_AVAILABLE:\n",
    "    @jit(nopython=True)\n",
    "    def polynomial_numba(x, coefficients):\n",
    "        \"\"\"\n",
    "        Numba JIT-compiled version.\n",
    "        \n",
    "        Compiles to machine code - no Python overhead.\n",
    "        \"\"\"\n",
    "        result = np.zeros_like(x)\n",
    "        \n",
    "        for i in range(len(x)):\n",
    "            for j, coef in enumerate(coefficients):\n",
    "                result[i] += coef * (x[i] ** j)\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "# Benchmark vectorization\n",
    "print(\"=\"*70)\n",
    "print(\"VECTORIZATION: Scalar vs Vectorized vs Numba\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "x = np.linspace(0, 10, 1_000_000)\n",
    "coefficients = [1.0, 2.0, -0.5, 0.1, -0.01]  # 4th degree polynomial\n",
    "\n",
    "print(f\"\\nInput size: {len(x):,} points\")\n",
    "print(f\"Polynomial degree: {len(coefficients)-1}\\n\")\n",
    "\n",
    "# Scalar version\n",
    "print(\"Scalar (Python loops)...\")\n",
    "start = time.time()\n",
    "result_scalar = polynomial_scalar(x, coefficients)\n",
    "time_scalar = time.time() - start\n",
    "print(f\"  Time: {time_scalar:.4f} seconds\")\n",
    "\n",
    "# Vectorized version\n",
    "print(\"\\nVectorized (NumPy)...\")\n",
    "start = time.time()\n",
    "result_vectorized = polynomial_vectorized(x, coefficients)\n",
    "time_vectorized = time.time() - start\n",
    "print(f\"  Time: {time_vectorized:.4f} seconds\")\n",
    "print(f\"  Speedup: {time_scalar/time_vectorized:.2f}x\")\n",
    "\n",
    "# Numba version\n",
    "if NUMBA_AVAILABLE:\n",
    "    print(\"\\nNumba JIT (warmup)...\")\n",
    "    _ = polynomial_numba(x[:100], coefficients)  # Warmup\n",
    "    \n",
    "    print(\"Numba JIT (compiled)...\")\n",
    "    start = time.time()\n",
    "    result_numba = polynomial_numba(x, coefficients)\n",
    "    time_numba = time.time() - start\n",
    "    print(f\"  Time: {time_numba:.4f} seconds\")\n",
    "    print(f\"  Speedup: {time_scalar/time_numba:.2f}x\")\n",
    "\n",
    "# Verify correctness\n",
    "assert np.allclose(result_scalar, result_vectorized)\n",
    "if NUMBA_AVAILABLE:\n",
    "    assert np.allclose(result_scalar, result_numba)\n",
    "print(\"\\n✓ All results match!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hybrid CPU-GPU Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"HYBRID CPU-GPU APPROACH\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "Strategy: Use both CPU and GPU simultaneously!\n",
    "\n",
    "1. DIVIDE WORK:\n",
    "   - 70% of data → GPU (faster for large batches)\n",
    "   - 30% of data → CPU (parallel with multiprocessing)\n",
    "   - Overlap computation: GPU and CPU work in parallel\n",
    "\n",
    "2. WHEN TO USE:\n",
    "   - GPU transfer overhead is significant\n",
    "   - Problem is large enough for both\n",
    "   - CPU has idle cycles while waiting for GPU\n",
    "\n",
    "3. IMPLEMENTATION:\n",
    "   - Use threading to launch GPU and CPU work simultaneously\n",
    "   - GPU works on large chunk (high throughput)\n",
    "   - CPU works on smaller chunk (low latency)\n",
    "   - Combine results at the end\n",
    "\n",
    "4. EXPECTED SPEEDUP:\n",
    "   - Better than GPU-only (uses idle CPU)\n",
    "   - Better than CPU-only (GPU handles bulk)\n",
    "   - Typically 1.2-1.5x improvement over best single approach\n",
    "\"\"\")\n",
    "\n",
    "if GPU_AVAILABLE:\n",
    "    print(\"\\nGPU available - hybrid approach is viable!\")\n",
    "    print(\"Recommended split:\")\n",
    "    print(f\"  GPU: 70% of work (maximize GPU utilization)\")\n",
    "    print(f\"  CPU: 30% of work ({num_cores} cores for parallelization)\")\n",
    "else:\n",
    "    print(\"\\nGPU not available - hybrid approach not demonstrated.\")\n",
    "    print(\"With GPU, you could achieve:\")\n",
    "    print(\"  - 5-10x from GPU on 70% of work\")\n",
    "    print(\"  - 2-4x from CPU parallel on 30% of work\")\n",
    "    print(\"  - Total: ~6-12x speedup (better than either alone)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Iterative Optimization Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ITERATIVE OPTIMIZATION METHODOLOGY\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "Step-by-step process for optimizing parallel programs:\n",
    "\n",
    "STEP 1: PROFILE & MEASURE\n",
    "  ✓ Identify bottleneck (CPU? Memory? Communication?)\n",
    "  ✓ Measure baseline performance\n",
    "  ✓ Use profiling tools (cProfile, line_profiler, NVIDIA Nsight)\n",
    "\n",
    "STEP 2: CHOOSE OPTIMIZATION TARGET\n",
    "  Focus on ONE thing at a time:\n",
    "  □ Cache optimization (memory access patterns)\n",
    "  □ Load balancing (work distribution)\n",
    "  □ Communication reduction (chunk sizing)\n",
    "  □ Vectorization (SIMD operations)\n",
    "  □ Algorithm improvement (O(n²) → O(n log n))\n",
    "\n",
    "STEP 3: IMPLEMENT CHANGE\n",
    "  ✓ Make ONE change at a time\n",
    "  ✓ Keep code version controlled (git)\n",
    "  ✓ Document what you changed and why\n",
    "\n",
    "STEP 4: MEASURE IMPACT\n",
    "  ✓ Run benchmarks (multiple iterations)\n",
    "  ✓ Calculate speedup\n",
    "  ✓ Check correctness (results still accurate?)\n",
    "\n",
    "STEP 5: DECIDE\n",
    "  ✓ If faster: Keep change, goto STEP 1\n",
    "  ✓ If slower: Revert change, try different approach\n",
    "  ✓ If no more bottlenecks: Done!\n",
    "\n",
    "GOLDEN RULES:\n",
    "  1. Measure, don't guess\n",
    "  2. Optimize the bottleneck, not everything\n",
    "  3. One change at a time\n",
    "  4. Document your progress\n",
    "  5. Know when to stop (diminishing returns)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Case Study: Image Processing Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_image(size=(1024, 1024)):\n",
    "    \"\"\"Create test image.\"\"\"\n",
    "    height, width = size\n",
    "    image = np.random.randint(0, 256, size=(height, width), dtype=np.uint8)\n",
    "    return image\n",
    "\n",
    "\n",
    "def gaussian_kernel(size=5, sigma=1.0):\n",
    "    \"\"\"Create Gaussian kernel.\"\"\"\n",
    "    if size % 2 == 0:\n",
    "        size += 1\n",
    "    ax = np.arange(-size // 2 + 1, size // 2 + 1)\n",
    "    xx, yy = np.meshgrid(ax, ax)\n",
    "    kernel = np.exp(-(xx**2 + yy**2) / (2 * sigma**2))\n",
    "    return kernel / np.sum(kernel)\n",
    "\n",
    "\n",
    "# Version 1: Baseline (naive loops)\n",
    "def blur_v1_naive(image, kernel):\n",
    "    \"\"\"Version 1: Naive nested loops.\"\"\"\n",
    "    height, width = image.shape\n",
    "    k_size = kernel.shape[0]\n",
    "    k_half = k_size // 2\n",
    "    \n",
    "    padded = np.pad(image, k_half, mode='edge')\n",
    "    result = np.zeros_like(image, dtype=np.float32)\n",
    "    \n",
    "    for i in range(height):\n",
    "        for j in range(width):\n",
    "            for ki in range(k_size):\n",
    "                for kj in range(k_size):\n",
    "                    result[i, j] += padded[i+ki, j+kj] * kernel[ki, kj]\n",
    "    \n",
    "    return result.astype(np.uint8)\n",
    "\n",
    "\n",
    "# Version 2: Optimized inner loops (NumPy slicing)\n",
    "def blur_v2_vectorized(image, kernel):\n",
    "    \"\"\"Version 2: Vectorized inner loop.\"\"\"\n",
    "    height, width = image.shape\n",
    "    k_size = kernel.shape[0]\n",
    "    k_half = k_size // 2\n",
    "    \n",
    "    padded = np.pad(image, k_half, mode='edge')\n",
    "    result = np.zeros_like(image, dtype=np.float32)\n",
    "    \n",
    "    for i in range(height):\n",
    "        for j in range(width):\n",
    "            region = padded[i:i+k_size, j:j+k_size]\n",
    "            result[i, j] = np.sum(region * kernel)\n",
    "    \n",
    "    return result.astype(np.uint8)\n",
    "\n",
    "\n",
    "# Version 3: Full vectorization (scipy)\n",
    "def blur_v3_scipy(image, kernel):\n",
    "    \"\"\"Version 3: Use scipy's optimized convolution.\"\"\"\n",
    "    from scipy.ndimage import convolve\n",
    "    return convolve(image.astype(np.float32), kernel, mode='nearest').astype(np.uint8)\n",
    "\n",
    "\n",
    "# Benchmark all versions\n",
    "print(\"=\"*70)\n",
    "print(\"CASE STUDY: Progressive Image Processing Optimization\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "image = create_test_image((512, 512))\n",
    "kernel = gaussian_kernel(size=5, sigma=1.5)\n",
    "\n",
    "print(f\"\\nImage: {image.shape}\")\n",
    "print(f\"Kernel: {kernel.shape}\\n\")\n",
    "\n",
    "versions = [\n",
    "    (\"V1: Naive loops\", blur_v1_naive),\n",
    "    (\"V2: Vectorized inner\", blur_v2_vectorized),\n",
    "    (\"V3: Scipy (optimized)\", blur_v3_scipy)\n",
    "]\n",
    "\n",
    "results = []\n",
    "baseline_time = None\n",
    "\n",
    "for name, func in versions:\n",
    "    print(f\"{name}...\")\n",
    "    \n",
    "    # Warmup\n",
    "    _ = func(image[:100, :100], kernel)\n",
    "    \n",
    "    # Time\n",
    "    times = []\n",
    "    for _ in range(3):\n",
    "        start = time.time()\n",
    "        result = func(image, kernel)\n",
    "        times.append(time.time() - start)\n",
    "    \n",
    "    mean_time = np.mean(times)\n",
    "    \n",
    "    if baseline_time is None:\n",
    "        baseline_time = mean_time\n",
    "        speedup = 1.0\n",
    "    else:\n",
    "        speedup = baseline_time / mean_time\n",
    "    \n",
    "    print(f\"  Time: {mean_time:.4f} seconds\")\n",
    "    print(f\"  Speedup: {speedup:.2f}x\\n\")\n",
    "    \n",
    "    results.append({\n",
    "        'Version': name,\n",
    "        'Time (s)': mean_time,\n",
    "        'Speedup': speedup\n",
    "    })\n",
    "\n",
    "# Summary\n",
    "df_results = pd.DataFrame(results)\n",
    "print(\"=\"*70)\n",
    "print(\"OPTIMIZATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(df_results.to_string(index=False))\n",
    "print(f\"\\nTotal improvement: {df_results['Speedup'].max():.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize optimization progress\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Execution time\n",
    "plt.subplot(121)\n",
    "versions_names = df_results['Version'].values\n",
    "times = df_results['Time (s)'].values\n",
    "colors = ['red', 'orange', 'green']\n",
    "\n",
    "bars = plt.bar(range(len(versions_names)), times, color=colors, alpha=0.7, edgecolor='black')\n",
    "plt.xticks(range(len(versions_names)), versions_names, rotation=15, ha='right')\n",
    "plt.ylabel('Execution Time (seconds)', fontsize=12)\n",
    "plt.title('Optimization Progress: Execution Time', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (bar, time_val) in enumerate(zip(bars, times)):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "            f'{time_val:.3f}s', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Plot 2: Speedup\n",
    "plt.subplot(122)\n",
    "speedups = df_results['Speedup'].values\n",
    "\n",
    "plt.plot(range(len(versions_names)), speedups, 'o-', linewidth=3, markersize=12, color='green')\n",
    "plt.xticks(range(len(versions_names)), versions_names, rotation=15, ha='right')\n",
    "plt.ylabel('Speedup vs Baseline', fontsize=12)\n",
    "plt.title('Cumulative Speedup', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=1, linestyle='--', color='gray', linewidth=1, label='Baseline')\n",
    "\n",
    "# Add value labels\n",
    "for i, speedup in enumerate(speedups):\n",
    "    plt.text(i, speedup + 0.5, f'{speedup:.1f}x', ha='center', va='bottom', \n",
    "            fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('optimization_progress.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Optimization progress plot saved to: optimization_progress.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Performance Optimization Checklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"PERFORMANCE OPTIMIZATION CHECKLIST\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "Before starting optimization:\n",
    "  □ Profile to find bottleneck\n",
    "  □ Measure baseline performance\n",
    "  □ Set performance target\n",
    "  □ Ensure correctness tests exist\n",
    "\n",
    "Algorithm level:\n",
    "  □ Use optimal algorithm (O(n log n) vs O(n²))\n",
    "  □ Avoid redundant computation\n",
    "  □ Cache intermediate results\n",
    "  □ Consider approximation algorithms\n",
    "\n",
    "Memory optimization:\n",
    "  □ Optimize memory access patterns (cache-friendly)\n",
    "  □ Use appropriate data structures\n",
    "  □ Minimize memory allocations\n",
    "  □ Reuse buffers when possible\n",
    "  □ Consider memory layout (row vs column major)\n",
    "\n",
    "Parallelization:\n",
    "  □ Identify parallelizable sections\n",
    "  □ Choose right parallel approach (CPU/GPU/hybrid)\n",
    "  □ Minimize synchronization\n",
    "  □ Balance load across workers\n",
    "  □ Use appropriate chunk sizes\n",
    "\n",
    "CPU optimization:\n",
    "  □ Vectorize operations (NumPy, SIMD)\n",
    "  □ Use JIT compilation (Numba)\n",
    "  □ Optimize loop structures\n",
    "  □ Minimize branch mispredictions\n",
    "  □ Use multi-threading/processing\n",
    "\n",
    "GPU optimization:\n",
    "  □ Minimize CPU-GPU transfers\n",
    "  □ Use shared memory\n",
    "  □ Coalesce memory access\n",
    "  □ Maximize occupancy\n",
    "  □ Avoid branch divergence\n",
    "\n",
    "Communication:\n",
    "  □ Reduce data transfer volume\n",
    "  □ Overlap communication and computation\n",
    "  □ Use large chunks (amortize overhead)\n",
    "  □ Consider compression\n",
    "\n",
    "After optimization:\n",
    "  □ Verify correctness\n",
    "  □ Measure performance gain\n",
    "  □ Check scalability\n",
    "  □ Document changes\n",
    "  □ Consider maintainability\n",
    "\n",
    "When to stop:\n",
    "  ✓ Reached performance target\n",
    "  ✓ Diminishing returns (<5% gain)\n",
    "  ✓ Code becoming unmaintainable\n",
    "  ✓ Hit hardware limits\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"OPTIMIZATION TECHNIQUES SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "Techniques covered in this module:\n",
    "\n",
    "1. CACHE OPTIMIZATION\n",
    "   Impact: 2-10x speedup\n",
    "   When: Memory-bound applications\n",
    "   How: Row-major access, tiling, data locality\n",
    "\n",
    "2. LOAD BALANCING\n",
    "   Impact: 1.5-3x speedup\n",
    "   When: Variable workload per task\n",
    "   How: Dynamic scheduling, work stealing\n",
    "\n",
    "3. COMMUNICATION REDUCTION\n",
    "   Impact: 2-5x speedup\n",
    "   When: Communication-bound parallel code\n",
    "   How: Large chunks, batching, minimize transfers\n",
    "\n",
    "4. VECTORIZATION\n",
    "   Impact: 5-50x speedup\n",
    "   When: Elementwise operations\n",
    "   How: NumPy operations, SIMD, Numba\n",
    "\n",
    "5. HYBRID CPU-GPU\n",
    "   Impact: 1.2-1.5x over best single approach\n",
    "   When: Large problems with both resources\n",
    "   How: Split work 70-30, overlap execution\n",
    "\n",
    "Recommended optimization order:\n",
    "  1st: Choose right algorithm (biggest impact)\n",
    "  2nd: Vectorize (often easy, big wins)\n",
    "  3rd: Parallelize (CPU cores or GPU)\n",
    "  4th: Cache optimization (if still memory-bound)\n",
    "  5th: Load balancing (if scaling poorly)\n",
    "  6th: Communication reduction (if overhead high)\n",
    "  7th: Hybrid approaches (squeeze last bits)\n",
    "\n",
    "Tools for optimization:\n",
    "  Profiling: cProfile, line_profiler, py-spy\n",
    "  CPU: NumPy, Numba, multiprocessing\n",
    "  GPU: Numba CUDA, CuPy, PyTorch\n",
    "  Memory: memory_profiler, tracemalloc\n",
    "  Benchmarking: timeit, pytest-benchmark\n",
    "\n",
    "Remember:\n",
    "  - Premature optimization is the root of all evil\n",
    "  - Measure, don't guess\n",
    "  - Optimize the bottleneck\n",
    "  - Keep it maintainable\n",
    "  - Know when to stop\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONGRATULATIONS!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "You've completed the parallel processing assignment modules!\n",
    "\n",
    "You now know:\n",
    "  ✓ How to implement serial baselines (Module 05)\n",
    "  ✓ Multi-core CPU parallelization (Module 06)\n",
    "  ✓ GPU acceleration with CUDA (Module 07)\n",
    "  ✓ Performance benchmarking (Module 08)\n",
    "  ✓ Advanced optimization techniques (Module 09)\n",
    "\n",
    "Next steps:\n",
    "  1. Apply these techniques to YOUR assignment problem\n",
    "  2. Create your own benchmarks\n",
    "  3. Write your performance analysis report\n",
    "  4. Compare serial, parallel, and GPU versions\n",
    "\n",
    "Good luck with your assignment!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this module, you learned:\n",
    "\n",
    "1. **Cache Optimization**\n",
    "   - Row-major vs column-major access\n",
    "   - Tiling for better locality\n",
    "   - Impact on performance (2-10x)\n",
    "\n",
    "2. **Load Balancing**\n",
    "   - Static vs dynamic scheduling\n",
    "   - Handling variable workloads\n",
    "   - Minimizing idle time\n",
    "\n",
    "3. **Communication Optimization**\n",
    "   - Chunk sizing strategies\n",
    "   - Reducing overhead\n",
    "   - Computation/communication ratio\n",
    "\n",
    "4. **Vectorization**\n",
    "   - NumPy broadcasting\n",
    "   - Numba JIT compilation\n",
    "   - SIMD operations\n",
    "\n",
    "5. **Hybrid Approaches**\n",
    "   - CPU-GPU work splitting\n",
    "   - Overlapping execution\n",
    "   - Maximizing utilization\n",
    "\n",
    "6. **Iterative Process**\n",
    "   - Profile → Optimize → Measure\n",
    "   - One change at a time\n",
    "   - Know when to stop\n",
    "\n",
    "**You're now ready to optimize your own parallel programs!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
