{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 07: GPU Acceleration with CUDA\n",
    "\n",
    "**Difficulty**: ⭐⭐⭐\n",
    "\n",
    "**Estimated Time**: 90 minutes\n",
    "\n",
    "**Prerequisites**: \n",
    "- Module 05: Serial Implementation\n",
    "- Module 06: Multi-Core Parallelization\n",
    "- Basic understanding of GPU architecture\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Understand GPU architecture and CUDA execution model\n",
    "2. Write CUDA kernels using Numba in Python\n",
    "3. Manage GPU memory (global, shared, constant)\n",
    "4. Optimize GPU code for maximum throughput\n",
    "5. Compare CPU vs GPU performance on various problems\n",
    "6. Identify when GPU acceleration is beneficial\n",
    "\n",
    "## Note on GPU Availability\n",
    "\n",
    "This notebook is designed to work whether you have a GPU or not:\n",
    "- **With GPU**: Full CUDA acceleration\n",
    "- **Without GPU**: Code examples and explanations (won't execute kernels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and GPU Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from typing import Tuple\n",
    "\n",
    "# Try to import CUDA support\n",
    "GPU_AVAILABLE = False\n",
    "try:\n",
    "    from numba import cuda\n",
    "    import numba\n",
    "    \n",
    "    # Check if GPU is actually available\n",
    "    if cuda.is_available():\n",
    "        GPU_AVAILABLE = True\n",
    "        print(\"GPU DETECTED!\")\n",
    "        print(f\"Numba version: {numba.__version__}\")\n",
    "        print(f\"\\nGPU Information:\")\n",
    "        gpu = cuda.get_current_device()\n",
    "        print(f\"  Name: {gpu.name.decode('utf-8')}\")\n",
    "        print(f\"  Compute Capability: {gpu.compute_capability}\")\n",
    "        print(f\"  Total Memory: {gpu.total_memory / 1e9:.2f} GB\")\n",
    "        print(f\"  Multiprocessors: {gpu.MULTIPROCESSOR_COUNT}\")\n",
    "        print(f\"  Max Threads per Block: {gpu.MAX_THREADS_PER_BLOCK}\")\n",
    "        print(f\"  Max Block Dimensions: {gpu.MAX_BLOCK_DIM_X} x {gpu.MAX_BLOCK_DIM_Y} x {gpu.MAX_BLOCK_DIM_Z}\")\n",
    "        print(f\"  Max Grid Dimensions: {gpu.MAX_GRID_DIM_X} x {gpu.MAX_GRID_DIM_Y} x {gpu.MAX_GRID_DIM_Z}\")\n",
    "    else:\n",
    "        print(\"Numba installed but no GPU detected.\")\n",
    "        print(\"This notebook will show code examples but won't execute GPU kernels.\")\n",
    "except ImportError:\n",
    "    print(\"Numba with CUDA support not installed.\")\n",
    "    print(\"To install: conda install numba cudatoolkit\")\n",
    "    print(\"or: pip install numba (if you have CUDA toolkit installed)\")\n",
    "    print(\"\\nThis notebook will show code examples and concepts.\")\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GPU Architecture Overview\n",
    "\n",
    "### CUDA Execution Model\n",
    "\n",
    "```\n",
    "CPU (Host)                    GPU (Device)\n",
    "┌─────────────┐              ┌──────────────────────────────┐\n",
    "│  Main Code  │              │  Grid                        │\n",
    "│             │              │  ┌────────────────────────┐  │\n",
    "│  Launch ────┼──────────────┼─>│  Block 0  Block 1  ... │  │\n",
    "│  Kernel     │              │  │  ┌─────┐  ┌─────┐      │  │\n",
    "│             │              │  │  │T T T│  │T T T│      │  │\n",
    "│  Wait  <────┼──────────────┼──│  │T T T│  │T T T│      │  │\n",
    "│             │              │  │  │T T T│  │T T T│      │  │\n",
    "└─────────────┘              │  │  └─────┘  └─────┘      │  │\n",
    "                             │  └────────────────────────┘  │\n",
    "                             └──────────────────────────────┘\n",
    "                             T = Thread\n",
    "```\n",
    "\n",
    "### Memory Hierarchy\n",
    "\n",
    "| Memory Type | Scope | Speed | Size | Latency |\n",
    "|-------------|-------|-------|------|----------|\n",
    "| **Registers** | Thread | Fastest | ~64KB | 1 cycle |\n",
    "| **Shared Memory** | Block | Very Fast | ~48KB | ~5 cycles |\n",
    "| **L1/L2 Cache** | Device | Fast | ~MB | ~50 cycles |\n",
    "| **Global Memory** | Device | Slow | GB | ~400 cycles |\n",
    "| **Constant Memory** | Device | Fast* | 64KB | ~5 cycles* |\n",
    "\n",
    "*if cached\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Grid**: Collection of blocks\n",
    "2. **Block**: Collection of threads (up to 1024 threads)\n",
    "3. **Thread**: Single execution unit\n",
    "4. **Warp**: Group of 32 threads executing together (SIMT)\n",
    "5. **Kernel**: Function that runs on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Your First CUDA Kernel: Vector Addition\n",
    "\n",
    "Let's start with a simple example to understand CUDA programming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU version (baseline)\n",
    "def vector_add_cpu(a, b, c):\n",
    "    \"\"\"\n",
    "    Add two vectors on CPU.\n",
    "    \n",
    "    Args:\n",
    "        a, b: Input vectors\n",
    "        c: Output vector (c = a + b)\n",
    "    \"\"\"\n",
    "    for i in range(a.size):\n",
    "        c[i] = a[i] + b[i]\n",
    "\n",
    "\n",
    "if GPU_AVAILABLE:\n",
    "    @cuda.jit\n",
    "    def vector_add_gpu(a, b, c):\n",
    "        \"\"\"\n",
    "        Add two vectors on GPU.\n",
    "        \n",
    "        Each thread computes one element of the result.\n",
    "        \n",
    "        CUDA Built-in Variables:\n",
    "        - cuda.threadIdx.x: Thread index within block\n",
    "        - cuda.blockIdx.x:  Block index within grid\n",
    "        - cuda.blockDim.x:  Number of threads per block\n",
    "        \"\"\"\n",
    "        # Calculate global thread index\n",
    "        idx = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
    "        \n",
    "        # Boundary check (in case we have more threads than elements)\n",
    "        if idx < c.size:\n",
    "            c[idx] = a[idx] + b[idx]\n",
    "    \n",
    "    print(\"GPU kernel defined!\")\n",
    "else:\n",
    "    print(\"GPU not available - showing kernel code for reference:\")\n",
    "    print(\"\"\"\n",
    "    @cuda.jit\n",
    "    def vector_add_gpu(a, b, c):\n",
    "        idx = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
    "        if idx < c.size:\n",
    "            c[idx] = a[idx] + b[idx]\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GPU_AVAILABLE:\n",
    "    # Test vector addition\n",
    "    n = 1_000_000\n",
    "    \n",
    "    # Create test data on CPU\n",
    "    a_cpu = np.random.rand(n).astype(np.float32)\n",
    "    b_cpu = np.random.rand(n).astype(np.float32)\n",
    "    c_cpu = np.zeros(n, dtype=np.float32)\n",
    "    \n",
    "    # CPU execution\n",
    "    print(f\"Testing with {n:,} elements...\\n\")\n",
    "    print(\"CPU version...\")\n",
    "    start = time.time()\n",
    "    vector_add_cpu(a_cpu, b_cpu, c_cpu)\n",
    "    cpu_time = time.time() - start\n",
    "    print(f\"  Time: {cpu_time:.4f} seconds\")\n",
    "    \n",
    "    # GPU execution\n",
    "    print(\"\\nGPU version...\")\n",
    "    \n",
    "    # Step 1: Copy data to GPU\n",
    "    a_gpu = cuda.to_device(a_cpu)\n",
    "    b_gpu = cuda.to_device(b_cpu)\n",
    "    c_gpu = cuda.device_array(n, dtype=np.float32)\n",
    "    \n",
    "    # Step 2: Configure kernel launch parameters\n",
    "    threads_per_block = 256\n",
    "    blocks_per_grid = (n + threads_per_block - 1) // threads_per_block\n",
    "    \n",
    "    print(f\"  Launching kernel with:\")\n",
    "    print(f\"    Threads per block: {threads_per_block}\")\n",
    "    print(f\"    Blocks per grid:   {blocks_per_grid}\")\n",
    "    print(f\"    Total threads:     {threads_per_block * blocks_per_grid:,}\")\n",
    "    \n",
    "    # Step 3: Launch kernel and measure time\n",
    "    start = time.time()\n",
    "    vector_add_gpu[blocks_per_grid, threads_per_block](a_gpu, b_gpu, c_gpu)\n",
    "    cuda.synchronize()  # Wait for GPU to finish\n",
    "    gpu_time = time.time() - start\n",
    "    \n",
    "    # Step 4: Copy result back to CPU\n",
    "    c_gpu_result = c_gpu.copy_to_host()\n",
    "    \n",
    "    print(f\"  Time: {gpu_time:.4f} seconds\")\n",
    "    \n",
    "    # Verify correctness\n",
    "    matches = np.allclose(c_cpu, c_gpu_result)\n",
    "    print(f\"\\nResults match: {matches}\")\n",
    "    \n",
    "    # Calculate speedup\n",
    "    speedup = cpu_time / gpu_time\n",
    "    print(f\"GPU speedup: {speedup:.2f}x\")\n",
    "    \n",
    "    if speedup < 1:\n",
    "        print(\"\\nNote: GPU might be slower for simple operations due to:\")\n",
    "        print(\"  - Memory transfer overhead (CPU ↔ GPU)\")\n",
    "        print(\"  - Kernel launch overhead\")\n",
    "        print(\"  - NumPy's vectorized operations are already optimized\")\n",
    "else:\n",
    "    print(\"GPU not available - skipping benchmark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Image Processing on GPU\n",
    "\n",
    "Image filtering is highly parallelizable - perfect for GPU!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_image(size=(1024, 1024)):\n",
    "    \"\"\"Create test image.\"\"\"\n",
    "    height, width = size\n",
    "    image = np.zeros((height, width), dtype=np.uint8)\n",
    "    \n",
    "    for i in range(height):\n",
    "        image[i, :] = int(255 * i / height)\n",
    "    \n",
    "    image[100:200, 100:300] = 200\n",
    "    \n",
    "    center_y, center_x = height // 2, width // 2\n",
    "    radius = 80\n",
    "    y, x = np.ogrid[:height, :width]\n",
    "    mask = (x - center_x)**2 + (y - center_y)**2 <= radius**2\n",
    "    image[mask] = 150\n",
    "    \n",
    "    noise = np.random.randint(-20, 20, size=(height, width))\n",
    "    image = np.clip(image.astype(np.int16) + noise, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    return image\n",
    "\n",
    "\n",
    "def create_gaussian_kernel(size=5, sigma=1.0):\n",
    "    \"\"\"Create Gaussian kernel.\"\"\"\n",
    "    if size % 2 == 0:\n",
    "        size += 1\n",
    "    ax = np.arange(-size // 2 + 1, size // 2 + 1)\n",
    "    xx, yy = np.meshgrid(ax, ax)\n",
    "    kernel = np.exp(-(xx**2 + yy**2) / (2 * sigma**2))\n",
    "    return kernel / np.sum(kernel)\n",
    "\n",
    "\n",
    "# Create test data\n",
    "test_image = create_test_image((1024, 1024))\n",
    "gaussian_kernel = create_gaussian_kernel(size=5, sigma=1.5).astype(np.float32)\n",
    "\n",
    "print(f\"Test image: {test_image.shape}, dtype={test_image.dtype}\")\n",
    "print(f\"Kernel: {gaussian_kernel.shape}, dtype={gaussian_kernel.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GPU_AVAILABLE:\n",
    "    @cuda.jit\n",
    "    def gaussian_blur_gpu(image, kernel, output):\n",
    "        \"\"\"\n",
    "        Apply Gaussian blur on GPU.\n",
    "        \n",
    "        Each thread processes one output pixel.\n",
    "        Uses 2D grid and block configuration.\n",
    "        \"\"\"\n",
    "        # Get 2D thread coordinates\n",
    "        row = cuda.blockIdx.y * cuda.blockDim.y + cuda.threadIdx.y\n",
    "        col = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
    "        \n",
    "        height, width = image.shape\n",
    "        k_height, k_width = kernel.shape\n",
    "        k_half = k_height // 2\n",
    "        \n",
    "        # Boundary check\n",
    "        if row < height and col < width:\n",
    "            # Apply convolution\n",
    "            value = 0.0\n",
    "            \n",
    "            for ki in range(k_height):\n",
    "                for kj in range(k_width):\n",
    "                    # Image coordinates (with boundary clamping)\n",
    "                    img_row = min(max(row + ki - k_half, 0), height - 1)\n",
    "                    img_col = min(max(col + kj - k_half, 0), width - 1)\n",
    "                    \n",
    "                    value += image[img_row, img_col] * kernel[ki, kj]\n",
    "            \n",
    "            # Clamp and store result\n",
    "            if value < 0:\n",
    "                output[row, col] = 0\n",
    "            elif value > 255:\n",
    "                output[row, col] = 255\n",
    "            else:\n",
    "                output[row, col] = int(value)\n",
    "    \n",
    "    print(\"GPU image filtering kernel defined!\")\n",
    "else:\n",
    "    print(\"GPU not available - kernel code for reference only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GPU_AVAILABLE:\n",
    "    print(\"=\"*70)\n",
    "    print(\"IMAGE FILTERING BENCHMARK: CPU vs GPU\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # CPU baseline (using NumPy vectorization)\n",
    "    def gaussian_blur_cpu(image, kernel):\n",
    "        \"\"\"CPU version using scipy.\"\"\"\n",
    "        from scipy.ndimage import convolve\n",
    "        return convolve(image.astype(np.float32), kernel, mode='nearest').astype(np.uint8)\n",
    "    \n",
    "    print(\"\\nCPU version (scipy.ndimage.convolve)...\")\n",
    "    start = time.time()\n",
    "    result_cpu = gaussian_blur_cpu(test_image, gaussian_kernel)\n",
    "    cpu_time = time.time() - start\n",
    "    print(f\"  Time: {cpu_time:.4f} seconds\")\n",
    "    \n",
    "    # GPU version\n",
    "    print(\"\\nGPU version...\")\n",
    "    \n",
    "    # Transfer to GPU\n",
    "    image_gpu = cuda.to_device(test_image)\n",
    "    kernel_gpu = cuda.to_device(gaussian_kernel)\n",
    "    output_gpu = cuda.device_array(test_image.shape, dtype=np.uint8)\n",
    "    \n",
    "    # Configure 2D grid\n",
    "    threads_per_block = (16, 16)  # 16x16 = 256 threads per block\n",
    "    blocks_per_grid_x = (test_image.shape[1] + threads_per_block[0] - 1) // threads_per_block[0]\n",
    "    blocks_per_grid_y = (test_image.shape[0] + threads_per_block[1] - 1) // threads_per_block[1]\n",
    "    blocks_per_grid = (blocks_per_grid_x, blocks_per_grid_y)\n",
    "    \n",
    "    print(f\"  Grid configuration:\")\n",
    "    print(f\"    Threads per block: {threads_per_block[0]}x{threads_per_block[1]}\")\n",
    "    print(f\"    Blocks per grid:   {blocks_per_grid[0]}x{blocks_per_grid[1]}\")\n",
    "    print(f\"    Total threads:     {blocks_per_grid[0]*blocks_per_grid[1]*threads_per_block[0]*threads_per_block[1]:,}\")\n",
    "    \n",
    "    # Launch kernel\n",
    "    start = time.time()\n",
    "    gaussian_blur_gpu[blocks_per_grid, threads_per_block](image_gpu, kernel_gpu, output_gpu)\n",
    "    cuda.synchronize()\n",
    "    gpu_time = time.time() - start\n",
    "    \n",
    "    # Copy back\n",
    "    result_gpu = output_gpu.copy_to_host()\n",
    "    \n",
    "    print(f\"  Time: {gpu_time:.4f} seconds\")\n",
    "    \n",
    "    # Calculate speedup\n",
    "    speedup = cpu_time / gpu_time\n",
    "    print(f\"\\nGPU speedup: {speedup:.2f}x\")\n",
    "    \n",
    "    # Visualize results\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(131)\n",
    "    plt.imshow(test_image, cmap='gray')\n",
    "    plt.title('Original Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(132)\n",
    "    plt.imshow(result_cpu, cmap='gray')\n",
    "    plt.title(f'CPU Result ({cpu_time:.3f}s)')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(133)\n",
    "    plt.imshow(result_gpu, cmap='gray')\n",
    "    plt.title(f'GPU Result ({gpu_time:.3f}s) - {speedup:.1f}x faster')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"GPU not available - skipping benchmark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Matrix Multiplication with Shared Memory\n",
    "\n",
    "Shared memory is key to high GPU performance. Let's implement tiled matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GPU_AVAILABLE:\n",
    "    @cuda.jit\n",
    "    def matmul_gpu_naive(A, B, C):\n",
    "        \"\"\"\n",
    "        Naive matrix multiplication on GPU.\n",
    "        \n",
    "        Each thread computes one element of C.\n",
    "        Uses only global memory (slow).\n",
    "        \"\"\"\n",
    "        row = cuda.blockIdx.y * cuda.blockDim.y + cuda.threadIdx.y\n",
    "        col = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
    "        \n",
    "        if row < C.shape[0] and col < C.shape[1]:\n",
    "            value = 0.0\n",
    "            for k in range(A.shape[1]):\n",
    "                value += A[row, k] * B[k, col]\n",
    "            C[row, col] = value\n",
    "    \n",
    "    \n",
    "    @cuda.jit\n",
    "    def matmul_gpu_shared(A, B, C):\n",
    "        \"\"\"\n",
    "        Tiled matrix multiplication using shared memory.\n",
    "        \n",
    "        Shared memory is ~100x faster than global memory.\n",
    "        We load tiles of A and B into shared memory, then compute.\n",
    "        \n",
    "        Algorithm:\n",
    "        1. Divide matrices into tiles (e.g., 16x16)\n",
    "        2. Load one tile of A and B into shared memory\n",
    "        3. Compute partial dot product\n",
    "        4. Repeat for all tiles\n",
    "        5. Write final result to global memory\n",
    "        \"\"\"\n",
    "        # Thread indices\n",
    "        tx = cuda.threadIdx.x\n",
    "        ty = cuda.threadIdx.y\n",
    "        \n",
    "        # Block indices\n",
    "        bx = cuda.blockIdx.x\n",
    "        by = cuda.blockIdx.y\n",
    "        \n",
    "        # Tile size (same as block size)\n",
    "        TILE_SIZE = 16\n",
    "        \n",
    "        # Allocate shared memory for tiles\n",
    "        # Each block has its own shared memory\n",
    "        sA = cuda.shared.array(shape=(16, 16), dtype=numba.float32)\n",
    "        sB = cuda.shared.array(shape=(16, 16), dtype=numba.float32)\n",
    "        \n",
    "        # Global position of this thread's output element\n",
    "        row = by * TILE_SIZE + ty\n",
    "        col = bx * TILE_SIZE + tx\n",
    "        \n",
    "        # Accumulator for dot product\n",
    "        value = 0.0\n",
    "        \n",
    "        # Loop over tiles\n",
    "        num_tiles = (A.shape[1] + TILE_SIZE - 1) // TILE_SIZE\n",
    "        \n",
    "        for tile_idx in range(num_tiles):\n",
    "            # Load tile of A into shared memory\n",
    "            a_col = tile_idx * TILE_SIZE + tx\n",
    "            if row < A.shape[0] and a_col < A.shape[1]:\n",
    "                sA[ty, tx] = A[row, a_col]\n",
    "            else:\n",
    "                sA[ty, tx] = 0.0\n",
    "            \n",
    "            # Load tile of B into shared memory\n",
    "            b_row = tile_idx * TILE_SIZE + ty\n",
    "            if b_row < B.shape[0] and col < B.shape[1]:\n",
    "                sB[ty, tx] = B[b_row, col]\n",
    "            else:\n",
    "                sB[ty, tx] = 0.0\n",
    "            \n",
    "            # Synchronize to ensure all threads loaded their data\n",
    "            cuda.syncthreads()\n",
    "            \n",
    "            # Compute partial dot product using shared memory\n",
    "            for k in range(TILE_SIZE):\n",
    "                value += sA[ty, k] * sB[k, tx]\n",
    "            \n",
    "            # Synchronize before loading next tile\n",
    "            cuda.syncthreads()\n",
    "        \n",
    "        # Write result to global memory\n",
    "        if row < C.shape[0] and col < C.shape[1]:\n",
    "            C[row, col] = value\n",
    "    \n",
    "    print(\"GPU matrix multiplication kernels defined!\")\n",
    "    print(\"  - matmul_gpu_naive: Uses only global memory\")\n",
    "    print(\"  - matmul_gpu_shared: Uses shared memory (optimized)\")\n",
    "else:\n",
    "    print(\"GPU not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GPU_AVAILABLE:\n",
    "    print(\"=\"*70)\n",
    "    print(\"MATRIX MULTIPLICATION BENCHMARK\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Test matrices\n",
    "    n = 512\n",
    "    A = np.random.rand(n, n).astype(np.float32)\n",
    "    B = np.random.rand(n, n).astype(np.float32)\n",
    "    \n",
    "    print(f\"\\nMatrix size: {n}x{n}\")\n",
    "    print(f\"Total operations: {2*n**3:,} (2n³ for n×n matrices)\\n\")\n",
    "    \n",
    "    # CPU baseline (NumPy)\n",
    "    print(\"CPU (NumPy)...\")\n",
    "    start = time.time()\n",
    "    C_cpu = np.dot(A, B)\n",
    "    cpu_time = time.time() - start\n",
    "    gflops_cpu = (2 * n**3) / cpu_time / 1e9\n",
    "    print(f\"  Time: {cpu_time:.4f} seconds\")\n",
    "    print(f\"  Performance: {gflops_cpu:.2f} GFLOPS\")\n",
    "    \n",
    "    # GPU naive\n",
    "    print(\"\\nGPU (naive - global memory only)...\")\n",
    "    A_gpu = cuda.to_device(A)\n",
    "    B_gpu = cuda.to_device(B)\n",
    "    C_gpu_naive = cuda.device_array((n, n), dtype=np.float32)\n",
    "    \n",
    "    threads_per_block = (16, 16)\n",
    "    blocks_per_grid_x = (n + threads_per_block[0] - 1) // threads_per_block[0]\n",
    "    blocks_per_grid_y = (n + threads_per_block[1] - 1) // threads_per_block[1]\n",
    "    blocks_per_grid = (blocks_per_grid_x, blocks_per_grid_y)\n",
    "    \n",
    "    start = time.time()\n",
    "    matmul_gpu_naive[blocks_per_grid, threads_per_block](A_gpu, B_gpu, C_gpu_naive)\n",
    "    cuda.synchronize()\n",
    "    gpu_naive_time = time.time() - start\n",
    "    gflops_gpu_naive = (2 * n**3) / gpu_naive_time / 1e9\n",
    "    \n",
    "    print(f\"  Time: {gpu_naive_time:.4f} seconds\")\n",
    "    print(f\"  Performance: {gflops_gpu_naive:.2f} GFLOPS\")\n",
    "    print(f\"  Speedup vs CPU: {cpu_time/gpu_naive_time:.2f}x\")\n",
    "    \n",
    "    # GPU shared memory\n",
    "    print(\"\\nGPU (optimized - shared memory)...\")\n",
    "    C_gpu_shared = cuda.device_array((n, n), dtype=np.float32)\n",
    "    \n",
    "    start = time.time()\n",
    "    matmul_gpu_shared[blocks_per_grid, threads_per_block](A_gpu, B_gpu, C_gpu_shared)\n",
    "    cuda.synchronize()\n",
    "    gpu_shared_time = time.time() - start\n",
    "    gflops_gpu_shared = (2 * n**3) / gpu_shared_time / 1e9\n",
    "    \n",
    "    print(f\"  Time: {gpu_shared_time:.4f} seconds\")\n",
    "    print(f\"  Performance: {gflops_gpu_shared:.2f} GFLOPS\")\n",
    "    print(f\"  Speedup vs CPU: {cpu_time/gpu_shared_time:.2f}x\")\n",
    "    print(f\"  Speedup vs GPU naive: {gpu_naive_time/gpu_shared_time:.2f}x\")\n",
    "    \n",
    "    # Verify correctness\n",
    "    C_gpu_result = C_gpu_shared.copy_to_host()\n",
    "    matches = np.allclose(C_cpu, C_gpu_result, rtol=1e-4)\n",
    "    print(f\"\\nResults match: {matches}\")\n",
    "    \n",
    "    # Summary table\n",
    "    results = pd.DataFrame([\n",
    "        {'Method': 'CPU (NumPy)', 'Time (s)': cpu_time, 'GFLOPS': gflops_cpu, 'Speedup': 1.0},\n",
    "        {'Method': 'GPU Naive', 'Time (s)': gpu_naive_time, 'GFLOPS': gflops_gpu_naive, 'Speedup': cpu_time/gpu_naive_time},\n",
    "        {'Method': 'GPU Shared', 'Time (s)': gpu_shared_time, 'GFLOPS': gflops_gpu_shared, 'Speedup': cpu_time/gpu_shared_time}\n",
    "    ])\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(results.to_string(index=False))\n",
    "    \n",
    "    print(\"\\nKey insight: Shared memory is crucial for GPU performance!\")\n",
    "    print(f\"Shared memory gave {gpu_naive_time/gpu_shared_time:.1f}x speedup over naive GPU implementation.\")\n",
    "else:\n",
    "    print(\"GPU not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Monte Carlo Simulation on GPU\n",
    "\n",
    "Monte Carlo is embarrassingly parallel - ideal for GPU!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GPU_AVAILABLE:\n",
    "    from numba.cuda.random import create_xoroshiro128p_states, xoroshiro128p_uniform_float32\n",
    "    \n",
    "    @cuda.jit\n",
    "    def monte_carlo_pi_gpu(rng_states, results, samples_per_thread):\n",
    "        \"\"\"\n",
    "        Estimate π using Monte Carlo on GPU.\n",
    "        \n",
    "        Each thread:\n",
    "        1. Generates random points\n",
    "        2. Counts points inside quarter circle\n",
    "        3. Stores count in results array\n",
    "        \n",
    "        Args:\n",
    "            rng_states: Random number generator states (one per thread)\n",
    "            results: Output array (one element per thread)\n",
    "            samples_per_thread: How many samples each thread should generate\n",
    "        \"\"\"\n",
    "        thread_id = cuda.grid(1)\n",
    "        \n",
    "        if thread_id < results.size:\n",
    "            inside = 0\n",
    "            \n",
    "            # Generate samples\n",
    "            for _ in range(samples_per_thread):\n",
    "                # Generate random point in [0,1] × [0,1]\n",
    "                x = xoroshiro128p_uniform_float32(rng_states, thread_id)\n",
    "                y = xoroshiro128p_uniform_float32(rng_states, thread_id)\n",
    "                \n",
    "                # Check if inside quarter circle\n",
    "                if x*x + y*y <= 1.0:\n",
    "                    inside += 1\n",
    "            \n",
    "            results[thread_id] = inside\n",
    "    \n",
    "    print(\"GPU Monte Carlo kernel defined!\")\n",
    "else:\n",
    "    print(\"GPU not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GPU_AVAILABLE:\n",
    "    print(\"=\"*70)\n",
    "    print(\"MONTE CARLO π ESTIMATION: CPU vs GPU\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    total_samples = 100_000_000  # 100 million\n",
    "    \n",
    "    # CPU baseline\n",
    "    print(f\"\\nCPU version ({total_samples:,} samples)...\")\n",
    "    start = time.time()\n",
    "    x = np.random.random(total_samples)\n",
    "    y = np.random.random(total_samples)\n",
    "    inside = np.sum(x*x + y*y <= 1.0)\n",
    "    pi_cpu = 4.0 * inside / total_samples\n",
    "    cpu_time = time.time() - start\n",
    "    \n",
    "    print(f\"  Time: {cpu_time:.4f} seconds\")\n",
    "    print(f\"  π estimate: {pi_cpu:.6f}\")\n",
    "    print(f\"  Error: {abs(pi_cpu - np.pi):.6f}\")\n",
    "    \n",
    "    # GPU version\n",
    "    print(f\"\\nGPU version ({total_samples:,} samples)...\")\n",
    "    \n",
    "    threads_per_block = 256\n",
    "    blocks = 1024\n",
    "    total_threads = threads_per_block * blocks\n",
    "    samples_per_thread = total_samples // total_threads\n",
    "    \n",
    "    print(f\"  Threads per block: {threads_per_block}\")\n",
    "    print(f\"  Blocks: {blocks}\")\n",
    "    print(f\"  Total threads: {total_threads:,}\")\n",
    "    print(f\"  Samples per thread: {samples_per_thread:,}\")\n",
    "    \n",
    "    # Initialize RNG states\n",
    "    rng_states = create_xoroshiro128p_states(total_threads, seed=42)\n",
    "    \n",
    "    # Allocate result array\n",
    "    results_gpu = cuda.device_array(total_threads, dtype=np.int32)\n",
    "    \n",
    "    # Launch kernel\n",
    "    start = time.time()\n",
    "    monte_carlo_pi_gpu[blocks, threads_per_block](rng_states, results_gpu, samples_per_thread)\n",
    "    cuda.synchronize()\n",
    "    \n",
    "    # Sum results\n",
    "    results_cpu = results_gpu.copy_to_host()\n",
    "    total_inside = np.sum(results_cpu)\n",
    "    pi_gpu = 4.0 * total_inside / (total_threads * samples_per_thread)\n",
    "    gpu_time = time.time() - start\n",
    "    \n",
    "    print(f\"  Time: {gpu_time:.4f} seconds\")\n",
    "    print(f\"  π estimate: {pi_gpu:.6f}\")\n",
    "    print(f\"  Error: {abs(pi_gpu - np.pi):.6f}\")\n",
    "    \n",
    "    # Calculate speedup\n",
    "    speedup = cpu_time / gpu_time\n",
    "    print(f\"\\nGPU speedup: {speedup:.2f}x\")\n",
    "    print(f\"Samples per second:\")\n",
    "    print(f\"  CPU: {total_samples/cpu_time/1e6:.2f} million/sec\")\n",
    "    print(f\"  GPU: {total_samples/gpu_time/1e6:.2f} million/sec\")\n",
    "else:\n",
    "    print(\"GPU not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. When to Use GPU: Decision Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"GPU ACCELERATION DECISION GUIDE\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "✓ GOOD FIT FOR GPU:\n",
    "  1. Data parallelism (same operation on many data elements)\n",
    "     - Image/video processing\n",
    "     - Matrix operations\n",
    "     - Monte Carlo simulations\n",
    "  \n",
    "  2. High arithmetic intensity (compute/memory ratio)\n",
    "     - Deep learning (convolutions, matrix multiplies)\n",
    "     - Physics simulations\n",
    "     - Computational geometry\n",
    "  \n",
    "  3. Large datasets (millions of elements)\n",
    "     - GPU overhead is amortized\n",
    "     - Memory bandwidth utilized\n",
    "  \n",
    "  4. Regular memory access patterns\n",
    "     - Coalesced memory reads/writes\n",
    "     - Predictable access patterns\n",
    "\n",
    "✗ POOR FIT FOR GPU:\n",
    "  1. Sequential algorithms\n",
    "     - Dynamic programming\n",
    "     - Recursive algorithms\n",
    "     - Algorithms with data dependencies\n",
    "  \n",
    "  2. Small datasets (< 10K elements)\n",
    "     - Transfer overhead dominates\n",
    "     - Can't saturate GPU\n",
    "  \n",
    "  3. Irregular/random memory access\n",
    "     - Sparse matrices\n",
    "     - Graph algorithms\n",
    "     - Tree traversals\n",
    "  \n",
    "  4. Frequent CPU ↔ GPU transfers\n",
    "     - PCIe bandwidth limited (~16 GB/s)\n",
    "     - Each transfer adds latency\n",
    "  \n",
    "  5. Heavy branching (if/else)\n",
    "     - SIMT execution = all threads follow same path\n",
    "     - Divergence causes serialization\n",
    "\n",
    "OPTIMIZATION PRIORITIES:\n",
    "  1. Minimize CPU ↔ GPU transfers (biggest bottleneck)\n",
    "  2. Maximize memory coalescing (aligned, sequential access)\n",
    "  3. Use shared memory for reused data (100x faster than global)\n",
    "  4. Maximize occupancy (enough threads to hide latency)\n",
    "  5. Avoid branch divergence within warps\n",
    "\n",
    "PERFORMANCE EXPECTATIONS:\n",
    "  - Image processing: 5-20x speedup\n",
    "  - Dense linear algebra: 10-50x speedup\n",
    "  - Monte Carlo: 50-200x speedup\n",
    "  - Deep learning: 10-100x speedup\n",
    "  \n",
    "  Note: Actual speedup depends on:\n",
    "  - GPU model (consumer vs datacenter)\n",
    "  - Problem size\n",
    "  - Memory access patterns\n",
    "  - Implementation quality\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Memory Bandwidth Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GPU_AVAILABLE:\n",
    "    @cuda.jit\n",
    "    def memory_bandwidth_test(data, result):\n",
    "        \"\"\"\n",
    "        Test memory bandwidth by reading and writing large arrays.\n",
    "        \"\"\"\n",
    "        idx = cuda.grid(1)\n",
    "        if idx < data.size:\n",
    "            # Read from global memory, do minimal computation, write back\n",
    "            result[idx] = data[idx] * 2.0\n",
    "    \n",
    "    print(\"Testing GPU memory bandwidth...\\n\")\n",
    "    \n",
    "    # Test different data sizes\n",
    "    sizes_mb = [1, 10, 100, 500]\n",
    "    \n",
    "    for size_mb in sizes_mb:\n",
    "        n_elements = size_mb * 1024 * 1024 // 4  # 4 bytes per float32\n",
    "        \n",
    "        # Create data\n",
    "        data = np.random.rand(n_elements).astype(np.float32)\n",
    "        \n",
    "        # Transfer to GPU\n",
    "        data_gpu = cuda.to_device(data)\n",
    "        result_gpu = cuda.device_array(n_elements, dtype=np.float32)\n",
    "        \n",
    "        # Configure launch\n",
    "        threads_per_block = 256\n",
    "        blocks = (n_elements + threads_per_block - 1) // threads_per_block\n",
    "        \n",
    "        # Run kernel\n",
    "        start = time.time()\n",
    "        memory_bandwidth_test[blocks, threads_per_block](data_gpu, result_gpu)\n",
    "        cuda.synchronize()\n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        # Calculate bandwidth (read + write)\n",
    "        bytes_transferred = 2 * n_elements * 4  # 2 operations × 4 bytes\n",
    "        bandwidth_gb_s = bytes_transferred / elapsed / 1e9\n",
    "        \n",
    "        print(f\"Size: {size_mb:3d} MB, Time: {elapsed:.4f}s, Bandwidth: {bandwidth_gb_s:.2f} GB/s\")\n",
    "    \n",
    "    print(\"\\nNote: GPU memory bandwidth typically 200-900 GB/s depending on model.\")\n",
    "    print(\"Lower than theoretical? Check for:\")\n",
    "    print(\"  - Uncoalesced memory access\")\n",
    "    print(\"  - Small transfer sizes\")\n",
    "    print(\"  - PCIe bottleneck (if including CPU-GPU transfer)\")\n",
    "else:\n",
    "    print(\"GPU not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"GPU PROGRAMMING BEST PRACTICES\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "1. MINIMIZE CPU ↔ GPU TRANSFERS:\n",
    "   - Transfer data once at start\n",
    "   - Keep data on GPU between kernel calls\n",
    "   - Use pinned memory for faster transfers\n",
    "   - Consider computing everything on GPU\n",
    "\n",
    "2. MEMORY OPTIMIZATION:\n",
    "   - Use shared memory for frequently accessed data\n",
    "   - Coalesce global memory accesses\n",
    "   - Minimize register usage per thread\n",
    "   - Use constant memory for read-only data\n",
    "\n",
    "3. THREAD CONFIGURATION:\n",
    "   - Threads per block: 128-256 (multiples of 32)\n",
    "   - Maximize occupancy (check with profiler)\n",
    "   - Consider warp size (32) for efficiency\n",
    "   - Use 2D/3D blocks for 2D/3D problems\n",
    "\n",
    "4. ALGORITHM DESIGN:\n",
    "   - Avoid branch divergence within warps\n",
    "   - Design for regular access patterns\n",
    "   - Use reduction for combining results\n",
    "   - Consider tiling for better cache usage\n",
    "\n",
    "5. DEBUGGING:\n",
    "   - Start with small data and single block\n",
    "   - Print from kernel (only for debugging!)\n",
    "   - Check bounds carefully (cuda.grid(1) calculations)\n",
    "   - Verify results against CPU version\n",
    "\n",
    "6. PROFILING:\n",
    "   - Use cuda.profile_start/stop\n",
    "   - Analyze with NVIDIA Nsight\n",
    "   - Check memory bandwidth utilization\n",
    "   - Look for bottlenecks (memory vs compute)\n",
    "\n",
    "7. COMMON PITFALLS:\n",
    "   - Forgetting cuda.synchronize()\n",
    "   - Not checking array bounds\n",
    "   - Race conditions in shared memory\n",
    "   - Bank conflicts in shared memory\n",
    "   - Excessive register usage (reduces occupancy)\n",
    "\"\"\")\n",
    "\n",
    "if GPU_AVAILABLE:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"YOUR GPU PERFORMANCE SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    gpu = cuda.get_current_device()\n",
    "    print(f\"GPU: {gpu.name.decode('utf-8')}\")\n",
    "    print(f\"Memory: {gpu.total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"Compute Capability: {gpu.compute_capability}\")\n",
    "    print(\"\\nRecommended configuration for this GPU:\")\n",
    "    print(f\"  Max threads per block: {min(gpu.MAX_THREADS_PER_BLOCK, 256)}\")\n",
    "    print(f\"  Optimal block size: 128-256 threads\")\n",
    "    print(f\"  Shared memory per block: ~48KB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"NEXT STEPS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "Module 08: Performance Benchmarking\n",
    "  - Comprehensive comparison: Serial vs Parallel vs GPU\n",
    "  - Scalability analysis (strong and weak scaling)\n",
    "  - Statistical analysis of performance\n",
    "  - Visualization of results\n",
    "\n",
    "Module 09: Optimization Techniques\n",
    "  - Cache optimization strategies\n",
    "  - Load balancing improvements\n",
    "  - Hybrid CPU-GPU approaches\n",
    "  - Iterative refinement process\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Parallel Reduction on GPU\n",
    "\n",
    "Implement a parallel sum reduction using shared memory.\n",
    "\n",
    "**Challenge**: How do you combine results from multiple blocks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GPU_AVAILABLE:\n",
    "    @cuda.jit\n",
    "    def parallel_sum_gpu(data, block_sums):\n",
    "        \"\"\"\n",
    "        Sum array elements using parallel reduction.\n",
    "        \n",
    "        Strategy:\n",
    "        1. Each thread loads one element into shared memory\n",
    "        2. Perform tree-based reduction within block\n",
    "        3. First thread writes block sum to global memory\n",
    "        \"\"\"\n",
    "        # TODO: Implement parallel reduction\n",
    "        # Hints:\n",
    "        # - Use cuda.shared.array() for temporary storage\n",
    "        # - Use cuda.syncthreads() between reduction steps\n",
    "        # - Reduction pattern: stride = 1, 2, 4, 8, ...\n",
    "        pass\n",
    "    \n",
    "    # Test your implementation\n",
    "    # data = np.random.rand(1000000).astype(np.float32)\n",
    "    # expected_sum = np.sum(data)\n",
    "    # ... launch kernel ...\n",
    "    # gpu_sum = np.sum(block_sums_cpu)  # Sum the block sums\n",
    "    # print(f\"CPU sum: {expected_sum:.6f}\")\n",
    "    # print(f\"GPU sum: {gpu_sum:.6f}\")\n",
    "    # print(f\"Match: {np.isclose(expected_sum, gpu_sum)}\")\n",
    "else:\n",
    "    print(\"GPU not available - exercise skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Image Histogram on GPU\n",
    "\n",
    "Compute image histogram using GPU.\n",
    "\n",
    "**Challenge**: Multiple threads may try to increment the same bin (atomic operations needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GPU_AVAILABLE:\n",
    "    @cuda.jit\n",
    "    def histogram_gpu(image, histogram):\n",
    "        \"\"\"\n",
    "        Compute histogram on GPU.\n",
    "        \n",
    "        Use cuda.atomic.add() to safely increment bins.\n",
    "        \"\"\"\n",
    "        # TODO: Implement GPU histogram\n",
    "        # Hint: cuda.atomic.add(array, index, value)\n",
    "        pass\n",
    "    \n",
    "    # Test\n",
    "    # test_img = create_test_image((1024, 1024))\n",
    "    # histogram_cpu = np.bincount(test_img.ravel(), minlength=256)\n",
    "    # ... GPU version ...\n",
    "    # Compare CPU vs GPU results\n",
    "else:\n",
    "    print(\"GPU not available - exercise skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Performance Analysis\n",
    "\n",
    "Measure the overhead of CPU-GPU transfers.\n",
    "\n",
    "**Task**: For different data sizes, measure:\n",
    "1. CPU → GPU transfer time\n",
    "2. Kernel execution time\n",
    "3. GPU → CPU transfer time\n",
    "4. Total time\n",
    "\n",
    "Plot the results to see when GPU becomes worthwhile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement transfer overhead analysis\n",
    "# Test with sizes: 1KB, 10KB, 100KB, 1MB, 10MB, 100MB\n",
    "# Plot: time vs data size, showing transfer vs compute breakdown\n",
    "\n",
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this module, you learned:\n",
    "\n",
    "1. **GPU Architecture**\n",
    "   - CUDA execution model (grid, blocks, threads)\n",
    "   - Memory hierarchy (global, shared, constant, registers)\n",
    "   - SIMT execution and warps\n",
    "\n",
    "2. **CUDA Programming with Numba**\n",
    "   - Writing GPU kernels in Python\n",
    "   - Managing GPU memory\n",
    "   - Thread indexing and synchronization\n",
    "\n",
    "3. **Optimization Techniques**\n",
    "   - Shared memory for data reuse\n",
    "   - Memory coalescing\n",
    "   - Minimizing CPU-GPU transfers\n",
    "   - Proper thread configuration\n",
    "\n",
    "4. **Performance Analysis**\n",
    "   - CPU vs GPU benchmarking\n",
    "   - When GPU acceleration is beneficial\n",
    "   - Memory bandwidth considerations\n",
    "\n",
    "**What's Next?**\n",
    "\n",
    "- **Module 08**: Comprehensive benchmarking framework\n",
    "- **Module 09**: Advanced optimization and hybrid approaches"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
