{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Module 09: Advanced Scripting Techniques\n",
    "\n",
    "**Difficulty**: ⭐⭐⭐ (Advanced)\n",
    "\n",
    "**Estimated Time**: 75 minutes\n",
    "\n",
    "**Prerequisites**: \n",
    "- Completed Modules 00-08\n",
    "- Strong Python programming skills\n",
    "- Understanding of processes and threading concepts\n",
    "- Familiarity with testing principles\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. **Implement** parallel processing for performance gains\n",
    "2. **Apply** advanced error handling and recovery patterns\n",
    "3. **Configure** structured logging for production systems\n",
    "4. **Write** testable automation scripts\n",
    "5. **Optimize** script performance using profiling\n",
    "6. **Build** production-ready automation tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Introduction: Professional Automation\n",
    "\n",
    "This module elevates your automation scripts from working prototypes to production-ready systems.\n",
    "\n",
    "### What Makes Code \"Production-Ready\"?\n",
    "\n",
    "**Basic Script** → **Production System**\n",
    "\n",
    "| Aspect | Basic | Production |\n",
    "|--------|-------|------------|\n",
    "| **Error Handling** | Try-except if needed | Comprehensive with recovery |\n",
    "| **Logging** | Print statements | Structured logging with levels |\n",
    "| **Performance** | \"Good enough\" | Profiled and optimized |\n",
    "| **Testing** | Manual testing | Automated test suite |\n",
    "| **Scalability** | Single-threaded | Parallel when beneficial |\n",
    "| **Monitoring** | None | Metrics and alerts |\n",
    "| **Documentation** | Minimal comments | Complete docs + examples |\n",
    "\n",
    "### When to Apply Advanced Techniques\n",
    "\n",
    "**Use parallel processing when:**\n",
    "- Processing many independent items (batch operations)\n",
    "- I/O-bound tasks (API calls, file operations)\n",
    "- CPU-bound tasks on multi-core systems\n",
    "\n",
    "**Use structured logging when:**\n",
    "- Scripts run unattended\n",
    "- Debugging production issues\n",
    "- Compliance/audit requirements\n",
    "- Multiple team members use the script\n",
    "\n",
    "**Write tests when:**\n",
    "- Code will be reused frequently\n",
    "- Multiple people contribute\n",
    "- Changes are risky\n",
    "- Regression bugs are costly\n",
    "\n",
    "This module teaches you **how** and **when** to apply each technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Import required libraries\n",
    "import time\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed\n",
    "import multiprocessing\n",
    "from functools import wraps\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "\n",
    "print(f\"CPU cores available: {multiprocessing.cpu_count()}\")\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 1. Parallel Processing\n",
    "\n",
    "Speed up batch operations by processing multiple items simultaneously.\n",
    "\n",
    "### Threading vs Multiprocessing\n",
    "\n",
    "| Method | Best For | Limitation |\n",
    "|--------|----------|------------|\n",
    "| **Threading** | I/O-bound (file/network) | GIL limits CPU usage |\n",
    "| **Multiprocessing** | CPU-bound (computation) | Higher memory overhead |\n",
    "\n",
    "**GIL (Global Interpreter Lock)**: Python limitation that prevents true parallel execution of Python code in threads. Use multiprocessing for CPU-heavy tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Process files in parallel\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def process_file(file_path):\n",
    "    \"\"\"\n",
    "    Simulate processing a file (I/O-bound operation).\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to file\n",
    "    \n",
    "    Returns:\n",
    "        dict: Processing results\n",
    "    \"\"\"\n",
    "    # Simulate I/O operation\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    return {\n",
    "        'file': str(file_path),\n",
    "        'status': 'success',\n",
    "        'lines': 100\n",
    "    }\n",
    "\n",
    "def process_files_sequential(files):\n",
    "    \"\"\"Process files one at a time.\"\"\"\n",
    "    results = []\n",
    "    for file in files:\n",
    "        result = process_file(file)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "def process_files_parallel(files, max_workers=4):\n",
    "    \"\"\"\n",
    "    Process files in parallel using ThreadPoolExecutor.\n",
    "    \n",
    "    Args:\n",
    "        files: List of file paths\n",
    "        max_workers: Maximum concurrent threads\n",
    "    \n",
    "    Returns:\n",
    "        list: Processing results\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all tasks\n",
    "        future_to_file = {executor.submit(process_file, f): f for f in files}\n",
    "        \n",
    "        # Collect results as they complete\n",
    "        for future in as_completed(future_to_file):\n",
    "            file = future_to_file[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                results.append(result)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file}: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Compare performance\n",
    "test_files = [f'file_{i}.txt' for i in range(10)]\n",
    "\n",
    "print(\"Processing 10 files...\\n\")\n",
    "\n",
    "# Sequential\n",
    "start = time.time()\n",
    "results_seq = process_files_sequential(test_files)\n",
    "time_seq = time.time() - start\n",
    "print(f\"Sequential: {time_seq:.2f}s\")\n",
    "\n",
    "# Parallel\n",
    "start = time.time()\n",
    "results_par = process_files_parallel(test_files, max_workers=4)\n",
    "time_par = time.time() - start\n",
    "print(f\"Parallel (4 workers): {time_par:.2f}s\")\n",
    "\n",
    "speedup = time_seq / time_par\n",
    "print(f\"\\nSpeedup: {speedup:.1f}x faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "### 1.1 CPU-Bound Parallel Processing\n",
    "\n",
    "For computation-heavy tasks, use multiprocessing to bypass the GIL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU-bound task example\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import multiprocessing\n",
    "\n",
    "def compute_intensive_task(n):\n",
    "    \"\"\"\n",
    "    Simulate CPU-intensive computation.\n",
    "    \n",
    "    Args:\n",
    "        n: Input value\n",
    "    \n",
    "    Returns:\n",
    "        int: Computed result\n",
    "    \"\"\"\n",
    "    # Simulate computation (calculate prime numbers)\n",
    "    result = sum(i for i in range(n) if all(i % j != 0 for j in range(2, int(i**0.5) + 1)))\n",
    "    return result\n",
    "\n",
    "def process_batch_parallel(items, max_workers=None):\n",
    "    \"\"\"\n",
    "    Process items in parallel using multiple processes.\n",
    "    \n",
    "    Args:\n",
    "        items: List of items to process\n",
    "        max_workers: Number of processes (None = CPU count)\n",
    "    \n",
    "    Returns:\n",
    "        list: Results\n",
    "    \"\"\"\n",
    "    if max_workers is None:\n",
    "        max_workers = multiprocessing.cpu_count()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_item = {executor.submit(compute_intensive_task, item): item for item in items}\n",
    "        \n",
    "        for future in as_completed(future_to_item):\n",
    "            item = future_to_item[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                results.append(result)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {item}: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Demo with smaller numbers for quick execution\n",
    "test_items = [1000, 2000, 3000, 4000]\n",
    "\n",
    "print(f\"Processing {len(test_items)} CPU-intensive tasks\")\n",
    "print(f\"Using {multiprocessing.cpu_count()} processes\\n\")\n",
    "\n",
    "start = time.time()\n",
    "results = process_batch_parallel(test_items)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"Completed in {elapsed:.2f}s\")\n",
    "print(f\"Results: {results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 2. Advanced Error Handling Patterns\n",
    "\n",
    "Production code needs robust error handling with recovery strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retry decorator with exponential backoff\n",
    "from functools import wraps\n",
    "import time\n",
    "\n",
    "def retry_with_backoff(max_retries=3, base_delay=1, max_delay=60, exceptions=(Exception,)):\n",
    "    \"\"\"\n",
    "    Decorator to retry function with exponential backoff.\n",
    "    \n",
    "    Args:\n",
    "        max_retries: Maximum number of retry attempts\n",
    "        base_delay: Initial delay in seconds\n",
    "        max_delay: Maximum delay between retries\n",
    "        exceptions: Tuple of exceptions to catch\n",
    "    \"\"\"\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            delay = base_delay\n",
    "            \n",
    "            for attempt in range(1, max_retries + 1):\n",
    "                try:\n",
    "                    return func(*args, **kwargs)\n",
    "                \n",
    "                except exceptions as e:\n",
    "                    if attempt == max_retries:\n",
    "                        # Last attempt failed, re-raise\n",
    "                        raise\n",
    "                    \n",
    "                    print(f\"Attempt {attempt}/{max_retries} failed: {e}\")\n",
    "                    print(f\"Retrying in {delay}s...\")\n",
    "                    \n",
    "                    time.sleep(delay)\n",
    "                    \n",
    "                    # Exponential backoff\n",
    "                    delay = min(delay * 2, max_delay)\n",
    "        \n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "# Example usage\n",
    "@retry_with_backoff(max_retries=3, base_delay=1)\n",
    "def unreliable_api_call():\n",
    "    \"\"\"Simulate API call that sometimes fails.\"\"\"\n",
    "    import random\n",
    "    if random.random() < 0.7:  # 70% failure rate for demo\n",
    "        raise ConnectionError(\"API temporarily unavailable\")\n",
    "    return {\"status\": \"success\", \"data\": [1, 2, 3]}\n",
    "\n",
    "# Test the retry mechanism\n",
    "print(\"Testing retry with exponential backoff:\\n\")\n",
    "try:\n",
    "    result = unreliable_api_call()\n",
    "    print(f\"\\n✓ Success: {result}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ Failed after all retries: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "### 2.1 Context Managers for Resource Management\n",
    "\n",
    "Ensure resources are always cleaned up, even when errors occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom context manager for automation tasks\n",
    "from contextlib import contextmanager\n",
    "import logging\n",
    "\n",
    "@contextmanager\n",
    "def automation_task(task_name, logger=None):\n",
    "    \"\"\"\n",
    "    Context manager for automation tasks.\n",
    "    Ensures cleanup and logging even if task fails.\n",
    "    \n",
    "    Args:\n",
    "        task_name: Name of the task\n",
    "        logger: Logger instance (optional)\n",
    "    \"\"\"\n",
    "    if logger is None:\n",
    "        logger = logging.getLogger(__name__)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    logger.info(f\"Starting task: {task_name}\")\n",
    "    \n",
    "    try:\n",
    "        yield\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        logger.info(f\"✓ Task completed: {task_name} ({elapsed:.2f}s)\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        elapsed = time.time() - start_time\n",
    "        logger.error(f\"✗ Task failed: {task_name} ({elapsed:.2f}s)\")\n",
    "        logger.error(f\"Error: {e}\")\n",
    "        logger.debug(traceback.format_exc())\n",
    "        raise\n",
    "    \n",
    "    finally:\n",
    "        # Cleanup code always runs\n",
    "        logger.debug(f\"Cleanup for task: {task_name}\")\n",
    "\n",
    "# Example usage\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Successful task\n",
    "with automation_task(\"Data Processing\", logger):\n",
    "    print(\"Processing data...\")\n",
    "    time.sleep(0.5)\n",
    "    print(\"Data processed!\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Failed task\n",
    "try:\n",
    "    with automation_task(\"Risky Operation\", logger):\n",
    "        print(\"Attempting risky operation...\")\n",
    "        raise ValueError(\"Something went wrong!\")\n",
    "except ValueError:\n",
    "    print(\"Handled the error gracefully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 3. Structured Logging\n",
    "\n",
    "Professional logging with levels, formatting, and multiple handlers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production-grade logging setup\n",
    "import logging\n",
    "import logging.handlers\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "def setup_production_logger(\n",
    "    name,\n",
    "    log_dir='logs',\n",
    "    console_level=logging.INFO,\n",
    "    file_level=logging.DEBUG,\n",
    "    max_bytes=10*1024*1024,  # 10 MB\n",
    "    backup_count=5\n",
    "):\n",
    "    \"\"\"\n",
    "    Setup production-grade logger with rotation.\n",
    "    \n",
    "    Args:\n",
    "        name: Logger name\n",
    "        log_dir: Directory for log files\n",
    "        console_level: Console logging level\n",
    "        file_level: File logging level\n",
    "        max_bytes: Max log file size before rotation\n",
    "        backup_count: Number of backup files to keep\n",
    "    \n",
    "    Returns:\n",
    "        logging.Logger: Configured logger\n",
    "    \"\"\"\n",
    "    # Create logger\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    logger.handlers.clear()\n",
    "    \n",
    "    # Create logs directory\n",
    "    log_dir = Path(log_dir)\n",
    "    log_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # File handler with rotation\n",
    "    log_file = log_dir / f\"{name}.log\"\n",
    "    file_handler = logging.handlers.RotatingFileHandler(\n",
    "        log_file,\n",
    "        maxBytes=max_bytes,\n",
    "        backupCount=backup_count\n",
    "    )\n",
    "    file_handler.setLevel(file_level)\n",
    "    \n",
    "    # Console handler\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setLevel(console_level)\n",
    "    \n",
    "    # Detailed formatter for files\n",
    "    file_formatter = logging.Formatter(\n",
    "        '%(asctime)s - %(name)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S'\n",
    "    )\n",
    "    \n",
    "    # Simple formatter for console\n",
    "    console_formatter = logging.Formatter(\n",
    "        '%(levelname)s: %(message)s'\n",
    "    )\n",
    "    \n",
    "    file_handler.setFormatter(file_formatter)\n",
    "    console_handler.setFormatter(console_formatter)\n",
    "    \n",
    "    logger.addHandler(file_handler)\n",
    "    logger.addHandler(console_handler)\n",
    "    \n",
    "    logger.info(f\"Logger initialized: {name}\")\n",
    "    logger.info(f\"Log file: {log_file}\")\n",
    "    \n",
    "    return logger\n",
    "\n",
    "# Example usage\n",
    "logger = setup_production_logger(\n",
    "    'demo_app',\n",
    "    log_dir='logs/demo',\n",
    "    console_level=logging.INFO,\n",
    "    file_level=logging.DEBUG\n",
    ")\n",
    "\n",
    "# Different log levels\n",
    "logger.debug(\"Detailed debugging information\")\n",
    "logger.info(\"General information about program execution\")\n",
    "logger.warning(\"Warning: Something unexpected happened\")\n",
    "logger.error(\"Error: Operation failed\")\n",
    "\n",
    "# Structured logging with context\n",
    "user_id = 12345\n",
    "action = \"data_export\"\n",
    "logger.info(f\"User {user_id} performed {action}\", extra={'user_id': user_id, 'action': action})\n",
    "\n",
    "print(\"\\n✓ Logs written to logs/demo/demo_app.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 4. Performance Profiling\n",
    "\n",
    "Measure and optimize script performance scientifically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance profiling decorator\n",
    "import time\n",
    "from functools import wraps\n",
    "\n",
    "def profile_performance(func):\n",
    "    \"\"\"\n",
    "    Decorator to profile function performance.\n",
    "    \n",
    "    Measures execution time and memory usage.\n",
    "    \"\"\"\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        import tracemalloc\n",
    "        \n",
    "        # Start profiling\n",
    "        tracemalloc.start()\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            result = func(*args, **kwargs)\n",
    "            return result\n",
    "        \n",
    "        finally:\n",
    "            # Stop profiling\n",
    "            elapsed_time = time.time() - start_time\n",
    "            current, peak = tracemalloc.get_traced_memory()\n",
    "            tracemalloc.stop()\n",
    "            \n",
    "            # Report\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Performance Profile: {func.__name__}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            print(f\"Execution time: {elapsed_time:.4f}s\")\n",
    "            print(f\"Current memory: {current / 1024 / 1024:.2f} MB\")\n",
    "            print(f\"Peak memory: {peak / 1024 / 1024:.2f} MB\")\n",
    "            print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return wrapper\n",
    "\n",
    "# Example: Profile a data processing function\n",
    "@profile_performance\n",
    "def process_large_dataset(size=100000):\n",
    "    \"\"\"\n",
    "    Simulate processing a large dataset.\n",
    "    \n",
    "    Args:\n",
    "        size: Dataset size\n",
    "    \"\"\"\n",
    "    # Create data\n",
    "    data = list(range(size))\n",
    "    \n",
    "    # Process data\n",
    "    result = [x * 2 for x in data if x % 2 == 0]\n",
    "    \n",
    "    return len(result)\n",
    "\n",
    "# Test the profiled function\n",
    "count = process_large_dataset(size=100000)\n",
    "print(f\"Processed {count} items\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "### 4.1 Code Optimization Strategies\n",
    "\n",
    "Common optimization techniques for automation scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization examples\n",
    "import time\n",
    "\n",
    "def compare_approaches(size=10000):\n",
    "    \"\"\"\n",
    "    Compare different approaches to the same task.\n",
    "    \"\"\"\n",
    "    data = list(range(size))\n",
    "    \n",
    "    # Approach 1: List comprehension (Pythonic)\n",
    "    start = time.time()\n",
    "    result1 = [x * 2 for x in data if x % 2 == 0]\n",
    "    time1 = time.time() - start\n",
    "    \n",
    "    # Approach 2: For loop with append (Verbose)\n",
    "    start = time.time()\n",
    "    result2 = []\n",
    "    for x in data:\n",
    "        if x % 2 == 0:\n",
    "            result2.append(x * 2)\n",
    "    time2 = time.time() - start\n",
    "    \n",
    "    # Approach 3: Filter + map (Functional)\n",
    "    start = time.time()\n",
    "    result3 = list(map(lambda x: x * 2, filter(lambda x: x % 2 == 0, data)))\n",
    "    time3 = time.time() - start\n",
    "    \n",
    "    # Report\n",
    "    print(\"Performance Comparison:\")\n",
    "    print(f\"List comprehension: {time1*1000:.2f}ms (baseline)\")\n",
    "    print(f\"For loop:           {time2*1000:.2f}ms ({time2/time1:.2f}x slower)\")\n",
    "    print(f\"Filter + map:       {time3*1000:.2f}ms ({time3/time1:.2f}x slower)\")\n",
    "    print(f\"\\nRecommendation: Use list comprehension (fastest and most readable)\")\n",
    "\n",
    "compare_approaches(size=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## 5. Testing Automation Scripts\n",
    "\n",
    "Write testable code and automated tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Testable automation function\n",
    "def validate_data_file(file_path, required_columns=None, max_missing_pct=0.1):\n",
    "    \"\"\"\n",
    "    Validate a data file meets quality standards.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to data file\n",
    "        required_columns: List of required column names\n",
    "        max_missing_pct: Maximum allowed missing data percentage\n",
    "    \n",
    "    Returns:\n",
    "        dict: Validation results\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If validation fails\n",
    "    \"\"\"\n",
    "    from pathlib import Path\n",
    "    \n",
    "    file_path = Path(file_path)\n",
    "    \n",
    "    # Check file exists\n",
    "    if not file_path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    # For demo, simulate validation\n",
    "    # In real code, would read CSV and check columns/data quality\n",
    "    \n",
    "    results = {\n",
    "        'valid': True,\n",
    "        'file': str(file_path),\n",
    "        'rows': 1000,\n",
    "        'columns': ['id', 'value', 'timestamp'],\n",
    "        'missing_pct': 0.05\n",
    "    }\n",
    "    \n",
    "    # Check required columns\n",
    "    if required_columns:\n",
    "        missing_cols = set(required_columns) - set(results['columns'])\n",
    "        if missing_cols:\n",
    "            results['valid'] = False\n",
    "            results['error'] = f\"Missing required columns: {missing_cols}\"\n",
    "            raise ValueError(results['error'])\n",
    "    \n",
    "    # Check missing data\n",
    "    if results['missing_pct'] > max_missing_pct:\n",
    "        results['valid'] = False\n",
    "        results['error'] = f\"Too much missing data: {results['missing_pct']:.1%} > {max_missing_pct:.1%}\"\n",
    "        raise ValueError(results['error'])\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Simple test function\n",
    "def test_validate_data_file():\n",
    "    \"\"\"Test data validation function.\"\"\"\n",
    "    from pathlib import Path\n",
    "    \n",
    "    # Create test file\n",
    "    test_file = Path('test_data.csv')\n",
    "    test_file.write_text('id,value,timestamp\\n1,100,2024-01-01')\n",
    "    \n",
    "    try:\n",
    "        # Test 1: Valid file\n",
    "        result = validate_data_file(test_file, required_columns=['id', 'value'])\n",
    "        assert result['valid'] == True\n",
    "        print(\"✓ Test 1 passed: Valid file\")\n",
    "        \n",
    "        # Test 2: Missing required column\n",
    "        try:\n",
    "            validate_data_file(test_file, required_columns=['missing_column'])\n",
    "            print(\"✗ Test 2 failed: Should have raised ValueError\")\n",
    "        except ValueError as e:\n",
    "            print(f\"✓ Test 2 passed: Caught expected error: {e}\")\n",
    "        \n",
    "        # Test 3: Non-existent file\n",
    "        try:\n",
    "            validate_data_file('nonexistent.csv')\n",
    "            print(\"✗ Test 3 failed: Should have raised FileNotFoundError\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"✓ Test 3 passed: Caught expected FileNotFoundError\")\n",
    "        \n",
    "        print(\"\\n✓ All tests passed!\")\n",
    "    \n",
    "    finally:\n",
    "        # Cleanup\n",
    "        test_file.unlink()\n",
    "\n",
    "# Run tests\n",
    "test_validate_data_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## 6. Practice Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "### Exercise 1: Parallel File Processor\n",
    "\n",
    "Build a production-grade parallel file processor:\n",
    "1. Process multiple files in parallel\n",
    "2. Implement retry logic for failed files\n",
    "3. Add comprehensive logging\n",
    "4. Profile performance (sequential vs parallel)\n",
    "5. Write tests for edge cases\n",
    "\n",
    "**Hint**: Combine ThreadPoolExecutor, retry decorator, and logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your solution here\n",
    "\n",
    "class ProductionFileProcessor:\n",
    "    \"\"\"\n",
    "    Production-ready parallel file processor.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_workers=4, logger=None):\n",
    "        # TODO: Initialize processor with logging\n",
    "        pass\n",
    "    \n",
    "    @retry_with_backoff(max_retries=3)\n",
    "    def process_single_file(self, file_path):\n",
    "        # TODO: Process one file with error handling\n",
    "        pass\n",
    "    \n",
    "    def process_batch(self, file_paths):\n",
    "        # TODO: Process files in parallel\n",
    "        pass\n",
    "    \n",
    "    def get_stats(self):\n",
    "        # TODO: Return processing statistics\n",
    "        pass\n",
    "\n",
    "# Test your processor\n",
    "# processor = ProductionFileProcessor(max_workers=4)\n",
    "# results = processor.process_batch(['file1.txt', 'file2.txt', 'file3.txt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "### Exercise 2: Performance Optimizer\n",
    "\n",
    "Create a tool to identify performance bottlenecks:\n",
    "1. Profile multiple functions automatically\n",
    "2. Compare execution times\n",
    "3. Track memory usage\n",
    "4. Generate optimization recommendations\n",
    "5. Export performance report\n",
    "\n",
    "**Hint**: Use the profile_performance decorator and extend it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your solution here\n",
    "\n",
    "class PerformanceAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyze and optimize script performance.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # TODO: Initialize analyzer\n",
    "        pass\n",
    "    \n",
    "    def profile_function(self, func, *args, **kwargs):\n",
    "        # TODO: Profile function execution\n",
    "        pass\n",
    "    \n",
    "    def compare_implementations(self, implementations):\n",
    "        # TODO: Compare different implementations\n",
    "        pass\n",
    "    \n",
    "    def generate_report(self):\n",
    "        # TODO: Generate performance report\n",
    "        pass\n",
    "\n",
    "# Test your analyzer\n",
    "# analyzer = PerformanceAnalyzer()\n",
    "# analyzer.profile_function(some_function, arg1, arg2)\n",
    "# analyzer.generate_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "### Exercise 3: Complete Production Pipeline\n",
    "\n",
    "Build an end-to-end data processing pipeline:\n",
    "1. Load data from multiple sources in parallel\n",
    "2. Validate data quality (use tests)\n",
    "3. Process with error handling and retries\n",
    "4. Log all operations comprehensively\n",
    "5. Generate performance metrics\n",
    "6. Save results with proper error handling\n",
    "\n",
    "**Hint**: Combine all techniques from this module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Your solution here\n",
    "\n",
    "class ProductionDataPipeline:\n",
    "    \"\"\"\n",
    "    Production-grade data processing pipeline.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config, logger=None):\n",
    "        # TODO: Initialize pipeline\n",
    "        pass\n",
    "    \n",
    "    def load_data_sources(self, sources):\n",
    "        # TODO: Load data in parallel\n",
    "        pass\n",
    "    \n",
    "    def validate_data(self, data):\n",
    "        # TODO: Validate data quality\n",
    "        pass\n",
    "    \n",
    "    def process_data(self, data):\n",
    "        # TODO: Process with error handling\n",
    "        pass\n",
    "    \n",
    "    def run(self):\n",
    "        # TODO: Execute complete pipeline\n",
    "        pass\n",
    "\n",
    "# Test your pipeline\n",
    "# pipeline = ProductionDataPipeline(config={'sources': ['s1', 's2']})\n",
    "# pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Parallel Processing**\n",
    "   - ThreadPoolExecutor for I/O-bound tasks\n",
    "   - ProcessPoolExecutor for CPU-bound tasks\n",
    "   - Measure speedup to validate parallelization\n",
    "\n",
    "2. **Advanced Error Handling**\n",
    "   - Retry with exponential backoff\n",
    "   - Context managers for resource cleanup\n",
    "   - Comprehensive exception handling\n",
    "\n",
    "3. **Structured Logging**\n",
    "   - Multiple handlers (file, console)\n",
    "   - Proper log levels (DEBUG, INFO, WARNING, ERROR)\n",
    "   - Log rotation for disk management\n",
    "   - Detailed formatting for debugging\n",
    "\n",
    "4. **Performance Optimization**\n",
    "   - Profile before optimizing\n",
    "   - Measure time and memory\n",
    "   - Use efficient Python patterns\n",
    "   - Document optimization decisions\n",
    "\n",
    "5. **Testing**\n",
    "   - Write testable functions\n",
    "   - Test edge cases and errors\n",
    "   - Automate test execution\n",
    "   - Maintain test coverage\n",
    "\n",
    "### Production Checklist\n",
    "\n",
    "Before deploying automation scripts:\n",
    "\n",
    "- [ ] Comprehensive error handling with retries\n",
    "- [ ] Structured logging (file + console)\n",
    "- [ ] Performance profiled and optimized\n",
    "- [ ] Tests written and passing\n",
    "- [ ] Resource cleanup (context managers)\n",
    "- [ ] Configuration externalized (no hardcoded values)\n",
    "- [ ] Documentation complete\n",
    "- [ ] Security reviewed (credentials, permissions)\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In **Module 10: Final Automation Project**, you'll:\n",
    "- Apply all learned techniques\n",
    "- Build a complete real-world automation system\n",
    "- Follow production best practices\n",
    "- Create deployment-ready code\n",
    "\n",
    "### Self-Assessment\n",
    "\n",
    "Before moving on, make sure you can:\n",
    "- [ ] Implement parallel processing for appropriate tasks\n",
    "- [ ] Add retry logic with exponential backoff\n",
    "- [ ] Setup structured logging with rotation\n",
    "- [ ] Profile and optimize script performance\n",
    "- [ ] Write automated tests for automation scripts\n",
    "- [ ] Use context managers for resource management\n",
    "\n",
    "---\n",
    "\n",
    "**Continue to Module 10** for the final capstone project!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
