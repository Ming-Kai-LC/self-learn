{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 06: Networking Basics for Data Science\n",
    "\n",
    "**Difficulty**: ⭐⭐ (Intermediate)\n",
    "\n",
    "**Estimated Time**: 60 minutes\n",
    "\n",
    "**Prerequisites**: \n",
    "- Completed Modules 00-05\n",
    "- Basic understanding of HTTP and APIs\n",
    "- Familiarity with requests library (will install if needed)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. **Check** network connectivity and diagnose issues\n",
    "2. **Test** port availability and service accessibility\n",
    "3. **Work with** HTTP APIs for data collection\n",
    "4. **Download** datasets and files reliably\n",
    "5. **Handle** proxy settings and authentication\n",
    "6. **Monitor** network usage for data transfers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction: Why Networking for Data Scientists?\n",
    "\n",
    "Data scientists constantly work with networked resources:\n",
    "\n",
    "### Common Scenarios\n",
    "\n",
    "**1. Data Collection**\n",
    "- Fetch data from REST APIs\n",
    "- Download datasets from cloud storage\n",
    "- Scrape data from websites (ethically)\n",
    "- Access databases over network\n",
    "\n",
    "**2. Model Deployment**\n",
    "- Serve predictions via HTTP API\n",
    "- Check if port is available\n",
    "- Test API endpoint accessibility\n",
    "- Monitor service health\n",
    "\n",
    "**3. Distributed Computing**\n",
    "- Connect to remote Jupyter servers\n",
    "- Access cloud GPU resources\n",
    "- Transfer large datasets\n",
    "- Check network bandwidth\n",
    "\n",
    "**4. Troubleshooting**\n",
    "- Debug connection failures\n",
    "- Test if API is reachable\n",
    "- Identify network bottlenecks\n",
    "- Verify firewall rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Import required libraries\n",
    "import socket\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import time\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Install requests if needed\n",
    "try:\n",
    "    import requests\n",
    "except ImportError:\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'requests', '-q'])\n",
    "    import requests\n",
    "\n",
    "print(f\"requests version: {requests.__version__}\")\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Network Connectivity Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test network connectivity to a host\n",
    "def check_connectivity(host, timeout=3):\n",
    "    \"\"\"\n",
    "    Check if a host is reachable.\n",
    "    \n",
    "    Args:\n",
    "        host: Hostname or IP address\n",
    "        timeout: Timeout in seconds\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (reachable: bool, latency_ms: float or None)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        start = time.time()\n",
    "        # Try to resolve hostname and connect\n",
    "        socket.setdefaulttimeout(timeout)\n",
    "        socket.gethostbyname(host)\n",
    "        latency_ms = (time.time() - start) * 1000\n",
    "        return True, latency_ms\n",
    "    except (socket.gaierror, socket.timeout):\n",
    "        return False, None\n",
    "\n",
    "# Test common hosts\n",
    "hosts = ['google.com', 'github.com', 'pypi.org']\n",
    "print(\"Testing connectivity:\")\n",
    "for host in hosts:\n",
    "    reachable, latency = check_connectivity(host)\n",
    "    if reachable:\n",
    "        print(f\"✓ {host}: {latency:.1f}ms\")\n",
    "    else:\n",
    "        print(f\"✗ {host}: Not reachable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Port Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if a specific port is open\n",
    "def check_port(host, port, timeout=3):\n",
    "    \"\"\"\n",
    "    Check if a port is open on a host.\n",
    "    \n",
    "    Args:\n",
    "        host: Hostname or IP\n",
    "        port: Port number\n",
    "        timeout: Timeout in seconds\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if port is open\n",
    "    \"\"\"\n",
    "    try:\n",
    "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        sock.settimeout(timeout)\n",
    "        result = sock.connect_ex((host, port))\n",
    "        sock.close()\n",
    "        return result == 0\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Test common ports\n",
    "print(\"\\nTesting ports on localhost:\")\n",
    "common_ports = {\n",
    "    80: 'HTTP',\n",
    "    443: 'HTTPS',\n",
    "    8888: 'Jupyter',\n",
    "    5432: 'PostgreSQL',\n",
    "    27017: 'MongoDB'\n",
    "}\n",
    "\n",
    "for port, service in common_ports.items():\n",
    "    is_open = check_port('localhost', port)\n",
    "    status = \"✓ Open\" if is_open else \"✗ Closed\"\n",
    "    print(f\"Port {port} ({service}): {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. HTTP Requests for Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch data from API with error handling\n",
    "def fetch_api_data(url, params=None, timeout=10):\n",
    "    \"\"\"\n",
    "    Safely fetch data from an API.\n",
    "    \n",
    "    Args:\n",
    "        url: API endpoint URL\n",
    "        params: Query parameters dict\n",
    "        timeout: Request timeout\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (success: bool, data: dict or str, error: str)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, params=params, timeout=timeout)\n",
    "        response.raise_for_status()  # Raise exception for 4xx/5xx\n",
    "        \n",
    "        # Try to parse as JSON\n",
    "        try:\n",
    "            data = response.json()\n",
    "        except:\n",
    "            data = response.text\n",
    "        \n",
    "        return True, data, None\n",
    "    \n",
    "    except requests.exceptions.Timeout:\n",
    "        return False, None, f\"Request timed out after {timeout}s\"\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        return False, None, \"Connection failed\"\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        return False, None, f\"HTTP error: {e}\"\n",
    "    except Exception as e:\n",
    "        return False, None, f\"Error: {e}\"\n",
    "\n",
    "# Example: Fetch public API data\n",
    "success, data, error = fetch_api_data('https://api.github.com/zen')\n",
    "if success:\n",
    "    print(f\"✓ API Response: {data}\")\n",
    "else:\n",
    "    print(f\"✗ Failed: {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Downloading Files Reliably"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download files with progress and resume capability\n",
    "def download_file(url, dest_path, chunk_size=8192):\n",
    "    \"\"\"\n",
    "    Download file with progress tracking.\n",
    "    \n",
    "    Args:\n",
    "        url: URL to download from\n",
    "        dest_path: Destination file path\n",
    "        chunk_size: Download chunk size in bytes\n",
    "    \n",
    "    Returns:\n",
    "        bool: Success status\n",
    "    \"\"\"\n",
    "    dest_path = Path(dest_path)\n",
    "    dest_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, stream=True, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "        \n",
    "        print(f\"Downloading: {dest_path.name}\")\n",
    "        print(f\"Size: {total_size / 1024 / 1024:.2f} MB\")\n",
    "        \n",
    "        downloaded = 0\n",
    "        with open(dest_path, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=chunk_size):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "                    downloaded += len(chunk)\n",
    "                    \n",
    "                    if total_size > 0:\n",
    "                        percent = (downloaded / total_size) * 100\n",
    "                        print(f\"\\rProgress: {percent:.1f}%\", end='')\n",
    "        \n",
    "        print(\"\\n✓ Download complete!\")\n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Download failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Example (small file)\n",
    "# download_file('https://example.com/file.csv', 'data/downloaded.csv')\n",
    "print(\"download_file() function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Practice Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: API Data Collector\n",
    "\n",
    "Create a tool to collect data from multiple APIs:\n",
    "1. Accept list of API URLs\n",
    "2. Fetch data from each with retries\n",
    "3. Save responses to JSON files\n",
    "4. Log success/failure rates\n",
    "\n",
    "**Hint**: Use `fetch_api_data()` with retry logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your solution here\n",
    "import json\n",
    "\n",
    "def collect_api_data(urls, output_dir, max_retries=3):\n",
    "    \"\"\"\n",
    "    Collect data from multiple APIs.\n",
    "    \n",
    "    Args:\n",
    "        urls: List of API URLs\n",
    "        output_dir: Directory to save responses\n",
    "        max_retries: Maximum retry attempts\n",
    "    \"\"\"\n",
    "    # TODO: Implement API data collector\n",
    "    pass\n",
    "\n",
    "# Test your collector\n",
    "# urls = ['https://api.example.com/data1', 'https://api.example.com/data2']\n",
    "# collect_api_data(urls, 'data/api_responses')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Service Health Checker\n",
    "\n",
    "Create a service health monitoring tool:\n",
    "1. Check if services are running (port check)\n",
    "2. Test HTTP endpoint response time\n",
    "3. Verify response status codes\n",
    "4. Alert if service is down\n",
    "\n",
    "**Hint**: Combine port checking and HTTP requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your solution here\n",
    "\n",
    "class ServiceHealthChecker:\n",
    "    \"\"\"\n",
    "    Monitor health of network services.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, services):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            services: List of (name, url, expected_status) tuples\n",
    "        \"\"\"\n",
    "        # TODO: Initialize checker\n",
    "        pass\n",
    "    \n",
    "    def check_all(self):\n",
    "        # TODO: Check all services\n",
    "        pass\n",
    "\n",
    "# Test your checker\n",
    "# services = [('API', 'http://localhost:8000/health', 200)]\n",
    "# checker = ServiceHealthChecker(services)\n",
    "# checker.check_all()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Dataset Downloader\n",
    "\n",
    "Create a robust dataset downloader:\n",
    "1. Download from list of URLs\n",
    "2. Resume interrupted downloads\n",
    "3. Verify file integrity (checksums)\n",
    "4. Extract archives automatically\n",
    "\n",
    "**Hint**: Use download_file() and add resume support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Your solution here\n",
    "\n",
    "def download_datasets(dataset_urls, output_dir):\n",
    "    \"\"\"\n",
    "    Download multiple datasets with resume support.\n",
    "    \n",
    "    Args:\n",
    "        dataset_urls: Dict of {name: url}\n",
    "        output_dir: Output directory\n",
    "    \"\"\"\n",
    "    # TODO: Implement dataset downloader\n",
    "    pass\n",
    "\n",
    "# Test your downloader\n",
    "# datasets = {'sample': 'https://example.com/dataset.zip'}\n",
    "# download_datasets(datasets, 'data/downloads')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Summary\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Connectivity Testing**\n",
    "   - Use `socket` for host/port checking\n",
    "   - Test before making requests\n",
    "   - Measure latency\n",
    "\n",
    "2. **HTTP Requests**\n",
    "   - Use `requests` library\n",
    "   - Handle timeouts and errors\n",
    "   - Parse JSON responses\n",
    "\n",
    "3. **File Downloads**\n",
    "   - Stream large files\n",
    "   - Track progress\n",
    "   - Handle interruptions\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In **Module 07: Scheduled Tasks & Automation**, you'll learn:\n",
    "- Windows Task Scheduler\n",
    "- Automated data collection\n",
    "- Cron-like scheduling\n",
    "- Email notifications\n",
    "\n",
    "---\n",
    "\n",
    "**Continue to Module 07** when ready!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
