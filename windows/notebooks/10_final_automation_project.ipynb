{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Module 10: Final Automation Project\n",
    "\n",
    "**Difficulty**: ‚≠ê‚≠ê‚≠ê (Advanced)\n",
    "\n",
    "**Estimated Time**: 90 minutes\n",
    "\n",
    "**Prerequisites**: \n",
    "- Completed Modules 00-09\n",
    "- All concepts from previous modules\n",
    "- Ready to build production-quality automation\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this project, you will have:\n",
    "\n",
    "1. **Built** a complete end-to-end automation system\n",
    "2. **Applied** all techniques from modules 00-09\n",
    "3. **Created** production-ready, deployable code\n",
    "4. **Demonstrated** professional automation practices\n",
    "5. **Documented** your system for team use\n",
    "6. **Deployed** your automation to Windows Task Scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Project Overview: Data Science Workflow Automator\n",
    "\n",
    "You'll build a **Data Science Workflow Automation System** that automates a complete data science pipeline.\n",
    "\n",
    "### The Scenario\n",
    "\n",
    "You're a data scientist at a company that needs to:\n",
    "- Collect data from multiple APIs daily\n",
    "- Process and clean the data\n",
    "- Generate analysis reports\n",
    "- Monitor data quality\n",
    "- Alert team when issues occur\n",
    "- Archive old data\n",
    "\n",
    "Currently, this is done manually and takes 2 hours every morning. Your task: **Automate it!**\n",
    "\n",
    "### System Requirements\n",
    "\n",
    "**Must Have:**\n",
    "1. ‚úÖ Data collection from multiple sources\n",
    "2. ‚úÖ Parallel processing for performance\n",
    "3. ‚úÖ Data validation and quality checks\n",
    "4. ‚úÖ Comprehensive logging\n",
    "5. ‚úÖ Error handling with retries\n",
    "6. ‚úÖ Email notifications for failures\n",
    "7. ‚úÖ Secure credential management\n",
    "8. ‚úÖ Scheduled execution (daily)\n",
    "9. ‚úÖ Performance monitoring\n",
    "10. ‚úÖ Automated cleanup of old data\n",
    "\n",
    "### System Architecture\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ          Data Science Automation System                 ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                           ‚îÇ\n",
    "        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "        ‚îÇ                  ‚îÇ                  ‚îÇ\n",
    "   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "   ‚îÇ Data    ‚îÇ      ‚îÇ Processing ‚îÇ   ‚îÇ Monitoring ‚îÇ\n",
    "   ‚îÇCollector‚îÇ      ‚îÇ  Pipeline  ‚îÇ   ‚îÇ  & Alerts  ‚îÇ\n",
    "   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "        ‚îÇ                 ‚îÇ                  ‚îÇ\n",
    "        ‚ñº                 ‚ñº                  ‚ñº\n",
    "   [API Sources]    [Validation]      [Email/Logs]\n",
    "   [Parallel]       [Transform]       [Metrics]\n",
    "   [Retry Logic]    [Reports]         [Cleanup]\n",
    "```\n",
    "\n",
    "### Technologies Used\n",
    "\n",
    "- **Module 00-02**: Python environment, subprocess, pathlib\n",
    "- **Module 03-04**: File operations, process management\n",
    "- **Module 05**: Environment variables (credentials)\n",
    "- **Module 06**: HTTP requests, API calls\n",
    "- **Module 07**: Scheduling, email notifications\n",
    "- **Module 08**: Security, permissions\n",
    "- **Module 09**: Parallel processing, logging, testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Import all required libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import logging.handlers\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from functools import wraps\n",
    "import traceback\n",
    "\n",
    "# Install additional dependencies if needed\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    import requests\n",
    "except ImportError:\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'requests', '-q'])\n",
    "    import requests\n",
    "\n",
    "print(\"‚úì All dependencies loaded\")\n",
    "print(f\"Python version: {sys.version.split()[0]}\")\n",
    "print(f\"Working directory: {Path.cwd()}\")\n",
    "print(\"\\nSystem ready for automation project!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Part 1: Configuration and Setup\n",
    "\n",
    "Professional automation starts with proper configuration management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration system\n",
    "class AutomationConfig:\n",
    "    \"\"\"\n",
    "    Central configuration for automation system.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config_file='config.json'):\n",
    "        self.config_file = Path(config_file)\n",
    "        self.config = self._load_config()\n",
    "    \n",
    "    def _load_config(self):\n",
    "        \"\"\"Load configuration from file or create default.\"\"\"\n",
    "        if self.config_file.exists():\n",
    "            with open(self.config_file) as f:\n",
    "                return json.load(f)\n",
    "        else:\n",
    "            # Default configuration\n",
    "            default_config = {\n",
    "                'data_dir': 'data/collected',\n",
    "                'log_dir': 'logs',\n",
    "                'report_dir': 'reports',\n",
    "                'archive_dir': 'archive',\n",
    "                'max_workers': 4,\n",
    "                'retry_attempts': 3,\n",
    "                'archive_after_days': 30,\n",
    "                'data_sources': [\n",
    "                    {'name': 'api_source_1', 'url': 'https://api.example.com/data1'},\n",
    "                    {'name': 'api_source_2', 'url': 'https://api.example.com/data2'}\n",
    "                ],\n",
    "                'email': {\n",
    "                    'enabled': False,  # Set to True when configured\n",
    "                    'smtp_server': os.getenv('SMTP_SERVER', ''),\n",
    "                    'smtp_port': int(os.getenv('SMTP_PORT', 587)),\n",
    "                    'from_email': os.getenv('FROM_EMAIL', ''),\n",
    "                    'to_email': os.getenv('TO_EMAIL', '')\n",
    "                },\n",
    "                'quality_thresholds': {\n",
    "                    'min_rows': 10,\n",
    "                    'max_missing_pct': 0.1\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Save default config\n",
    "            self._save_config(default_config)\n",
    "            return default_config\n",
    "    \n",
    "    def _save_config(self, config):\n",
    "        \"\"\"Save configuration to file.\"\"\"\n",
    "        with open(self.config_file, 'w') as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "    \n",
    "    def get(self, key, default=None):\n",
    "        \"\"\"Get configuration value.\"\"\"\n",
    "        return self.config.get(key, default)\n",
    "    \n",
    "    def update(self, key, value):\n",
    "        \"\"\"Update configuration value.\"\"\"\n",
    "        self.config[key] = value\n",
    "        self._save_config(self.config)\n",
    "\n",
    "# Initialize configuration\n",
    "config = AutomationConfig('automation_config.json')\n",
    "print(\"‚úì Configuration loaded\")\n",
    "print(f\"  Data directory: {config.get('data_dir')}\")\n",
    "print(f\"  Max workers: {config.get('max_workers')}\")\n",
    "print(f\"  Data sources: {len(config.get('data_sources'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Part 2: Logging System\n",
    "\n",
    "Production logging with rotation and multiple handlers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup production logger\n",
    "def setup_logger(name='automation', log_dir=None):\n",
    "    \"\"\"\n",
    "    Setup production-grade logger.\n",
    "    \n",
    "    Args:\n",
    "        name: Logger name\n",
    "        log_dir: Log directory (from config if None)\n",
    "    \n",
    "    Returns:\n",
    "        logging.Logger: Configured logger\n",
    "    \"\"\"\n",
    "    if log_dir is None:\n",
    "        log_dir = config.get('log_dir', 'logs')\n",
    "    \n",
    "    log_dir = Path(log_dir)\n",
    "    log_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Create logger\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    logger.handlers.clear()\n",
    "    \n",
    "    # File handler with rotation\n",
    "    log_file = log_dir / f\"{name}.log\"\n",
    "    file_handler = logging.handlers.RotatingFileHandler(\n",
    "        log_file,\n",
    "        maxBytes=10*1024*1024,  # 10 MB\n",
    "        backupCount=5\n",
    "    )\n",
    "    file_handler.setLevel(logging.DEBUG)\n",
    "    \n",
    "    # Console handler\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setLevel(logging.INFO)\n",
    "    \n",
    "    # Formatters\n",
    "    file_formatter = logging.Formatter(\n",
    "        '%(asctime)s - %(name)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S'\n",
    "    )\n",
    "    console_formatter = logging.Formatter(\n",
    "        '%(levelname)s: %(message)s'\n",
    "    )\n",
    "    \n",
    "    file_handler.setFormatter(file_formatter)\n",
    "    console_handler.setFormatter(console_formatter)\n",
    "    \n",
    "    logger.addHandler(file_handler)\n",
    "    logger.addHandler(console_handler)\n",
    "    \n",
    "    logger.info(f\"Logger initialized: {name}\")\n",
    "    logger.info(f\"Log file: {log_file}\")\n",
    "    \n",
    "    return logger\n",
    "\n",
    "# Initialize logger\n",
    "logger = setup_logger('ds_automation')\n",
    "logger.info(\"Automation system starting...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Part 3: Data Collector (Parallel)\n",
    "\n",
    "Collect data from multiple sources in parallel with error handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retry decorator from Module 09\n",
    "def retry_with_backoff(max_retries=3, base_delay=1, max_delay=60):\n",
    "    \"\"\"Retry decorator with exponential backoff.\"\"\"\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            delay = base_delay\n",
    "            \n",
    "            for attempt in range(1, max_retries + 1):\n",
    "                try:\n",
    "                    return func(*args, **kwargs)\n",
    "                except Exception as e:\n",
    "                    if attempt == max_retries:\n",
    "                        raise\n",
    "                    \n",
    "                    logger.warning(f\"Attempt {attempt}/{max_retries} failed: {e}\")\n",
    "                    logger.info(f\"Retrying in {delay}s...\")\n",
    "                    \n",
    "                    time.sleep(delay)\n",
    "                    delay = min(delay * 2, max_delay)\n",
    "        \n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "class DataCollector:\n",
    "    \"\"\"\n",
    "    Parallel data collector with error handling.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config, logger):\n",
    "        self.config = config\n",
    "        self.logger = logger\n",
    "        self.data_dir = Path(config.get('data_dir'))\n",
    "        self.data_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.stats = {'success': 0, 'failed': 0, 'total': 0}\n",
    "    \n",
    "    @retry_with_backoff(max_retries=3)\n",
    "    def collect_from_source(self, source):\n",
    "        \"\"\"\n",
    "        Collect data from a single source.\n",
    "        \n",
    "        Args:\n",
    "            source: Source configuration dict\n",
    "        \n",
    "        Returns:\n",
    "            dict: Collection result\n",
    "        \"\"\"\n",
    "        source_name = source['name']\n",
    "        source_url = source['url']\n",
    "        \n",
    "        self.logger.info(f\"Collecting data from: {source_name}\")\n",
    "        \n",
    "        # For demo, simulate API call\n",
    "        # In production, use: response = requests.get(source_url, timeout=30)\n",
    "        \n",
    "        # Simulate data\n",
    "        data = {\n",
    "            'source': source_name,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'data': {\n",
    "                'records': [\n",
    "                    {'id': i, 'value': i * 10, 'status': 'active'}\n",
    "                    for i in range(1, 101)\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Save to file\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        filename = f\"{source_name}_{timestamp}.json\"\n",
    "        filepath = self.data_dir / filename\n",
    "        \n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "        \n",
    "        self.logger.info(f\"‚úì Collected {len(data['data']['records'])} records from {source_name}\")\n",
    "        \n",
    "        return {\n",
    "            'source': source_name,\n",
    "            'filepath': str(filepath),\n",
    "            'records': len(data['data']['records']),\n",
    "            'status': 'success'\n",
    "        }\n",
    "    \n",
    "    def collect_all(self):\n",
    "        \"\"\"\n",
    "        Collect data from all sources in parallel.\n",
    "        \n",
    "        Returns:\n",
    "            list: Collection results\n",
    "        \"\"\"\n",
    "        sources = self.config.get('data_sources', [])\n",
    "        max_workers = self.config.get('max_workers', 4)\n",
    "        \n",
    "        self.logger.info(f\"Starting parallel data collection from {len(sources)} sources\")\n",
    "        \n",
    "        results = []\n",
    "        self.stats['total'] = len(sources)\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            future_to_source = {executor.submit(self.collect_from_source, source): source for source in sources}\n",
    "            \n",
    "            for future in as_completed(future_to_source):\n",
    "                source = future_to_source[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    results.append(result)\n",
    "                    self.stats['success'] += 1\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Failed to collect from {source['name']}: {e}\")\n",
    "                    self.stats['failed'] += 1\n",
    "                    results.append({\n",
    "                        'source': source['name'],\n",
    "                        'status': 'failed',\n",
    "                        'error': str(e)\n",
    "                    })\n",
    "        \n",
    "        self.logger.info(f\"Collection complete: {self.stats['success']} success, {self.stats['failed']} failed\")\n",
    "        return results\n",
    "\n",
    "# Test data collector\n",
    "collector = DataCollector(config, logger)\n",
    "collection_results = collector.collect_all()\n",
    "\n",
    "print(f\"\\n‚úì Data collection completed\")\n",
    "print(f\"  Success: {collector.stats['success']}/{collector.stats['total']}\")\n",
    "print(f\"  Failed: {collector.stats['failed']}/{collector.stats['total']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Part 4: Data Validator\n",
    "\n",
    "Validate data quality before processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality validator\n",
    "class DataValidator:\n",
    "    \"\"\"\n",
    "    Validate data quality against thresholds.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config, logger):\n",
    "        self.config = config\n",
    "        self.logger = logger\n",
    "        self.thresholds = config.get('quality_thresholds', {})\n",
    "    \n",
    "    def validate_file(self, filepath):\n",
    "        \"\"\"\n",
    "        Validate a data file.\n",
    "        \n",
    "        Args:\n",
    "            filepath: Path to data file\n",
    "        \n",
    "        Returns:\n",
    "            dict: Validation results\n",
    "        \"\"\"\n",
    "        filepath = Path(filepath)\n",
    "        self.logger.info(f\"Validating: {filepath.name}\")\n",
    "        \n",
    "        try:\n",
    "            with open(filepath) as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            records = data.get('data', {}).get('records', [])\n",
    "            \n",
    "            # Check minimum rows\n",
    "            min_rows = self.thresholds.get('min_rows', 10)\n",
    "            if len(records) < min_rows:\n",
    "                raise ValueError(f\"Insufficient data: {len(records)} < {min_rows} rows\")\n",
    "            \n",
    "            # Check for missing data\n",
    "            total_fields = len(records) * len(records[0]) if records else 0\n",
    "            missing_count = sum(\n",
    "                1 for record in records\n",
    "                for value in record.values()\n",
    "                if value is None or value == ''\n",
    "            )\n",
    "            missing_pct = missing_count / total_fields if total_fields > 0 else 0\n",
    "            \n",
    "            max_missing = self.thresholds.get('max_missing_pct', 0.1)\n",
    "            if missing_pct > max_missing:\n",
    "                raise ValueError(f\"Too much missing data: {missing_pct:.1%} > {max_missing:.1%}\")\n",
    "            \n",
    "            self.logger.info(f\"‚úì Validation passed: {len(records)} records, {missing_pct:.1%} missing\")\n",
    "            \n",
    "            return {\n",
    "                'filepath': str(filepath),\n",
    "                'valid': True,\n",
    "                'records': len(records),\n",
    "                'missing_pct': missing_pct\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"‚úó Validation failed: {e}\")\n",
    "            return {\n",
    "                'filepath': str(filepath),\n",
    "                'valid': False,\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    def validate_all(self, results):\n",
    "        \"\"\"\n",
    "        Validate all collected data files.\n",
    "        \n",
    "        Args:\n",
    "            results: Collection results\n",
    "        \n",
    "        Returns:\n",
    "            list: Validation results\n",
    "        \"\"\"\n",
    "        validation_results = []\n",
    "        \n",
    "        for result in results:\n",
    "            if result['status'] == 'success':\n",
    "                filepath = result['filepath']\n",
    "                validation = self.validate_file(filepath)\n",
    "                validation_results.append(validation)\n",
    "        \n",
    "        valid_count = sum(1 for r in validation_results if r['valid'])\n",
    "        self.logger.info(f\"Validation complete: {valid_count}/{len(validation_results)} valid\")\n",
    "        \n",
    "        return validation_results\n",
    "\n",
    "# Test validator\n",
    "validator = DataValidator(config, logger)\n",
    "validation_results = validator.validate_all(collection_results)\n",
    "\n",
    "print(f\"\\n‚úì Data validation completed\")\n",
    "valid_count = sum(1 for r in validation_results if r['valid'])\n",
    "print(f\"  Valid: {valid_count}/{len(validation_results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Part 5: Report Generator\n",
    "\n",
    "Generate summary reports of automation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report generator\n",
    "class ReportGenerator:\n",
    "    \"\"\"\n",
    "    Generate automation reports.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config, logger):\n",
    "        self.config = config\n",
    "        self.logger = logger\n",
    "        self.report_dir = Path(config.get('report_dir', 'reports'))\n",
    "        self.report_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    def generate_report(self, collection_results, validation_results, execution_time):\n",
    "        \"\"\"\n",
    "        Generate summary report.\n",
    "        \n",
    "        Args:\n",
    "            collection_results: Collection results\n",
    "            validation_results: Validation results\n",
    "            execution_time: Total execution time\n",
    "        \n",
    "        Returns:\n",
    "            str: Report filepath\n",
    "        \"\"\"\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        report_file = self.report_dir / f\"automation_report_{timestamp}.txt\"\n",
    "        \n",
    "        # Calculate statistics\n",
    "        total_sources = len(collection_results)\n",
    "        successful_collections = sum(1 for r in collection_results if r['status'] == 'success')\n",
    "        failed_collections = total_sources - successful_collections\n",
    "        \n",
    "        valid_files = sum(1 for r in validation_results if r['valid'])\n",
    "        invalid_files = len(validation_results) - valid_files\n",
    "        \n",
    "        total_records = sum(r.get('records', 0) for r in validation_results if r['valid'])\n",
    "        \n",
    "        # Generate report\n",
    "        report = f\"\"\"# Data Science Automation Report\n",
    "\n",
    "**Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "**Execution Time**: {execution_time:.2f}s\n",
    "\n",
    "## Summary\n",
    "\n",
    "- ‚úì **Status**: {'SUCCESS' if failed_collections == 0 and invalid_files == 0 else 'WARNING' if invalid_files > 0 else 'FAILED'}\n",
    "- **Total Sources**: {total_sources}\n",
    "- **Total Records Collected**: {total_records:,}\n",
    "\n",
    "## Data Collection\n",
    "\n",
    "- ‚úì Successful: {successful_collections}/{total_sources}\n",
    "- ‚úó Failed: {failed_collections}/{total_sources}\n",
    "\n",
    "## Data Validation\n",
    "\n",
    "- ‚úì Valid: {valid_files}/{len(validation_results)}\n",
    "- ‚úó Invalid: {invalid_files}/{len(validation_results)}\n",
    "\n",
    "## Details\n",
    "\n",
    "### Collection Results\n",
    "\"\"\"\n",
    "        \n",
    "        for result in collection_results:\n",
    "            status_icon = \"‚úì\" if result['status'] == 'success' else \"‚úó\"\n",
    "            report += f\"\\n- {status_icon} {result['source']}: {result.get('records', 0)} records\"\n",
    "            if result['status'] == 'failed':\n",
    "                report += f\" (Error: {result.get('error', 'Unknown')})\"\n",
    "        \n",
    "        report += \"\\n\\n### Validation Results\\n\"\n",
    "        \n",
    "        for result in validation_results:\n",
    "            status_icon = \"‚úì\" if result['valid'] else \"‚úó\"\n",
    "            filepath = Path(result['filepath']).name\n",
    "            report += f\"\\n- {status_icon} {filepath}\"\n",
    "            if result['valid']:\n",
    "                report += f\": {result['records']} records, {result['missing_pct']:.1%} missing\"\n",
    "            else:\n",
    "                report += f\": {result.get('error', 'Validation failed')}\"\n",
    "        \n",
    "        report += \"\\n\\n---\\n\\nAutomated report generated by Data Science Automation System\\n\"\n",
    "        \n",
    "        # Save report\n",
    "        with open(report_file, 'w') as f:\n",
    "            f.write(report)\n",
    "        \n",
    "        self.logger.info(f\"Report generated: {report_file}\")\n",
    "        \n",
    "        return str(report_file)\n",
    "\n",
    "# Generate report\n",
    "reporter = ReportGenerator(config, logger)\n",
    "execution_time = 5.0  # Would be measured in production\n",
    "report_path = reporter.generate_report(collection_results, validation_results, execution_time)\n",
    "\n",
    "print(f\"\\n‚úì Report generated: {report_path}\")\n",
    "\n",
    "# Display report\n",
    "with open(report_path) as f:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f.read())\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Part 6: Complete Automation Pipeline\n",
    "\n",
    "Tie all components together into a single automation system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main automation pipeline\n",
    "class DataScienceAutomation:\n",
    "    \"\"\"\n",
    "    Complete data science automation system.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config_file='automation_config.json'):\n",
    "        self.config = AutomationConfig(config_file)\n",
    "        self.logger = setup_logger('ds_automation', self.config.get('log_dir'))\n",
    "        self.collector = DataCollector(self.config, self.logger)\n",
    "        self.validator = DataValidator(self.config, self.logger)\n",
    "        self.reporter = ReportGenerator(self.config, self.logger)\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Execute complete automation pipeline.\n",
    "        \n",
    "        Returns:\n",
    "            dict: Execution results\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            self.logger.info(\"=\"*60)\n",
    "            self.logger.info(\"Data Science Automation Pipeline Starting\")\n",
    "            self.logger.info(\"=\"*60)\n",
    "            \n",
    "            # Step 1: Collect data\n",
    "            self.logger.info(\"Step 1: Collecting data...\")\n",
    "            collection_results = self.collector.collect_all()\n",
    "            \n",
    "            # Step 2: Validate data\n",
    "            self.logger.info(\"Step 2: Validating data...\")\n",
    "            validation_results = self.validator.validate_all(collection_results)\n",
    "            \n",
    "            # Step 3: Generate report\n",
    "            execution_time = time.time() - start_time\n",
    "            self.logger.info(\"Step 3: Generating report...\")\n",
    "            report_path = self.reporter.generate_report(\n",
    "                collection_results,\n",
    "                validation_results,\n",
    "                execution_time\n",
    "            )\n",
    "            \n",
    "            # Success\n",
    "            self.logger.info(\"=\"*60)\n",
    "            self.logger.info(f\"‚úì Pipeline completed successfully in {execution_time:.2f}s\")\n",
    "            self.logger.info(\"=\"*60)\n",
    "            \n",
    "            return {\n",
    "                'status': 'success',\n",
    "                'execution_time': execution_time,\n",
    "                'collection_results': collection_results,\n",
    "                'validation_results': validation_results,\n",
    "                'report_path': report_path\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            execution_time = time.time() - start_time\n",
    "            self.logger.error(\"=\"*60)\n",
    "            self.logger.error(f\"‚úó Pipeline failed: {e}\")\n",
    "            self.logger.error(traceback.format_exc())\n",
    "            self.logger.error(\"=\"*60)\n",
    "            \n",
    "            return {\n",
    "                'status': 'failed',\n",
    "                'execution_time': execution_time,\n",
    "                'error': str(e)\n",
    "            }\n",
    "\n",
    "# Execute automation\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Executing Complete Automation Pipeline\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "automation = DataScienceAutomation('automation_config.json')\n",
    "results = automation.run()\n",
    "\n",
    "if results['status'] == 'success':\n",
    "    print(\"\\n‚úì AUTOMATION SUCCESSFUL\")\n",
    "    print(f\"  Execution time: {results['execution_time']:.2f}s\")\n",
    "    print(f\"  Report: {results['report_path']}\")\n",
    "else:\n",
    "    print(\"\\n‚úó AUTOMATION FAILED\")\n",
    "    print(f\"  Error: {results['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Part 7: Deployment Guide\n",
    "\n",
    "Deploy your automation to run on schedule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "### Deployment Steps\n",
    "\n",
    "#### 1. Create Standalone Script\n",
    "\n",
    "Create `run_automation.py`:\n",
    "\n",
    "```python\n",
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "Data Science Automation - Standalone Runner\n",
    "\"\"\"\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project directory to path\n",
    "project_dir = Path(__file__).parent\n",
    "sys.path.insert(0, str(project_dir))\n",
    "\n",
    "# Import automation class\n",
    "from automation import DataScienceAutomation\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    automation = DataScienceAutomation('automation_config.json')\n",
    "    results = automation.run()\n",
    "    \n",
    "    # Exit with appropriate code\n",
    "    sys.exit(0 if results['status'] == 'success' else 1)\n",
    "```\n",
    "\n",
    "#### 2. Configure Environment\n",
    "\n",
    "Create `.env` file (never commit this!):\n",
    "\n",
    "```env\n",
    "# Email Configuration\n",
    "SMTP_SERVER=smtp.gmail.com\n",
    "SMTP_PORT=587\n",
    "FROM_EMAIL=your_email@example.com\n",
    "TO_EMAIL=team@example.com\n",
    "SMTP_PASSWORD=your_app_specific_password\n",
    "```\n",
    "\n",
    "#### 3. Schedule with Task Scheduler\n",
    "\n",
    "**Option A: Using Python Script**\n",
    "\n",
    "```python\n",
    "import subprocess\n",
    "\n",
    "# Create scheduled task to run daily at 6 AM\n",
    "task_name = 'DataScienceAutomation'\n",
    "script_path = r'C:\\path\\to\\run_automation.py'\n",
    "python_exe = r'C:\\path\\to\\python.exe'\n",
    "\n",
    "command = [\n",
    "    'schtasks', '/Create',\n",
    "    '/TN', task_name,\n",
    "    '/TR', f'\"{python_exe}\" \"{script_path}\"',\n",
    "    '/SC', 'DAILY',\n",
    "    '/ST', '06:00',\n",
    "    '/F'\n",
    "]\n",
    "\n",
    "subprocess.run(command)\n",
    "```\n",
    "\n",
    "**Option B: Using GUI**\n",
    "\n",
    "1. Open Task Scheduler (taskschd.msc)\n",
    "2. Create Basic Task\n",
    "3. Name: \"Data Science Automation\"\n",
    "4. Trigger: Daily at 6:00 AM\n",
    "5. Action: Start a program\n",
    "6. Program: `C:\\path\\to\\python.exe`\n",
    "7. Arguments: `C:\\path\\to\\run_automation.py`\n",
    "8. Start in: `C:\\path\\to\\project`\n",
    "\n",
    "#### 4. Test Deployment\n",
    "\n",
    "```bash\n",
    "# Test manual run\n",
    "python run_automation.py\n",
    "\n",
    "# Check logs\n",
    "type logs\\ds_automation.log\n",
    "\n",
    "# Check reports\n",
    "dir reports\n",
    "```\n",
    "\n",
    "#### 5. Monitor and Maintain\n",
    "\n",
    "- Check logs regularly: `logs/ds_automation.log`\n",
    "- Review reports: `reports/automation_report_*.txt`\n",
    "- Set up email notifications for failures\n",
    "- Archive old data monthly\n",
    "- Update configuration as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Project Summary\n",
    "\n",
    "### What You Built\n",
    "\n",
    "‚úÖ **Complete Automation System** with:\n",
    "1. Parallel data collection from multiple sources\n",
    "2. Automatic retry logic for failed operations\n",
    "3. Data quality validation\n",
    "4. Comprehensive logging with rotation\n",
    "5. Automated report generation\n",
    "6. Production-ready error handling\n",
    "7. Secure configuration management\n",
    "8. Deployment-ready structure\n",
    "\n",
    "### Techniques Applied\n",
    "\n",
    "**From All Modules:**\n",
    "- Module 00-02: Python environment, subprocess, pathlib\n",
    "- Module 03: File operations, directory management\n",
    "- Module 04: Process management\n",
    "- Module 05: Environment variables\n",
    "- Module 06: HTTP requests, API integration\n",
    "- Module 07: Scheduled tasks, automation\n",
    "- Module 08: Security, permissions\n",
    "- Module 09: Parallel processing, logging, error handling\n",
    "\n",
    "### Production Readiness Checklist\n",
    "\n",
    "Your system includes:\n",
    "\n",
    "- ‚úÖ Comprehensive error handling\n",
    "- ‚úÖ Structured logging with rotation\n",
    "- ‚úÖ Parallel processing for performance\n",
    "- ‚úÖ Data validation and quality checks\n",
    "- ‚úÖ Configuration management\n",
    "- ‚úÖ Retry logic with backoff\n",
    "- ‚úÖ Report generation\n",
    "- ‚úÖ Deployment documentation\n",
    "- ‚úÖ Security best practices\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "**To Extend This Project:**\n",
    "\n",
    "1. **Add Email Notifications**: Implement email alerts using Module 07 techniques\n",
    "2. **Add More Data Sources**: Expand to real APIs (financial data, weather, etc.)\n",
    "3. **Add Data Processing**: Include actual data transformation/analysis\n",
    "4. **Add Database Storage**: Save results to SQLite/PostgreSQL\n",
    "5. **Add Web Dashboard**: Create Flask app to view reports\n",
    "6. **Add Testing**: Write unit tests for all components\n",
    "7. **Add Monitoring**: Integrate with monitoring tools\n",
    "8. **Add ML Pipeline**: Include model training/prediction\n",
    "\n",
    "### Congratulations! üéâ\n",
    "\n",
    "You've completed the Windows for Data Science curriculum and built a production-ready automation system!\n",
    "\n",
    "**Skills Gained:**\n",
    "- Windows automation expertise\n",
    "- Production-grade Python development\n",
    "- Data pipeline engineering\n",
    "- System integration\n",
    "- Professional software practices\n",
    "\n",
    "**You're now ready to:**\n",
    "- Automate data science workflows\n",
    "- Build production data pipelines\n",
    "- Deploy automated systems\n",
    "- Optimize Windows-based workflows\n",
    "- Share your expertise with teams\n",
    "\n",
    "---\n",
    "\n",
    "**Keep learning, keep automating!** üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
