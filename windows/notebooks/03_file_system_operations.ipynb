{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 03: File System Operations for Data Science\n",
    "\n",
    "**Difficulty**: ⭐⭐ (Intermediate)\n",
    "\n",
    "**Estimated Time**: 75 minutes\n",
    "\n",
    "**Prerequisites**: \n",
    "- Completed Modules 00-02\n",
    "- Understanding of pathlib basics\n",
    "- Familiarity with file I/O operations\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. **Perform** batch file operations on hundreds of files efficiently\n",
    "2. **Search** for files using patterns and filters\n",
    "3. **Monitor** file system changes for data pipelines\n",
    "4. **Organize** datasets with automated file management\n",
    "5. **Work with** archives and compressed files\n",
    "6. **Handle** file permissions and attributes on Windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction: Why File Operations Matter for Data Scientists\n",
    "\n",
    "Data scientists work with files constantly:\n",
    "- **Datasets**: CSV, JSON, Parquet, HDF5 files\n",
    "- **Models**: Pickle files, SavedModel directories, checkpoint files\n",
    "- **Results**: Plots, reports, logs, predictions\n",
    "- **Code**: Notebooks, scripts, configuration files\n",
    "\n",
    "### Real-World Scenarios\n",
    "\n",
    "**Scenario 1: Daily Data Ingestion**\n",
    "- 50 CSV files arrive daily in a folder\n",
    "- Need to process new files only (not already processed)\n",
    "- Move processed files to archive\n",
    "- Generate summary report\n",
    "\n",
    "**Scenario 2: Model Checkpoint Management**\n",
    "- Deep learning training saves checkpoints every epoch\n",
    "- 100+ checkpoint files accumulate\n",
    "- Need to keep only best 5 and latest 3\n",
    "- Delete others to save disk space\n",
    "\n",
    "**Scenario 3: Dataset Organization**\n",
    "- Downloaded dataset has messy structure\n",
    "- Images scattered across folders\n",
    "- Need to reorganize by category\n",
    "- Create train/validation/test splits\n",
    "\n",
    "This module teaches you to automate all these tasks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Import required libraries\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import glob\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Advanced pathlib Techniques\n",
    "\n",
    "While we covered pathlib basics in Module 00, let's explore advanced features essential for data science workflows.\n",
    "\n",
    "### Why pathlib Over os.path?\n",
    "\n",
    "**Old way (os.path)**:\n",
    "```python\n",
    "import os\n",
    "data_dir = os.path.join(os.getcwd(), 'data', 'raw')\n",
    "files = [os.path.join(data_dir, f) for f in os.listdir(data_dir) \n",
    "         if os.path.isfile(os.path.join(data_dir, f))]\n",
    "```\n",
    "\n",
    "**New way (pathlib)**:\n",
    "```python\n",
    "from pathlib import Path\n",
    "data_dir = Path.cwd() / 'data' / 'raw'\n",
    "files = [f for f in data_dir.iterdir() if f.is_file()]\n",
    "```\n",
    "\n",
    "**Benefits**:\n",
    "- More readable and intuitive\n",
    "- Object-oriented (paths are objects with methods)\n",
    "- Cross-platform (handles Windows/Linux path differences)\n",
    "- Chainable operations\n",
    "- Better error messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Finding Files with Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all files matching a pattern\n",
    "# Essential for working with datasets split across multiple files\n",
    "\n",
    "def find_files_by_pattern(directory, pattern='*'):\n",
    "    \"\"\"\n",
    "    Find all files matching a pattern in directory (recursive).\n",
    "    \n",
    "    Args:\n",
    "        directory: Path to search in\n",
    "        pattern: Glob pattern (e.g., '*.csv', 'data_*.json')\n",
    "    \n",
    "    Returns:\n",
    "        list: List of Path objects matching pattern\n",
    "    \"\"\"\n",
    "    directory = Path(directory)\n",
    "    \n",
    "    # ** means \"any subdirectory\"\n",
    "    return sorted(directory.rglob(pattern))\n",
    "\n",
    "# Example: Find all .ipynb files in project\n",
    "project_root = Path.cwd().parent\n",
    "notebooks = find_files_by_pattern(project_root, '*.ipynb')\n",
    "\n",
    "print(f\"Found {len(notebooks)} notebook(s):\")\n",
    "for nb in notebooks[:5]:  # Show first 5\n",
    "    # Get path relative to project root for clean display\n",
    "    rel_path = nb.relative_to(project_root)\n",
    "    print(f\"  {rel_path}\")\n",
    "\n",
    "if len(notebooks) > 5:\n",
    "    print(f\"  ... and {len(notebooks) - 5} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Filtering Files by Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter files by size, date, extension, etc.\n",
    "# Useful for finding large datasets, recent files, etc.\n",
    "\n",
    "def filter_files(directory, \n",
    "                 min_size=None,  # bytes\n",
    "                 max_size=None,  # bytes\n",
    "                 extensions=None,  # list of extensions\n",
    "                 modified_after=None,  # datetime object\n",
    "                 modified_before=None):  # datetime object\n",
    "    \"\"\"\n",
    "    Filter files by multiple criteria.\n",
    "    \n",
    "    Args:\n",
    "        directory: Directory to search\n",
    "        min_size: Minimum file size in bytes\n",
    "        max_size: Maximum file size in bytes\n",
    "        extensions: List of extensions (e.g., ['.csv', '.json'])\n",
    "        modified_after: Files modified after this datetime\n",
    "        modified_before: Files modified before this datetime\n",
    "    \n",
    "    Returns:\n",
    "        list: Filtered list of Path objects\n",
    "    \"\"\"\n",
    "    directory = Path(directory)\n",
    "    results = []\n",
    "    \n",
    "    for file_path in directory.rglob('*'):\n",
    "        if not file_path.is_file():\n",
    "            continue\n",
    "        \n",
    "        # Check size\n",
    "        file_size = file_path.stat().st_size\n",
    "        if min_size and file_size < min_size:\n",
    "            continue\n",
    "        if max_size and file_size > max_size:\n",
    "            continue\n",
    "        \n",
    "        # Check extension\n",
    "        if extensions and file_path.suffix.lower() not in extensions:\n",
    "            continue\n",
    "        \n",
    "        # Check modification time\n",
    "        mod_time = datetime.fromtimestamp(file_path.stat().st_mtime)\n",
    "        if modified_after and mod_time < modified_after:\n",
    "            continue\n",
    "        if modified_before and mod_time > modified_before:\n",
    "            continue\n",
    "        \n",
    "        results.append(file_path)\n",
    "    \n",
    "    return sorted(results)\n",
    "\n",
    "# Example: Find large files (>100KB) modified in last 7 days\n",
    "seven_days_ago = datetime.now() - timedelta(days=7)\n",
    "large_recent_files = filter_files(\n",
    "    project_root,\n",
    "    min_size=100 * 1024,  # 100KB\n",
    "    modified_after=seven_days_ago\n",
    ")\n",
    "\n",
    "print(f\"\\nLarge files (>100KB) modified in last 7 days: {len(large_recent_files)}\")\n",
    "for file_path in large_recent_files[:3]:\n",
    "    size_kb = file_path.stat().st_size / 1024\n",
    "    mod_time = datetime.fromtimestamp(file_path.stat().st_mtime)\n",
    "    print(f\"  {file_path.name}: {size_kb:.1f}KB, modified {mod_time.strftime('%Y-%m-%d')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Batch File Operations\n",
    "\n",
    "When working with datasets, you often need to process hundreds or thousands of files. Doing this manually is error-prone and time-consuming.\n",
    "\n",
    "### Common Batch Operations in Data Science\n",
    "\n",
    "1. **Renaming**: Standardize filenames (e.g., `IMG001.jpg` → `train_001.jpg`)\n",
    "2. **Moving**: Organize files into folders (e.g., by date, category)\n",
    "3. **Copying**: Create backups, duplicate for different experiments\n",
    "4. **Deleting**: Remove old checkpoints, temporary files\n",
    "\n",
    "### Safety Best Practices\n",
    "\n",
    "Before batch operations:\n",
    "1. ✅ **Test on small subset first** (5-10 files)\n",
    "2. ✅ **Create backups** of important data\n",
    "3. ✅ **Log all operations** for audit trail\n",
    "4. ✅ **Use dry-run mode** to preview changes\n",
    "5. ✅ **Validate** after operation completes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Batch Renaming with Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch rename files with pattern-based rules\n",
    "# Common for preparing datasets for ML training\n",
    "\n",
    "def batch_rename(directory, pattern, replacement, dry_run=True):\n",
    "    \"\"\"\n",
    "    Rename multiple files using pattern matching.\n",
    "    \n",
    "    Args:\n",
    "        directory: Directory containing files\n",
    "        pattern: String pattern to find in filenames\n",
    "        replacement: String to replace pattern with\n",
    "        dry_run: If True, only show what would be renamed (default: True)\n",
    "    \n",
    "    Returns:\n",
    "        int: Number of files renamed (or would be renamed if dry_run)\n",
    "    \"\"\"\n",
    "    directory = Path(directory)\n",
    "    renamed_count = 0\n",
    "    \n",
    "    print(f\"{'DRY RUN: ' if dry_run else ''}Renaming files in {directory}\")\n",
    "    print(f\"Pattern: '{pattern}' → '{replacement}'\")\n",
    "    print()\n",
    "    \n",
    "    for file_path in directory.iterdir():\n",
    "        if not file_path.is_file():\n",
    "            continue\n",
    "        \n",
    "        # Check if pattern exists in filename\n",
    "        if pattern in file_path.name:\n",
    "            new_name = file_path.name.replace(pattern, replacement)\n",
    "            new_path = file_path.parent / new_name\n",
    "            \n",
    "            print(f\"  {file_path.name} → {new_name}\")\n",
    "            \n",
    "            if not dry_run:\n",
    "                # Check if target already exists\n",
    "                if new_path.exists():\n",
    "                    print(f\"    ⚠ Skipped: {new_name} already exists\")\n",
    "                    continue\n",
    "                \n",
    "                file_path.rename(new_path)\n",
    "            \n",
    "            renamed_count += 1\n",
    "    \n",
    "    print(f\"\\n{'Would rename' if dry_run else 'Renamed'} {renamed_count} file(s)\")\n",
    "    if dry_run:\n",
    "        print(\"Set dry_run=False to actually rename files\")\n",
    "    \n",
    "    return renamed_count\n",
    "\n",
    "# Example: Preview renaming (dry run)\n",
    "# This won't actually rename anything\n",
    "notebooks_dir = Path.cwd()\n",
    "batch_rename(notebooks_dir, '_', '-', dry_run=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Organizing Files by Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize files into date-based folders\n",
    "# Useful for organizing daily data dumps or logs\n",
    "\n",
    "def organize_by_date(source_dir, dest_dir, dry_run=True):\n",
    "    \"\"\"\n",
    "    Organize files into YYYY/MM/DD folder structure based on modification date.\n",
    "    \n",
    "    Args:\n",
    "        source_dir: Directory containing files to organize\n",
    "        dest_dir: Destination directory for organized structure\n",
    "        dry_run: If True, only show what would be done\n",
    "    \n",
    "    Returns:\n",
    "        int: Number of files organized\n",
    "    \"\"\"\n",
    "    source_dir = Path(source_dir)\n",
    "    dest_dir = Path(dest_dir)\n",
    "    organized_count = 0\n",
    "    \n",
    "    print(f\"{'DRY RUN: ' if dry_run else ''}Organizing files by date\")\n",
    "    print(f\"Source: {source_dir}\")\n",
    "    print(f\"Destination: {dest_dir}\")\n",
    "    print()\n",
    "    \n",
    "    for file_path in source_dir.iterdir():\n",
    "        if not file_path.is_file():\n",
    "            continue\n",
    "        \n",
    "        # Get modification date\n",
    "        mod_time = datetime.fromtimestamp(file_path.stat().st_mtime)\n",
    "        \n",
    "        # Create date-based path: YYYY/MM/DD/filename\n",
    "        date_folder = dest_dir / str(mod_time.year) / f\"{mod_time.month:02d}\" / f\"{mod_time.day:02d}\"\n",
    "        dest_file = date_folder / file_path.name\n",
    "        \n",
    "        print(f\"  {file_path.name} → {date_folder.relative_to(dest_dir)}/\")\n",
    "        \n",
    "        if not dry_run:\n",
    "            # Create directory structure\n",
    "            date_folder.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            # Move file\n",
    "            shutil.move(str(file_path), str(dest_file))\n",
    "        \n",
    "        organized_count += 1\n",
    "    \n",
    "    print(f\"\\n{'Would organize' if dry_run else 'Organized'} {organized_count} file(s)\")\n",
    "    return organized_count\n",
    "\n",
    "# Example: Preview organization (dry run)\n",
    "# organize_by_date(source_dir, archive_dir, dry_run=True)\n",
    "print(\"File organization function ready!\")\n",
    "print(\"Example usage: organize_by_date('data/raw', 'data/archive', dry_run=True)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Batch Copying with Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy multiple files with progress tracking\n",
    "# Essential for backing up datasets or creating train/test splits\n",
    "\n",
    "def batch_copy(source_files, dest_dir, show_progress=True):\n",
    "    \"\"\"\n",
    "    Copy multiple files to a destination directory.\n",
    "    \n",
    "    Args:\n",
    "        source_files: List of Path objects or strings\n",
    "        dest_dir: Destination directory\n",
    "        show_progress: Print progress updates\n",
    "    \n",
    "    Returns:\n",
    "        dict: Statistics (copied, failed, skipped)\n",
    "    \"\"\"\n",
    "    dest_dir = Path(dest_dir)\n",
    "    dest_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    stats = {'copied': 0, 'failed': 0, 'skipped': 0}\n",
    "    total = len(source_files)\n",
    "    \n",
    "    for i, source_file in enumerate(source_files, 1):\n",
    "        source_file = Path(source_file)\n",
    "        dest_file = dest_dir / source_file.name\n",
    "        \n",
    "        try:\n",
    "            # Skip if destination exists\n",
    "            if dest_file.exists():\n",
    "                stats['skipped'] += 1\n",
    "                if show_progress:\n",
    "                    print(f\"[{i}/{total}] Skipped: {source_file.name} (already exists)\")\n",
    "                continue\n",
    "            \n",
    "            # Copy file\n",
    "            shutil.copy2(source_file, dest_file)  # copy2 preserves metadata\n",
    "            stats['copied'] += 1\n",
    "            \n",
    "            if show_progress:\n",
    "                size_mb = source_file.stat().st_size / (1024 * 1024)\n",
    "                print(f\"[{i}/{total}] Copied: {source_file.name} ({size_mb:.2f} MB)\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            stats['failed'] += 1\n",
    "            print(f\"[{i}/{total}] Failed: {source_file.name} - {e}\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nSummary:\")\n",
    "    print(f\"  Copied: {stats['copied']}\")\n",
    "    print(f\"  Skipped: {stats['skipped']}\")\n",
    "    print(f\"  Failed: {stats['failed']}\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Example: Copy all notebooks to backup folder\n",
    "# backup_dir = project_root / 'backup'\n",
    "# batch_copy(notebooks, backup_dir)\n",
    "print(\"Batch copy function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Working with Archives\n",
    "\n",
    "Data scientists frequently work with compressed files:\n",
    "- Downloaded datasets (often .zip or .tar.gz)\n",
    "- Model backups (compress checkpoints)\n",
    "- Sharing results (compress output folder)\n",
    "\n",
    "Python's `shutil` module provides easy archive handling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Creating Archives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create compressed archives of directories\n",
    "# Useful for backing up models or sharing datasets\n",
    "\n",
    "def create_archive(source_dir, archive_name, format='zip'):\n",
    "    \"\"\"\n",
    "    Create a compressed archive of a directory.\n",
    "    \n",
    "    Args:\n",
    "        source_dir: Directory to archive\n",
    "        archive_name: Name for archive (without extension)\n",
    "        format: Archive format ('zip', 'tar', 'gztar', 'bztar', 'xztar')\n",
    "    \n",
    "    Returns:\n",
    "        Path: Path to created archive\n",
    "    \"\"\"\n",
    "    source_dir = Path(source_dir)\n",
    "    \n",
    "    if not source_dir.exists():\n",
    "        raise FileNotFoundError(f\"Source directory not found: {source_dir}\")\n",
    "    \n",
    "    print(f\"Creating {format} archive: {archive_name}\")\n",
    "    print(f\"Source: {source_dir}\")\n",
    "    \n",
    "    # shutil.make_archive returns path without extension\n",
    "    archive_path = shutil.make_archive(\n",
    "        base_name=archive_name,\n",
    "        format=format,\n",
    "        root_dir=source_dir.parent,\n",
    "        base_dir=source_dir.name\n",
    "    )\n",
    "    \n",
    "    archive_path = Path(archive_path)\n",
    "    size_mb = archive_path.stat().st_size / (1024 * 1024)\n",
    "    \n",
    "    print(f\"✓ Created: {archive_path.name} ({size_mb:.2f} MB)\")\n",
    "    \n",
    "    return archive_path\n",
    "\n",
    "# Example: Create zip archive of notebooks directory\n",
    "# archive = create_archive(notebooks_dir, 'notebooks_backup', format='zip')\n",
    "print(\"Archive creation function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Extracting Archives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract compressed archives\n",
    "# Useful for working with downloaded datasets\n",
    "\n",
    "def extract_archive(archive_path, extract_to=None):\n",
    "    \"\"\"\n",
    "    Extract a compressed archive.\n",
    "    \n",
    "    Args:\n",
    "        archive_path: Path to archive file\n",
    "        extract_to: Directory to extract to (default: same as archive)\n",
    "    \n",
    "    Returns:\n",
    "        Path: Path to extraction directory\n",
    "    \"\"\"\n",
    "    archive_path = Path(archive_path)\n",
    "    \n",
    "    if not archive_path.exists():\n",
    "        raise FileNotFoundError(f\"Archive not found: {archive_path}\")\n",
    "    \n",
    "    if extract_to is None:\n",
    "        extract_to = archive_path.parent\n",
    "    else:\n",
    "        extract_to = Path(extract_to)\n",
    "    \n",
    "    extract_to.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"Extracting: {archive_path.name}\")\n",
    "    print(f\"To: {extract_to}\")\n",
    "    \n",
    "    # shutil.unpack_archive detects format automatically\n",
    "    shutil.unpack_archive(archive_path, extract_to)\n",
    "    \n",
    "    print(f\"✓ Extracted successfully\")\n",
    "    \n",
    "    return extract_to\n",
    "\n",
    "# Example: Extract dataset\n",
    "# extract_archive('data/raw/dataset.zip', 'data/raw/')\n",
    "print(\"Archive extraction function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. File Monitoring and Watching\n",
    "\n",
    "Sometimes you need to detect when new files appear (e.g., for automated data pipelines).\n",
    "\n",
    "### Use Cases\n",
    "- **Data ingestion**: Process new CSV files as they arrive\n",
    "- **Model monitoring**: Detect when new checkpoints are saved\n",
    "- **Log monitoring**: Parse new log entries in real-time\n",
    "\n",
    "### Simple Polling Approach\n",
    "\n",
    "We'll implement a simple file watcher using polling. For production, consider using `watchdog` library for more efficient file watching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple file watcher using polling\n",
    "# Detects new files added to a directory\n",
    "\n",
    "class SimpleFileWatcher:\n",
    "    \"\"\"\n",
    "    Watch a directory for new files.\n",
    "    \n",
    "    Example:\n",
    "        watcher = SimpleFileWatcher('data/incoming')\n",
    "        new_files = watcher.check_for_new_files()\n",
    "        for file in new_files:\n",
    "            process_file(file)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, directory):\n",
    "        \"\"\"Initialize watcher for a directory.\"\"\"\n",
    "        self.directory = Path(directory)\n",
    "        self.known_files = set()\n",
    "        \n",
    "        # Initialize with existing files\n",
    "        if self.directory.exists():\n",
    "            self.known_files = {f for f in self.directory.iterdir() if f.is_file()}\n",
    "    \n",
    "    def check_for_new_files(self):\n",
    "        \"\"\"\n",
    "        Check for files added since last check.\n",
    "        \n",
    "        Returns:\n",
    "            set: Set of new file Path objects\n",
    "        \"\"\"\n",
    "        if not self.directory.exists():\n",
    "            return set()\n",
    "        \n",
    "        current_files = {f for f in self.directory.iterdir() if f.is_file()}\n",
    "        new_files = current_files - self.known_files\n",
    "        \n",
    "        # Update known files\n",
    "        self.known_files = current_files\n",
    "        \n",
    "        return new_files\n",
    "    \n",
    "    def watch(self, callback, interval=1, duration=10):\n",
    "        \"\"\"\n",
    "        Watch directory and call callback for new files.\n",
    "        \n",
    "        Args:\n",
    "            callback: Function to call with new file path\n",
    "            interval: Seconds between checks\n",
    "            duration: Total seconds to watch (None for infinite)\n",
    "        \"\"\"\n",
    "        print(f\"Watching {self.directory} for new files...\")\n",
    "        print(f\"Interval: {interval}s, Duration: {duration}s\")\n",
    "        print(\"Press Ctrl+C to stop\\n\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            while True:\n",
    "                new_files = self.check_for_new_files()\n",
    "                \n",
    "                for file_path in new_files:\n",
    "                    print(f\"New file detected: {file_path.name}\")\n",
    "                    callback(file_path)\n",
    "                \n",
    "                # Check if duration exceeded\n",
    "                if duration and (time.time() - start_time) >= duration:\n",
    "                    print(f\"\\nWatch duration completed ({duration}s)\")\n",
    "                    break\n",
    "                \n",
    "                time.sleep(interval)\n",
    "        \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nWatch stopped by user\")\n",
    "\n",
    "# Example callback function\n",
    "def process_new_file(file_path):\n",
    "    \"\"\"Example: Process a new file.\"\"\"\n",
    "    print(f\"  Processing: {file_path.name}\")\n",
    "    # Add your processing logic here\n",
    "\n",
    "# Example usage (commented out):\n",
    "# watcher = SimpleFileWatcher('data/incoming')\n",
    "# watcher.watch(process_new_file, interval=2, duration=10)\n",
    "\n",
    "print(\"File watcher ready!\")\n",
    "print(\"Example: watcher = SimpleFileWatcher('data/incoming')\")\n",
    "print(\"         watcher.watch(process_new_file, interval=2, duration=10)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Practical Automation Example: Dataset Cleanup\n",
    "\n",
    "Let's combine everything into a real-world automation script: cleaning up old model checkpoints while keeping the best ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automated checkpoint cleanup\n",
    "# Keeps only best N and latest M checkpoints, deletes rest\n",
    "\n",
    "def cleanup_checkpoints(checkpoint_dir, \n",
    "                       keep_best=5, \n",
    "                       keep_latest=3,\n",
    "                       dry_run=True):\n",
    "    \"\"\"\n",
    "    Clean up old model checkpoints intelligently.\n",
    "    \n",
    "    Assumes checkpoint filenames contain metric value:\n",
    "    e.g., 'model_epoch10_val_acc_0.95.h5'\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_dir: Directory containing checkpoints\n",
    "        keep_best: Number of best checkpoints to keep (by metric)\n",
    "        keep_latest: Number of most recent checkpoints to keep\n",
    "        dry_run: If True, only show what would be deleted\n",
    "    \n",
    "    Returns:\n",
    "        dict: Statistics about cleanup\n",
    "    \"\"\"\n",
    "    checkpoint_dir = Path(checkpoint_dir)\n",
    "    \n",
    "    if not checkpoint_dir.exists():\n",
    "        print(f\"Checkpoint directory not found: {checkpoint_dir}\")\n",
    "        return {'deleted': 0, 'kept': 0}\n",
    "    \n",
    "    # Get all checkpoint files\n",
    "    checkpoints = sorted(\n",
    "        [f for f in checkpoint_dir.glob('*.h5')],\n",
    "        key=lambda x: x.stat().st_mtime\n",
    "    )\n",
    "    \n",
    "    if not checkpoints:\n",
    "        print(\"No checkpoint files found\")\n",
    "        return {'deleted': 0, 'kept': 0}\n",
    "    \n",
    "    print(f\"Found {len(checkpoints)} checkpoint(s)\")\n",
    "    print(f\"Policy: Keep best {keep_best} + latest {keep_latest}\")\n",
    "    print()\n",
    "    \n",
    "    # Keep latest N\n",
    "    latest_checkpoints = set(checkpoints[-keep_latest:])\n",
    "    \n",
    "    # Keep best N by modification time (proxy for training progress)\n",
    "    # In real scenario, parse metric from filename\n",
    "    best_checkpoints = set(checkpoints[-keep_best:])\n",
    "    \n",
    "    # Files to keep\n",
    "    keep_files = latest_checkpoints | best_checkpoints\n",
    "    \n",
    "    # Files to delete\n",
    "    delete_files = set(checkpoints) - keep_files\n",
    "    \n",
    "    # Show what will be kept\n",
    "    print(f\"Keeping {len(keep_files)} checkpoint(s):\")\n",
    "    for f in sorted(keep_files, key=lambda x: x.stat().st_mtime):\n",
    "        size_mb = f.stat().st_size / (1024 * 1024)\n",
    "        tags = []\n",
    "        if f in latest_checkpoints:\n",
    "            tags.append('latest')\n",
    "        if f in best_checkpoints:\n",
    "            tags.append('best')\n",
    "        print(f\"  ✓ {f.name} ({size_mb:.2f} MB) [{', '.join(tags)}]\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Show/perform deletions\n",
    "    if delete_files:\n",
    "        total_size_mb = sum(f.stat().st_size for f in delete_files) / (1024 * 1024)\n",
    "        \n",
    "        print(f\"{'Would delete' if dry_run else 'Deleting'} {len(delete_files)} checkpoint(s) ({total_size_mb:.2f} MB):\")\n",
    "        for f in sorted(delete_files, key=lambda x: x.stat().st_mtime):\n",
    "            size_mb = f.stat().st_size / (1024 * 1024)\n",
    "            print(f\"  ✗ {f.name} ({size_mb:.2f} MB)\")\n",
    "            \n",
    "            if not dry_run:\n",
    "                f.unlink()\n",
    "        \n",
    "        if dry_run:\n",
    "            print(f\"\\nDRY RUN: No files actually deleted\")\n",
    "            print(f\"Set dry_run=False to perform cleanup\")\n",
    "    else:\n",
    "        print(\"No checkpoints need deletion\")\n",
    "    \n",
    "    return {'deleted': len(delete_files), 'kept': len(keep_files)}\n",
    "\n",
    "# Example usage (dry run)\n",
    "# cleanup_checkpoints('models/checkpoints', keep_best=5, keep_latest=3, dry_run=True)\n",
    "print(\"Checkpoint cleanup function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Practice Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Dataset Organization Tool\n",
    "\n",
    "Create a function that organizes image files into train/val/test folders:\n",
    "1. Find all .jpg and .png files in a directory\n",
    "2. Split them: 70% train, 20% validation, 10% test\n",
    "3. Copy (don't move) files to respective folders\n",
    "4. Print statistics (count and total size per split)\n",
    "\n",
    "**Hint**: Use `random.sample()` for random splitting, `batch_copy()` from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your solution here\n",
    "import random\n",
    "\n",
    "def create_train_val_test_split(source_dir, output_dir, train_ratio=0.7, val_ratio=0.2, test_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Split images into train/val/test sets.\n",
    "    \n",
    "    Args:\n",
    "        source_dir: Directory containing images\n",
    "        output_dir: Output directory for splits\n",
    "        train_ratio: Fraction for training set\n",
    "        val_ratio: Fraction for validation set\n",
    "        test_ratio: Fraction for test set\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    pass\n",
    "\n",
    "# Test your function\n",
    "# create_train_val_test_split('data/images', 'data/split')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Duplicate File Finder\n",
    "\n",
    "Create a function to find duplicate files based on content:\n",
    "1. Calculate hash (MD5 or SHA256) for each file\n",
    "2. Group files with same hash\n",
    "3. Report duplicates with sizes\n",
    "4. Calculate total wasted space\n",
    "\n",
    "**Hint**: Use `hashlib` module for hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your solution here\n",
    "import hashlib\n",
    "\n",
    "def find_duplicates(directory):\n",
    "    \"\"\"\n",
    "    Find duplicate files in directory tree.\n",
    "    \n",
    "    Args:\n",
    "        directory: Directory to search\n",
    "    \n",
    "    Returns:\n",
    "        dict: Hash -> list of duplicate file paths\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    pass\n",
    "\n",
    "# Test your function\n",
    "# duplicates = find_duplicates('data/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Automated Backup System\n",
    "\n",
    "Create an automated backup system:\n",
    "1. Monitor a source directory for changes (use SimpleFileWatcher)\n",
    "2. When new files appear, copy them to backup directory\n",
    "3. Organize backups by date in YYYY-MM-DD folders\n",
    "4. Keep only last 7 days of backups (delete older)\n",
    "5. Log all operations to a file\n",
    "\n",
    "**Hint**: Combine file watching, organization, and cleanup techniques from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Your solution here\n",
    "\n",
    "class AutomatedBackupSystem:\n",
    "    \"\"\"\n",
    "    Automated backup system with retention policy.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, source_dir, backup_dir, retention_days=7):\n",
    "        # TODO: Initialize the backup system\n",
    "        pass\n",
    "    \n",
    "    def backup_file(self, file_path):\n",
    "        # TODO: Backup a file with date organization\n",
    "        pass\n",
    "    \n",
    "    def cleanup_old_backups(self):\n",
    "        # TODO: Remove backups older than retention_days\n",
    "        pass\n",
    "    \n",
    "    def start_watching(self, interval=5):\n",
    "        # TODO: Start watching for new files\n",
    "        pass\n",
    "\n",
    "# Test your system\n",
    "# backup_system = AutomatedBackupSystem('data/source', 'data/backup', retention_days=7)\n",
    "# backup_system.start_watching(interval=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Smart Archiver\n",
    "\n",
    "Create an intelligent archiving tool:\n",
    "1. Find all folders in a directory\n",
    "2. For folders not modified in last 30 days:\n",
    "   - Create compressed archive\n",
    "   - Verify archive integrity\n",
    "   - Delete original folder\n",
    "3. Report space saved\n",
    "4. Create an index file listing archived folders\n",
    "\n",
    "**Hint**: Check modification time, use `create_archive()`, verify by extracting to temp location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Your solution here\n",
    "\n",
    "def smart_archiver(base_dir, archive_after_days=30, dry_run=True):\n",
    "    \"\"\"\n",
    "    Archive old folders automatically.\n",
    "    \n",
    "    Args:\n",
    "        base_dir: Directory to search\n",
    "        archive_after_days: Archive folders older than this\n",
    "        dry_run: Preview only if True\n",
    "    \n",
    "    Returns:\n",
    "        dict: Statistics about archiving\n",
    "    \"\"\"\n",
    "    # TODO: Implement smart archiving\n",
    "    pass\n",
    "\n",
    "# Test your archiver\n",
    "# smart_archiver('data/experiments', archive_after_days=30, dry_run=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Advanced pathlib**\n",
    "   - `rglob()` for recursive pattern matching\n",
    "   - Filtering by size, date, extension\n",
    "   - Cross-platform path handling\n",
    "\n",
    "2. **Batch Operations**\n",
    "   - Always use dry-run mode first\n",
    "   - Show progress for user feedback\n",
    "   - Handle errors gracefully\n",
    "   - Log operations for audit trail\n",
    "\n",
    "3. **Archive Management**\n",
    "   - `shutil.make_archive()` for compression\n",
    "   - `shutil.unpack_archive()` for extraction\n",
    "   - Automatic format detection\n",
    "\n",
    "4. **File Monitoring**\n",
    "   - Simple polling with set operations\n",
    "   - Callback-based processing\n",
    "   - Production: use `watchdog` library\n",
    "\n",
    "5. **Practical Patterns**\n",
    "   - Checkpoint cleanup (keep best + latest)\n",
    "   - Date-based organization\n",
    "   - Backup automation\n",
    "   - Duplicate detection\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "- **Data pipelines**: Auto-process incoming files\n",
    "- **Model management**: Clean up old checkpoints\n",
    "- **Dataset preparation**: Organize and split data\n",
    "- **Backup automation**: Scheduled backups with retention\n",
    "- **Space management**: Archive old experiments\n",
    "\n",
    "### Safety Checklist\n",
    "\n",
    "Before running file operations:\n",
    "- [ ] Test on small subset first\n",
    "- [ ] Use dry-run mode\n",
    "- [ ] Have backups of important data\n",
    "- [ ] Validate results after operation\n",
    "- [ ] Log all changes for audit trail\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In **Module 04: Process & Service Management**, you'll learn:\n",
    "- Monitor running processes\n",
    "- Manage Windows services\n",
    "- Kill hung processes\n",
    "- Restart failed services automatically\n",
    "\n",
    "### Self-Assessment\n",
    "\n",
    "Before moving on, make sure you can:\n",
    "- [ ] Find files using patterns and filters\n",
    "- [ ] Perform batch operations safely (with dry-run)\n",
    "- [ ] Create and extract archives\n",
    "- [ ] Watch directories for new files\n",
    "- [ ] Organize files programmatically\n",
    "\n",
    "---\n",
    "\n",
    "**Continue to Module 04** when ready!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
